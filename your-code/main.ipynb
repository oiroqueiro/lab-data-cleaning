{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Import users table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patool: Extracting datasets.rar ...\n",
      "patool: running /usr/bin/unrar x -- \"/home/roque/01. IronHack/00. Data Analytics/01. Course/05. Week 2 - Day 2/git/lab-data-cleaning/your-code/datasets.rar\"\n",
      "patool:     with cwd='./Unpack__uf2mszo'\n",
      "patool: ... datasets.rar extracted to `datasets1' (multiple files in root).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'datasets1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import patoolib\n",
    "patoolib.extract_archive('datasets.rar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>DownVotes</th>\n",
       "      <th>AccountId</th>\n",
       "      <th>Age</th>\n",
       "      <th>ProfileImageUrl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>1920</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "      <td>2010-07-19 14:01:36</td>\n",
       "      <td>Geoff Dalgas</td>\n",
       "      <td>2013-11-12 22:07:23</td>\n",
       "      <td>http://stackoverflow.com</td>\n",
       "      <td>Corvallis, OR</td>\n",
       "      <td>&lt;p&gt;Developer on the StackOverflow team.  Find ...</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>101</td>\n",
       "      <td>2010-07-19 15:34:50</td>\n",
       "      <td>Jarrod Dixon</td>\n",
       "      <td>2014-08-08 06:42:58</td>\n",
       "      <td>http://stackoverflow.com</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>&lt;p&gt;&lt;a href=\"http://blog.stackoverflow.com/2009...</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>101</td>\n",
       "      <td>2010-07-19 19:03:27</td>\n",
       "      <td>Emmett</td>\n",
       "      <td>2014-01-02 09:31:02</td>\n",
       "      <td>http://minesweeperonline.com</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>&lt;p&gt;currently at a startup in SF&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1998</td>\n",
       "      <td>28.0</td>\n",
       "      <td>http://i.stack.imgur.com/d1oHX.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6792</td>\n",
       "      <td>2010-07-19 19:03:57</td>\n",
       "      <td>Shane</td>\n",
       "      <td>2014-08-13 00:23:47</td>\n",
       "      <td>http://www.statalgo.com</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>&lt;p&gt;Quantitative researcher focusing on statist...</td>\n",
       "      <td>1145</td>\n",
       "      <td>662</td>\n",
       "      <td>5</td>\n",
       "      <td>54503</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40320</th>\n",
       "      <td>55743</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-09-13 21:03:50</td>\n",
       "      <td>AussieMeg</td>\n",
       "      <td>2014-09-13 21:18:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5026902</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://graph.facebook.com/665821703/picture?ty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40321</th>\n",
       "      <td>55744</td>\n",
       "      <td>6</td>\n",
       "      <td>2014-09-13 21:39:30</td>\n",
       "      <td>Mia Maria</td>\n",
       "      <td>2014-09-13 21:39:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5026998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40322</th>\n",
       "      <td>55745</td>\n",
       "      <td>101</td>\n",
       "      <td>2014-09-13 23:45:27</td>\n",
       "      <td>tronbabylove</td>\n",
       "      <td>2014-09-13 23:45:27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>481766</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.gravatar.com/avatar/faa7a3fdbd8308...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40323</th>\n",
       "      <td>55746</td>\n",
       "      <td>106</td>\n",
       "      <td>2014-09-14 00:29:41</td>\n",
       "      <td>GPP</td>\n",
       "      <td>2014-09-14 02:05:17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Stats noobie, product, marketing &amp;amp; medi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>976289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.gravatar.com/avatar/6d9e9fa6b783a3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40324</th>\n",
       "      <td>55747</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-09-14 01:01:44</td>\n",
       "      <td>Shivam Agrawal</td>\n",
       "      <td>2014-09-14 01:19:04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>India</td>\n",
       "      <td>&lt;p&gt;Maths Enthusiast &lt;/p&gt;\\r\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5027354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://lh4.googleusercontent.com/-ZsXhwVaFmiY...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40325 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id  Reputation         CreationDate     DisplayName  \\\n",
       "0         -1           1  2010-07-19 06:55:26       Community   \n",
       "1          2         101  2010-07-19 14:01:36    Geoff Dalgas   \n",
       "2          3         101  2010-07-19 15:34:50    Jarrod Dixon   \n",
       "3          4         101  2010-07-19 19:03:27          Emmett   \n",
       "4          5        6792  2010-07-19 19:03:57           Shane   \n",
       "...      ...         ...                  ...             ...   \n",
       "40320  55743           1  2014-09-13 21:03:50       AussieMeg   \n",
       "40321  55744           6  2014-09-13 21:39:30       Mia Maria   \n",
       "40322  55745         101  2014-09-13 23:45:27    tronbabylove   \n",
       "40323  55746         106  2014-09-14 00:29:41             GPP   \n",
       "40324  55747           1  2014-09-14 01:01:44  Shivam Agrawal   \n",
       "\n",
       "            LastAccessDate                      WebsiteUrl  \\\n",
       "0      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "1      2013-11-12 22:07:23        http://stackoverflow.com   \n",
       "2      2014-08-08 06:42:58        http://stackoverflow.com   \n",
       "3      2014-01-02 09:31:02    http://minesweeperonline.com   \n",
       "4      2014-08-13 00:23:47         http://www.statalgo.com   \n",
       "...                    ...                             ...   \n",
       "40320  2014-09-13 21:18:52                             NaN   \n",
       "40321  2014-09-13 21:39:30                             NaN   \n",
       "40322  2014-09-13 23:45:27                             NaN   \n",
       "40323  2014-09-14 02:05:17                             NaN   \n",
       "40324  2014-09-14 01:19:04                             NaN   \n",
       "\n",
       "                 Location                                            AboutMe  \\\n",
       "0      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "1           Corvallis, OR  <p>Developer on the StackOverflow team.  Find ...   \n",
       "2            New York, NY  <p><a href=\"http://blog.stackoverflow.com/2009...   \n",
       "3       San Francisco, CA  <p>currently at a startup in SF</p>\\r\\n\\r\\n<p>...   \n",
       "4            New York, NY  <p>Quantitative researcher focusing on statist...   \n",
       "...                   ...                                                ...   \n",
       "40320                 NaN                                                NaN   \n",
       "40321                 NaN                                                NaN   \n",
       "40322       United States                                                NaN   \n",
       "40323                 NaN  <p>Stats noobie, product, marketing &amp; medi...   \n",
       "40324               India                       <p>Maths Enthusiast </p>\\r\\n   \n",
       "\n",
       "       Views  UpVotes  DownVotes  AccountId   Age  \\\n",
       "0          0     5007       1920         -1   NaN   \n",
       "1         25        3          0          2  37.0   \n",
       "2         22       19          0          3  35.0   \n",
       "3         11        0          0       1998  28.0   \n",
       "4       1145      662          5      54503  35.0   \n",
       "...      ...      ...        ...        ...   ...   \n",
       "40320      0        0          0    5026902   NaN   \n",
       "40321      1        0          0    5026998   NaN   \n",
       "40322      0        0          0     481766   NaN   \n",
       "40323      1        0          0     976289   NaN   \n",
       "40324      0        0          0    5027354   NaN   \n",
       "\n",
       "                                         ProfileImageUrl  \n",
       "0                                                    NaN  \n",
       "1                                                    NaN  \n",
       "2                                                    NaN  \n",
       "3                     http://i.stack.imgur.com/d1oHX.jpg  \n",
       "4                                                    NaN  \n",
       "...                                                  ...  \n",
       "40320  http://graph.facebook.com/665821703/picture?ty...  \n",
       "40321                                                NaN  \n",
       "40322  https://www.gravatar.com/avatar/faa7a3fdbd8308...  \n",
       "40323  https://www.gravatar.com/avatar/6d9e9fa6b783a3...  \n",
       "40324  https://lh4.googleusercontent.com/-ZsXhwVaFmiY...  \n",
       "\n",
       "[40325 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = pd.read_csv('datasets/users_table.csv')\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Rename Id column to userId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>DownVotes</th>\n",
       "      <th>AccountId</th>\n",
       "      <th>Age</th>\n",
       "      <th>ProfileImageUrl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>1920</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "      <td>2010-07-19 14:01:36</td>\n",
       "      <td>Geoff Dalgas</td>\n",
       "      <td>2013-11-12 22:07:23</td>\n",
       "      <td>http://stackoverflow.com</td>\n",
       "      <td>Corvallis, OR</td>\n",
       "      <td>&lt;p&gt;Developer on the StackOverflow team.  Find ...</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>101</td>\n",
       "      <td>2010-07-19 15:34:50</td>\n",
       "      <td>Jarrod Dixon</td>\n",
       "      <td>2014-08-08 06:42:58</td>\n",
       "      <td>http://stackoverflow.com</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>&lt;p&gt;&lt;a href=\"http://blog.stackoverflow.com/2009...</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>101</td>\n",
       "      <td>2010-07-19 19:03:27</td>\n",
       "      <td>Emmett</td>\n",
       "      <td>2014-01-02 09:31:02</td>\n",
       "      <td>http://minesweeperonline.com</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>&lt;p&gt;currently at a startup in SF&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1998</td>\n",
       "      <td>28.0</td>\n",
       "      <td>http://i.stack.imgur.com/d1oHX.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6792</td>\n",
       "      <td>2010-07-19 19:03:57</td>\n",
       "      <td>Shane</td>\n",
       "      <td>2014-08-13 00:23:47</td>\n",
       "      <td>http://www.statalgo.com</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>&lt;p&gt;Quantitative researcher focusing on statist...</td>\n",
       "      <td>1145</td>\n",
       "      <td>662</td>\n",
       "      <td>5</td>\n",
       "      <td>54503</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40320</th>\n",
       "      <td>55743</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-09-13 21:03:50</td>\n",
       "      <td>AussieMeg</td>\n",
       "      <td>2014-09-13 21:18:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5026902</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://graph.facebook.com/665821703/picture?ty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40321</th>\n",
       "      <td>55744</td>\n",
       "      <td>6</td>\n",
       "      <td>2014-09-13 21:39:30</td>\n",
       "      <td>Mia Maria</td>\n",
       "      <td>2014-09-13 21:39:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5026998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40322</th>\n",
       "      <td>55745</td>\n",
       "      <td>101</td>\n",
       "      <td>2014-09-13 23:45:27</td>\n",
       "      <td>tronbabylove</td>\n",
       "      <td>2014-09-13 23:45:27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>481766</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.gravatar.com/avatar/faa7a3fdbd8308...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40323</th>\n",
       "      <td>55746</td>\n",
       "      <td>106</td>\n",
       "      <td>2014-09-14 00:29:41</td>\n",
       "      <td>GPP</td>\n",
       "      <td>2014-09-14 02:05:17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Stats noobie, product, marketing &amp;amp; medi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>976289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.gravatar.com/avatar/6d9e9fa6b783a3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40324</th>\n",
       "      <td>55747</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-09-14 01:01:44</td>\n",
       "      <td>Shivam Agrawal</td>\n",
       "      <td>2014-09-14 01:19:04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>India</td>\n",
       "      <td>&lt;p&gt;Maths Enthusiast &lt;/p&gt;\\r\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5027354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://lh4.googleusercontent.com/-ZsXhwVaFmiY...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40325 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  Reputation         CreationDate     DisplayName  \\\n",
       "0          -1           1  2010-07-19 06:55:26       Community   \n",
       "1           2         101  2010-07-19 14:01:36    Geoff Dalgas   \n",
       "2           3         101  2010-07-19 15:34:50    Jarrod Dixon   \n",
       "3           4         101  2010-07-19 19:03:27          Emmett   \n",
       "4           5        6792  2010-07-19 19:03:57           Shane   \n",
       "...       ...         ...                  ...             ...   \n",
       "40320   55743           1  2014-09-13 21:03:50       AussieMeg   \n",
       "40321   55744           6  2014-09-13 21:39:30       Mia Maria   \n",
       "40322   55745         101  2014-09-13 23:45:27    tronbabylove   \n",
       "40323   55746         106  2014-09-14 00:29:41             GPP   \n",
       "40324   55747           1  2014-09-14 01:01:44  Shivam Agrawal   \n",
       "\n",
       "            LastAccessDate                      WebsiteUrl  \\\n",
       "0      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "1      2013-11-12 22:07:23        http://stackoverflow.com   \n",
       "2      2014-08-08 06:42:58        http://stackoverflow.com   \n",
       "3      2014-01-02 09:31:02    http://minesweeperonline.com   \n",
       "4      2014-08-13 00:23:47         http://www.statalgo.com   \n",
       "...                    ...                             ...   \n",
       "40320  2014-09-13 21:18:52                             NaN   \n",
       "40321  2014-09-13 21:39:30                             NaN   \n",
       "40322  2014-09-13 23:45:27                             NaN   \n",
       "40323  2014-09-14 02:05:17                             NaN   \n",
       "40324  2014-09-14 01:19:04                             NaN   \n",
       "\n",
       "                 Location                                            AboutMe  \\\n",
       "0      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "1           Corvallis, OR  <p>Developer on the StackOverflow team.  Find ...   \n",
       "2            New York, NY  <p><a href=\"http://blog.stackoverflow.com/2009...   \n",
       "3       San Francisco, CA  <p>currently at a startup in SF</p>\\r\\n\\r\\n<p>...   \n",
       "4            New York, NY  <p>Quantitative researcher focusing on statist...   \n",
       "...                   ...                                                ...   \n",
       "40320                 NaN                                                NaN   \n",
       "40321                 NaN                                                NaN   \n",
       "40322       United States                                                NaN   \n",
       "40323                 NaN  <p>Stats noobie, product, marketing &amp; medi...   \n",
       "40324               India                       <p>Maths Enthusiast </p>\\r\\n   \n",
       "\n",
       "       Views  UpVotes  DownVotes  AccountId   Age  \\\n",
       "0          0     5007       1920         -1   NaN   \n",
       "1         25        3          0          2  37.0   \n",
       "2         22       19          0          3  35.0   \n",
       "3         11        0          0       1998  28.0   \n",
       "4       1145      662          5      54503  35.0   \n",
       "...      ...      ...        ...        ...   ...   \n",
       "40320      0        0          0    5026902   NaN   \n",
       "40321      1        0          0    5026998   NaN   \n",
       "40322      0        0          0     481766   NaN   \n",
       "40323      1        0          0     976289   NaN   \n",
       "40324      0        0          0    5027354   NaN   \n",
       "\n",
       "                                         ProfileImageUrl  \n",
       "0                                                    NaN  \n",
       "1                                                    NaN  \n",
       "2                                                    NaN  \n",
       "3                     http://i.stack.imgur.com/d1oHX.jpg  \n",
       "4                                                    NaN  \n",
       "...                                                  ...  \n",
       "40320  http://graph.facebook.com/665821703/picture?ty...  \n",
       "40321                                                NaN  \n",
       "40322  https://www.gravatar.com/avatar/faa7a3fdbd8308...  \n",
       "40323  https://www.gravatar.com/avatar/6d9e9fa6b783a3...  \n",
       "40324  https://lh4.googleusercontent.com/-ZsXhwVaFmiY...  \n",
       "\n",
       "[40325 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = users.rename(columns={'Id':'userId'})\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Import posts table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>CreaionDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Body</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>LasActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>...</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>CommunityOwnedDate</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>ClosedDate</th>\n",
       "      <th>OwnerDisplayName</th>\n",
       "      <th>LastEditorDisplayName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2010-07-19 19:12:12</td>\n",
       "      <td>23</td>\n",
       "      <td>1278.0</td>\n",
       "      <td>&lt;p&gt;How should I elicit prior distributions fro...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2010-09-15 21:08:26</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2010-07-19 19:12:57</td>\n",
       "      <td>22</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>&lt;p&gt;In many different statistical methods there...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2012-11-12 09:21:54</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2010-08-07 17:56:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2010-07-19 19:13:28</td>\n",
       "      <td>54</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>&lt;p&gt;What are some valuable Statistical Analysis...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2013-05-27 14:48:36</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4</td>\n",
       "      <td>36.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2011-02-12 05:50:03</td>\n",
       "      <td>2010-07-19 19:13:28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>135.0</td>\n",
       "      <td>2010-07-19 19:13:31</td>\n",
       "      <td>13</td>\n",
       "      <td>5224.0</td>\n",
       "      <td>&lt;p&gt;I have two groups of data.  Each with a dif...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2010-09-08 03:00:19</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-07-19 19:14:43</td>\n",
       "      <td>81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;The R-project&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\"http://www...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2010-07-19 19:21:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2010-07-19 19:21:15</td>\n",
       "      <td>2010-07-19 19:14:43</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>48321</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-23 09:00:01</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;you can use the matlab codes for svm and co...</td>\n",
       "      <td>19966.0</td>\n",
       "      <td>2013-01-23 09:00:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45118.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>48322</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-23 09:09:34</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;I use &lt;a href=\"http://www.gnu.org/software/...</td>\n",
       "      <td>892.0</td>\n",
       "      <td>2013-01-23 13:13:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>892.0</td>\n",
       "      <td>2013-01-23 13:13:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48311.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>48323</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-23 09:16:44</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;If I understand your question correctly, yo...</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>2013-01-23 09:16:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48247.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>48324</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-23 09:36:07</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Doesn't really help you with your question,...</td>\n",
       "      <td>19914.0</td>\n",
       "      <td>2013-01-23 09:36:07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48297.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>48325</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-23 09:44:07</td>\n",
       "      <td>-1</td>\n",
       "      <td>116.0</td>\n",
       "      <td>&lt;p&gt;I have 10 vectors each having 100,000 point...</td>\n",
       "      <td>19968.0</td>\n",
       "      <td>2013-02-22 11:23:54</td>\n",
       "      <td>are data sets obtained from a Normal distribut...</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id  PostTypeId  AcceptedAnswerId          CreaionDate  Score  \\\n",
       "0          1           1              15.0  2010-07-19 19:12:12     23   \n",
       "1          2           1              59.0  2010-07-19 19:12:57     22   \n",
       "2          3           1               5.0  2010-07-19 19:13:28     54   \n",
       "3          4           1             135.0  2010-07-19 19:13:31     13   \n",
       "4          5           2               NaN  2010-07-19 19:14:43     81   \n",
       "...      ...         ...               ...                  ...    ...   \n",
       "39995  48321           2               NaN  2013-01-23 09:00:01      0   \n",
       "39996  48322           2               NaN  2013-01-23 09:09:34      3   \n",
       "39997  48323           2               NaN  2013-01-23 09:16:44      1   \n",
       "39998  48324           2               NaN  2013-01-23 09:36:07      3   \n",
       "39999  48325           1               NaN  2013-01-23 09:44:07     -1   \n",
       "\n",
       "       ViewCount                                               Body  \\\n",
       "0         1278.0  <p>How should I elicit prior distributions fro...   \n",
       "1         8198.0  <p>In many different statistical methods there...   \n",
       "2         3613.0  <p>What are some valuable Statistical Analysis...   \n",
       "3         5224.0  <p>I have two groups of data.  Each with a dif...   \n",
       "4            NaN  <p>The R-project</p>\\n\\n<p><a href=\"http://www...   \n",
       "...          ...                                                ...   \n",
       "39995        NaN  <p>you can use the matlab codes for svm and co...   \n",
       "39996        NaN  <p>I use <a href=\"http://www.gnu.org/software/...   \n",
       "39997        NaN  <p>If I understand your question correctly, yo...   \n",
       "39998        NaN  <p>Doesn't really help you with your question,...   \n",
       "39999      116.0  <p>I have 10 vectors each having 100,000 point...   \n",
       "\n",
       "       OwnerUserId      LasActivityDate  \\\n",
       "0              8.0  2010-09-15 21:08:26   \n",
       "1             24.0  2012-11-12 09:21:54   \n",
       "2             18.0  2013-05-27 14:48:36   \n",
       "3             23.0  2010-09-08 03:00:19   \n",
       "4             23.0  2010-07-19 19:21:15   \n",
       "...            ...                  ...   \n",
       "39995      19966.0  2013-01-23 09:00:01   \n",
       "39996        892.0  2013-01-23 13:13:30   \n",
       "39997       2020.0  2013-01-23 09:16:44   \n",
       "39998      19914.0  2013-01-23 09:36:07   \n",
       "39999      19968.0  2013-02-22 11:23:54   \n",
       "\n",
       "                                                   Title  ... AnswerCount  \\\n",
       "0                          Eliciting priors from experts  ...         5.0   \n",
       "1                                     What is normality?  ...         7.0   \n",
       "2      What are some valuable Statistical Analysis op...  ...        19.0   \n",
       "3      Assessing the significance of differences in d...  ...         5.0   \n",
       "4                                                    NaN  ...         NaN   \n",
       "...                                                  ...  ...         ...   \n",
       "39995                                                NaN  ...         NaN   \n",
       "39996                                                NaN  ...         NaN   \n",
       "39997                                                NaN  ...         NaN   \n",
       "39998                                                NaN  ...         NaN   \n",
       "39999  are data sets obtained from a Normal distribut...  ...         2.0   \n",
       "\n",
       "       CommentCount  FavoriteCount  LastEditorUserId         LastEditDate  \\\n",
       "0                 1           14.0               NaN                  NaN   \n",
       "1                 1            8.0              88.0  2010-08-07 17:56:44   \n",
       "2                 4           36.0             183.0  2011-02-12 05:50:03   \n",
       "3                 2            2.0               NaN                  NaN   \n",
       "4                 3            NaN              23.0  2010-07-19 19:21:15   \n",
       "...             ...            ...               ...                  ...   \n",
       "39995             0            NaN               NaN                  NaN   \n",
       "39996             2            NaN             892.0  2013-01-23 13:13:30   \n",
       "39997             0            NaN               NaN                  NaN   \n",
       "39998             0            NaN               NaN                  NaN   \n",
       "39999             4            NaN               NaN                  NaN   \n",
       "\n",
       "        CommunityOwnedDate ParentId  ClosedDate OwnerDisplayName  \\\n",
       "0                      NaN      NaN         NaN              NaN   \n",
       "1                      NaN      NaN         NaN              NaN   \n",
       "2      2010-07-19 19:13:28      NaN         NaN              NaN   \n",
       "3                      NaN      NaN         NaN              NaN   \n",
       "4      2010-07-19 19:14:43      3.0         NaN              NaN   \n",
       "...                    ...      ...         ...              ...   \n",
       "39995                  NaN  45118.0         NaN              NaN   \n",
       "39996                  NaN  48311.0         NaN              NaN   \n",
       "39997                  NaN  48247.0         NaN              NaN   \n",
       "39998                  NaN  48297.0         NaN              NaN   \n",
       "39999                  NaN      NaN         NaN              NaN   \n",
       "\n",
       "      LastEditorDisplayName  \n",
       "0                       NaN  \n",
       "1                       NaN  \n",
       "2                       NaN  \n",
       "3                       NaN  \n",
       "4                       NaN  \n",
       "...                     ...  \n",
       "39995                   NaN  \n",
       "39996                   NaN  \n",
       "39997                   NaN  \n",
       "39998                   NaN  \n",
       "39999                   NaN  \n",
       "\n",
       "[40000 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = pd.read_csv('datasets/posts_table.csv')\n",
    "posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Rename Id column to postId and OwnerUserId to userId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postId</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>CreaionDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Body</th>\n",
       "      <th>userId</th>\n",
       "      <th>LasActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>...</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>CommunityOwnedDate</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>ClosedDate</th>\n",
       "      <th>OwnerDisplayName</th>\n",
       "      <th>LastEditorDisplayName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2010-07-19 19:12:12</td>\n",
       "      <td>23</td>\n",
       "      <td>1278.0</td>\n",
       "      <td>&lt;p&gt;How should I elicit prior distributions fro...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2010-09-15 21:08:26</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2010-07-19 19:12:57</td>\n",
       "      <td>22</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>&lt;p&gt;In many different statistical methods there...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2012-11-12 09:21:54</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2010-08-07 17:56:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2010-07-19 19:13:28</td>\n",
       "      <td>54</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>&lt;p&gt;What are some valuable Statistical Analysis...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2013-05-27 14:48:36</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4</td>\n",
       "      <td>36.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2011-02-12 05:50:03</td>\n",
       "      <td>2010-07-19 19:13:28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>135.0</td>\n",
       "      <td>2010-07-19 19:13:31</td>\n",
       "      <td>13</td>\n",
       "      <td>5224.0</td>\n",
       "      <td>&lt;p&gt;I have two groups of data.  Each with a dif...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2010-09-08 03:00:19</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-07-19 19:14:43</td>\n",
       "      <td>81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;The R-project&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\"http://www...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2010-07-19 19:21:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2010-07-19 19:21:15</td>\n",
       "      <td>2010-07-19 19:14:43</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>48321</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-23 09:00:01</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;you can use the matlab codes for svm and co...</td>\n",
       "      <td>19966.0</td>\n",
       "      <td>2013-01-23 09:00:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45118.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>48322</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-23 09:09:34</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;I use &lt;a href=\"http://www.gnu.org/software/...</td>\n",
       "      <td>892.0</td>\n",
       "      <td>2013-01-23 13:13:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>892.0</td>\n",
       "      <td>2013-01-23 13:13:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48311.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>48323</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-23 09:16:44</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;If I understand your question correctly, yo...</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>2013-01-23 09:16:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48247.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>48324</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-23 09:36:07</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Doesn't really help you with your question,...</td>\n",
       "      <td>19914.0</td>\n",
       "      <td>2013-01-23 09:36:07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48297.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>48325</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-23 09:44:07</td>\n",
       "      <td>-1</td>\n",
       "      <td>116.0</td>\n",
       "      <td>&lt;p&gt;I have 10 vectors each having 100,000 point...</td>\n",
       "      <td>19968.0</td>\n",
       "      <td>2013-02-22 11:23:54</td>\n",
       "      <td>are data sets obtained from a Normal distribut...</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       postId  PostTypeId  AcceptedAnswerId          CreaionDate  Score  \\\n",
       "0           1           1              15.0  2010-07-19 19:12:12     23   \n",
       "1           2           1              59.0  2010-07-19 19:12:57     22   \n",
       "2           3           1               5.0  2010-07-19 19:13:28     54   \n",
       "3           4           1             135.0  2010-07-19 19:13:31     13   \n",
       "4           5           2               NaN  2010-07-19 19:14:43     81   \n",
       "...       ...         ...               ...                  ...    ...   \n",
       "39995   48321           2               NaN  2013-01-23 09:00:01      0   \n",
       "39996   48322           2               NaN  2013-01-23 09:09:34      3   \n",
       "39997   48323           2               NaN  2013-01-23 09:16:44      1   \n",
       "39998   48324           2               NaN  2013-01-23 09:36:07      3   \n",
       "39999   48325           1               NaN  2013-01-23 09:44:07     -1   \n",
       "\n",
       "       ViewCount                                               Body   userId  \\\n",
       "0         1278.0  <p>How should I elicit prior distributions fro...      8.0   \n",
       "1         8198.0  <p>In many different statistical methods there...     24.0   \n",
       "2         3613.0  <p>What are some valuable Statistical Analysis...     18.0   \n",
       "3         5224.0  <p>I have two groups of data.  Each with a dif...     23.0   \n",
       "4            NaN  <p>The R-project</p>\\n\\n<p><a href=\"http://www...     23.0   \n",
       "...          ...                                                ...      ...   \n",
       "39995        NaN  <p>you can use the matlab codes for svm and co...  19966.0   \n",
       "39996        NaN  <p>I use <a href=\"http://www.gnu.org/software/...    892.0   \n",
       "39997        NaN  <p>If I understand your question correctly, yo...   2020.0   \n",
       "39998        NaN  <p>Doesn't really help you with your question,...  19914.0   \n",
       "39999      116.0  <p>I have 10 vectors each having 100,000 point...  19968.0   \n",
       "\n",
       "           LasActivityDate                                              Title  \\\n",
       "0      2010-09-15 21:08:26                      Eliciting priors from experts   \n",
       "1      2012-11-12 09:21:54                                 What is normality?   \n",
       "2      2013-05-27 14:48:36  What are some valuable Statistical Analysis op...   \n",
       "3      2010-09-08 03:00:19  Assessing the significance of differences in d...   \n",
       "4      2010-07-19 19:21:15                                                NaN   \n",
       "...                    ...                                                ...   \n",
       "39995  2013-01-23 09:00:01                                                NaN   \n",
       "39996  2013-01-23 13:13:30                                                NaN   \n",
       "39997  2013-01-23 09:16:44                                                NaN   \n",
       "39998  2013-01-23 09:36:07                                                NaN   \n",
       "39999  2013-02-22 11:23:54  are data sets obtained from a Normal distribut...   \n",
       "\n",
       "       ... AnswerCount  CommentCount  FavoriteCount  LastEditorUserId  \\\n",
       "0      ...         5.0             1           14.0               NaN   \n",
       "1      ...         7.0             1            8.0              88.0   \n",
       "2      ...        19.0             4           36.0             183.0   \n",
       "3      ...         5.0             2            2.0               NaN   \n",
       "4      ...         NaN             3            NaN              23.0   \n",
       "...    ...         ...           ...            ...               ...   \n",
       "39995  ...         NaN             0            NaN               NaN   \n",
       "39996  ...         NaN             2            NaN             892.0   \n",
       "39997  ...         NaN             0            NaN               NaN   \n",
       "39998  ...         NaN             0            NaN               NaN   \n",
       "39999  ...         2.0             4            NaN               NaN   \n",
       "\n",
       "              LastEditDate   CommunityOwnedDate ParentId  ClosedDate  \\\n",
       "0                      NaN                  NaN      NaN         NaN   \n",
       "1      2010-08-07 17:56:44                  NaN      NaN         NaN   \n",
       "2      2011-02-12 05:50:03  2010-07-19 19:13:28      NaN         NaN   \n",
       "3                      NaN                  NaN      NaN         NaN   \n",
       "4      2010-07-19 19:21:15  2010-07-19 19:14:43      3.0         NaN   \n",
       "...                    ...                  ...      ...         ...   \n",
       "39995                  NaN                  NaN  45118.0         NaN   \n",
       "39996  2013-01-23 13:13:30                  NaN  48311.0         NaN   \n",
       "39997                  NaN                  NaN  48247.0         NaN   \n",
       "39998                  NaN                  NaN  48297.0         NaN   \n",
       "39999                  NaN                  NaN      NaN         NaN   \n",
       "\n",
       "      OwnerDisplayName LastEditorDisplayName  \n",
       "0                  NaN                   NaN  \n",
       "1                  NaN                   NaN  \n",
       "2                  NaN                   NaN  \n",
       "3                  NaN                   NaN  \n",
       "4                  NaN                   NaN  \n",
       "...                ...                   ...  \n",
       "39995              NaN                   NaN  \n",
       "39996              NaN                   NaN  \n",
       "39997              NaN                   NaN  \n",
       "39998              NaN                   NaN  \n",
       "39999              NaN                   NaN  \n",
       "\n",
       "[40000 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = posts.rename(columns={'Id':'postId','OwnerUserId':'userId'})\n",
    "posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Define new dataframes for users and posts with the following selected columns:\n",
    "    **users columns**: userId, Reputation,Views,UpVotes,DownVotes\n",
    "    **posts columns**: postId, Score,userId,ViewCount,CommentCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_new = users[['userId','Reputation','Views','UpVotes','DownVotes']]\n",
    "posts_new = posts[['postId','Score','userId','ViewCount','CommentCount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           -1\n",
       "1            2\n",
       "2            3\n",
       "3            4\n",
       "4            5\n",
       "         ...  \n",
       "40320    55743\n",
       "40321    55744\n",
       "40322    55745\n",
       "40323    55746\n",
       "40324    55747\n",
       "Name: userId, Length: 40325, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_new.head()\n",
    "users_new['userId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postId</th>\n",
       "      <th>Score</th>\n",
       "      <th>userId</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>CommentCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1278.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5224.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>81</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>48321</td>\n",
       "      <td>0</td>\n",
       "      <td>19966.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>48322</td>\n",
       "      <td>3</td>\n",
       "      <td>892.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>48323</td>\n",
       "      <td>1</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>48324</td>\n",
       "      <td>3</td>\n",
       "      <td>19914.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>48325</td>\n",
       "      <td>-1</td>\n",
       "      <td>19968.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       postId  Score   userId  ViewCount  CommentCount\n",
       "0           1     23      8.0     1278.0             1\n",
       "1           2     22     24.0     8198.0             1\n",
       "2           3     54     18.0     3613.0             4\n",
       "3           4     13     23.0     5224.0             2\n",
       "4           5     81     23.0        NaN             3\n",
       "...       ...    ...      ...        ...           ...\n",
       "39995   48321      0  19966.0        NaN             0\n",
       "39996   48322      3    892.0        NaN             2\n",
       "39997   48323      1   2020.0        NaN             0\n",
       "39998   48324      3  19914.0        NaN             0\n",
       "39999   48325     -1  19968.0      116.0             4\n",
       "\n",
       "[40000 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postId</th>\n",
       "      <th>Score</th>\n",
       "      <th>userId</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>CommentCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1278.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5224.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>81</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>48321</td>\n",
       "      <td>0</td>\n",
       "      <td>19966.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>48322</td>\n",
       "      <td>3</td>\n",
       "      <td>892.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>48323</td>\n",
       "      <td>1</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>48324</td>\n",
       "      <td>3</td>\n",
       "      <td>19914.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>48325</td>\n",
       "      <td>-1</td>\n",
       "      <td>19968.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       postId  Score   userId  ViewCount  CommentCount\n",
       "0           1     23      8.0     1278.0             1\n",
       "1           2     22     24.0     8198.0             1\n",
       "2           3     54     18.0     3613.0             4\n",
       "3           4     13     23.0     5224.0             2\n",
       "4           5     81     23.0        NaN             3\n",
       "...       ...    ...      ...        ...           ...\n",
       "39995   48321      0  19966.0        NaN             0\n",
       "39996   48322      3    892.0        NaN             2\n",
       "39997   48323      1   2020.0        NaN             0\n",
       "39998   48324      3  19914.0        NaN             0\n",
       "39999   48325     -1  19968.0      116.0             4\n",
       "\n",
       "[40000 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postId</th>\n",
       "      <th>Score</th>\n",
       "      <th>userId</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>CommentCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1278.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5224.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>81</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>48321</td>\n",
       "      <td>0</td>\n",
       "      <td>19966.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>48322</td>\n",
       "      <td>3</td>\n",
       "      <td>892.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>48323</td>\n",
       "      <td>1</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>48324</td>\n",
       "      <td>3</td>\n",
       "      <td>19914.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>48325</td>\n",
       "      <td>-1</td>\n",
       "      <td>19968.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38962 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       postId  Score   userId  ViewCount  CommentCount\n",
       "0           1     23      8.0     1278.0             1\n",
       "1           2     22     24.0     8198.0             1\n",
       "2           3     54     18.0     3613.0             4\n",
       "3           4     13     23.0     5224.0             2\n",
       "4           5     81     23.0        NaN             3\n",
       "...       ...    ...      ...        ...           ...\n",
       "39995   48321      0  19966.0        NaN             0\n",
       "39996   48322      3    892.0        NaN             2\n",
       "39997   48323      1   2020.0        NaN             0\n",
       "39998   48324      3  19914.0        NaN             0\n",
       "39999   48325     -1  19968.0      116.0             4\n",
       "\n",
       "[38962 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "postId            int64\n",
       "Score             int64\n",
       "userId          float64\n",
       "ViewCount       float64\n",
       "CommentCount      int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "postId            int64\n",
       "Score             int64\n",
       "userId            int64\n",
       "ViewCount       float64\n",
       "CommentCount      int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(posts_new)\n",
    "\n",
    "posts_new = posts_new[((posts_new['userId']%1)==0)]\n",
    "display(posts_new)\n",
    "\n",
    "display(posts_new.dtypes)\n",
    "posts_new = posts_new.astype({'userId':'int'})\n",
    "display(posts_new.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postId</th>\n",
       "      <th>Score</th>\n",
       "      <th>userId</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>CommentCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>1278.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>18</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>5224.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>81</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>48321</td>\n",
       "      <td>0</td>\n",
       "      <td>19966</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>48322</td>\n",
       "      <td>3</td>\n",
       "      <td>892</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>48323</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>48324</td>\n",
       "      <td>3</td>\n",
       "      <td>19914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>48325</td>\n",
       "      <td>-1</td>\n",
       "      <td>19968</td>\n",
       "      <td>116.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38962 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       postId  Score  userId  ViewCount  CommentCount\n",
       "0           1     23       8     1278.0             1\n",
       "1           2     22      24     8198.0             1\n",
       "2           3     54      18     3613.0             4\n",
       "3           4     13      23     5224.0             2\n",
       "4           5     81      23        NaN             3\n",
       "...       ...    ...     ...        ...           ...\n",
       "39995   48321      0   19966        NaN             0\n",
       "39996   48322      3     892        NaN             2\n",
       "39997   48323      1    2020        NaN             0\n",
       "39998   48324      3   19914        NaN             0\n",
       "39999   48325     -1   19968      116.0             4\n",
       "\n",
       "[38962 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(posts_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Merge both dataframes, users and posts. \n",
    "You will need to make a [merge](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) of posts and users dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>...</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>CommunityOwnedDate</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>ClosedDate</th>\n",
       "      <th>OwnerDisplayName</th>\n",
       "      <th>LastEditorDisplayName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2014-04-23 13:43:43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2011-03-21 17:40:28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2011-03-21 17:46:43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>919.0</td>\n",
       "      <td>2011-03-30 19:23:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>919.0</td>\n",
       "      <td>2011-03-30 19:23:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38957</th>\n",
       "      <td>45934</td>\n",
       "      <td>11</td>\n",
       "      <td>2014-05-21 17:13:23</td>\n",
       "      <td>jasonweiyi</td>\n",
       "      <td>2014-06-08 13:36:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>805.0</td>\n",
       "      <td>2013-05-13 04:41:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jasonweiyi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38958</th>\n",
       "      <td>46192</td>\n",
       "      <td>36</td>\n",
       "      <td>2014-05-26 15:29:30</td>\n",
       "      <td>user1738753</td>\n",
       "      <td>2014-09-10 19:52:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>930.0</td>\n",
       "      <td>2012-10-18 15:07:40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>user1738753</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38959</th>\n",
       "      <td>46522</td>\n",
       "      <td>235</td>\n",
       "      <td>2014-06-01 17:25:18</td>\n",
       "      <td>Andy Blankertz</td>\n",
       "      <td>2014-09-13 18:03:07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Actuary for a life insurance company.&lt;/p&gt;\\r\\n</td>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>user1009703</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38960</th>\n",
       "      <td>52371</td>\n",
       "      <td>221</td>\n",
       "      <td>2014-07-19 13:36:58</td>\n",
       "      <td>Karel Petranek</td>\n",
       "      <td>2014-07-20 09:32:16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-07-30 20:09:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dark_charlie</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38961</th>\n",
       "      <td>55226</td>\n",
       "      <td>119</td>\n",
       "      <td>2014-09-04 07:34:48</td>\n",
       "      <td>Klas Lindbäck</td>\n",
       "      <td>2014-09-08 11:07:33</td>\n",
       "      <td>http://N/A</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>&lt;p&gt;Working with a wide range of languages, but...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16174.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Klas Lindbäck</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38962 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  Reputation         CreationDate     DisplayName  \\\n",
       "0          -1           1  2010-07-19 06:55:26       Community   \n",
       "1          -1           1  2010-07-19 06:55:26       Community   \n",
       "2          -1           1  2010-07-19 06:55:26       Community   \n",
       "3          -1           1  2010-07-19 06:55:26       Community   \n",
       "4          -1           1  2010-07-19 06:55:26       Community   \n",
       "...       ...         ...                  ...             ...   \n",
       "38957   45934          11  2014-05-21 17:13:23      jasonweiyi   \n",
       "38958   46192          36  2014-05-26 15:29:30     user1738753   \n",
       "38959   46522         235  2014-06-01 17:25:18  Andy Blankertz   \n",
       "38960   52371         221  2014-07-19 13:36:58  Karel Petranek   \n",
       "38961   55226         119  2014-09-04 07:34:48   Klas Lindbäck   \n",
       "\n",
       "            LastAccessDate                      WebsiteUrl  \\\n",
       "0      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "1      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "2      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "3      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "4      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "...                    ...                             ...   \n",
       "38957  2014-06-08 13:36:47                             NaN   \n",
       "38958  2014-09-10 19:52:34                             NaN   \n",
       "38959  2014-09-13 18:03:07                             NaN   \n",
       "38960  2014-07-20 09:32:16                             NaN   \n",
       "38961  2014-09-08 11:07:33                      http://N/A   \n",
       "\n",
       "                 Location                                            AboutMe  \\\n",
       "0      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "1      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "2      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "3      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "4      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "...                   ...                                                ...   \n",
       "38957                 NaN                                                NaN   \n",
       "38958                 NaN                                                NaN   \n",
       "38959                 NaN   <p>Actuary for a life insurance company.</p>\\r\\n   \n",
       "38960                 NaN                                                NaN   \n",
       "38961              Sweden  <p>Working with a wide range of languages, but...   \n",
       "\n",
       "       Views  UpVotes  ...  AnswerCount  CommentCount  FavoriteCount  \\\n",
       "0          0     5007  ...          NaN             0            NaN   \n",
       "1          0     5007  ...          NaN             0            NaN   \n",
       "2          0     5007  ...          NaN             0            NaN   \n",
       "3          0     5007  ...          NaN             0            NaN   \n",
       "4          0     5007  ...          NaN             0            NaN   \n",
       "...      ...      ...  ...          ...           ...            ...   \n",
       "38957      1        0  ...          1.0             2            NaN   \n",
       "38958      1        0  ...          1.0             2            1.0   \n",
       "38959     13       27  ...          3.0             0            NaN   \n",
       "38960      2        0  ...          7.0             5            8.0   \n",
       "38961      2        3  ...          NaN             0            NaN   \n",
       "\n",
       "      LastEditorUserId         LastEditDate   CommunityOwnedDate  ParentId  \\\n",
       "0                 -1.0  2014-04-23 13:43:43                  NaN       NaN   \n",
       "1                 -1.0  2011-03-21 17:40:28                  NaN       NaN   \n",
       "2                 -1.0  2011-03-21 17:46:43                  NaN       NaN   \n",
       "3                919.0  2011-03-30 19:23:14                  NaN       NaN   \n",
       "4                919.0  2011-03-30 19:23:14                  NaN       NaN   \n",
       "...                ...                  ...                  ...       ...   \n",
       "38957            805.0  2013-05-13 04:41:41                  NaN       NaN   \n",
       "38958            930.0  2012-10-18 15:07:40                  NaN       NaN   \n",
       "38959              NaN                  NaN                  NaN       NaN   \n",
       "38960              NaN                  NaN  2014-07-30 20:09:14       NaN   \n",
       "38961              NaN                  NaN                  NaN   16174.0   \n",
       "\n",
       "      ClosedDate  OwnerDisplayName  LastEditorDisplayName  \n",
       "0            NaN               NaN                    NaN  \n",
       "1            NaN               NaN                    NaN  \n",
       "2            NaN               NaN                    NaN  \n",
       "3            NaN               NaN                    NaN  \n",
       "4            NaN               NaN                    NaN  \n",
       "...          ...               ...                    ...  \n",
       "38957        NaN        jasonweiyi                    NaN  \n",
       "38958        NaN       user1738753                    NaN  \n",
       "38959        NaN       user1009703                    NaN  \n",
       "38960        NaN      dark_charlie                    NaN  \n",
       "38961        NaN     Klas Lindbäck                    NaN  \n",
       "\n",
       "[38962 rows x 34 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "users_posts = users.merge(posts,left_on='userId',right_on='userId')\n",
    "display(users_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. How many missing values do you have in your merged dataframe? On which columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484716"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DisplayName                  9\n",
       "WebsiteUrl               21496\n",
       "Location                 18057\n",
       "AboutMe                  18488\n",
       "Age                      24862\n",
       "ProfileImageUrl          36137\n",
       "AcceptedAnswerId         31813\n",
       "ViewCount                23572\n",
       "Body                       115\n",
       "Title                    23572\n",
       "Tags                     23572\n",
       "AnswerCount              23572\n",
       "FavoriteCount            31906\n",
       "LastEditorUserId         19624\n",
       "LastEditDate             19434\n",
       "CommunityOwnedDate       37136\n",
       "ParentId                 15923\n",
       "ClosedDate               38451\n",
       "OwnerDisplayName         38224\n",
       "LastEditorDisplayName    38753\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sum(users_posts.isnull().sum()))\n",
    "null_cols = users_posts.isnull().sum()\n",
    "display(null_cols[null_cols>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. You will need to make something with missing values.  Will you clean or filling them? Explain. \n",
    "**Remember** to check the results of your code before passing to the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>percent_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DisplayName</th>\n",
       "      <td>DisplayName</td>\n",
       "      <td>0.023099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <td>WebsiteUrl</td>\n",
       "      <td>55.171706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <td>Location</td>\n",
       "      <td>46.345157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AboutMe</th>\n",
       "      <td>AboutMe</td>\n",
       "      <td>47.451363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>Age</td>\n",
       "      <td>63.810893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProfileImageUrl</th>\n",
       "      <td>ProfileImageUrl</td>\n",
       "      <td>92.749346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <td>AcceptedAnswerId</td>\n",
       "      <td>81.651353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ViewCount</th>\n",
       "      <td>ViewCount</td>\n",
       "      <td>60.499974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Body</th>\n",
       "      <td>Body</td>\n",
       "      <td>0.295159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Title</th>\n",
       "      <td>Title</td>\n",
       "      <td>60.499974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>Tags</td>\n",
       "      <td>60.499974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AnswerCount</th>\n",
       "      <td>AnswerCount</td>\n",
       "      <td>60.499974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FavoriteCount</th>\n",
       "      <td>FavoriteCount</td>\n",
       "      <td>81.890047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <td>LastEditorUserId</td>\n",
       "      <td>50.367024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LastEditDate</th>\n",
       "      <td>LastEditDate</td>\n",
       "      <td>49.879370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CommunityOwnedDate</th>\n",
       "      <td>CommunityOwnedDate</td>\n",
       "      <td>95.313382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ParentId</th>\n",
       "      <td>ParentId</td>\n",
       "      <td>40.868025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ClosedDate</th>\n",
       "      <td>ClosedDate</td>\n",
       "      <td>98.688466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OwnerDisplayName</th>\n",
       "      <td>OwnerDisplayName</td>\n",
       "      <td>98.105847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LastEditorDisplayName</th>\n",
       "      <td>LastEditorDisplayName</td>\n",
       "      <td>99.463580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 column_name  percent_missing\n",
       "DisplayName                      DisplayName         0.023099\n",
       "WebsiteUrl                        WebsiteUrl        55.171706\n",
       "Location                            Location        46.345157\n",
       "AboutMe                              AboutMe        47.451363\n",
       "Age                                      Age        63.810893\n",
       "ProfileImageUrl              ProfileImageUrl        92.749346\n",
       "AcceptedAnswerId            AcceptedAnswerId        81.651353\n",
       "ViewCount                          ViewCount        60.499974\n",
       "Body                                    Body         0.295159\n",
       "Title                                  Title        60.499974\n",
       "Tags                                    Tags        60.499974\n",
       "AnswerCount                      AnswerCount        60.499974\n",
       "FavoriteCount                  FavoriteCount        81.890047\n",
       "LastEditorUserId            LastEditorUserId        50.367024\n",
       "LastEditDate                    LastEditDate        49.879370\n",
       "CommunityOwnedDate        CommunityOwnedDate        95.313382\n",
       "ParentId                            ParentId        40.868025\n",
       "ClosedDate                        ClosedDate        98.688466\n",
       "OwnerDisplayName            OwnerDisplayName        98.105847\n",
       "LastEditorDisplayName  LastEditorDisplayName        99.463580"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_missing = users_posts.isnull().sum() * 100 / len(users_posts)\n",
    "missing_value_df = pd.DataFrame({'column_name': users_posts.columns,\n",
    "                                 'percent_missing': percent_missing})\n",
    "missing_value_df[(percent_missing>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Thomas O',\n",
       " nan,\n",
       " 'Tanishka',\n",
       " 'Sebastian Horn',\n",
       " 'Alex Flint',\n",
       " 'Cecile',\n",
       " 'Shankarooni',\n",
       " 'akbertram',\n",
       " 'Varun',\n",
       " 'R_usr',\n",
       " 'Ste Trevors',\n",
       " 'Ricardo Altamirano',\n",
       " 'Charles',\n",
       " 'jrshrenk',\n",
       " 'user19869',\n",
       " 'Marianne',\n",
       " 'Mattijs Schipper',\n",
       " 'Pyramis',\n",
       " 'wolf.rauch',\n",
       " 'user16204',\n",
       " 'derrek',\n",
       " 'Aaron',\n",
       " 'Michael Litvin',\n",
       " 'user1366',\n",
       " 'alis',\n",
       " 'Happy',\n",
       " 'Zara',\n",
       " 'nibot',\n",
       " 'confusion',\n",
       " 'Saptarshi',\n",
       " 'test',\n",
       " 'user2721',\n",
       " 'yongtw123',\n",
       " 'reyman64',\n",
       " 'Terry',\n",
       " 'Chen',\n",
       " 'user13283',\n",
       " 'zsero',\n",
       " 'Serg',\n",
       " 'Jake Westfall',\n",
       " 'Julian',\n",
       " 'Jeff Boggs',\n",
       " 'EXP0',\n",
       " 'Jeruza',\n",
       " 'Dylan Sabulsky',\n",
       " 'Junkie Dolphin',\n",
       " 'miku',\n",
       " 'Ryan Ellis',\n",
       " 'PepsiCo',\n",
       " 'JMS',\n",
       " 'Jakob',\n",
       " 'PKP',\n",
       " 'andrija',\n",
       " 'Morten',\n",
       " 'alwinhoff',\n",
       " 'user2932',\n",
       " 'dato datuashvili',\n",
       " 'Masood Moshref Javadi',\n",
       " 'chk',\n",
       " 'Trevor Lohrbeer',\n",
       " 'MALL',\n",
       " 'onyourmark',\n",
       " 'kmm',\n",
       " 'user181813',\n",
       " 'dougk',\n",
       " 'Michele',\n",
       " 'FloppyDisk',\n",
       " 'Mélissa',\n",
       " 'user1643060',\n",
       " 'Johan Kullingsjo',\n",
       " 'mugetsu',\n",
       " 'Mohit Ranka',\n",
       " 'nimrodm',\n",
       " 'j-a',\n",
       " 'vzn',\n",
       " 'Theodor',\n",
       " 'A.S. Snijders',\n",
       " 'Nataraj',\n",
       " 'Akash Krishnan',\n",
       " 'moooeeeep',\n",
       " 'MattF',\n",
       " 'user14357',\n",
       " 'Sriram',\n",
       " 'Andrew',\n",
       " 'vrish88',\n",
       " 'Orsino',\n",
       " 'victor',\n",
       " 'user17601',\n",
       " 'Uwe Ziegenhagen',\n",
       " 'Mike G',\n",
       " 'Hernan_L',\n",
       " 'lokheart',\n",
       " 'user1785104',\n",
       " 'Baha',\n",
       " 'Peter Ellis',\n",
       " 'user497804',\n",
       " 'Cristofero',\n",
       " 'Qnan',\n",
       " 'user2303',\n",
       " 'Fiona',\n",
       " 'Els',\n",
       " 'user1151406',\n",
       " 'Reid',\n",
       " 'bobobobo',\n",
       " 'nbren12',\n",
       " 'Craig Wright',\n",
       " 'Patrick',\n",
       " 'user',\n",
       " 'TheBug',\n",
       " 'dimitris',\n",
       " 'Tyron James Hall',\n",
       " 'bili',\n",
       " 'Mika Tiihonen',\n",
       " 'frenchy',\n",
       " 'Toby',\n",
       " 'MigDus',\n",
       " 'avj',\n",
       " 'Benoit_Plante',\n",
       " 'Rhubarb Joker',\n",
       " 'user13226',\n",
       " 'Ken Dunn',\n",
       " 'wvguy8258',\n",
       " 'John Doe',\n",
       " 'Sunil Kosuri',\n",
       " 'LX_Gould',\n",
       " 'Em Ae',\n",
       " 'alsocasey',\n",
       " 'c00kiemonster',\n",
       " 'Figaro',\n",
       " 'Farbod',\n",
       " 'mnel',\n",
       " 'Ross T',\n",
       " 'Nicholas Mancuso',\n",
       " 'VBR',\n",
       " 'random_forest_fanatic',\n",
       " 'gozero',\n",
       " 'Ｊ. Ｍ.',\n",
       " 'Erin',\n",
       " 'MarkDollar',\n",
       " 'Sagarika',\n",
       " 'dianna',\n",
       " 'samc',\n",
       " 'Matthias',\n",
       " 'Sejanus',\n",
       " 'Barun',\n",
       " 'user1723765',\n",
       " 'JTL',\n",
       " 'J M',\n",
       " 'user1197460',\n",
       " 'Rrr',\n",
       " 'Kurtosis',\n",
       " 'Mark B',\n",
       " 'shima',\n",
       " 'Joris Meys',\n",
       " 'Espinho',\n",
       " 'Femi',\n",
       " 'Rok',\n",
       " 'ICR',\n",
       " 'Geff',\n",
       " 'Peter Hopkins',\n",
       " 'jtizzle36',\n",
       " 'Cardano',\n",
       " 'Kristian',\n",
       " 'Jim Bo',\n",
       " 'user1837966',\n",
       " 'fantomore',\n",
       " 'emanuele',\n",
       " 'olchauvin',\n",
       " 'Suzanne',\n",
       " 'Pavo',\n",
       " 'Cerin',\n",
       " 'joseph',\n",
       " 'Glenlivet',\n",
       " 'Benno',\n",
       " 'Georg M. Goerg',\n",
       " 'user13968',\n",
       " 'Sandra',\n",
       " 'Smandoli',\n",
       " 'Dima',\n",
       " 'Anish',\n",
       " 'user14583',\n",
       " 'mycat',\n",
       " 'Xavier',\n",
       " 'swh5',\n",
       " 'A.M.',\n",
       " 'Leena',\n",
       " 'Gustavo',\n",
       " 'user13985',\n",
       " 'Donna',\n",
       " 'EvKohl',\n",
       " 'a different ben',\n",
       " 'Mickaël S',\n",
       " 'Thilo',\n",
       " 'Ting Qian',\n",
       " 'William A. Josephson',\n",
       " 'tedddd',\n",
       " 'ansate',\n",
       " 'Greg Slodkowicz',\n",
       " 'user4341',\n",
       " 'taotree',\n",
       " 'Amit',\n",
       " 'Ilik',\n",
       " 'bgbg',\n",
       " 'Kai Feng Chew',\n",
       " 'Timo',\n",
       " 'Hugo S Ferreira',\n",
       " 'Bashayer Turkustani',\n",
       " 'Noam Kremen',\n",
       " 'Andy Lu',\n",
       " 'Georgette',\n",
       " 'ezbentley',\n",
       " 'zeferino',\n",
       " 'mhh',\n",
       " 'regulatethis',\n",
       " 'tshauck',\n",
       " 'tomaz',\n",
       " 'Shep',\n",
       " 'MP Sylvestre',\n",
       " 'Moritz',\n",
       " 'Randolph Chou',\n",
       " 'RobF',\n",
       " 'Mark Adler',\n",
       " 'Ana Maria Popescu',\n",
       " 'The August',\n",
       " 'edallme',\n",
       " 'cbeleites',\n",
       " 'Ari B. Friedman',\n",
       " 'remo',\n",
       " 'Girishkumar',\n",
       " 'Dennis Prangle',\n",
       " 'Oliver',\n",
       " 'DaveG',\n",
       " 'Hibernating',\n",
       " 'Peter Taylor',\n",
       " 'Frank',\n",
       " 'Brent',\n",
       " 'mataap',\n",
       " 'EliezerSilva',\n",
       " 'Misha',\n",
       " 'Or Zuk',\n",
       " 'Andrey',\n",
       " 'alps',\n",
       " 'madCode',\n",
       " 'Max Li',\n",
       " 'beginner',\n",
       " 'vonPetrushev',\n",
       " 'Tal Bashan',\n",
       " 'Aaron Stewart',\n",
       " 'ProbablePattern',\n",
       " 'itzy',\n",
       " 'Clarence Green',\n",
       " 'Suminda Dharmasena',\n",
       " 'sjm.majewski',\n",
       " 'Djellel Eddine',\n",
       " 'slotishtype',\n",
       " 'Jerry Gagelman',\n",
       " 'mpeytonjones',\n",
       " 'KalEl',\n",
       " 'Jeremy Hageman',\n",
       " 'Mikko',\n",
       " 'datarunner',\n",
       " 'Brabster',\n",
       " 'Will Newton',\n",
       " 'Ron Coleman',\n",
       " 'Anastasia',\n",
       " 'robbisg',\n",
       " 'Thomas Levine',\n",
       " 'Millsy11',\n",
       " 'chrisamiller',\n",
       " 'Aurimas',\n",
       " 'nimcap',\n",
       " 'arinte',\n",
       " 'BioGeek',\n",
       " 'gub',\n",
       " 'Cara',\n",
       " 'Preets',\n",
       " 'peuhp',\n",
       " 'Pavel',\n",
       " 'Framester',\n",
       " 'user1203297',\n",
       " 'Kevin Kane',\n",
       " 'João Daniel',\n",
       " 'Nick Sabbe',\n",
       " 'Dave Gerrard',\n",
       " 'user1474074',\n",
       " 'Masood_mj',\n",
       " 'DPS',\n",
       " 'Dylan',\n",
       " 'misha',\n",
       " 'Bi-Gnomial',\n",
       " 'Caroline Brorsson',\n",
       " 'user3430',\n",
       " 'AJay',\n",
       " 'Tomy',\n",
       " 'reisner',\n",
       " 'Niousha',\n",
       " 'nograpes',\n",
       " 'Jason Davies',\n",
       " 'craig',\n",
       " 'Gail Palubiak',\n",
       " 'Puzzled Paper Reviewer',\n",
       " 'Simon Nickerson',\n",
       " 'Bryce Thomas',\n",
       " 'A. Singh',\n",
       " 'Andre Holzner',\n",
       " 'Dragos',\n",
       " 'Pedro A. Ortega',\n",
       " 'Muhammad Alkarouri',\n",
       " 'tom brown',\n",
       " 'user3434',\n",
       " 'user7417',\n",
       " 'user1587692',\n",
       " 'std1976',\n",
       " 'totemob',\n",
       " 'George Dontas',\n",
       " 'John Smith',\n",
       " 'infoholic_anonymous',\n",
       " 'GrayR',\n",
       " 'Chthonic Project',\n",
       " 'khoda',\n",
       " 'Lionel',\n",
       " 'JooMing',\n",
       " 'chris',\n",
       " 'EOL',\n",
       " 'fra',\n",
       " 'ehmicky',\n",
       " 'osilocks',\n",
       " 'Matt Simpson',\n",
       " 'Jonny Cundall',\n",
       " 'JJ01',\n",
       " 'nonaxiomatic',\n",
       " 'Mahalanobis student',\n",
       " 'user9323',\n",
       " 'Ben Jackson',\n",
       " 'repied2',\n",
       " 'obesechicken13',\n",
       " 'kam',\n",
       " 'Rafael Maia',\n",
       " 'gianluca',\n",
       " 'kolar',\n",
       " 'Joe Pairman',\n",
       " 'clyfe',\n",
       " 'ruben baetens',\n",
       " 'Jakob Borg',\n",
       " 'maxTC',\n",
       " 'user18208',\n",
       " 'Tony James',\n",
       " 'Thomas Darling',\n",
       " 'KatyB',\n",
       " 'Mostafa Mahdieh',\n",
       " 'Freya Harrison',\n",
       " 'romkyns',\n",
       " 'asker123',\n",
       " 'Eric Marsh',\n",
       " 'Dr. Suhail Sarwar',\n",
       " 'SLD',\n",
       " 'mali',\n",
       " 'pkvprakash',\n",
       " 'Christine Forrester',\n",
       " 'tryingtoremovetheuserprofile',\n",
       " 'Jakub Hampl',\n",
       " 'mosaic',\n",
       " 'proton',\n",
       " 'glassy',\n",
       " 'Erwin',\n",
       " 'Kiudee',\n",
       " 'Add',\n",
       " 'Ron Gejman',\n",
       " 'Frank Rodgers',\n",
       " 'ikz',\n",
       " 'kat',\n",
       " 'Beltrame',\n",
       " 'JEquihua',\n",
       " 'GuillaumeThomas',\n",
       " 'nyer24a',\n",
       " 'JeffJo',\n",
       " 'Matt Krause',\n",
       " 'zneak',\n",
       " 'ScienceGuyRob',\n",
       " 'David Joubert',\n",
       " 'rezakhorshidi',\n",
       " 'DSG',\n",
       " 'alopex',\n",
       " 'Reza',\n",
       " 'Duopixel',\n",
       " 'Chris Beeley',\n",
       " 'Manuel Ramón',\n",
       " 'ImAlsoGreg',\n",
       " 'antoine',\n",
       " 'Ian Stuart',\n",
       " 'sabra',\n",
       " 'user176105',\n",
       " 'Tobin Baker',\n",
       " 'swanson',\n",
       " 'Erel Segal Halevi',\n",
       " 'Dirk Nachbar',\n",
       " 'Justin Solomon',\n",
       " 'Stefan',\n",
       " 'Raivo Kolde',\n",
       " 'heltonbiker',\n",
       " 'Thank you',\n",
       " 'John Barnes',\n",
       " 'Wikis',\n",
       " 'Greg Laughlin',\n",
       " 'Shadok',\n",
       " 'alya',\n",
       " 'pete',\n",
       " 'kellyjeglum',\n",
       " 'Larry Wasserman',\n",
       " 'Jimmy',\n",
       " 'Georgi',\n",
       " 'BEF',\n",
       " 'user4775',\n",
       " 'Shawn',\n",
       " 'zara',\n",
       " 'Ady',\n",
       " 'lowndrul',\n",
       " 'Alexandros',\n",
       " 'Erogol',\n",
       " 'Pierre',\n",
       " 'Kris',\n",
       " 'Robert Hill',\n",
       " 'Jens Roth',\n",
       " 'styfle',\n",
       " 'siger',\n",
       " 'wyrobekj',\n",
       " 'Aleksandr Zubatyi',\n",
       " 'Steve Bennett',\n",
       " 'peckjonk',\n",
       " 'Mashud',\n",
       " 'Sycren',\n",
       " 'friendpine',\n",
       " 'sas user',\n",
       " 'Greg Snow',\n",
       " 'Shuvankar Mukherjee',\n",
       " 'Guy',\n",
       " 'sortega',\n",
       " 'marino89',\n",
       " 'Alby',\n",
       " 'Vivek Kumar',\n",
       " 'yoplait',\n",
       " 'madereal',\n",
       " 'Sicnarf',\n",
       " 'O_Devinyak',\n",
       " 'tharen',\n",
       " 'KarloKatz',\n",
       " 'Statisfactions',\n",
       " 'joker',\n",
       " 'TJM',\n",
       " 'JPC',\n",
       " 'mike',\n",
       " 'aurora1625',\n",
       " 'Luke404',\n",
       " 'Kurtis Voris',\n",
       " 'John Lehmann',\n",
       " 'Adam Bailey',\n",
       " 'bwalenz',\n",
       " 'Robert',\n",
       " 'mirror2image',\n",
       " 'Vebjorn Ljosa',\n",
       " 'user1363251',\n",
       " 'jerry',\n",
       " 'nournia',\n",
       " 'RTM',\n",
       " 'demongolem',\n",
       " 'user4917',\n",
       " 'RobTeszka',\n",
       " 'RobJackson28',\n",
       " 'barerd',\n",
       " 'ThomasBayes',\n",
       " 'Vagif Abilov',\n",
       " 'skhullar',\n",
       " 'user863882',\n",
       " 'James Erl',\n",
       " 'yang1986',\n",
       " 'Leau',\n",
       " 'pazam',\n",
       " 'Marc-André Lafortune',\n",
       " 'Anvaka',\n",
       " 'Pinar Donmez',\n",
       " 'Johann Hibschman',\n",
       " 'Olivier',\n",
       " 'PeterRabbit',\n",
       " 'abhinavkulkarni',\n",
       " 'matcheek',\n",
       " 'Adele Tommers',\n",
       " 'edA-qa mort-ora-y',\n",
       " 'Rorschach',\n",
       " 'Grafix1',\n",
       " 'littleEinstein',\n",
       " 'Mukul',\n",
       " 'tristan',\n",
       " 'user1753987',\n",
       " 'giovanna',\n",
       " 'Nathalie',\n",
       " 'Tal Fishman',\n",
       " 'user17645',\n",
       " 'kara',\n",
       " 'alex',\n",
       " 'Anony-Mousse',\n",
       " 'Sigvard',\n",
       " 'Ashutosh',\n",
       " 'Roanne',\n",
       " 'bregt',\n",
       " 'Abhischek',\n",
       " 'Jeffrey',\n",
       " 'Lars Kotthoff',\n",
       " 'Julia Braeutigam',\n",
       " 'Tiago Pasqualini',\n",
       " 'wh1t3cat1k',\n",
       " 'Duke of Lizards',\n",
       " 'AruniRC',\n",
       " 'warren',\n",
       " 'Kerim Atasoy',\n",
       " 'user19950',\n",
       " 'cboettig',\n",
       " 'Arun',\n",
       " 'Vegard Larsen',\n",
       " 'HornetsFan',\n",
       " 'user1257313',\n",
       " 'jkd',\n",
       " 'Johan',\n",
       " 'DATTARAM',\n",
       " 'Will Jagy',\n",
       " 'Timothy P. Jurka',\n",
       " 'pyrole',\n",
       " 'Dave Harris',\n",
       " 'kiki',\n",
       " 'Bear',\n",
       " 'marc',\n",
       " 'dhess',\n",
       " 'gerrit',\n",
       " 'Alexandru Luchian',\n",
       " 'JoFrhwld',\n",
       " 'tortilla',\n",
       " 'Pepps',\n",
       " 'dani',\n",
       " 'BrutForce',\n",
       " 'Andree',\n",
       " 'Pedro Pires',\n",
       " 'The Eremite',\n",
       " 'Steven Noble',\n",
       " 'CarrKnight',\n",
       " 'Innuo',\n",
       " 'Tano',\n",
       " 'John Buonagurio',\n",
       " 'kayaker',\n",
       " 'priyasaha10',\n",
       " 'RAH',\n",
       " 'Jacques Tardie',\n",
       " 'Mhc',\n",
       " 'shigeta',\n",
       " 'rae_mil',\n",
       " 'Mike Bijon',\n",
       " 'vitalik',\n",
       " 'Chase Mason',\n",
       " 'sfuj',\n",
       " 'bob',\n",
       " 'Alexandre Passos',\n",
       " 'A Lion',\n",
       " 'newprint',\n",
       " 'pertsevin',\n",
       " 'Stef van Buuren',\n",
       " 'Tiago',\n",
       " 'gam',\n",
       " 'ipadawan',\n",
       " 'Rachel Gibson',\n",
       " 'Missing Bob',\n",
       " 'Manuel Kuehner',\n",
       " 'E P',\n",
       " 'Learn Stat',\n",
       " 'AlexPof',\n",
       " 'Christine Hong',\n",
       " 'Melissa Tso',\n",
       " 'user1600',\n",
       " 'Vlad Niculae',\n",
       " 'Ujjwal',\n",
       " 'Salvador',\n",
       " 'arnsholt',\n",
       " 'Miroslav Sabo',\n",
       " 'kaja',\n",
       " 'Mike Young',\n",
       " 'ben w',\n",
       " 'hazan',\n",
       " 'Andreas Wittberg',\n",
       " 'Grittathh',\n",
       " 'Krisrs1128',\n",
       " 'Ryan Barnhart',\n",
       " 'ali_nad_syed',\n",
       " 'ganesh reddy',\n",
       " 'guest',\n",
       " 'Stanley',\n",
       " 'Ian Mackinnon',\n",
       " 'John Sjölander',\n",
       " 'user1029',\n",
       " 'Lara',\n",
       " 'AlanD',\n",
       " 'JKP',\n",
       " 'user765195',\n",
       " 'Garuda Ikëtum',\n",
       " 'Gonten',\n",
       " 'Tommy',\n",
       " 'Gilbert',\n",
       " 'Cheng',\n",
       " 'Matthew Leingang',\n",
       " 'Adam Ingmansson',\n",
       " 'user12798',\n",
       " 'Edward',\n",
       " 'yuk',\n",
       " 'guenis',\n",
       " 'user1775772',\n",
       " 'Firefeather',\n",
       " 'EpiGurlz',\n",
       " 'Bar',\n",
       " 'Mark Heckmann',\n",
       " 'user22',\n",
       " 'TMOD',\n",
       " 'Ron',\n",
       " 'KGB',\n",
       " 'Robert Price',\n",
       " 'Jason O. Jensen',\n",
       " 'Kolibris',\n",
       " 'Shafi',\n",
       " 'Menno',\n",
       " 'dontangg',\n",
       " 'flxlex',\n",
       " 'Herman Haugland',\n",
       " 'Daniel Ryan',\n",
       " 'Spencer',\n",
       " 'dytchay',\n",
       " 'nik.shornikov',\n",
       " '2scoops',\n",
       " 'Ernie',\n",
       " 'mnemonic',\n",
       " 'alan',\n",
       " 'rolando2',\n",
       " 'Siddhant',\n",
       " 'Linda',\n",
       " 'stacy',\n",
       " 'Graham Jones',\n",
       " 'user3136',\n",
       " 'Cyan',\n",
       " 'Ankush',\n",
       " 'xmjx',\n",
       " 'Sam Saffron',\n",
       " 'Pablo Marin-Garcia',\n",
       " 'johanvdw',\n",
       " 'Florian Bw',\n",
       " 'Vian Chinner',\n",
       " 'weezybizzle',\n",
       " 'Saber CN',\n",
       " 'Alex Ott',\n",
       " 'jawns317',\n",
       " 'Shelby',\n",
       " 'Vinh Nguyen',\n",
       " 'ulidtko',\n",
       " 'drm',\n",
       " 'Steffi',\n",
       " 'mushroom',\n",
       " 'Vijay',\n",
       " 'Travis R',\n",
       " 'twk',\n",
       " 'bonCodigo',\n",
       " 'hung',\n",
       " 'Fernando Sanchez',\n",
       " 'Andreas Mueller',\n",
       " 'brian',\n",
       " 'Isaac',\n",
       " 'dannytoone',\n",
       " 'Aren Cambre',\n",
       " 'Luxspes',\n",
       " 'tim',\n",
       " 'ssx6',\n",
       " 'MikeRand',\n",
       " 'gakera',\n",
       " 'Wee',\n",
       " 'Rho',\n",
       " 'Andy ',\n",
       " 'ektrules',\n",
       " 'user4167',\n",
       " 'asaaki',\n",
       " 'fionn',\n",
       " 'Michal Illich',\n",
       " 'user10720',\n",
       " 'dirk5959',\n",
       " 'user16093',\n",
       " 'max356',\n",
       " 'SiegeX',\n",
       " 's_a',\n",
       " 'user2137',\n",
       " 'egbutter',\n",
       " 'Kywia',\n",
       " 'Unna',\n",
       " 'Monica',\n",
       " 'ozlem aktas',\n",
       " 'Prabhu M',\n",
       " 'psandersen',\n",
       " 'josetanago',\n",
       " 'Rider_X',\n",
       " 'feelfree',\n",
       " 'Fred',\n",
       " 'Richard Herron',\n",
       " 'Kathy',\n",
       " 'Deyaa',\n",
       " 'user9968',\n",
       " 'Anubhav',\n",
       " 'dr.bunsen',\n",
       " 'Digital Gal',\n",
       " 'Jay',\n",
       " 'sharky',\n",
       " 'Clamstew',\n",
       " 'user4624',\n",
       " 'mrsteve',\n",
       " 'biomarker',\n",
       " 'JanD',\n",
       " 'probabilityislogic',\n",
       " 'Tom Wright',\n",
       " 'Nickparsa',\n",
       " 'zzk',\n",
       " 'debuism',\n",
       " 'UncleDJ',\n",
       " 'Lorin Hochstein',\n",
       " 'thomas',\n",
       " 'Ista',\n",
       " 'mitch',\n",
       " 'user900978',\n",
       " 'nellaivijay',\n",
       " 'Aufziehvogel',\n",
       " 'twerdster',\n",
       " 'David Jensen',\n",
       " 'dcl',\n",
       " 'J. Abrahamson',\n",
       " 'Matthias Greiner',\n",
       " 'Rich',\n",
       " 'Matt Parker',\n",
       " 'usεr11852',\n",
       " 'SheldonCooper',\n",
       " 'Christin',\n",
       " 'Dan Dunn',\n",
       " 'zhang',\n",
       " 'user1690846',\n",
       " 'user14470',\n",
       " 'Ether Desf',\n",
       " 'Davoud Taghawi-Nejad',\n",
       " 'Luca V',\n",
       " 'zaxtax',\n",
       " 'Rasmus',\n",
       " 'Josh Bleecher Snyder',\n",
       " 'akshayl',\n",
       " 'Marco A',\n",
       " 'Christopher Manning',\n",
       " 'rajan sthapit',\n",
       " 'dnagirl',\n",
       " 'Coder',\n",
       " 'geek girl',\n",
       " 'Joshua Frank',\n",
       " 'Miriam',\n",
       " 'Claudio Albertin',\n",
       " 't l',\n",
       " 'user3155',\n",
       " 'bliako',\n",
       " 'Pamela Edmond',\n",
       " 'Peteris',\n",
       " 'ALV',\n",
       " 'M. Alaggan',\n",
       " 'Neil Toronto',\n",
       " 'Ethan Bartolic',\n",
       " 'Patrick McCann',\n",
       " 'jitendra',\n",
       " 'aaronlevin',\n",
       " 'Michael.Z',\n",
       " 'Dementic',\n",
       " 'GregM',\n",
       " 'trideceth12',\n",
       " 'user5563',\n",
       " 'cjauvin',\n",
       " 'user1239631',\n",
       " 'Groundskeeper Willie',\n",
       " 'pnewhook',\n",
       " 'wipeout',\n",
       " 'dram',\n",
       " 'stevejb',\n",
       " 'Eugene Osovetsky',\n",
       " 'Abhinav Gujjar',\n",
       " 'vkubicki',\n",
       " 'user1140126',\n",
       " 'chhhhhh',\n",
       " 'Polshgiant',\n",
       " 'rafael caballero',\n",
       " 'capoluca',\n",
       " 'alwin hoff',\n",
       " 'Carlos Accioly',\n",
       " 'user12414',\n",
       " 'mzalikhan',\n",
       " 'Sean',\n",
       " 'Jun',\n",
       " 'JosephH',\n",
       " 'Simon Andrews',\n",
       " 'Emx',\n",
       " 'Bravo',\n",
       " 'joran',\n",
       " 'Yug',\n",
       " 'Fletcher Duran',\n",
       " 'sarah',\n",
       " 'Ravi',\n",
       " 'James Skidmore',\n",
       " 'John Reed',\n",
       " 'marcin63',\n",
       " 'JL.Yap',\n",
       " 'Ikuyasu',\n",
       " 'Fernando',\n",
       " 'Keith Winstein',\n",
       " 'Will Clyne',\n",
       " 'Vít Tuček',\n",
       " 'Ayse Elvan Gunduz',\n",
       " 'B_Dev',\n",
       " 'Harrison',\n",
       " 'user12208',\n",
       " 'Antonio Tirri',\n",
       " 'James Forsythe',\n",
       " 'alenyb',\n",
       " 'Nabil',\n",
       " 'MvG',\n",
       " 'fioghual',\n",
       " 'RealKnight',\n",
       " 'ilprincipe',\n",
       " 'icobes',\n",
       " 'Fenix',\n",
       " 'hypothizer',\n",
       " 'Stuart',\n",
       " 'Dimitriy V. Masterov',\n",
       " 'kjearns',\n",
       " 'chainsaw riot',\n",
       " 'adavid',\n",
       " 'Amateur',\n",
       " 'Michael Hardy',\n",
       " 'Phil',\n",
       " 'astrofrog',\n",
       " 'huda',\n",
       " 'danilofreire',\n",
       " 'Mike Dunlavey',\n",
       " 'regressand',\n",
       " 'Vicente Cartas',\n",
       " 'Unleashed',\n",
       " 'jeffalstott',\n",
       " 'sylowtheorems',\n",
       " 'Hzmy',\n",
       " 'Roddy G',\n",
       " 'KronoS',\n",
       " 'user1800340',\n",
       " 'siegfried',\n",
       " 'alang',\n",
       " 'Dean Portelli',\n",
       " 'Peter Deplewski',\n",
       " 'athula herath',\n",
       " 'Shrey',\n",
       " 'Maysam',\n",
       " 'Arne',\n",
       " 'ph_singer',\n",
       " 'Erika Hernández',\n",
       " 'Steve Trawley',\n",
       " 'napsterockoeur',\n",
       " 'gabgoh',\n",
       " 'biased_estimator',\n",
       " 'Bill718',\n",
       " 'Anthony Labarre',\n",
       " 'Vikas',\n",
       " 'atiretoo',\n",
       " 'Roy Yip',\n",
       " 'MsSnowy',\n",
       " 'user16540',\n",
       " 'Noah Watkins',\n",
       " 'Python_R',\n",
       " 'Ghasem',\n",
       " 'Jo Lewis',\n",
       " 'scottyaz',\n",
       " 'xiaohan2012',\n",
       " 'Ben',\n",
       " 'user13910',\n",
       " 'Julio Diaz',\n",
       " 'alexpotato',\n",
       " 'drezha',\n",
       " 'postrational',\n",
       " 'yimyom',\n",
       " 'Wai Yip Tung',\n",
       " 'Yannick Wurm',\n",
       " 'JavaCake',\n",
       " 'user11920',\n",
       " 'TAD',\n",
       " 'abmarie',\n",
       " 'Dave Anderson',\n",
       " 'Leo',\n",
       " 'Matti Lyra',\n",
       " 'Stats',\n",
       " 'Ghillie Dhu',\n",
       " 'dmckee',\n",
       " 'bnaul',\n",
       " 'jthurman',\n",
       " 'pb1',\n",
       " 'kvista',\n",
       " 'William Fernandes',\n",
       " 'MSR',\n",
       " 'Richard E. Gilder',\n",
       " 'Joshua',\n",
       " 'wal',\n",
       " 'Erik J',\n",
       " 'Qamber',\n",
       " 'Serene',\n",
       " 'Sridhar R',\n",
       " 'Kirthi Raman',\n",
       " 'neuron',\n",
       " 'coen',\n",
       " 'biku',\n",
       " 'CLS',\n",
       " 'user1421972',\n",
       " 'ebony1',\n",
       " 'bart',\n",
       " 'Dason',\n",
       " 'DeeperUnderstanding',\n",
       " 'suresh',\n",
       " 'Doug Johnson',\n",
       " 'WoA',\n",
       " 'msh210',\n",
       " 'Noro',\n",
       " 'Iva Rashkova',\n",
       " 'Mark Miller',\n",
       " 'bmu',\n",
       " 'user5012',\n",
       " 'sucuriju',\n",
       " 'Gim',\n",
       " 'Ion Caciula',\n",
       " 'mindmatters',\n",
       " 'bootstrapping CI question',\n",
       " 'andrew strathclyde',\n",
       " 'puzzle',\n",
       " 'Sarah  ',\n",
       " 'duckworthd',\n",
       " 'GMC',\n",
       " 'SUBHABRATA',\n",
       " 'Andrew Dalke',\n",
       " 'Steven',\n",
       " 'J_L',\n",
       " 'claws',\n",
       " 'Sideshow Bob',\n",
       " 'Florian Pilz',\n",
       " 'griffin',\n",
       " 'Aliya',\n",
       " 'rumtscho',\n",
       " 'Hiwa',\n",
       " 'nullglob',\n",
       " 'nzcoops',\n",
       " 'biomed',\n",
       " 'Omar',\n",
       " 'Maciej Jończyk',\n",
       " 'Bonoboticians',\n",
       " 'Juha-Matti S.',\n",
       " 'Omnium',\n",
       " 'Dan M.',\n",
       " 'bill_080',\n",
       " 'Juan',\n",
       " 'Andrew Latham',\n",
       " 'wmmurrah',\n",
       " 'Zach Sheffler',\n",
       " 'Ed Hyer',\n",
       " 'hila',\n",
       " 'user1626730',\n",
       " 'mona',\n",
       " 'jorges',\n",
       " 'Gil Kalai',\n",
       " 'Kimmeke',\n",
       " 'Jehu',\n",
       " 'jack',\n",
       " 'Quantitative Historian',\n",
       " 'Slak',\n",
       " 'Matt Burland',\n",
       " 'Alex319',\n",
       " 'Hypermnestra',\n",
       " 'Jane',\n",
       " 'user16309',\n",
       " 'bmc',\n",
       " 'RyanB',\n",
       " 'Penguin_Knight',\n",
       " 'skybreaker',\n",
       " 'M.R.Garmsiri',\n",
       " 'Ian Turner',\n",
       " 'alwaysean',\n",
       " 'Petra',\n",
       " 'izhar',\n",
       " 'eWizardII',\n",
       " 'Epifunky',\n",
       " 'user14185',\n",
       " 'tha4',\n",
       " 'Affine',\n",
       " 'user3438',\n",
       " 'f1r3br4nd',\n",
       " 'Jelly',\n",
       " 'OverKAnalytics',\n",
       " 'Donnie',\n",
       " 'user4594',\n",
       " 'Noah',\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " 'http://htt://ecome.wordpress.com',\n",
       " 'http://www.astro.puc.cl/~nespino',\n",
       " 'http://gplus.to/dlras2/about',\n",
       " 'https://github.com/weishi',\n",
       " 'http://research.microsoft.com/~minka/',\n",
       " 'http://www.enigmaportal.com',\n",
       " 'http://ie.linkedin.com/in/danielvassallo',\n",
       " 'http://www.caspershouse.com',\n",
       " 'http://ronandaly.org/',\n",
       " 'http://www.clsnyder.com',\n",
       " 'http://stackoverflow.com/users/1493368/metrics',\n",
       " 'http://www.cs.bilkent.edu.tr/~cant',\n",
       " 'http://www.simontoth.cz',\n",
       " 'http://www.championkitchens.com',\n",
       " 'http://in.linkedin.com/in/sayandasgupta',\n",
       " 'http://www.google.com/profiles/ivan.mashchenko',\n",
       " 'http://www.celtic-knot-creator.com',\n",
       " 'http://www.about.me/tonplomp',\n",
       " 'http://chetvericov.ru',\n",
       " 'http://www.brc.net.pk',\n",
       " 'http://Nonewhatsoever',\n",
       " 'http://dirknachbar.com',\n",
       " 'http://www.footballthoughtsfromsweden.wordpress.com',\n",
       " 'http://dipanmehta.wordpress.com',\n",
       " 'http://research.sourcebyte.com',\n",
       " 'http://www.gwern.net',\n",
       " 'http://arsphysica.wordpress.com',\n",
       " 'http://oswco.com',\n",
       " 'http://pranphy.wordpress.com/',\n",
       " 'http://www.stiglerdiet.com',\n",
       " 'http://www.setzkorn.eu',\n",
       " 'http://www.ics.uci.edu/~duboisc',\n",
       " 'http://dahtah.wordpress.com',\n",
       " 'http://BG.SE',\n",
       " 'http://justinbozonier.posterous.com',\n",
       " 'http://twitter.com/siah',\n",
       " 'http://www.ime.usp.br/~patriota',\n",
       " 'http://www.bookmarkz.us',\n",
       " 'http://jhoward.fastmail.fm',\n",
       " 'http://blog.programet.org',\n",
       " 'http://stronginference.com',\n",
       " 'http://careers.stackoverflow.com/bjoernpollex',\n",
       " 'http://fsteeg.wordpress.com',\n",
       " 'http://barry.rowlingson.com/',\n",
       " 'http://blog.explainmydata.com',\n",
       " 'http://allencch.wordpress.com/',\n",
       " 'http://andrewbauerband.weebly.com',\n",
       " 'http://about.me/lior.kogan',\n",
       " 'http://michal.illich.cz/',\n",
       " 'http://www.select-statistics.co.uk',\n",
       " 'http://chil.rice.edu/jzemla',\n",
       " 'https://simao.io',\n",
       " 'http://warwickmasson.com',\n",
       " 'http://www.wolfsheadsoftware.co.uk',\n",
       " 'http://bschwehn.de',\n",
       " 'http://twitter.com/zjelveh',\n",
       " 'http://floorplanner.com',\n",
       " 'http://www.cs.mcgill.ca/~akazna/',\n",
       " 'http://synthesis.williamgunn.org',\n",
       " 'http://grey.colorado.edu/mingus',\n",
       " 'http://www.stat.berkeley.edu/~drosen/',\n",
       " 'http://idebamarketing.com',\n",
       " 'http://oracleaide.wordpress.com',\n",
       " 'http://www.annemphillip.com',\n",
       " 'http://blog.postmaster.gr',\n",
       " 'http://mcfromnz.wordpress.com/',\n",
       " 'http://qmaxim.com',\n",
       " 'http://www.calvin.edu/~mkh2',\n",
       " 'http://www.dcsc.tudelft.nl/~itkachev/',\n",
       " 'http://cs.unitbv.ro/~lmsasu',\n",
       " 'http://www.fittestmodel.com',\n",
       " 'http://inajourney.wordpress.com/',\n",
       " 'http://nicolas.kruchten.com/',\n",
       " 'http://lizzaninline.com',\n",
       " 'http://www.sowinglight.com',\n",
       " 'http://phihag.de/',\n",
       " 'http://www.getpsprep.org',\n",
       " 'http://www.montegraphia.com',\n",
       " 'http://nl.linkedin.com/in/jochemdonkers',\n",
       " 'http://hetland.org',\n",
       " 'http://www.kerimatasoy.com',\n",
       " 'http://www.matdiesel.co.uk',\n",
       " 'http://www.dadabik.org',\n",
       " 'http://profiles.google.com/114451807329956829391/about',\n",
       " 'http://degi.posterous.com',\n",
       " 'http://www.cherevko.net',\n",
       " 'http://twitter.com/mariosangiorgio',\n",
       " 'http://www.stat.ualberta.ca/people/schmu/dept_page.html',\n",
       " 'http://www.kbrandt.com',\n",
       " 'http://vkreynin.wordpress.com',\n",
       " 'http://ventirisk.com',\n",
       " 'http://gettinggeneticsdone.blogspot.com/',\n",
       " 'http://www.kdnuggets.com/gps.html',\n",
       " 'http://www.cherhan.net',\n",
       " 'http://www.twitter.com/neilkod',\n",
       " 'https://www.researchgate.net/profile/Kaveh_Vakili/?ev=hdr_xprf',\n",
       " 'http://www.fennel.rcast.u-tokyo.ac.jp/profilee_ktakahashi.html',\n",
       " 'http://modernthings.org',\n",
       " 'http://berndweiss.net',\n",
       " 'http://myprofile.cos.com/gilder',\n",
       " 'http://about.me/siddhantgoel/',\n",
       " 'http://shape-of-code.coding-guidelines.com',\n",
       " 'http://www.quantdec.com',\n",
       " 'http://www.redf.org',\n",
       " 'http://christianjauv.in',\n",
       " 'http://dnsdebug.com',\n",
       " 'http://gplus.to/alexthompson',\n",
       " 'http://www.reedcopsey.com',\n",
       " 'http://www.bioinformatics.babraham.ac.uk',\n",
       " 'http://tirandocodigo.net',\n",
       " 'http://dafeda.wordpress.com',\n",
       " 'http://dssheep.blogspot.com/',\n",
       " 'http://gabrieljperez.com',\n",
       " 'http://lesinigo.it/',\n",
       " 'http://www.csiro.au/science/TasICTCentre.html',\n",
       " 'http://edgarhassler.com',\n",
       " 'https://sites.google.com/site/gregorpassolt/',\n",
       " 'http://cpuguru.net',\n",
       " 'http://harithimanshu.wordpress.com',\n",
       " 'http://blog.adamsmith.cc/',\n",
       " 'http://energynumbers.info/?ref=stackexchange',\n",
       " 'http://dalkescientific.com/writings/',\n",
       " 'http://www.stats.stackexchange.com',\n",
       " 'https://www.facebook.com/paterz1',\n",
       " 'http://xkcd.com',\n",
       " 'http://www.riskmath.eu',\n",
       " 'http://www.thenativeweb.io',\n",
       " 'http://petrustheron.com/',\n",
       " 'https://github.com/vegard',\n",
       " 'http://math.pugetsound.edu/~mspivey/',\n",
       " 'http://scholar.google.com/citations?user=L5t2HBYAAAAJ&hl=en',\n",
       " 'http://www.catherinedalzell.ca',\n",
       " 'http://openwetware.org/wiki/User:Thaman_Chand',\n",
       " 'http://ineed.coffee',\n",
       " 'http://amanahuja.me',\n",
       " 'http://google.com',\n",
       " 'http://www.su3analytics.com',\n",
       " 'http://polstat.org',\n",
       " 'http://www.rommil.com',\n",
       " 'http://Unnown',\n",
       " 'http://www.msebera.cz',\n",
       " 'http://about.sjrdesign.net',\n",
       " 'http://stackoverflow.com/users/298389',\n",
       " 'http://themanthursday.com',\n",
       " 'http://www.econinfo.de',\n",
       " 'http://www.itr.unisa.edu.au/~mckillrg',\n",
       " 'http://moderntoolmaking.blogspot.com/',\n",
       " 'http://ncatlab.org/davidroberts/show/HomePage',\n",
       " 'http://student.ucr.edu/~ddelo001',\n",
       " 'http://dat-berger.de',\n",
       " 'http://conlections.com',\n",
       " 'http://www.Kormanik.com',\n",
       " 'http://aaboyles.com',\n",
       " 'http://joelreyesnoche.wordpress.com/tag/mathematics/',\n",
       " 'http://www.dandexter.com',\n",
       " 'http://www.anovisions.com',\n",
       " 'http://yandex.ru/search',\n",
       " 'http://www.genroe.com',\n",
       " 'http://kdl.cs.umass.edu/people/jensen/',\n",
       " 'http://sapir.us',\n",
       " 'http://koblik.blogspot.com',\n",
       " 'https://github.com/bontibon',\n",
       " 'http://www.jasonmtroos.com',\n",
       " 'http://en.wikipedia.org/wiki/User:MikeDunlavey',\n",
       " 'http://www.danclarke.com',\n",
       " 'http://Www.phastar.co.uk',\n",
       " 'http://www.facebook.com/eshwar.andhavarapu',\n",
       " 'http://www.yuan-shuai.info',\n",
       " 'http://birdventure.blogspot.com',\n",
       " 'http://www.kieranhealy.org',\n",
       " 'http://blog.ouseful.info',\n",
       " 'http://vzn1.wordpress.com/',\n",
       " 'http://aparamon.msk.ru',\n",
       " 'http://Grieu',\n",
       " 'http://stuartlayton.com',\n",
       " 'http://fr.linkedin.com/in/ayseulgen',\n",
       " 'http://scholar.google.com/citations?user=WfzzY4cAAAAJ',\n",
       " 'http://belikoff.net',\n",
       " 'http://www.cs.utah.edu/~moeller',\n",
       " 'http://www.masoudmoshref.com',\n",
       " 'http://pantheon.yale.edu/~ejw3/',\n",
       " 'https://sites.google.com/site/antonioirpino/',\n",
       " 'http://stackoverflow.com/users/717441/benjamin',\n",
       " 'http://www.facebook.com/home.php#!/profile.php?id=754791381',\n",
       " 'http://www.unur.com/',\n",
       " 'http://www.stanford.edu/~justso1',\n",
       " 'http://kuk.ac.in',\n",
       " 'http://www.mintuz.co.uk',\n",
       " 'http://www.jabsy.com',\n",
       " 'http://www.softwest.com',\n",
       " 'http://www.kotbcorp.blogspot.com',\n",
       " 'http://tinyurl.com/ashwinpanchapakesan',\n",
       " 'http://nattinger.net',\n",
       " 'http://www.kalisch.biz',\n",
       " 'http://stackoverflow.com/users/11123/philip-fourie',\n",
       " 'http://scholar.google.com/citations?user=J72tQy4AAAAJ',\n",
       " 'http://neurov.is/on',\n",
       " 'http://english-jack.blogspot.com',\n",
       " 'http://lambert.geek.nz/',\n",
       " 'http://cataclysmicmutation.com',\n",
       " 'http://www.cse.iitb.ac.in/~aruniyer/',\n",
       " 'http://www.tgohome.com',\n",
       " 'http://thecellarroom.net',\n",
       " 'http://djhurio.wordpress.com/',\n",
       " 'http://neuroelf.net/',\n",
       " 'http://blue-feet.com',\n",
       " 'http://twitter.com/jholloway7',\n",
       " 'http://mathoverflow.net/users/3324/will-jagy',\n",
       " 'http://www.speakingjudging.com',\n",
       " 'http://c4il.jp',\n",
       " 'http://micans.org/stijn/',\n",
       " 'http://www.le.ac.uk/gg/staff/academic_brunsdon.html',\n",
       " 'http://www.miskatonic.org/',\n",
       " 'http://www.failuretorefrain.com',\n",
       " 'http://home.arcor.de/hirnstrom',\n",
       " 'http://www.onlineinvestingai.com/blog/',\n",
       " 'http://dropbit.com',\n",
       " 'http://pavel.kabir.ru',\n",
       " 'http://gray.clhn.co',\n",
       " 'http://www.terrymatula.com',\n",
       " 'http://about.me/brycethomas',\n",
       " 'http://www.linkedin.com/in/matthewlesko',\n",
       " 'http://facebook.com/chriswillis',\n",
       " 'http://android-argentina.blogspot.com',\n",
       " 'http://geneorama.com',\n",
       " 'http://onur-gungor.com',\n",
       " 'http://www.thenewsbeforethenews.com',\n",
       " 'http://www.forthgo.com/blog/',\n",
       " 'http://gregguida.com/',\n",
       " 'http://adamgreenhall.com',\n",
       " 'http://stackoverflow.com/users/edit/1003565',\n",
       " 'http://arek-paterek.com',\n",
       " 'http://bloggingabout.net/blogs/vagif/default.aspx',\n",
       " 'http://rachel53461.wordpress.com/',\n",
       " 'http://osiloke.com',\n",
       " 'http://www.reilurekrytointi.fi/video/sepp%C3%A4l%C3%A4-jaakko',\n",
       " 'http://nickrobison.com',\n",
       " 'http://sjuvekar.appspot.com/blog',\n",
       " 'http://gr.linkedin.com/in/eriksperling',\n",
       " 'http://danielswan.net',\n",
       " 'http://www.stats.ox.ac.uk/~evans',\n",
       " 'http://www.hrnutshell.com',\n",
       " 'http://leidegren.se/',\n",
       " 'http://www.ianolasov.com/Home.htm',\n",
       " 'https://developers.google.com/',\n",
       " 'http://www.twitter.com/palanski',\n",
       " 'http://www.whythawk.com/',\n",
       " 'http://herbsusmann.com',\n",
       " 'http://tingletech.tumblr.com/',\n",
       " 'http://www.reglue.org',\n",
       " 'http://www.ua.es/personal/jose.zubcoff/',\n",
       " 'http://mlstat.wordpress.com',\n",
       " 'http://stevenljohnson.org',\n",
       " 'http://www.sharpsteen.net',\n",
       " 'http://www.ieortools.com',\n",
       " 'http://espenhh.com',\n",
       " 'http://blog.thegrandlocus.com',\n",
       " 'http://stackoverflow.com/users/679318/rlcabral',\n",
       " 'http://arencambre.com',\n",
       " 'http://www.atmos.uw.edu/~akchen0/',\n",
       " 'http://www.vanheusden.com/',\n",
       " 'http://jameshoward.us',\n",
       " 'http://www.oslolso.tumblr.com',\n",
       " 'http://ff',\n",
       " 'http://www.yossale.com',\n",
       " 'http://blogs.erichoch.org/eric',\n",
       " 'https://twitter.com/__carlsmith__',\n",
       " 'http://www.thomaslevine.com',\n",
       " 'http://www.andrewmao.net',\n",
       " 'http://www.fabriziobianchi.it',\n",
       " 'http://Google',\n",
       " 'http://www.ms.unimelb.edu.au/~andrewpr',\n",
       " 'http://www.indolering.com',\n",
       " 'http://www.benjamindeschamps.ca',\n",
       " 'http://www.tweelingenregister.org',\n",
       " 'http://cmenguy.github.io',\n",
       " 'http://aminpractice.blogspot.com',\n",
       " 'http://errorstatistics.com',\n",
       " 'http://jamesscottbrown.com',\n",
       " 'http://www.eamann.com',\n",
       " 'http://redhardsupra.blogspot.com',\n",
       " 'http://www.google.com/profiles/timothy.s.lau',\n",
       " 'http://www.gdr-mascotnum.fr/doku.php?id=iooss1',\n",
       " 'http://notebookonthewebs.tumblr.com/',\n",
       " 'http://codevanced.net/',\n",
       " 'http://djpirtu.deviantart.com',\n",
       " 'http://google.it',\n",
       " 'http://Correlations',\n",
       " 'http://tushman.com',\n",
       " 'http://about.me/paul.mason',\n",
       " 'http://rguha.net',\n",
       " 'http://about.me/andy_dent',\n",
       " 'http://zmjones.com',\n",
       " 'http://www.cquotient.com',\n",
       " 'http://users.softlab.ntua.gr/~ttsiod/index.html',\n",
       " 'http://carlboettiger.info',\n",
       " 'http://csferrie.com',\n",
       " 'http://en.wikipedia.org/wiki/Gongsun_Long',\n",
       " 'http://fatchat121.blogspot.com',\n",
       " 'http://www.alstatr.blogspot.com',\n",
       " 'http://prod.campuscruiser.com/cruiser/widener/maalexander',\n",
       " 'http://www.designingforthefuture.com',\n",
       " 'http://crypticvaibhav.wordpress.com',\n",
       " 'http://sidmitra.com',\n",
       " 'http://preetiedul.wordpress.com/',\n",
       " 'http://www.hightechrealm.com',\n",
       " 'http://lamontconsulting.com',\n",
       " 'http://twitter.com/muzhig',\n",
       " 'http://johannes.jakeapp.com/blog/',\n",
       " 'http://www.pdmi.ras.ru/~ssp',\n",
       " 'http://paper.li/FindProz/1307414225',\n",
       " 'http://holyvier.blogspot.com/',\n",
       " 'http://strugglingthroughproblems.blogspot.com/',\n",
       " 'http://www.moviri.com',\n",
       " 'http://kartones.net/blogs/jadengine',\n",
       " 'http://tullo.ch',\n",
       " 'http://meta.stackexchange.com/',\n",
       " 'http://statisticaconr.blogspot.com',\n",
       " 'http://mako.cc',\n",
       " 'http://www.princeton.edu/~sims',\n",
       " 'http://pacomet.wordpress.com',\n",
       " 'http://bayesianthink.blogspot.com/2012/12/the-best-books-to-learn-probability.html',\n",
       " 'http://www.luigip.com/',\n",
       " 'http://sychopx.wordpress.com',\n",
       " 'http://qsar4u.com',\n",
       " 'http://volomike.com',\n",
       " 'http://stackoverflow.com/users/1049893/vutukuri',\n",
       " 'http://linbaba.wordpress.com/',\n",
       " 'http://www.stubbornmule.net',\n",
       " 'http://kitmonisit.com',\n",
       " 'http://stangirala.wordpress.com',\n",
       " 'http://www.deslondesoftware.com',\n",
       " 'http://www.statystyka.net',\n",
       " 'http://www.linkedin.com/in/guidogarcia',\n",
       " 'http://www.lucaswilkins.com',\n",
       " 'http://www.BillTheLizard.com',\n",
       " 'http://jon-coleman.net',\n",
       " 'http://blog.meta-spaces.com',\n",
       " 'http://nass.usda.gov',\n",
       " 'http://vkuzo.com',\n",
       " 'http://theatavism.blogspot.com',\n",
       " 'http://evincarofautumn.blogspot.com/',\n",
       " 'http://lospi.net',\n",
       " 'http://radekstepan.com',\n",
       " 'http://www.cse.ust.hk/~derekhh/',\n",
       " 'http://www.globalccs.net/~vanstat',\n",
       " 'https://heatma.ps',\n",
       " 'http://1.618034.com',\n",
       " 'http://braintrace.ru',\n",
       " 'http://www0.cs.ucl.ac.uk/people/E.Challis.html',\n",
       " 'http://www.talkstats.com',\n",
       " 'http://NUR',\n",
       " 'http://www.nestorsulikowski.com',\n",
       " 'http://www.topjaklont.org/',\n",
       " 'http://www.integrativestatistics.com',\n",
       " 'http://www.jcachat.com',\n",
       " 'http://sysadmin.jprice.org',\n",
       " 'http://www.cs.uml.edu/~kuttecht/',\n",
       " 'http://www.univ-rouen.fr/LMRS/Persopage/Giraudo/',\n",
       " 'http://http//twitter.com/nitin',\n",
       " 'http://www.zloto-jubiler.pl',\n",
       " 'http://www.staygeo.com',\n",
       " 'http://www.usemarkup.com',\n",
       " 'http://axiomofcats.com',\n",
       " 'http://www.furia.com',\n",
       " 'http://nym.se/',\n",
       " 'http://vega.rd.no',\n",
       " 'http://www.linkedin.com/in/yurkor',\n",
       " 'http://clee.kr/',\n",
       " 'http://www.cs.mcgill.ca/~pmangg',\n",
       " 'http://twitter.com/marmistead',\n",
       " 'http://harrak.net',\n",
       " 'http://mathiasbynens.be/',\n",
       " 'http://www.soundcheks.com',\n",
       " 'http://www.squobble.com',\n",
       " 'http://twitter.com/#!/imbenzene',\n",
       " 'http://www.yang-cs.com',\n",
       " 'http://nsaunders.wordpress.com',\n",
       " 'https://www.researchgate.net/profile/Richard_Warnung/imgsrc=https://www.researchgate.net/images/public/profile_share_badge.pngalt=RichardWarnung//a',\n",
       " 'http://twitter.com/brocktibert',\n",
       " 'http://www.gallamine.com',\n",
       " 'http://slvsef.org',\n",
       " 'https://sites.google.com/site/jorpppp/',\n",
       " 'http://www.iovanalex.ro',\n",
       " 'http://statsoft.com',\n",
       " 'http://theInPUT.org',\n",
       " 'http://jiangzuoyan.blogspot.com',\n",
       " 'http://www.rtwilson.com',\n",
       " 'http://www.vermorel.com',\n",
       " 'http://www.columbia.edu/~cds81',\n",
       " 'https://twitter.com/sameersoi',\n",
       " 'http://snowl.net',\n",
       " 'https://christopherdardis.wordpress.com/',\n",
       " 'http://blogs.wiki-dot.net/alaudo',\n",
       " 'https://twitter.com/MLWiDA',\n",
       " 'http://www.thomas-steinbrenner.net',\n",
       " 'http://www.drsteen.com/andrewdsteen',\n",
       " 'http://www.jasondavies.com/',\n",
       " 'http://shadow6976@gmail.com',\n",
       " 'http://www.cs.ru.nl/~gori/',\n",
       " 'http://www.socsci.ru.nl/~advdv/',\n",
       " 'http://matchbookit.com',\n",
       " 'http://mindcode.org',\n",
       " 'http://aramis.hostman.us',\n",
       " 'http://www.linkedin.com/profile/view?id=155522394&trk=nav_responsive_tab_profile',\n",
       " 'http://f.briatte.org/',\n",
       " 'http://blog.hansdude.com',\n",
       " 'http://www2.jabsom.hawaii.edu/igert/?page_id=453',\n",
       " 'http://www.mathworks.com/matlabcentral/fileexchange/authors/228021',\n",
       " 'http://n/a',\n",
       " 'http://xhochy.com',\n",
       " 'https://plus.google.com/109439195292129836824',\n",
       " 'http://frontalattackonanenglishwriter.com',\n",
       " 'http://gua.com',\n",
       " 'http://www.webservius.com',\n",
       " 'http://martin.ankerl.com',\n",
       " 'http://mikestumpf.com',\n",
       " 'http://www.samuels-art.com',\n",
       " 'http://statisticalskier.com',\n",
       " 'http://blog.donwilson.net',\n",
       " 'http://www.markfisherevolution.com',\n",
       " 'http://www.boardgamegeek.com/user/CrankyPants',\n",
       " 'http://brassfly.com',\n",
       " 'http://tuwien.ac.at',\n",
       " 'http://www.harlan.harris.name',\n",
       " 'http://jfcoder.com',\n",
       " 'http://stackoverflow.com/users/163080',\n",
       " 'http://www.statisticalconsulting.org',\n",
       " 'http://www.fisica.unam.mx/ifunam_english/clusters_english/',\n",
       " 'http://Www.facebook.com/entropyas',\n",
       " 'http://sites.google.com/site/spatialpython/',\n",
       " 'http://www.s2temsc.org/staffprofiles/gregorymacdougall',\n",
       " 'http://about.me/mkozhevnikov',\n",
       " 'http://www.thenerdiestshirts.com',\n",
       " 'http://www.ics.uci.edu/~yganjisa',\n",
       " 'http://www.johndcook.com',\n",
       " 'http://annova',\n",
       " 'http://www.kcl.ac.uk/iop/depts/neuroimaging/people/Ginestet,Cedric.aspx',\n",
       " 'http://hirejamesbradshaw.com',\n",
       " 'http://skiminog.livejournal.com',\n",
       " 'http://www.danieljhocking.worpress.com',\n",
       " 'http://ch.linkedin.com/in/jnlopes',\n",
       " 'http://wrongnotes.blogspot.com',\n",
       " 'http://www.broadinstitute.org/~mghandi',\n",
       " 'http://infonomics.ltd.uk',\n",
       " 'http://www.cse.iitb.ac.in/~salilj',\n",
       " 'http://www.chenyuzhao.net',\n",
       " 'http://cgibbons.us',\n",
       " 'http://bioinformatics.ucdavis.edu',\n",
       " 'http://www.souhaib-bentaieb.com',\n",
       " 'http://mohitranka.wordpress.com',\n",
       " 'http://www.scheme.dk/blog/',\n",
       " 'http://www.ccl.gatech.edu/',\n",
       " 'http://matthias.vallentin.net',\n",
       " 'http://www.gotgenes.com/',\n",
       " 'http://hsigrist.github.io/blog',\n",
       " 'http://www.mat.univie.ac.at/~neum/',\n",
       " 'http://www.ucl.ac.uk/energy',\n",
       " 'http://www.kirkouimet.com/',\n",
       " 'http://salman-w.blogspot.com',\n",
       " 'http://www.amaracademy.com',\n",
       " 'http://www.emakalic.org/blog',\n",
       " 'http://kniganapolke.wordpress.com',\n",
       " 'http://www.jakob-r.de',\n",
       " 'http://www.r-statistics.com',\n",
       " 'http://i-coded.blogspot.com',\n",
       " 'http://www.naught101.org',\n",
       " 'http://www.newyorkgeek.com',\n",
       " 'http://mormon.org/me/38ZB',\n",
       " 'http://www.enac.fr/recherche/leea/Steve%20Lawford/steve_site1.html',\n",
       " 'http://6v8.gamboni.org',\n",
       " 'http://faculty.washington.edu/mrich',\n",
       " 'http://alex.smola.org',\n",
       " 'http://isomorphismes.tumblr.com',\n",
       " 'http://www.centerspace.net',\n",
       " 'http://bentilly.blogspot.com/',\n",
       " 'http://Gallagher',\n",
       " 'http://akshayl.posterous.com',\n",
       " 'http://stackoverflow.com/users/167973/carl',\n",
       " 'http://www.twitter.com/jdalton',\n",
       " 'http://matthewrocklin.com',\n",
       " 'http://www.johnmyleswhite.com',\n",
       " 'http://ediab.com/',\n",
       " 'http://jedifran.com',\n",
       " 'http://bangor.academia.edu/BronsonHarry',\n",
       " 'http://vouldjeff.freewildart.com',\n",
       " 'http://interazioneinnaturale.tumblr.com/',\n",
       " 'http://xianblog.wordpress.com',\n",
       " 'http://www.jameskoppel.com',\n",
       " 'http://www.linkedin.com/in/kenahoo',\n",
       " 'http://www.equate-design.com',\n",
       " 'http://user.uni-frankfurt.de/~rauchw',\n",
       " 'http://farmacokratia.blogspot.com',\n",
       " 'http://Yahoo',\n",
       " 'http://www.nus.edu.sg',\n",
       " 'http://lorinhochstein.org',\n",
       " 'http://www.accioly-biz.com',\n",
       " 'http://whathecode.wordpress.com/',\n",
       " 'http://www.lordabbett.com/',\n",
       " 'http://telliott99.blogspot.com',\n",
       " 'http://www.michaelbarton.me.uk',\n",
       " 'http://www.jirilukavsky.info',\n",
       " 'http://www.lathamcity.com',\n",
       " 'http://Gra.ve',\n",
       " 'http://cerebralmastication.com',\n",
       " 'http://siyandawrites.wordpress.com',\n",
       " 'http://donthave.com',\n",
       " 'https://github.com/mkemnetz',\n",
       " 'http://www.cqlcorp.com',\n",
       " 'http://www.flickr.com/photos/cryptic_star/',\n",
       " 'http://miningtext.blogspot.com',\n",
       " 'http://about.me/LiaoGongYi',\n",
       " 'http://menugget.blogspot.com/',\n",
       " 'http://zeus.calacademy.org/roopnarine/peter.html',\n",
       " 'http://www.erichfw.co.za',\n",
       " 'http://www.pariser.me/',\n",
       " 'http://www.whoisdanbarrett.com',\n",
       " 'http://eighty-b.com',\n",
       " 'http://daob.org',\n",
       " 'http://neal.mcburnett.org',\n",
       " 'http://www.JoalahDesigns.com',\n",
       " 'http://v1v3kn.tumblr.com',\n",
       " 'http://www.vpgcentral,com',\n",
       " 'http://www.stocktickr.com',\n",
       " 'http://mhermans.net',\n",
       " 'http://vk.com/id32453/javascript5678',\n",
       " 'http://www.beavercreekfire.org',\n",
       " 'http://www.multiplyleadership.com',\n",
       " 'http://www.romeovansnick.be',\n",
       " 'http://jeromyanglim.blogspot.com/',\n",
       " 'http://www.nbr-graphs.com',\n",
       " 'http://www.limepepper.co.uk',\n",
       " 'http://www.masseria.org',\n",
       " 'http://www.pushpendre.in',\n",
       " 'http://hypergeometric.wordpress.com',\n",
       " 'http://www.chrisgagne.com',\n",
       " 'http://refactor.it',\n",
       " 'http://dropbearcode.blogspot.com/',\n",
       " 'http://www.meta-numerics.net',\n",
       " 'http://www.daleidoscope.com',\n",
       " 'http://scienceblogs.com/developingintelligence',\n",
       " 'http://www.mattbilskie.com',\n",
       " 'http://had.co.nz',\n",
       " 'http://kev.inburke.com',\n",
       " 'http://quadraforte.com',\n",
       " 'http://blog.joshuaenfield.com',\n",
       " 'http://anthonykong.tumblr.com/',\n",
       " 'http://www.cse.wustl.edu/~gazihan/',\n",
       " 'http://yokozar.org/',\n",
       " 'http://tungwaiyip.info/',\n",
       " 'http://nonerightnow',\n",
       " 'http://yes',\n",
       " 'http://www.hutchinsonlee.com',\n",
       " 'http://jvangael.github.io',\n",
       " 'http://ram.rachum.com',\n",
       " 'http://utsa.academia.edu/CoreySparks',\n",
       " 'http://objektorient.blogspot.com/',\n",
       " 'http://kldavenport.com',\n",
       " 'http://www.movage.nl',\n",
       " 'http://egonw.github.com',\n",
       " 'http://www.knowing.net',\n",
       " 'http://www.johnsjolander.com',\n",
       " 'http://gordonwatts.wordpress.com',\n",
       " 'http://www.linchiat.com',\n",
       " 'http://mike.axiak.net',\n",
       " 'http://stevenvh.net/steven.php',\n",
       " 'http://sethpaxton.tumblr.com',\n",
       " 'http://www.adamscorner.com',\n",
       " 'http://tora.us.fm/erelsgl',\n",
       " 'http://eduardoleoni.com',\n",
       " 'http://stanford.edu/~mbloem/',\n",
       " 'https://mbq.me',\n",
       " 'http://www.implicit.ugent.be/index.php?position=1x3x3#.UFic5WFuItk',\n",
       " 'http://ce.sharif.edu/~mahdieh',\n",
       " 'http://mypetprojects.blogspot.com/',\n",
       " 'http://www.DavidDLewis.com',\n",
       " 'http://creatapreneur.com',\n",
       " 'http://ziroby.com',\n",
       " 'http://samuelhulick.com',\n",
       " 'http://www.rwlindell.com',\n",
       " 'http://xn.pinkhamster.net/',\n",
       " 'http://mbd.uni-potsdam.de/HTML/people/CVhohenstein.html',\n",
       " 'http://twitter.com/jajathilo',\n",
       " 'http://cloudartisan.com',\n",
       " 'http://www.applieddatalabs.com',\n",
       " 'http://www.iiasa.ac.at/Research/POP/Staff/wazir.html',\n",
       " 'https://plus.google.com/109915142458111672265/about',\n",
       " 'http://www.flavors.me/flxlex',\n",
       " 'http://thomblake.com',\n",
       " 'http://nedbatchelder.com',\n",
       " 'http://www.linkedin.com/pub/oleksandr-pavlyk/4/403/39a',\n",
       " 'http://www.cc.gatech.edu/~sbakhshi/',\n",
       " 'http://www.linkedin.com/in/marckvaisman',\n",
       " 'http://toddsnotes.blogspot.com/',\n",
       " 'http://socialgraphpaper.blogspot.com',\n",
       " 'http://idris.heroku.com',\n",
       " 'http://sbotond.github.io',\n",
       " 'http://acm.timus.ru/author.aspx?id=57538&locale=en',\n",
       " 'http://matteodefelice.name/research',\n",
       " 'http://www.cs.illinois.edu/homes/rsamdan2/',\n",
       " 'http://rivita.ru/spssmacros_en.shtml',\n",
       " 'http://www.autobox.com',\n",
       " 'http://craicpropagation.blogspot.com/',\n",
       " 'http://sites.google.com/site/mwtoews/',\n",
       " 'http://manasto.info',\n",
       " 'http://reader.differentialist.info',\n",
       " 'http://rumpelkammer87@gmx.de',\n",
       " 'http://bazzilic.me/',\n",
       " 'http://vrabie.net',\n",
       " 'http://www.xmwconsulting.co.uk',\n",
       " 'http://www.withoutthesarcasm.com/',\n",
       " 'http://felixcloutier.com/',\n",
       " 'http://dirk.eddelbuettel.com',\n",
       " 'http://www.wvbauer.com/',\n",
       " 'http://dcm.ffclrp.usp.br/~augusto',\n",
       " 'http://www.codeulike.com',\n",
       " 'http://gatton.uky.edu/gradstudents/cress',\n",
       " 'http://wvega.com',\n",
       " 'http://memming.wordpress.com',\n",
       " 'http://www.mymindleaks.com',\n",
       " 'http://www.bytemining.com',\n",
       " 'http://www.DrAnkush.com',\n",
       " 'http://www.bioss.ac.uk/students/alexm.html',\n",
       " 'http://dadadom.de',\n",
       " 'http://www.traviswolfe.com',\n",
       " 'http://danachandler.com',\n",
       " 'http://migdal.wikidot.com/en',\n",
       " 'http://www.tuhh.de',\n",
       " 'http://www.somethinkodd.com/oddthinking',\n",
       " 'http://in.linkedin.com/pub/shashank-gupta/28/2bb/518',\n",
       " 'http://twitter.com/ClaudioAlbertin',\n",
       " 'http://kenmankoff',\n",
       " 'http://twitter.com/evoroth',\n",
       " 'http://swolter.sdf1.org',\n",
       " 'http://recologia.com.br',\n",
       " 'http://www.sumsar.net',\n",
       " 'http://dgrtwo.github.io',\n",
       " 'http://twitter.com/saneef',\n",
       " 'http://erikvold.com/index.cfm',\n",
       " 'http://joefu.net/blog',\n",
       " 'http://www.otago.ac.nz/wellington/departments/biostatisticalservices/staff/otago019097.html',\n",
       " 'http://secretdruidsociety.org',\n",
       " 'http://www.biologie.uni-hamburg.de/bzf/fbda005/fbda005_e.htm',\n",
       " 'http://cellagames.com',\n",
       " 'http://akshayshah.org',\n",
       " 'http://about.me/stephen.rush',\n",
       " 'http://john.jersdesk.com',\n",
       " 'http://ntnu.no',\n",
       " 'http://lpenz.org',\n",
       " 'http://www.martianwabbit.com',\n",
       " 'http://blogs.sas.com/content/iml',\n",
       " 'http://www.sbitzer.eu',\n",
       " 'http://bronzebeard.wordpress.com',\n",
       " 'http://nickerson.org.uk/blog',\n",
       " 'http://thinkingaboutthinking.org',\n",
       " 'http://NA',\n",
       " 'http://stackoverflow.com',\n",
       " 'http://navarroj.com/latex',\n",
       " 'http://www.drowtales.com',\n",
       " 'http://www.stevenmaude.co.uk',\n",
       " 'http://drpaulbrewer.com',\n",
       " 'http://www.med.govt.nz/sectors-industries/tourism/tourism-research-data',\n",
       " 'http://www.annezelenka.com',\n",
       " 'http://Toobusy',\n",
       " 'http://artax.karlin.mff.cuni.cz/~tucev2am/',\n",
       " 'http://bradleygermain.com',\n",
       " 'http://tech-and-philosophy.blogspot.com/',\n",
       " 'http://www.tau.ac.il/~saharon',\n",
       " 'http://cyrussamii.com',\n",
       " 'http://127.0.0.1/index.html',\n",
       " 'http://gileadslostson.blogspot.com',\n",
       " 'http://davharris.github.io',\n",
       " 'http://website.com',\n",
       " 'http://nkls.schmckr.de',\n",
       " 'http://www.markus-lanthaler.com',\n",
       " 'http://-',\n",
       " 'https://formr.org',\n",
       " 'http://sharedcount.com',\n",
       " 'http://www.thomasrehman.net/',\n",
       " 'http://www.aaronmcdaid.com',\n",
       " 'http://www.overkillanalytics.net',\n",
       " 'http://goo.gl/0TJHX',\n",
       " 'http://wbarczynski.pl/wp',\n",
       " 'http://www.tunnuz.net',\n",
       " 'http://alea.fr.eu.org',\n",
       " 'http://artscitech.blogspot.com',\n",
       " 'http://www.firstheartland.com',\n",
       " 'http://ecs.victoria.ac.nz/Main/GradKeithCassell',\n",
       " 'http://ultracold.uchicago.edu/people',\n",
       " 'http://www.stat.cmu.edu/~nmv',\n",
       " 'http://chrisbeeley.net',\n",
       " 'http://www.junuxx.net',\n",
       " 'http://www.faciletek.com',\n",
       " 'http://adini.me',\n",
       " 'http://www.thiloschneider.net',\n",
       " 'http://marianosemelman.com.ar',\n",
       " 'http://twitter.com/mike_jenkins',\n",
       " 'http://Itsprivate...',\n",
       " 'http://kermit.epska.org',\n",
       " 'http://bilge1',\n",
       " 'http://thetarzan.wordpress.com',\n",
       " 'http://aquaya.org',\n",
       " 'http://blog.crossedstreams.com',\n",
       " 'http://www.looper.hu',\n",
       " 'http://harshhpareek.com',\n",
       " 'http://www.atino.name',\n",
       " 'http://dlaptev.org',\n",
       " 'http://blog.hartleybrody.com',\n",
       " 'http://www.stefan-koch.name/',\n",
       " 'http://twitter.com/mehper',\n",
       " 'http://www.gillesmaes.be',\n",
       " 'http://blog.rootle.it',\n",
       " 'http://varyc.com',\n",
       " 'http://www.culturalcognition.net/kahan/',\n",
       " 'http://www.cs.mu',\n",
       " 'http://www.confounding.net',\n",
       " 'http://msutherl.net/',\n",
       " 'http://www.ccc.uga.edu',\n",
       " 'http://hlangeveld.nl',\n",
       " 'http://www.seizurerobots.com/',\n",
       " 'http://www.di.uoa.gr/~std07150',\n",
       " 'http://deteriormusic.blogspot.com/',\n",
       " 'http://lookingatdata.blogspot.com/',\n",
       " 'http://alexott.net/en/',\n",
       " 'http://elehack.net/michael',\n",
       " 'http://www.ms.uky.edu/~jack/',\n",
       " 'http://www.graphpad.com',\n",
       " 'http://doingbayesiandataanalysis.blogspot.com/',\n",
       " 'http://blog.hello-math.dreamhosters.com/',\n",
       " 'http://kracken.cs.ucla.edu',\n",
       " 'http://www.economics.mcmaster.ca/faculty/racinej',\n",
       " 'https://plus.google.com/108351331401552240682/posts',\n",
       " 'http://www.nicolaromano.net',\n",
       " 'http://www.wegnerdesign.com',\n",
       " 'http://wp7agile.wordpress.com/',\n",
       " 'http://www.remin.pl',\n",
       " 'http://www.kevinjspring.com',\n",
       " 'https://twitter.com/ConstantPieters',\n",
       " 'http://onlyvix.blogspot.com',\n",
       " 'http://www.sparrowtail.com',\n",
       " 'http://aaronlevin.ca',\n",
       " 'http://berkeleybiolabs.com',\n",
       " 'http://wannik.wordpress.com',\n",
       " 'http://www.crmportals.com',\n",
       " 'http://plindenbaum.blogspot.com',\n",
       " 'https://whymarrh.com',\n",
       " 'http://www.u-psud.fr',\n",
       " 'http://www.sleeper.cz',\n",
       " 'http://reidster.net',\n",
       " 'http://phpdevpad.blogspot.com/',\n",
       " 'http://mikekr.blogspot.com',\n",
       " 'http://www.twodesk.com',\n",
       " 'http://cometapps.com',\n",
       " 'http://danganothererror.wordpress.com',\n",
       " 'https://github.com/alixaxel/',\n",
       " 'http://scausa.com',\n",
       " 'http://www.johnnylogic.org/',\n",
       " 'http://www.openwetware.org/wiki/User:David_S_LeBauer',\n",
       " 'http://www.inbo.be',\n",
       " 'http://www.phredtek.com',\n",
       " 'http://hughperkins.com/techblog',\n",
       " 'http://afrolegs.com',\n",
       " 'https://github.com/riyadparvez',\n",
       " 'http://www.stat.purdue.edu/~jdatta',\n",
       " 'http://peekabo-vision.blogspot.com',\n",
       " 'http://stas.kolenikov.name',\n",
       " 'http://www.kpa-group.com',\n",
       " 'http://www.likestripes.com',\n",
       " 'http://inside.mines.edu/~ddanford',\n",
       " 'http://www.mrcorey.com',\n",
       " 'http://www.willdampier.info',\n",
       " 'http://compbiome.com',\n",
       " 'http://www.bertelsen.ca',\n",
       " 'http://jeroen.vangoey.be',\n",
       " 'http://www.staff.ncl.ac.uk/d.j.wilkinson/',\n",
       " 'http://www.martinlarsson.net',\n",
       " 'http://rushdishams.googlepages.com',\n",
       " 'http://www.john-joseph-horton.com/',\n",
       " 'https://sites.google.com/site/mathsworkmusic/',\n",
       " 'http://www.timday.com',\n",
       " 'http://www.aliquote.org',\n",
       " 'http://cloud101.eu',\n",
       " 'http://www.stat.umn.edu/~arendahl/',\n",
       " 'http://slashhome.be',\n",
       " 'http://kurthaeusler.wordpress.com',\n",
       " 'http://www.cdc.gov/tb/topic/research/tbtc/introduction.htm',\n",
       " 'https://www.linkedin.com/profile/view?id=7644414',\n",
       " 'http://vene.ro',\n",
       " 'http://www.navsea.navy.mil/nswc/Corona',\n",
       " 'http://www.csi.ucd.ie/users/thomas-holz',\n",
       " 'http://www.linkedin.com/in/gauravj49',\n",
       " 'http://www.andrewheiss.com',\n",
       " 'http://LeanEntrepreneur.co',\n",
       " 'http://abstractlabor.blogspot.com',\n",
       " 'http://mdswanson.com',\n",
       " 'http://facebook.com/nickcowans',\n",
       " 'http://amyglen.wordpress.com',\n",
       " 'http://jakebowers.org',\n",
       " 'http://sds.podval.org',\n",
       " 'http://www.icmc.usp.br/~alceufc/',\n",
       " 'http://www.rosshartshorn.net',\n",
       " 'http://sociology.uga.edu/people/faculty/jeremy-reynolds',\n",
       " 'http://www.ic.unicamp.br/~tachard/',\n",
       " 'http://codeonipad.com/',\n",
       " 'http://probabilicity.wordpress.com',\n",
       " 'http://web.engr.illinois.edu/~mvakili2/',\n",
       " 'http://www.cs.helsinki.fi/fabian.fagerholm/',\n",
       " 'http://www.twitter.com/gappy3000',\n",
       " 'http://sites.google.com/site/johannesbeller/',\n",
       " 'http://nlp.stanford.edu/~manning/',\n",
       " 'http://fjordist.wordpress.com',\n",
       " 'http://users.ics.tkk.fi/peter/',\n",
       " 'http://geogrouper.appspot.com/',\n",
       " 'http://www.daviddoria.com',\n",
       " 'http://participate-in-research.org.uk',\n",
       " 'http://biology.unm.edu/mmfuller/',\n",
       " 'http://yannick.poulet.org',\n",
       " 'http://pp.hillrippers.ch/',\n",
       " 'http://careers.stackoverflow.com/shannonquinn',\n",
       " 'http://maanskyn.info',\n",
       " 'http://personal.cis.strath.ac.uk/robert.moss',\n",
       " 'http://logfc.wordpress.com',\n",
       " 'http://mark.reid.name',\n",
       " 'http://www.ignician.net',\n",
       " 'http://peacelovebeer.wordpress.com/',\n",
       " 'http://about.me/mttsndrs',\n",
       " 'http://www.baltimark.com',\n",
       " 'https://www.cs.tcd.ie/~mcaulejj',\n",
       " 'http://prognoz.ck.ua',\n",
       " 'http://about.me/michael.chary',\n",
       " 'http://luftgesheft.wordpress.com/',\n",
       " 'http://N/A',\n",
       " 'http://schissel.org',\n",
       " 'http://www.rileydutton.com/',\n",
       " 'http://kalx.net',\n",
       " 'http://lemire.me/en/',\n",
       " 'http://www.29ways.co.uk',\n",
       " 'http://www.rkuykendall.com/',\n",
       " 'http://lepo.it.da.ut.ee/~timo_p',\n",
       " 'http://www.bipede.de',\n",
       " 'http://www.nimbox.com',\n",
       " 'http://li-tianyang.com',\n",
       " 'http://www.deus.co.uk',\n",
       " 'http://www.geektrading.com',\n",
       " 'http://soma.denkt.org',\n",
       " 'http://circleci.com',\n",
       " 'http://milktrader.net',\n",
       " 'http://www.slimjim.name',\n",
       " 'http://www.quantum.at',\n",
       " 'http://www.ngcrawford.com',\n",
       " 'http://izbicki.me',\n",
       " 'http://quantcorner.wordpress.com',\n",
       " 'http://prototoast.blogspot.com',\n",
       " 'http://factbased.blogspot.com',\n",
       " 'http://github.com/beyeran',\n",
       " 'http://www.jkndrkn.com',\n",
       " 'http://singyourownlullaby.blogspot.com',\n",
       " 'http://cwick.co.nz',\n",
       " 'http://about.me/samilosoi',\n",
       " 'http://www.statisticaladvisor.com',\n",
       " 'http://oharab.github.com',\n",
       " 'http://jayraj.net',\n",
       " 'http://None',\n",
       " 'http://non-existent',\n",
       " 'http://jamisondance.com/',\n",
       " 'http://heliosphan.org/',\n",
       " 'http://celenius.com',\n",
       " 'http://en.wikipedia.org/wiki/Wiener_sausage',\n",
       " 'http://csfowler.com',\n",
       " 'http://jmanga.tk',\n",
       " 'http://ben.com',\n",
       " 'http://www.circulumvite.com',\n",
       " 'http://www.statalgo.com',\n",
       " 'http://www.organizingcreativity.com/',\n",
       " 'http://www.joyofdata.de',\n",
       " 'http://williammurrah.com',\n",
       " 'http://therandomtexan.wordpress.com/',\n",
       " 'http://jspha.com',\n",
       " 'http://www.linkedin.com/in/bjansen',\n",
       " 'http://www.marc-andre.ca',\n",
       " 'http://joepairman.com',\n",
       " 'http://twitter.com/suncoolsu',\n",
       " 'http://www.flickr.com/photos/pulkitsinha/',\n",
       " 'http://20-100.ch/blog',\n",
       " 'http://www.flickr.com/photos/nimrodm/',\n",
       " 'http://www.johndierks.com',\n",
       " 'http://tpbitcalc.appspot.com/',\n",
       " 'http://shadyacres.tumblr.com/',\n",
       " 'http://blog.locut.us/',\n",
       " 'http://webscraping.com',\n",
       " 'http://www.hongliangjie.com',\n",
       " 'http://cdn2.knowyourmeme.com/i/5378/original/zero.jpg?1247534859',\n",
       " 'http://people.ysu.edu/~gkerns/',\n",
       " 'http://www.tylerstreeter.net',\n",
       " 'https://twitter.com/#!/geracleous',\n",
       " 'http://genderfrenchnouns.yolasite.com/',\n",
       " 'http://www.chrisamiller.com',\n",
       " 'http://cpd.wustl.edu',\n",
       " 'http://amaryazd.blogfa.com',\n",
       " 'http://www.mines-paristech.fr/Services/Annuaire/&?id=8828',\n",
       " 'http://maze5.net/',\n",
       " 'http://it.linkedin.com/in/luigisaggese',\n",
       " 'http://www.mauvedeity.com/',\n",
       " 'http://works.bepress.com/lucemia/',\n",
       " 'http://vincentdavis.net',\n",
       " 'http://martin.von-gagern.net/',\n",
       " 'http://www.cs.ucr.edu/~eamonn/',\n",
       " 'http://skila.pl',\n",
       " 'http://vzemlys.wordpress.com',\n",
       " 'http://drfloob.com',\n",
       " 'http://coding.pressbin.com',\n",
       " 'http://wikipedia.org',\n",
       " 'http://www.airts.co.uk',\n",
       " 'http://www.wyzant.com/Tutors/NY/Hollis/7894492/',\n",
       " 'http://binatron.wordpress.com/',\n",
       " 'http://etienne.webhop.org/',\n",
       " 'http://vince.vu',\n",
       " 'http://github.com/aomidpanah',\n",
       " 'http://www.carsonfarmer.com',\n",
       " 'http://litvak.eu/pyfi',\n",
       " 'http://josephh.mailinator.com',\n",
       " 'http://www.parsprogrammers.ir',\n",
       " 'http://adamlynch.ie',\n",
       " 'http://salebarn.com',\n",
       " 'http://www.tellesfera.com/',\n",
       " 'http://www.depthfirstsearch.net',\n",
       " 'http://kyle.mathews2000.com',\n",
       " 'http://alexholcombe.wordpress.com/',\n",
       " 'http://stevetjoa.com',\n",
       " 'http://antipaucity.com',\n",
       " 'http://www.pa-mar.net',\n",
       " 'http://blog.crmguru.co.uk',\n",
       " 'http://op.to/sO+',\n",
       " 'http://zv.github.com',\n",
       " 'http://statland.org',\n",
       " 'http://linuxcs.com',\n",
       " 'http://twitter.com/DevdattaTengshe',\n",
       " 'http://jermdemo.blogspot.com',\n",
       " 'http://twitter.com/tonybreyal',\n",
       " 'http://alexsvd.blogspot.com',\n",
       " 'http://arasbm.com',\n",
       " 'http://www.optimisationbeacon.com/',\n",
       " 'https://sites.google.com/site/rgayler/',\n",
       " 'http://www.aaronsw.com/',\n",
       " 'http://www.davidechicco.it',\n",
       " 'http://about.me/prakhar9',\n",
       " 'http://www.inquisitor.ru/',\n",
       " 'http://cran.r-project.org/web/packages/excel.link/index.html',\n",
       " 'http://www.morethannothing.co.uk',\n",
       " 'http://www.cs.berkeley.edu/~dcoetzee/',\n",
       " 'http://matan.name',\n",
       " 'http://www.dbruhn.de',\n",
       " 'http://jaheruddin.nl',\n",
       " 'http://www.eraservis.com',\n",
       " 'http://mpastell.com',\n",
       " 'http://www.growse.com',\n",
       " 'http://www.cs.princeton.edu/~asuleime/',\n",
       " 'https://konrad.foerstner.org',\n",
       " 'http://www.sentient.nl/',\n",
       " 'https://sites.google.com/site/miro165sabo/',\n",
       " 'http://www.nmt.edu/~borchers',\n",
       " 'http://www.oddskool.net',\n",
       " 'http://ayushb.blogspot.com',\n",
       " 'http://thesteve0.wordpress.com',\n",
       " 'http://localhost/',\n",
       " 'http://numericalrecipes.wordpress.com',\n",
       " 'http://twitter.com/kshameer',\n",
       " 'http://mic.wustl.edu',\n",
       " 'http://stud4.tuwien.ac.at/~e0302966/joomla',\n",
       " 'http://dotnetpanda.blogspot.com',\n",
       " 'http://www.rsginc.com',\n",
       " 'http://remembr.in',\n",
       " 'http://davidquigley.com',\n",
       " 'http://kronoskoders.logdown.com/',\n",
       " 'http://kylewurtz.com',\n",
       " 'http://stackoverflow.com/users/172261/rcs',\n",
       " 'http://twitter.com/pchalasani',\n",
       " 'http://www.drit.dk',\n",
       " 'http://xpda.com',\n",
       " 'https://m-elshamy.appspot.com/',\n",
       " 'http://richcooks.blogspot.com',\n",
       " 'http://blog.this.com.ar',\n",
       " 'http://fishpondfever.wordpress.com/',\n",
       " 'http://rpostcard.blogspot.com/2014/03/my-missing-countries.html',\n",
       " 'http://www.berkustun.com',\n",
       " 'http://www.cdeszaq.com',\n",
       " 'http://digitheadslabnotebook.blogspot.com/',\n",
       " 'http://www.endycahyono.com/',\n",
       " 'http://lrandroid.com',\n",
       " 'http://github.com/miku',\n",
       " 'http://gossetsstudent.wordpress.com/',\n",
       " 'https://twitter.com/mvholmes',\n",
       " 'http://www.inthehaystack.com',\n",
       " 'http://www.watermarquee.com',\n",
       " 'http://www.drewconway.com/',\n",
       " 'http://www.empiricist.ca',\n",
       " 'http://biomath.ugent.be/',\n",
       " 'http://binq.com',\n",
       " 'http://mikegioia@gmail.com',\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " '94025',\n",
       " 'AHMEDABD,INDIA',\n",
       " 'Aachen, Germany',\n",
       " 'Aalto University, Finland',\n",
       " 'Abersychan, United Kingdom',\n",
       " 'Acton Vale, Canada',\n",
       " 'Adelaide, Australia',\n",
       " 'Aiken, SC',\n",
       " 'Alabama',\n",
       " 'Alamo, CA',\n",
       " 'Alaska',\n",
       " 'Albany, NY',\n",
       " 'Alberta, Canada',\n",
       " 'Albuquerque, NM',\n",
       " 'Alexandria, Egypt',\n",
       " 'Altadena, CA',\n",
       " 'America',\n",
       " 'Ames, IA',\n",
       " 'Amherst, MA',\n",
       " 'Amritsar, India',\n",
       " 'Amsterdam',\n",
       " 'Amsterdam, Netherlands',\n",
       " 'Amsterdam, The Netherlands',\n",
       " 'Ankara, Turkey',\n",
       " 'Ann Arbor, MI',\n",
       " 'Antwerp, Belgium',\n",
       " 'Antwerp/Leuven, Belgium',\n",
       " 'Arcadia, CA',\n",
       " 'Arenys de Mar, Spain',\n",
       " 'Argentina',\n",
       " 'Arizona',\n",
       " 'Arkansas',\n",
       " 'Arlington, MA',\n",
       " 'Arlington, VA',\n",
       " 'Ashburn, VA',\n",
       " 'Asheville, NC',\n",
       " 'Asia',\n",
       " 'Athens, GA',\n",
       " 'Athens, Greece',\n",
       " 'Atlanta',\n",
       " 'Atlanta, GA',\n",
       " 'Auckland, New Zealand',\n",
       " 'Austin, TX',\n",
       " 'Australia',\n",
       " 'Austria',\n",
       " 'Baden-Wurttemberg, Germany',\n",
       " 'Bahia, Brazil',\n",
       " 'Baltimore, MD',\n",
       " 'Bangalore',\n",
       " 'Bangalore, India',\n",
       " 'Bangladesh',\n",
       " 'Bangor University',\n",
       " 'Barcelona, Spain',\n",
       " 'Basel, Switzerland',\n",
       " 'Basingstoke, United Kingdom',\n",
       " 'Bath',\n",
       " 'Bath, United Kingdom',\n",
       " 'Bay Area, CA',\n",
       " 'Bayreuth, Germany',\n",
       " 'Beavercreek, Ohio',\n",
       " 'Beijing',\n",
       " 'Beijing, China',\n",
       " 'Belgium',\n",
       " 'Bellevue, WA',\n",
       " 'Bellingham, WA',\n",
       " 'Belo Horizonte, Brazil',\n",
       " 'Bergen, Norway',\n",
       " 'Bergen-Norway',\n",
       " 'Berkeley, CA',\n",
       " 'Berkeley, CA, USA',\n",
       " 'Berkeley, California',\n",
       " 'Berlin',\n",
       " 'Berlin, Germany',\n",
       " 'Berne, Switzerland',\n",
       " 'Bethel Park, PA',\n",
       " 'Bethesda, MD',\n",
       " 'Between Glen_b and cardinal',\n",
       " 'Beverly Hills, CA',\n",
       " 'Bieberville, NM',\n",
       " 'Billings, MO',\n",
       " 'Birmingham, AL',\n",
       " 'Birmingham, UK',\n",
       " 'Birmingham, United Kingdom',\n",
       " 'Blacksburg, VA',\n",
       " 'Bloomington, IN',\n",
       " 'Bogota, Colombia',\n",
       " 'Bologna, Italy',\n",
       " 'Bolton, United Kingdom',\n",
       " 'Bolzano, Italy',\n",
       " 'Bonn, Germany',\n",
       " 'Boston MA',\n",
       " 'Boston, MA',\n",
       " 'Botswana',\n",
       " 'Boulder, CO',\n",
       " 'Boyle, Ireland',\n",
       " 'Bozeman, MT',\n",
       " 'Brasilia, Brazil',\n",
       " 'Bratislava, Slovakia',\n",
       " 'Braunschweig/Hannover, Germany',\n",
       " 'Brazil',\n",
       " 'Bremen, Germany',\n",
       " 'Brighton',\n",
       " 'Brighton, United Kingdom',\n",
       " 'Brisbane, Australia',\n",
       " 'Bristol, United Kingdom',\n",
       " 'British Columbia',\n",
       " 'Brno, Czech Republic',\n",
       " 'Bronx, NY',\n",
       " 'Brookline, MA',\n",
       " 'Brooklyn, AL',\n",
       " 'Brooklyn, NY',\n",
       " 'Brownbackistan (Kansas)',\n",
       " 'Brussels',\n",
       " 'Brussels, Belgium',\n",
       " 'Bucharest, Romania',\n",
       " 'Budapest, Hungary',\n",
       " 'Buenos Aires',\n",
       " 'Buenos Aires, Argentina',\n",
       " 'Buffalo, NY',\n",
       " 'Burlington, VT',\n",
       " 'CA',\n",
       " 'Cairo, Egypt',\n",
       " 'Calgary, Canada',\n",
       " 'Calgary, Canaday',\n",
       " 'California',\n",
       " 'Camarines Sur, Philippines',\n",
       " 'Cambridge, ID',\n",
       " 'Cambridge, MA',\n",
       " 'Cambridge, UK',\n",
       " 'Cambridge, United Kingdom',\n",
       " 'Cambridgeshire, United Kingdom',\n",
       " 'Campinas, Brazil',\n",
       " 'Can',\n",
       " 'Canada',\n",
       " 'Canberra, Australia',\n",
       " 'Cape Town, South Africa',\n",
       " 'Caracas, Venezuela',\n",
       " 'Cardiff, UK',\n",
       " 'Carnegie Mellon University, PA',\n",
       " 'Cartagena, Spain',\n",
       " 'Cary, NC',\n",
       " 'Castelldefels, Spain',\n",
       " 'Catalonia, Spain',\n",
       " 'Cayman Islands',\n",
       " 'Cedarville University, OH',\n",
       " 'Chalon-sur-Saône, France',\n",
       " 'Chambery, France',\n",
       " 'Champaign, IL',\n",
       " 'Channel Islands, United Kingdom',\n",
       " 'Chapel Hill, NC',\n",
       " 'Charlotte, NC',\n",
       " 'Charlottesville, VA',\n",
       " 'Chelmsford, MA',\n",
       " 'Cheltenham, United Kingdom',\n",
       " 'Chennai, India',\n",
       " 'Chester, United Kingdom',\n",
       " 'Chicago, IL',\n",
       " 'Chicagoland',\n",
       " 'Chile',\n",
       " 'China',\n",
       " 'Chisinau, Moldova',\n",
       " 'Christiansburg, VA',\n",
       " 'Cincinnati, OH',\n",
       " 'Class',\n",
       " 'Classified',\n",
       " 'Cleveland, OH',\n",
       " 'Cleveland, Ohio',\n",
       " 'Coimbatore,India',\n",
       " 'Coimbra, Portugal',\n",
       " 'College Station, TX',\n",
       " 'Cologne, Germany',\n",
       " 'Colombo, Sri Lanka',\n",
       " 'Colorado',\n",
       " 'Columbia, MO',\n",
       " 'Columbia, Maryland',\n",
       " 'Columbus, OH',\n",
       " 'Connecticut',\n",
       " 'Copenhagen, Denmark',\n",
       " 'Copenhagen/Warsaw',\n",
       " 'Cork, Ireland',\n",
       " 'Coruscant',\n",
       " 'Costa Rica',\n",
       " 'Coventry, United Kingdom',\n",
       " 'Covington, KY',\n",
       " 'Croatia',\n",
       " 'Crosslake, MN',\n",
       " 'Curitiba, Brazil',\n",
       " 'Cyberjaya, Malaysia',\n",
       " 'Czech Republic',\n",
       " 'Cáceres, Spain',\n",
       " 'DE',\n",
       " 'Dallas, TX',\n",
       " 'Dallas, TX, USA',\n",
       " 'Davis, CA',\n",
       " 'Dayton, OH',\n",
       " 'Delaware',\n",
       " 'Delft, Netherlands',\n",
       " 'Delhi',\n",
       " 'Delhi, India',\n",
       " 'Denmark',\n",
       " 'Denver',\n",
       " 'Denver, CO',\n",
       " 'Dingli, Malta',\n",
       " 'District of Columbia',\n",
       " 'District of Columbia Metro area',\n",
       " 'Distrito Federal, Mexico',\n",
       " 'Dubai, United Arab Emirates',\n",
       " 'Dublin',\n",
       " 'Dublin, Ireland',\n",
       " 'Duchy of Grand Fenwick',\n",
       " 'Duluth, MN',\n",
       " 'Dundee, Scotland',\n",
       " 'Dunedin',\n",
       " 'Dunedin, New Zealand',\n",
       " 'Durham',\n",
       " 'Durham, NC',\n",
       " 'Düsseldorf, Germany',\n",
       " 'Earth',\n",
       " 'Earth, TX',\n",
       " 'East Anglia',\n",
       " 'East Coast',\n",
       " 'East Greenwich, RI',\n",
       " 'Eau Claire, WI',\n",
       " 'Edinburgh, United Kingdom',\n",
       " 'Edmonton, Canada',\n",
       " 'Egypt',\n",
       " 'Eindhoven, Netherlands',\n",
       " 'Elgin, IL',\n",
       " 'England',\n",
       " 'England, United Kingdom',\n",
       " 'Espoo, Finland',\n",
       " 'Essen, Germany',\n",
       " 'Essex, UK',\n",
       " 'Estonia',\n",
       " 'Europe',\n",
       " 'FL',\n",
       " 'Fair Oaks, CA',\n",
       " 'Fairfax, CA',\n",
       " 'Fairfax, VA',\n",
       " 'Falcon Heights, MN',\n",
       " 'Ferrara, Italy',\n",
       " 'Fiji',\n",
       " 'Finland',\n",
       " 'Flagstaff, AZ',\n",
       " 'Flanders, Belgium',\n",
       " 'Florence, Italy',\n",
       " 'Florianópolis, Brazil',\n",
       " 'Florida',\n",
       " 'Flower Mound, Texas',\n",
       " 'Formiga, Brazil',\n",
       " 'France',\n",
       " 'Francia',\n",
       " 'Frankfurt, Germany',\n",
       " 'Fredericton, Canada',\n",
       " 'Fredrikstad, Norway',\n",
       " 'Freiburg, Germany',\n",
       " 'GB',\n",
       " 'Gainesville, AL',\n",
       " 'Gainesville, FL',\n",
       " 'Gainesville, Florida',\n",
       " 'Garner, NC',\n",
       " 'Gelderland',\n",
       " 'Geneva, Switzerland',\n",
       " 'Genoa, Italy',\n",
       " 'Georgia',\n",
       " 'Germantown, MD',\n",
       " 'Germany',\n",
       " 'Ghent, Belgium',\n",
       " 'Giessen, Germany',\n",
       " 'Gilbert, AZ',\n",
       " 'Glasgow, United Kingdom',\n",
       " 'Gold Coast',\n",
       " 'Golden, CO',\n",
       " 'Google, Inc.',\n",
       " 'Gothenburg, Sweden',\n",
       " 'Gouda, Netherlands',\n",
       " 'Grand Rapids, MI',\n",
       " 'Great Cacapon, WV',\n",
       " 'Greece',\n",
       " 'Greenville, SC',\n",
       " 'Grimsö, Sweden',\n",
       " 'Gryon, Switzerland',\n",
       " 'Gurgaon, India',\n",
       " 'Göttingen, Germany',\n",
       " 'Halifax, Canada',\n",
       " 'Hamburg, Germany',\n",
       " 'Hamburg/Kiel/Flensburg, Germany',\n",
       " 'Hamilton, Canada',\n",
       " 'Hamilton, New Zealand',\n",
       " 'Hanover, Germany',\n",
       " 'Havre de Grace, MD',\n",
       " 'Heidelberg, Germany',\n",
       " 'Hellas',\n",
       " 'Hobart, Australia',\n",
       " 'Hollywood, FL',\n",
       " 'Honduras',\n",
       " 'Hong Kong',\n",
       " 'Hong Kong, Hong Kong',\n",
       " 'Honolulu, HI',\n",
       " 'Houston',\n",
       " 'Houston, TX',\n",
       " 'Hungary',\n",
       " 'Hyderabad, India',\n",
       " 'Hyrule',\n",
       " 'I am here',\n",
       " \"I'm right here\",\n",
       " 'ITALY',\n",
       " 'Iligan City, Philippines',\n",
       " 'Illinois',\n",
       " 'India',\n",
       " 'Indiana',\n",
       " 'Indianapolis',\n",
       " 'Indianapolis, IN',\n",
       " 'Indonesia',\n",
       " 'Innsbruck, Austria',\n",
       " 'Internet Cloud',\n",
       " 'Iowa',\n",
       " 'Iowa City, IA',\n",
       " 'Iran',\n",
       " 'Ireland',\n",
       " 'Irvine, CA',\n",
       " 'Irving, TX',\n",
       " 'Israel',\n",
       " 'Istanbul, Turkey',\n",
       " 'Istanbul, Turkiye',\n",
       " 'Italy',\n",
       " 'Ithaca, NY',\n",
       " 'Jakarta, Indonesia',\n",
       " 'Japan',\n",
       " 'Jersey City, NJ',\n",
       " 'Jerusalem',\n",
       " 'Jerusalem, Israel',\n",
       " 'Jordan',\n",
       " 'Jupiter, FL',\n",
       " 'Kailua Kona, HI',\n",
       " 'Kailua, HI',\n",
       " 'Kaiserslautern, Germany',\n",
       " 'Kaneohe, HI',\n",
       " 'Kansas',\n",
       " 'Kansas City, MO',\n",
       " 'Kaohsiung, Taiwan',\n",
       " 'Karachi, Pakistan',\n",
       " 'Karlsruhe, Germany',\n",
       " 'Kassel, Germany',\n",
       " 'Kaunas, Republic of Lithuania',\n",
       " 'Kenya',\n",
       " 'Kerala, India',\n",
       " 'Kiel, Germany',\n",
       " 'Kiev, Ukraine',\n",
       " 'Kilifi, Kenya',\n",
       " \"King's College London, United Kingdom\",\n",
       " 'Kingdom of Zhao',\n",
       " 'Kingston, Canada',\n",
       " 'Kitchener, Canada',\n",
       " 'Knoxville, TN',\n",
       " 'Kolkata, India',\n",
       " 'Korea',\n",
       " 'Kuala Lumpur, Malysia.',\n",
       " 'Kuopio, Finland',\n",
       " 'Kyoto, Japan',\n",
       " 'Kyoto-shi, Japan',\n",
       " 'La Jolla',\n",
       " 'La Jolla, CA',\n",
       " 'Lahore, Pakistan',\n",
       " 'Lakewood, CO',\n",
       " 'Lancaster, United Kingdom',\n",
       " 'Las Vegas, NV',\n",
       " 'Latvia',\n",
       " 'Laurel, MD',\n",
       " 'Lausanne, Switzerland',\n",
       " 'Leamington, United Kingdom',\n",
       " 'Leeds, United Kingdom',\n",
       " 'Leicester, United Kingdom',\n",
       " 'Leiculeşti, Romania',\n",
       " 'Leiden, Netherlands',\n",
       " 'Leuven, Belgium',\n",
       " 'Lexington, KY',\n",
       " 'Leyte',\n",
       " 'Lille, France',\n",
       " 'Lima, Peru',\n",
       " 'Lincoln, NE',\n",
       " 'Linz, Austria',\n",
       " 'Lisbon, Portugal',\n",
       " 'Lithuania',\n",
       " 'Lititz, PA',\n",
       " 'Little Rock, AR',\n",
       " 'Little Venice',\n",
       " 'Liverpool, United Kingdom',\n",
       " 'Livingston, LA',\n",
       " 'Ljubljana, Slovenia',\n",
       " 'Lodz, Poland',\n",
       " 'Lomas De Zamora, Argentina',\n",
       " 'London',\n",
       " 'London, Canada',\n",
       " 'London, Europe',\n",
       " 'London, UK',\n",
       " 'London, United Kingdom',\n",
       " 'London, uk',\n",
       " 'London,UK',\n",
       " 'Longwood University, VA',\n",
       " 'Los Angeles',\n",
       " 'Los Angeles, CA',\n",
       " 'Los Gatos, CA',\n",
       " 'Louisiana',\n",
       " 'Ludwigsburg, Germany',\n",
       " 'Lund, Sweden',\n",
       " 'Luxembourg & Heidelberg',\n",
       " 'Lviv, Ukraine',\n",
       " 'Maastricht, Netherlands',\n",
       " 'Macedonia',\n",
       " 'Macquarie University, Australia',\n",
       " 'Madison, WI',\n",
       " 'Madison, Wisconsin',\n",
       " 'Madrid, Spain',\n",
       " 'Madurai, India',\n",
       " 'Maine',\n",
       " 'Malaysia',\n",
       " 'Malta',\n",
       " 'Manchester, United Kingdom',\n",
       " 'Manhattan, KS',\n",
       " 'Manila, Philippines',\n",
       " 'Mansfield, CT',\n",
       " 'Mars',\n",
       " 'Maryland',\n",
       " 'Massachusetts',\n",
       " 'Mcmaster University, Canada',\n",
       " 'Medellin, Colombia',\n",
       " 'Melboure, Australia',\n",
       " 'Melbourne',\n",
       " 'Melbourne, Australia',\n",
       " 'Memphis, TN',\n",
       " 'Menlo Park, CA',\n",
       " 'Mexico',\n",
       " 'Mexico City, Mexico',\n",
       " 'Michigan',\n",
       " 'Michigan State University, MI',\n",
       " 'Mid West, US',\n",
       " 'Midland, TX',\n",
       " 'Milan, Italy',\n",
       " 'Milton, GA',\n",
       " 'Milwaukee, WI',\n",
       " 'Minga',\n",
       " 'Minneapolis, MN',\n",
       " 'Minnesota',\n",
       " 'Missouri',\n",
       " 'Monterey, CA',\n",
       " 'Montreal',\n",
       " 'Montreal, Canada',\n",
       " 'Montreal, QC',\n",
       " 'Montreal, Quebec',\n",
       " 'Morgantown, WV',\n",
       " 'Morocco',\n",
       " 'Moscow, Russia',\n",
       " 'Mount Sinai, NY',\n",
       " 'Mountain View, CA',\n",
       " 'Mountain View, California',\n",
       " 'Mumbai, India',\n",
       " 'Munich',\n",
       " 'Munich, Germany',\n",
       " 'Myrtle Beach, SC',\n",
       " 'NC, USA',\n",
       " 'ND',\n",
       " 'NE UK',\n",
       " 'NY',\n",
       " 'NYC',\n",
       " 'NYC, WHOI, or Antarctica',\n",
       " 'Nairobi, Kenya',\n",
       " 'Naples, Italy',\n",
       " 'Nashua, NH',\n",
       " 'Nashville, TN',\n",
       " 'Natick, MA',\n",
       " 'National University Of Singapore',\n",
       " 'Nepal',\n",
       " 'Nerdvana',\n",
       " 'Netherlands',\n",
       " 'New England',\n",
       " 'New Haven, CT',\n",
       " 'New Haven, CT, USA',\n",
       " 'New Jersey',\n",
       " 'New Mexico',\n",
       " 'New Orleans, LA',\n",
       " 'New Orleans, Louisiana',\n",
       " 'New York',\n",
       " 'New York, NY',\n",
       " 'New York, New York',\n",
       " 'New York, United States',\n",
       " 'New Zealand',\n",
       " 'Newark, DE',\n",
       " 'Newcastle UK',\n",
       " 'Newcastle, CA',\n",
       " 'Newcastle, United Kingdom',\n",
       " 'Newport Beach, CA',\n",
       " 'Nish, Serbia',\n",
       " 'Nomad, MI',\n",
       " 'Norrtälje, Sweden',\n",
       " 'North America',\n",
       " 'North Carolina',\n",
       " 'North Pole',\n",
       " 'North Salt Lake, UT',\n",
       " 'Northeastern US',\n",
       " 'Northern Europe',\n",
       " 'Norway',\n",
       " 'Norwich',\n",
       " 'Not from around here',\n",
       " 'Nottingham, United Kingdom',\n",
       " 'Nova Scotia, Canada',\n",
       " 'Oakland, CA',\n",
       " 'Odessa, Ukraine',\n",
       " 'Oklahoma',\n",
       " 'Omaha, NE',\n",
       " 'Ontario',\n",
       " 'Ontario, Canada',\n",
       " 'Oregon',\n",
       " 'Oregon/Washington',\n",
       " 'Orlando',\n",
       " 'Orlando, FL',\n",
       " 'Osaka-shi, Japan',\n",
       " 'Oslo',\n",
       " 'Oslo, Norway',\n",
       " 'Ottawa',\n",
       " 'Ottawa, Canada',\n",
       " 'Oxford, UK',\n",
       " 'Oxford, United Kingdom',\n",
       " 'Oz',\n",
       " 'PVD, RI',\n",
       " 'Pacific Northwest',\n",
       " 'Pacific Palisades, CA',\n",
       " 'Pakistan',\n",
       " 'Palm Beach, FL',\n",
       " 'Palo Alto, CA',\n",
       " 'Panama',\n",
       " 'Paris France',\n",
       " 'Paris, France',\n",
       " 'Passau, Germany',\n",
       " 'Pennsylvania',\n",
       " 'Persian Gulf',\n",
       " 'Perth, Australia',\n",
       " 'Petaluma, CA',\n",
       " 'Peterborough, Canada',\n",
       " 'Phase space',\n",
       " 'Philadelphia, PA',\n",
       " 'Philippines',\n",
       " 'Phillips, ME',\n",
       " 'Phoenix, AZ',\n",
       " 'Phoenix, AZ, USA',\n",
       " 'Piliscsaba, Hungary',\n",
       " 'Pilsen, Czech Republic',\n",
       " 'Piracicaba, SP, Brazil',\n",
       " 'Pisa (Italy)',\n",
       " 'Pittsburgh, PA',\n",
       " 'Plymouth, United Kingdom',\n",
       " 'Poland',\n",
       " 'Portland, OR',\n",
       " 'Portland, Oregon',\n",
       " 'Portland,OR',\n",
       " 'Porto Alegre, Brazil',\n",
       " 'Porto, Portugal',\n",
       " 'Portugal',\n",
       " 'Potchefstroom, South Africa',\n",
       " 'Potsdam, Germany',\n",
       " 'Poznan, Poland',\n",
       " 'Prague',\n",
       " 'Prague, Czech Republic',\n",
       " 'Pratteln, Switzerland',\n",
       " 'Pretoria, South Africa',\n",
       " 'Princeton, NJ',\n",
       " 'Providence RI',\n",
       " 'Providence, RI',\n",
       " 'Provo, UT',\n",
       " 'Pryor, OK',\n",
       " 'Pune, India',\n",
       " 'Qatar',\n",
       " 'Quebec City, Canada',\n",
       " 'Queen Mary University London',\n",
       " 'Rabat, Morocco',\n",
       " 'Raleigh, NC',\n",
       " 'Redmond, WA',\n",
       " 'Redmond, WA, United States',\n",
       " 'Redwood City, CA',\n",
       " 'Regina, Canada',\n",
       " 'Rennes, France',\n",
       " 'Research Triangle Park',\n",
       " 'Reston, VA',\n",
       " 'Reykjavik, Iceland',\n",
       " 'Rhode Island',\n",
       " 'Richardson, TX',\n",
       " 'Riegel am Kaiserstuhl, Germany',\n",
       " 'Riga, Republic of Latvia',\n",
       " 'Rio De Janeiro, Brazil',\n",
       " 'Rio de Janeiro',\n",
       " 'Rio de Janeiro, Brazil',\n",
       " 'Riverside, CA',\n",
       " 'Rochester, MN',\n",
       " 'Rochester, NY',\n",
       " 'Rockefeller University, NY',\n",
       " 'Romania',\n",
       " 'Rome, Italy',\n",
       " 'Rotterdam',\n",
       " 'Rotterdam, Netherlands',\n",
       " 'Rotterdam, The Netherlands',\n",
       " 'Rouen, France',\n",
       " 'Russia',\n",
       " 'Russia, Moscow, Yandex',\n",
       " 'SF',\n",
       " 'Saarbrücken, Germany',\n",
       " 'Saint Moscow',\n",
       " 'Salem, OR',\n",
       " 'Salt Lake City, UT',\n",
       " 'Salt Lake City, Utah',\n",
       " 'Salto, Brazil',\n",
       " 'Salvador, Brazil',\n",
       " 'San Antonio, TX',\n",
       " 'San Diego',\n",
       " 'San Diego, CA',\n",
       " 'San Francisco Bay Area',\n",
       " 'San Francisco, CA',\n",
       " 'San Jose, CA',\n",
       " 'San Marcos, CA',\n",
       " 'San Marcos, TX',\n",
       " 'San Mateo, CA',\n",
       " \"San Michele all'Adige, Italy\",\n",
       " 'San francisco',\n",
       " 'Santa Barbara, CA',\n",
       " 'Santa Clara, CA',\n",
       " 'Santa Cruz, CA',\n",
       " 'Santa Fe, NM',\n",
       " 'Santa Monica CA',\n",
       " 'Santa Monica, CA',\n",
       " 'Santiago, Chile',\n",
       " 'Sao Paulo, Brazil',\n",
       " 'Sapporo-shi, Japan',\n",
       " 'Sarasota, FL',\n",
       " 'Sarrebruck, Germany',\n",
       " 'Saskatoon, Canada',\n",
       " 'Scotland, United Kingdom',\n",
       " 'Seattle',\n",
       " 'Seattle, WA',\n",
       " 'Seattle, Washington',\n",
       " 'Senegal',\n",
       " 'Seoul, Korea',\n",
       " 'Seoul, South Korea',\n",
       " \"Sevastopol', Ukraine\",\n",
       " 'Shandong, China',\n",
       " 'Shanghai',\n",
       " 'Shanghai, China',\n",
       " 'Sheffield, United Kingdom',\n",
       " 'Sherbrooke, Canada',\n",
       " 'Silicon Valley',\n",
       " 'Silver Spring, MD, US, Earth',\n",
       " 'Singapore',\n",
       " 'Singapore, Singapore',\n",
       " 'Slovenia',\n",
       " 'Socorro, NM',\n",
       " 'Sofia, Bulgaria',\n",
       " 'Somerville, Massachusetts',\n",
       " 'Sorocaba, Brazil',\n",
       " 'South Africa',\n",
       " 'South Dakota',\n",
       " 'South Korea',\n",
       " 'Southern California',\n",
       " 'Spain',\n",
       " 'Spanish Fork, UT',\n",
       " 'Spokane, WA',\n",
       " 'Springfield, MO',\n",
       " 'Sri Lanka',\n",
       " 'St Louis, MO',\n",
       " 'St Paul, MN',\n",
       " 'St Petersburg, FL',\n",
       " 'St. Gallen',\n",
       " \"St. John's, Canada\",\n",
       " 'St. Louis',\n",
       " 'St. Louis, MO',\n",
       " 'St. Paul, MN',\n",
       " 'St. Petersburg, Russia',\n",
       " 'Stanford, CA',\n",
       " 'State College, PA',\n",
       " 'Stellenbosch, South Africa',\n",
       " 'Steyr, Austria',\n",
       " 'Stockholm',\n",
       " 'Stockholm, Sweden',\n",
       " 'Stony Brook, NY',\n",
       " 'Subang Jaya, Malaysia',\n",
       " 'Sundsvall, Sweden',\n",
       " 'Sunnyvale, CA',\n",
       " 'Surrey, United Kingdom',\n",
       " 'Swansea MA',\n",
       " 'Swansea, United Kingdom',\n",
       " 'Sweden',\n",
       " 'Switzerland',\n",
       " 'Sydney, Australia',\n",
       " 'São Carlos, Brazil',\n",
       " 'São Paulo - Brazil',\n",
       " 'Tacoma, WA',\n",
       " 'Taipei & Hong Kong',\n",
       " 'Taiwan',\n",
       " 'Taloon',\n",
       " 'Tampa, FL',\n",
       " 'Tartu, Estonia',\n",
       " 'Taunton, United Kingdom',\n",
       " 'Tbilisi, Georgia',\n",
       " 'Tehran, Iran',\n",
       " 'Tel Aviv',\n",
       " 'Tel Aviv, Israel',\n",
       " 'Tempe, AZ',\n",
       " 'Tennessee',\n",
       " 'Texas',\n",
       " 'Thailand',\n",
       " 'The Earth',\n",
       " 'The Hague',\n",
       " 'The Netherlands',\n",
       " 'The Woodlands, TX',\n",
       " 'Thessaloniki, Greece',\n",
       " 'Thunder Bay, Canada',\n",
       " 'Tiburon, CA',\n",
       " 'Tokyo',\n",
       " 'Tokyo, Japan',\n",
       " 'Tornado, WV',\n",
       " 'Toronto, Canada',\n",
       " 'Toronto, Ontario, Canada',\n",
       " 'Toulouse, France',\n",
       " 'Townsville, Australia',\n",
       " 'Trento, Italy',\n",
       " 'Trieste, Italy',\n",
       " 'Tucson, AZ',\n",
       " 'Tunis, Tunisia',\n",
       " 'Turin, Italy',\n",
       " 'Turkey',\n",
       " 'Turku, Finland',\n",
       " 'Tønsberg, Norway',\n",
       " 'Tübingen, Germany',\n",
       " 'U.S.',\n",
       " 'UCL, United Kingdom',\n",
       " 'UCSF',\n",
       " 'UK',\n",
       " 'UK, Iran',\n",
       " 'US',\n",
       " 'USA',\n",
       " 'Ukraine',\n",
       " 'United Kingdom',\n",
       " 'United States',\n",
       " 'United States of America',\n",
       " 'University of Chicago, IL',\n",
       " 'University of Colorado, Boulder',\n",
       " 'University of Connecticut',\n",
       " 'University of Detroit Mercy',\n",
       " 'University of Southern California, CA',\n",
       " 'University of Waterloo, Canada',\n",
       " 'Uppsala, Sweden',\n",
       " 'Urbana, IL',\n",
       " 'Utah',\n",
       " 'Utah, United States',\n",
       " 'Uzhhorod, Ukraine',\n",
       " 'Valladolid, Spain',\n",
       " 'Vancouver',\n",
       " 'Vancouver, B.C.',\n",
       " 'Vancouver, Canada',\n",
       " 'Vathorst,Netherlands',\n",
       " 'Vay, France',\n",
       " 'Venezuela',\n",
       " 'Versailles, KY',\n",
       " 'Vienna',\n",
       " 'Vienna, Austria',\n",
       " 'Vilnius, Lithuania',\n",
       " 'Virginia',\n",
       " 'Walnut Creek, CA',\n",
       " 'Warsaw, Poland',\n",
       " 'Washington D.C.',\n",
       " 'Washington DC, United States',\n",
       " 'Washington, DC',\n",
       " 'Washington, District of Columbia, United States',\n",
       " 'Washington, United States',\n",
       " 'Waterloo, Canada',\n",
       " 'Weimar, Germany',\n",
       " 'Wellesley, MA',\n",
       " 'Wellington, New Zealand',\n",
       " 'Wenatchee, WA',\n",
       " 'West Chester, PA',\n",
       " 'West Kelowna, Canada',\n",
       " 'West Lafayette, IN',\n",
       " 'West Virginia',\n",
       " 'Westmont, IL',\n",
       " 'Westwood, MA',\n",
       " 'Wichita, KS',\n",
       " 'Williamsport, PA',\n",
       " 'Willich, Germany',\n",
       " 'Winnipeg, Canada',\n",
       " 'Winston-Salem, NC',\n",
       " 'Wisconsin',\n",
       " 'Worcester, MA',\n",
       " 'Wuhan, China',\n",
       " 'Wurzburg, Germany',\n",
       " 'Yaounde, Cameroon',\n",
       " 'Yerevan, Armenia',\n",
       " 'Youngstown, OH',\n",
       " 'Zagreb, Croatia',\n",
       " 'Zaragoza, Spain',\n",
       " 'Ziqim, Israel',\n",
       " 'Zurich, Switzerland',\n",
       " 'amsterdam',\n",
       " 'berlin.de',\n",
       " 'if it was your ass you would know',\n",
       " 'los angeles',\n",
       " nan,\n",
       " 'on the server farm',\n",
       " 'paris',\n",
       " 'san diego',\n",
       " 'seattle',\n",
       " 'Île Maurice',\n",
       " 'Östersund, Sweden',\n",
       " 'İstanbul, Turkey'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " '<p>\"The spiritual quest for elegance can turn the hacker into an artist.\"</p>\\r\\n',\n",
       " '<p>Environmental scientist working for an engineering consultancy, with interests in GIS and programming.</p>\\r\\n',\n",
       " '<p>(your <em>about me</em> is currently <em>not</em> blank)</p>\\r\\n',\n",
       " '<p>Working with a wide range of languages, but mainly Oracle PL/SQL, Java and C.</p>\\r\\n',\n",
       " 'Have been working as a programmer since September 2007 and loving it. Learned some C++ and Java while studying, HTML, PHP, JavaScript and CSS for my hobby website, and AppleScript, Flash ActionScript and Python at work.',\n",
       " '<p>see blog</p>\\r\\n',\n",
       " '<p>I work in computational biology in New York and Boston.  I do a great deal of numerical analysis, optimization and parallel algorithm design.</p>\\r\\n\\r\\n<p>You can learn more about my professional research and work at <a href=\"http://andrewmatteson.com\" rel=\"nofollow\">andrewmatteson.com</a>.  I host a blog about all things math, programming and biology at <a href=\"http://axiomofcats.com\" rel=\"nofollow\">axiomofcats.com</a>.</p>\\r\\n',\n",
       " '<p>\\r\\n<a href=\"http://www.lkb.ens.fr/-Metrology-of-simple-sistems-and-?lang=en\" rel=\"nofollow\">Physicist</a>.\\r\\n<a href=\"http://packages.python.org/uncertainties/\" rel=\"nofollow\">Open-source programmer</a>.\\r\\n<a href=\"http://easyit.weebly.com/\" rel=\"nofollow\">Science & IT consultant.</a>\\r\\n<a href=\"http://www.normalesup.org/~lebigot/music\" rel=\"nofollow\">Music composer</a>. \\r\\n</p>\\r\\n\\r\\n<p>Have been loving science since 1980.</p>\\r\\n\\r\\n<p>Started programming in 1983.  Worked with a dozen languages (imperative [Python, C++, IDL,…], functional [Caml], stack-based [Postscript, HP RPL], constraint-based [Prolog], flow-based [LabView], and assembly [Motorola 68000, HP Saturn]).</p>\\r\\n\\r\\n<p>Have been working with Unix since 1994.</p>\\r\\n\\r\\n<p>Started programming in Python in 2006, and still loving it!  Have been teaching Python to researchers, engineers and graduate students since 2009.</p>\\r\\n',\n",
       " \"<p>I'm Quantitative Environmental Scientist in the Institute of Environmental Change &amp; Society, at the University of Regina, Canada. I undertake research on environmental problems, including climate change and atmospheric pollution, affecting lakes. I use lake sediments to look back in time at the history of lakes to look at what organisms are present and how the species in the lake have changed through time and how lakes evolve and respond to pollution and perturbations.</p>\\r\\n\\r\\n<p>I'm also an Adjunct Professor in the Department of Biology at the University of Regina.</p>\\r\\n\",\n",
       " '<p>R User, board game enthusiast.</p>\\r\\n',\n",
       " 'Not a programmer but not afraid to use simple programming / macroing such as manipulate and analyze data in R or write 3-10 line autohotkey commands.',\n",
       " '<p>Interested in statistics, gis and programming.</p>\\r\\n',\n",
       " '<p>I work at Twilio.</p>\\r\\n\\r\\n<p>Here are some side projects I\\'ve been working on: <a href=\"http://kev.inburke.com/projects\" rel=\"nofollow\">http://kev.inburke.com/projects</a></p>\\r\\n',\n",
       " '<p>PhD student on Electrical Engineering, Marine Department\\r\\nInterest: Machine Learning, Mathematics, Statistics, Robotics, Marine Technology</p>\\r\\n',\n",
       " '<p>Long-time R user, new Stata user, occasional Stan user, and currently learning Scala. Data Scientist at <a href=\"http://datamininglab.com/\" rel=\"nofollow\">ERI</a>, an incredible company where we do everything from fraud detection and visualization, to training and and text mining.</p>\\r\\n',\n",
       " '<p>Student of Computer Science.</p>\\r\\n',\n",
       " '<p>nothing to see here, move along now</p>\\r\\n',\n",
       " '<p>Just a random...person.</p>\\r\\n',\n",
       " \"<p>I'm here to learn more and more about Statistics</p>\\r\\n\",\n",
       " '<p>Mathematics/statistics and physics student.</p>\\r\\n',\n",
       " '<p>ecology about global change and population</p>\\r\\n',\n",
       " \"<p>I'm a PhD student in the joint Statistics and Public Policy program at Carnegie Mellon University.</p>\\r\\n\",\n",
       " '<p>I like to learn things but to pay my way I will aim to teach 1 person something for every 117 things I learn by virtue of contributions of others</p>\\r\\n',\n",
       " '<p>1st first PhD student in Psychology at Stanford under Professor Jay McClelland. All neural network modeling, all of the time.</p>\\r\\n',\n",
       " '<p>I do research in behavioural change.</p>\\r\\n',\n",
       " '<p>Bioinformaticist, hobbyist iPhone developer</p>\\r\\n',\n",
       " '<p>C++, python, django, PostgreSQL</p>\\r\\n\\r\\n<p>Machine Learning, Statistics, NLP</p>\\r\\n',\n",
       " '<p>Research Interests: Data mining, refactoring, software engineering, metrics, knowledge representation.</p>\\r\\n\\r\\n<p>25+ years software development experience in Java, C++, Lisp, Prolog, Ada,...</p>\\r\\n',\n",
       " '<p>Statistician; part of the development team for Stan (http://www.mc-stan.org). </p>\\r\\n\\r\\n<p>You can reach me at daniel.lee@hutchinsonlee.com or bearlee@alum.mit.edu.</p>\\r\\n',\n",
       " '<p>A student in physics doing scientific computing.</p>\\r\\n',\n",
       " '<p>@JnBrymn</p>\\r\\n\\r\\n<ul>\\r\\n<li>I do Solr </li>\\r\\n<li>I do Cassandra </li>\\r\\n<li>I do Python </li>\\r\\n<li>I do Java </li>\\r\\n<li>I do Software Architecture</li>\\r\\n<li>I do Data Mining</li>\\r\\n</ul>\\r\\n\\r\\n<p>... and I do them all well</p>\\r\\n',\n",
       " '<p>A computer science lecturer at University College London. I do stuff in logic, theorem proving, program analysis and verification. I\\'m a very enthusiastic TeX/LaTeX user, most days.</p>\\r\\n\\r\\n<p>I also run a small site for LaTeX beginners in spanish: <a href=\"http://navarroj.com/latex/\" rel=\"nofollow\">LaTeX Fácil</a>.</p>\\r\\n',\n",
       " 'Java, Python',\n",
       " 'Professor of Radiology<br>\\\\\\\\nUniversity of Washington<br>\\\\\\\\nSeattle, WA',\n",
       " '<p>A coder.</p>\\r\\n',\n",
       " '<p>Avid Emacs and Vim hacker (I use Emacs with <strong><a href=\"http://www.emacswiki.org/emacs/Evil\" rel=\"nofollow\">Evil</a></strong> to get the best of both worlds ;)</p>\\r\\n\\r\\n<p>I\\'m experienced with Python, C#, Java, and Javascript, and have dabbled in C++, C, objective-C, Lisp, Ruby, Ocaml, and Scala. While I have a keen interest for all things code, I\\'m especially interested in the design and creation of programming languages themselves. </p>\\r\\n\\r\\n<p>Currently a Computer Science Sophomore at Cornell University. </p>\\r\\n\\r\\n<p>Formerly known as @CrazyJugglerDrummer.</p>\\r\\n',\n",
       " '<p>Professional Software Development Engineer.\\r\\nAlumni of Rose-Hulman.</p>\\r\\n',\n",
       " '<p>Hello. I am a programmer, musician, and volunteer.</p>\\r\\n\\r\\n<p>I work in education lead media doing mostly back-end programming in Ruby with some front-end programming in Rails. I also occasionally work with Clojure.</p>\\r\\n\\r\\n<p>Visit my website for links to various social networking sites, music recordings, band websites, and my programming portfolio.</p>\\r\\n',\n",
       " '<p>Computational Biologist \\r\\nBroad Institute of Harvard and MIT</p>\\r\\n',\n",
       " '<p>Senior developer.</p>\\r\\n',\n",
       " '<p>PhD student studying at James Cook University, Australia</p>\\r\\n',\n",
       " '<p>SAS programmer, statistician, and blogger. My blog (blogs.sas.com/content/iml) deals with data analysis, statistical programming, and the SAS/IML matrix language. Author of the books <em>Statistical Programming with SAS/IML Software</em> (2010) and <em>Simulating Data with SAS</em> (2013).</p>\\r\\n',\n",
       " '<p>Just another geek</p>\\r\\n\\r\\n<p><a href=\"https://www.google.com/+SalmanArshad2000\" rel=\"nofollow\">Google+ Profile</a><br>\\r\\n<a href=\"http://www.linkedin.com/in/salmanarshad2000\" rel=\"nofollow\">LinkedIn Profile</a><br>\\r\\n<a href=\"http://salman-w.blogspot.com\" rel=\"nofollow\">Web Development Blog</a></p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://salman-w.blogspot.com/2013/05/jquery-ui-dialog-examples.html\" rel=\"nofollow\">5 jQuery UI Dialog Examples</a></li>\\r\\n<li><a href=\"http://salman-w.blogspot.com/2013/12/jquery-ui-autocomplete-examples.html\" rel=\"nofollow\">5 jQuery UI Autocomplete Examples</a></li>\\r\\n<li><a href=\"http://salman-w.blogspot.com/2013/01/jquery-ui-datepicker-examples.html\" rel=\"nofollow\">5 jQuery UI Datepicker Examples</a></li>\\r\\n<li><a href=\"http://salman-w.blogspot.com/2014/06/google-adsense-responsive-ads-why-not.html\" rel=\"nofollow\">AdSense Responsive Ads - Why Not?</a></li>\\r\\n<li><a href=\"http://salman-w.blogspot.com/2012/11/fix-the-php-script-does-not-work-problem.html\" rel=\"nofollow\">Find and Fix PHP Problems Yourself</a></li>\\r\\n</ul>\\r\\n',\n",
       " '<p>Twitter @ChrisBeeley</p>\\r\\n',\n",
       " '<p>Infectious disease epidemiologist specializing in the intersection between mathematical models of disease transmission and observational methods. Crafter of artisanal simulation models for the discerning scientist. Oddly fond of enteric pathogens.</p>\\r\\n\\r\\n<p>A fair hand at SAS, Python and R</p>\\r\\n\\r\\n<p><a href=\"http://twitter.com/EpiGrad\" rel=\"nofollow\">@GermsAndNumbers</a></p>\\r\\n',\n",
       " '<p>Student of PhD in Rural and regional development Planning at Asian Institute of technology Thailand.</p>\\r\\n',\n",
       " '<p>I design stuff for Stack Exchange. Also a professional bacon eater.</p>\\r\\n\\r\\n<p>blog: <a href=\"http://www.8164.org\" rel=\"nofollow\">8164.org</a></p>\\r\\n\\r\\n<p>twitter: <a href=\"http://twitter.com/jzy\" rel=\"nofollow\">@jzy</a></p>\\r\\n',\n",
       " '<p>I work as a senior software engineer in a small, research oriented company. We program mostly in C#. </p>\\r\\n\\r\\n<p>Interest areas: object oriented design patterns, functional programming, data structures and algorithms. </p>\\r\\n',\n",
       " \"<p>I am a graduate student in Zoology with a specialization in Marine Science at the Hawai'i Institute of Marine Biology. I use R and MatLab for population models, statistics, and various graphic aids. To learn more about my project, check out my research blog, Fishpond Fever.</p>\\r\\n\",\n",
       " '<p>I graduated with a <a href=\"http://www.se.rit.edu/\" rel=\"nofollow\">Bachelor of Science in Software Engineering</a> from the <a href=\"http://www.rit.edu\" rel=\"nofollow\">Rochester Institute of Technology</a> in May 2011. I currently work for <a href=\"http://utcaerospacesystems.com/\" rel=\"nofollow\">UTC Aerospace Systems</a> in Massachusetts. My full CV is available on <a href=\"http://careers.stackoverflow.com/thomasjowens\">StackOverflow Careers</a> and <a href=\"http://www.linkedin.com/in/thomasjowens\" rel=\"nofollow\">LinkedIn</a>.</p>\\r\\n\\r\\n<p>My professional interests include software project management, software engineering process, software measurements and metrics, leadership, and professionalism in software engineering. I\\'m also a casual student of psychology and sociology, especially as they apply to a business context. Personally, I have taken up photography as a hobby. I\\'m also a casual gamer.</p>\\r\\n',\n",
       " '<p>This account is no longer in use.</p>\\r\\n',\n",
       " '<p>PostDoc in Experimental Psychology (vision, Flavor)</p>\\r\\n',\n",
       " '<p>Ph.D student in Biostatistics at UC Davis. Interested in Bayesian statistics, high performance computing, and MCMC.</p>\\r\\n\\r\\n<p><strong>Commonly-Used Languages:</strong> C, Python, R, SAS, SQL</p>\\r\\n\\r\\n<p><strong>Statistical:</strong> Computational statistics, Bayesian statistics, clinical trials, insurance and fraud</p>\\r\\n\\r\\n<p><strong>Interests:</strong> R, statistics education, biostatistics, Android, NoSQL vs. RDBMS arguments, functional programming  </p>\\r\\n\\r\\n<p><strong>Religious Affiliations:</strong> Bayesian statistics, vi</p>\\r\\n',\n",
       " '<p>Coder. Toolmaker. Cubmaster. Walrus.</p>\\r\\n',\n",
       " '<p>I consider myself web application programmer and a database developer. Although I started programming in 1991, I have 10 years of professional experience. I have 3 years of Project Management and Team Leading experience. I have experience in several web application development platforms and Database Systems. I have numerous Microsoft certifications (MCSD,MCPD,MCT..). I am certified in Oracle (11g) and Sql Server (2000-2008) Databases. I have Electrical Engineering BS, Compuer Engineering ,MS. I pursue PHD degree in Electrical Engineering (Machine Learning , Intrusion Detection)</p>\\r\\n',\n",
       " \"<p>I'm a physicist by training, with experience of working with financial data sets. I'm now moving into bioinformatics and trying to learn statistics properly!</p>\\r\\n\",\n",
       " '<p>MBA in Multimedia Marketing student</p>\\r\\n',\n",
       " '<p><code>2014/04/23:</code>  I develop Emergency Medical Services (EMS) software for <a href=\"http://www.sansio.com/\" rel=\"nofollow\">Sansio/Physio</a> to enable EMS personnel to track patient encounter data in the field and notify hospitals of incoming patients in real time.  </p>\\r\\n\\r\\n<p>I\\'m also launching <a href=\"http://www.greatPixelHunt.com\" rel=\"nofollow\">Great Pixel Hunt</a> as a side project.  This is an online sweepstakes where businesses pay to reach local users and local users/charities win the majority of those profits.</p>\\r\\n\\r\\n<p><code>2011/09/28:</code>  Working at my 2nd software-development job full time and working on my 1st web-site and spending time with my wife an baby girl in my free time.  I look forward to the day where my \"full time\" becomes my \"free time\".</p>\\r\\n',\n",
       " '<p>If you need to, you can contact me at: <strong>alix</strong> [<em>dot</em>] <strong>axel</strong> [<em>at</em>] <strong>gmail</strong> [<em>dot</em>] <strong>com</strong></p>\\r\\n\\r\\n<p>Some of my GitHub repositories:</p>\\r\\n\\r\\n<ul>\\r\\n<li><strong><a href=\"https://github.com/alixaxel/phunction/\" rel=\"nofollow\">phunction</a>, a minimalistic PHP HMVC Framework</strong>.</li>\\r\\n<li><strong><a href=\"https://github.com/alixaxel/halBox/\" rel=\"nofollow\">halBox</a>, bash script to bootstrap Debian/Ubuntu servers</strong>.</li>\\r\\n<li><strong><a href=\"https://github.com/alixaxel/ArrestDB/\" rel=\"nofollow\">ArrestDB</a>, RESTful API for SQLite, MySQL and PostgreSQL databases</strong>.</li>\\r\\n</ul>\\r\\n',\n",
       " '<p><a href=\"http://stackexchange.com/users/216182/blunders\"><img src=\"http://stackexchange.com/users/flair/216182.png\" width=\"208\" height=\"58\" alt=\"profile for blunders on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for blunders on Stack Exchange, a network of free, community-driven Q&amp;A sites\" /></a></p>\\r\\n',\n",
       " '<p>Engineer</p>\\r\\n',\n",
       " '<p>Brazilian PhD student in Automatic Control with topic in Diagnosis of Industrial Robots. I like the idea of stackexchange and would like to contribute some.</p>\\r\\n',\n",
       " '<p>Electrical and computer engineering student. Currently 2nd year.</p>\\r\\n',\n",
       " '<p>Gavin Chait is a Senior Risk Analyst and Strategist at consulting firm, Whythawk, which focuses on economic modelling, and market and investment risk analysis.</p>\\r\\n\\r\\n<p>Gavin\\'s core interests revolve around researching, analysing and writing about the frontiers of human development, particularly those of technology, and of wealth and poverty.</p>\\r\\n\\r\\n<p>Follow me at my <a href=\"http://www.whythawk.com/blog\" rel=\"nofollow\">blog</a> or <a href=\"http://twitter.com/#!/GavinChait/\" rel=\"nofollow\">@GavinChait</a></p>\\r\\n',\n",
       " '<p>Currently a PhD student at NC State University.</p>\\r\\n',\n",
       " '<p>facinated by simplicity.</p>\\r\\n',\n",
       " '<p>PHD student. Not in statistics. Trying to run a statistics blog anyway... Check it out at <a href=\"http://www.sumsar.net\" rel=\"nofollow\">http://www.sumsar.net</a> .</p>\\r\\n',\n",
       " '<p>My general StackExchange profile</p>\\r\\n\\r\\n<p><a href=\"http://stackexchange.com/users/916955/martin?tab=accounts\"><img src=\"http://stackexchange.com/users/flair/916955.png\" width=\"208\" height=\"58\" alt=\"profile for Martin on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Martin on Stack Exchange, a network of free, community-driven Q&amp;A sites\"></a></p>\\r\\n\\r\\n<p>Feel free to connect with me. <a href=\"https://plus.google.com/111471790388609536603/\" rel=\"nofollow\"><img src=\"http://www.hdicon.com/wp-content/uploads/2012/01/google_plus.png\" height=\"35\" /></a>\\r\\n<a href=\"http://il.linkedin.com/in/ramzimartinkahil\" rel=\"nofollow\">\\r\\n<img src=\"https://static.licdn.com/scds/common/u/img/webpromo/btn_viewmy_120x33.png\"></a></p>\\r\\n',\n",
       " '<p>Currently a Ph.D. student in ecology, frequently using R, Python, and ESRI software for data analysis.</p>\\r\\n',\n",
       " '<p>University of Coruña, Spain</p>\\r\\n',\n",
       " \"<p>Luckily I attended the University of Michigan before the invention of video games, so I did finish my PhD. I've been employed in the private sector my entire career, so I prefer to post under a pseudonym. I'm a data guy, rather than a math guy.</p>\\r\\n\\r\\n<p>I really like this site. Very helpful answers, very civilized tone. Thanks to those organize it and those who contribute to it.</p>\\r\\n\",\n",
       " '<p>PhD student in statistical machine learning</p>\\r\\n',\n",
       " \"<p>I'm an MS student in Statistics at the University of Minnesota.</p>\\r\\n\",\n",
       " '<p>I\\'m a ♦ moderator on Super User and research Quality of Experience for video and multimedia applications. I\\'m also a software developer with a focus on web stuff.</p>\\r\\n\\r\\n<p>If you have any questions about moderation, feel free to ping me in the <a href=\"http://chat.stackexchange.com/rooms/114/ask-a-super-user-moderator\">Ask a Super User moderator</a> chat room. For everything else, don\\'t hesitate to write me a mail to <code>slhck</code> at <code>me.com</code>.</p>\\r\\n\\r\\n<p><a href=\"http://stackexchange.com/users/f54b004d985f4ccbb6b82ad573dccb6d\">\\r\\n<img src=\"http://stackexchange.com/users/flair/f54b004d985f4ccbb6b82ad573dccb6d.png\" width=\"208\" height=\"58\" alt=\"profile for slhck on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for slhck on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\\r\\n</a></p>\\r\\n',\n",
       " '<p>I\\'m a graduate student in physics doing research in high-energy particle physics. I also have a hobby interest in computer programming.</p>\\r\\n\\r\\n<p>You can find me on <a href=\"http://twitter.com/ellipsix\" rel=\"nofollow\">Twitter</a>, or check out my <a href=\"http://www.ellipsix.net\" rel=\"nofollow\">blog and personal website</a>!</p>\\r\\n\\r\\n<p>For matters <em>not related to Stack Exchange</em>, I can be contacted by email at <strong>stack@ellipsix.net</strong>.</p>\\r\\n\\r\\n\\r\\n',\n",
       " \"<p>I'm an international economics student at Bocconi University.<br/> My passions are data crunching and quantitative finance.<br/> Former Google intern!</p>\\r\\n\",\n",
       " \"I have been always interested in programming, data analysis, statistics and probability. I have been used Gnu/Linux OS's since I started using a computer back in 2001.\",\n",
       " '<p>Purdue Statistics Ph.D Candidate</p>\\r\\n',\n",
       " \"<p>I'm tilly on Perlmonks, btilly on Hacker News, and am active in many forums.  I know Perl and math pretty well, but dabble in lots of things.</p>\\r\\n\",\n",
       " \"<p>I am a demographer and applied statistician. Been using SAS since 1998 and R since 2000. I'm currently doing work using a lot of spatial statistical methods (spatial regression and some spatial Bayesian methods) and applying them to lots of different problems in population science and public policy. I teach graduate stats courses in linear models, hazard models and spatial statistics.</p>\\r\\n\",\n",
       " '<p>I am a Analytics consultant at REDWOOD ASSOCIATES. We also provide training on Analytics to students in India.\\r\\nI am also a part time Forex Trader.</p>\\r\\n',\n",
       " '<p>Postdoc at the University of Toronto</p>\\r\\n',\n",
       " '<p>interested in applied, frequentist statistics</p>\\r\\n',\n",
       " '<p>Archaeologist</p>\\r\\n',\n",
       " '<p>I am currently a PhD student in Statistics.</p>\\r\\n',\n",
       " '<p>Professional and enthusiast currently working at <a href=\"http://www.yelp.com\" rel=\"nofollow\">Yelp</a>. As a logician, I believe anything can be made possible, though not all roads lead to tractable solutions. </p>\\r\\n\\r\\n<p>As an eclectic programmer, I\\'m not always the best person to answer your question. But, I do my best.</p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://www.linkedin.com/in/trobinso\" rel=\"nofollow\">LinkedIn Profile</a></li>\\r\\n</ul>\\r\\n',\n",
       " '<p>I ♥ Ruby!</p>\\r\\n\\r\\n<p>\\r\\nI\\'m a committer on:\\r\\n<ul><li>Ruby (MRI)</li>\\r\\n<li>Rubyspec</li>\\r\\n<li>Rubinius</li>\\r\\n</ul>\\r\\n</p>\\r\\n\\r\\n<p>Don\\'t be surprised if I refer to my <a href=\"http://github.com/marcandre/backports\" rel=\"nofollow\">extensive list of backports</a> that makes it easier to write Ruby code for different versions of Ruby.</p>\\r\\n',\n",
       " '<p><a href=\"http://stackexchange.com/users/d941154e3fa54592b111a6b8350d64c0\">\\r\\n<img src=\"http://stackexchange.com/users/flair/d941154e3fa54592b111a6b8350d64c0.png?theme=clean\" width=\"208\" height=\"58\" alt=\"profile for Mehper C. Palavuzlar on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Mehper C. Palavuzlar on Stack Exchange, a network of free, community-driven Q&amp;A sites\"> \\r\\n</a>\\r\\n<a href=\"http://mehper.tr.gg\" rel=\"nofollow\"><b>mehper.tr.gg</b></a></p>\\r\\n\\r\\n<p><b>Industrial Engineer <sup>M.Sc.</sup> </b></p>\\r\\n\\r\\n<hr/>\\r\\n\\r\\n<ul>\\r\\n<li>Author of <em><a href=\"http://rads.stackoverflow.com/amzn/click/383834555X\" rel=\"nofollow\">Distribution Planning of Magazines: A Practical Approach.</a></em></li>\\r\\n<li>Author of <em><a href=\"http://rads.stackoverflow.com/amzn/click/383836208X\" rel=\"nofollow\">Random Variate Generation If the Density Is Not Known: Basics, Methods, Implementations.</a></em></li>\\r\\n<li>Dealing with data analysis, system development and optimization.</li>\\r\\n<li>Using R, SQL, VBA languages.</li>\\r\\n<li>Tech-savvy.</li>\\r\\n<li>XBox 360 fan.</li>\\r\\n<br>\\r\\n</ul>\\r\\n',\n",
       " '<p>Developing in C++. Getting started with Python.</p>\\r\\n',\n",
       " \"<p>Technical marketing manager for MATLAB's Statistics Toolbox.</p>\\r\\n\",\n",
       " '<p>I like machine learning, functional programming, distributed computing, and probabilistic programming.</p>\\r\\n',\n",
       " '<p>I am interested in such topics: </p>\\r\\n\\r\\n<ul>\\r\\n<li>usability and accessibility of user interfaces,  </li>\\r\\n<li>design of programming languages,  </li>\\r\\n<li>natural language processing,  </li>\\r\\n<li>machine translation, </li>\\r\\n<li>data mining.</li>\\r\\n</ul>\\r\\n\\r\\n<p>Currently I am developing compiler and programming language Skila -- read more at <a href=\"http://skila.pl\" rel=\"nofollow\">skila.pl</a>.</p>\\r\\n',\n",
       " '<p>[Tools I use]<br/></p>\\r\\n\\r\\n<ul><li>UnitTest++</li>\\r\\n<li>Boost</li>\\r\\n<li>LibBeecrypt</li>\\r\\n<li>SQLite3</li>\\r\\n<li>CppSQLite</li>\\r\\n<li>stxxl( learning )</li>\\r\\n</ul>\\r\\n\\r\\n<p>[Build Tools]</p>\\r\\n\\r\\n<ul>\\r\\n<li>cmake</li>\\r\\n<li>scons</li>\\r\\n<li>make</li>\\r\\n<li>nmake</li>\\r\\n</ul>\\r\\n\\r\\n<p>[Compilers]</p>\\r\\n\\r\\n<ul>\\r\\n<li>MSVC 2012</li>\\r\\n<li>MSVC 2010</li>\\r\\n<li>MSVC 2008</li>\\r\\n<li>MSVC .NET 2005</li>\\r\\n<li>MSVC .NET 2003</li>\\r\\n<li>MSVC 6.0</li>\\r\\n<li>GCC 4.x</li>\\r\\n<li>GCC 3.3</li>\\r\\n</ul>\\r\\n',\n",
       " '<p>PhD student in particle physics</p>\\r\\n',\n",
       " '<p>Strats in a commodity trading firm in Hong Kong.</p>\\r\\n',\n",
       " '<p>I am a student of statistics. </p>\\r\\n',\n",
       " '<p>The blind leading the blind will do just fine as long as Google is available in Braille.</p>\\r\\n',\n",
       " '<p>Author of NumPy Beginner\\'s Guide <a href=\"http://www.packtpub.com/numpy-1-5-using-real-world-examples-beginners-guide/book\" rel=\"nofollow\">http://www.packtpub.com/numpy-1-5-using-real-world-examples-beginners-guide/book</a></p>\\r\\n',\n",
       " 'Long time Schemer.\\\\\\\\n\\\\\\\\n',\n",
       " '<p>Industrial researcher with expertise in biomedical signal processing and pattern recognition.</p>\\r\\n',\n",
       " '<p>Game Developper, Python fan.</p>\\r\\n',\n",
       " '<p>Just developing...</p>\\r\\n',\n",
       " '<p>Member of <a href=\"http://heal.heuristiclab.com\" rel=\"nofollow\">HEAL</a> Heuristics and Evolutionary Algorithm Laboratory in Hagenberg, Austria. We\\'re doing research on metaheuristic algorithms (Genetic Algorithm, Evolution Strategy, Simulated Annealing, Tabu Search, Variable Neighborhood Search, Particle Swarm Optimization, etc.) and application to optimization problems (Facility Layout and Assignment, Storage Location Assignment, Vehicle Routing/Pickup&amp;Delivery, Job Shop Scheduling, Data Mining).</p>\\r\\n\\r\\n<p>Co-Architect and Developer of <a href=\"http://dev.heuristiclab.com\" rel=\"nofollow\">HeuristicLab</a>.</p>\\r\\n',\n",
       " '<p>PhD student in Medical Physics, studying medical ultrasound.</p>\\r\\n',\n",
       " '<p>Novice programmer, Profesional visionary.</p>\\r\\n',\n",
       " '<p>Distance runner, analytics geek, classical music lover.</p>\\r\\n',\n",
       " '<p>I\\'m a PhD student at the <a href=\"http://www.icss.soton.ac.uk\" rel=\"nofollow\">Institute for Complex Systems Simulation</a> at the <a href=\"http://www.soton.ac.uk\" rel=\"nofollow\">University of Southampton</a>, UK studying complexity in Remote Sensing.</p>\\r\\n\\r\\n<p>As part of my work I am heavily involved in Remote Sensing and GIS technologies - particularly ENVI/IDL programming and ArcGIS scripting using Python.</p>\\r\\n\\r\\n<p>My <a href=\"http://www.rtwilson.com/academic\" rel=\"nofollow\">academic website</a> shows some examples of my work, and links to some of the <a href=\"http://www.rtwilson.com/academic/software\" rel=\"nofollow\">software I have written</a>.</p>\\r\\n',\n",
       " 'Physicist, interested in agent-based simulation and statistical physics of models inspired by economic and biological reasoning.\\\\\\\\n\\\\\\\\nProgramming in C and Python.',\n",
       " '<p>Bioinformatics student, interested in Python, .NET, Unix Systems, R, Heuristics, Machine Learning, Math, ...</p>\\r\\n',\n",
       " '<p>I teach honours statistics in a psychology department primarily using R and simulation.</p>\\r\\n',\n",
       " '<p>Medical Student </p>\\r\\n',\n",
       " '<p>Playing with data.</p>\\r\\n',\n",
       " '<p>I\\'m a <a href=\"http://www.johndcook.com/veryappliedmath.html\" rel=\"nofollow\">very applied mathematician</a> and software developer. </p>\\\\\\\\n\\\\\\\\n<p>I blog at <a href=\"http://www.johndcook.com/blog\" rel=\"nofollow\">The Endeavour</a> and I\\'m <a href=\"http://twitter.com/johndcook\" rel=\"nofollow\">@JohnDCook</a> on Twitter.</p> \\\\\\\\n\\\\\\\\nOther <a href=\"http://www.johndcook.com/contact.html\" rel=\"nofollow\">contact info</a>.',\n",
       " \"<p>“It is not that I'm so smart. But I stay with the questions much longer.”\\r\\n― Albert Einstein</p>\\r\\n\\r\\n<p>“The more I read, the more I acquire, the more certain I am that I know nothing.”\\r\\n― Voltaire</p>\\r\\n\\r\\n<p>“Study hard what interests you the most in the most undisciplined, irreverent and original manner possible.”\\r\\n― Richard P. Feynman</p>\\r\\n\\r\\n<p>“I think the big mistake in schools is trying to teach children anything, and by using fear as the basic motivation. Fear of getting failing grades, fear of not staying with your class, etc. Interest can produce learning on a scale compared to fear as a nuclear explosion to a firecracker.”\\r\\n― Stanley Kubrick</p>\\r\\n\\r\\n<p>“We are all failures- at least the best of us are.”\\r\\n― J.M. Barrie</p>\\r\\n\\r\\n<p>“Learning is not child's play; we cannot learn without pain.”\\r\\n― Aristotle</p>\\r\\n\\r\\n<p>“Learning never exhausts the mind.”\\r\\n― Leonardo da Vinci</p>\\r\\n\\r\\n<p>“I’ve seen how you can’t learn anything when you’re trying to look like the smartest person in the room.”\\r\\n― Barbara Kingsolver</p>\\r\\n\\r\\n<p>“The authority of those who teach is often an obstacle to those who want to learn.”\\r\\n― Cicero</p>\\r\\n\\r\\n<p>“Real learning comes about when the competitive spirit has ceased.”\\r\\n― Jiddu Krishnamurti</p>\\r\\n\\r\\n<p>“For the best return on your money, pour your purse into your head.”\\r\\n― Benjamin Franklin</p>\\r\\n\\r\\n<p>“The formulation of the problem is often more essential than its solution, which may be merely a matter of mathematical or experimental skill.”\\r\\n― Albert Einstein</p>\\r\\n\",\n",
       " '<p>I make software.</p>\\r\\n',\n",
       " '<p>A .NET developer with an MCSA in SQL Server 2012. I love programming in Python and learning new skills.</p>\\r\\n',\n",
       " '<p>Programmer, fanatical about scripting languages</p>\\r\\n',\n",
       " '<p>I write a puzzles blog at <a href=\"http://sett.com/puzzler-on-the-roof\" rel=\"nofollow\">http://sett.com/puzzler-on-the-roof</a></p>\\r\\n\\r\\n<p>Personal blog at <a href=\"http://hickford.github.io/\" rel=\"nofollow\">http://hickford.github.io/</a> . GitHub  <a href=\"https://github.com/hickford\" rel=\"nofollow\">https://github.com/hickford</a> </p>\\r\\n',\n",
       " '<p>I write automated trading applications in Java using the TWS API from Interactive Brokers.  I use Python and C++ for data analysis and exploration.</p>\\r\\n',\n",
       " '<p>Data Scientist at <a href=\"http://frogtek.org\" rel=\"nofollow\">Frogtek</a></p>\\r\\n\\r\\n<p>Statistician/Data Scientist. Máster en Estadística Aplicada en la U. Granada. Diplomado en Estadística en la U. Zaragoza <a href=\"http://es.linkedin.com/in/calejero\" rel=\"nofollow\">http://es.linkedin.com/in/calejero</a></p>\\r\\n\\r\\n<p>I am a statistician, specializing in the application of data mining and multivariate analysis methods to various explanatory and predictive problems. I have produced models, reports, and visualizations for many purposes, including sovereign debt, identity resolution, reputation scoring and environmental analysis.</p>\\r\\n',\n",
       " '<p>Biochemist working in prenatal screening in UK trying to formalise and expand the statistics I use daily.</p>\\r\\n',\n",
       " '<p>Find out about me on <a href=\"http://www.linkedin.com/in/jeremygollehon\" rel=\"nofollow\" title=\"LinkedIn Profile\">LinkedIn</a> and <a href=\"http://www.facebook.com/GollyJer\" rel=\"nofollow\" title=\"Facebook\">Facebook</a>.</p>\\r\\n',\n",
       " '<p>Physician, programmer, beginner statistician and data miner, fan of Above and Beyond and Ferry Corsten! Yeas ;)</p>\\r\\n',\n",
       " '<p><a href=\"http://stackoverflow.com/users/833975/matthew-kemnetz\">\\r\\n<img src=\"http://stackoverflow.com/users/flair/833975.png\" width=\"208\" height=\"58\" alt=\"profile for Matthew Kemnetz at Stack Overflow, Q&amp;A for professional and enthusiast programmers\" title=\"profile for Matthew Kemnetz at Stack Overflow, Q&amp;A for professional and enthusiast programmers\">\\r\\n</a></p>\\r\\n',\n",
       " '<p>I am a cognitive neuroscientist primarily studying spoken language and language impairments.</p>\\r\\n',\n",
       " \"I've been a software engineer for a few years. I love what I do and I'm always trying to learn more. Advice and critical discussions are welcome!\\\\\\\\n<br />\\\\\\\\nCurrently studying discrete math, multivariable calculus, algorithm design and analysis, and data structures (mainly graphs). I am also researching what I think my future field will be. I'm leaning towards soft computing/machine learning currently. Right now it's academia, but most things start out that way. I think it will be huge when enough people completely grok it.\",\n",
       " '<p>PhD in Genetics</p>\\r\\n',\n",
       " '<p>I am into building web application these days. I tend to work on the frontend where I can combine my love for great user experience with programming.</p>\\r\\n',\n",
       " \"<p>What's the answer?</p>\\r\\n\",\n",
       " '<p>Finance, law, statistics, machine learning, and programming.  Somehow this tries to all live in one place. </p>\\r\\n',\n",
       " '<p>Student in Statistics</p>\\r\\n',\n",
       " '<p>Reality is finally beginning to resemble my dream world, where there is plenty of data, storage and number crunching capacity. Now there is no excuse for going with gut feeling instead of hard data.</p>\\r\\n',\n",
       " '<p>interested in signal processing for univariate time series.</p>\\r\\n',\n",
       " '<p>Happily getting paid to learn at <a href=\"https://www.hackerschool.com\" rel=\"nofollow\">Hacker School</a>.</p>\\r\\n\\r\\n<p>Used to work at an medical imaging lab.</p>\\r\\n',\n",
       " '<p>Hi! I\\'m Joseph. </p>\\r\\n\\r\\n<p>I am a computer programmer (my <a href=\"https://github.com/jweissman\" rel=\"nofollow\">Github</a>) and thinker/writer (my working group\\'s <a href=\"http://ontology.io/\" rel=\"nofollow\">blog</a>.) </p>\\r\\n\\r\\n<p>I\\'m also one of the friendly neighborhood moderators <em>pro tempore</em> at <a href=\"http://philosophy.stackexchange.com/\">Philosophy StackExchange</a>. </p>\\r\\n\\r\\n<p><strong>Education:</strong> I graduated from Georgia College and State University in 2008 with a B.S. Degree in Computer Science. I took upper-level classes in both philosophy and mathematics, becoming so interested in the study of philosophy that I ended up taking a sufficient upper-level philosophy hours to satisfy the coursework requirements (if I had taken a bit more foreign languages and written up a thesis, I would have qualified for a B.A. in Philosophy also.)</p>\\r\\n\\r\\n<p><strong>Work:</strong> I\\'m a <a href=\"http://www.thoughtworks.com\" rel=\"nofollow\">ThoughtWorker</a>. I sometimes speak at <a href=\"http://atlanta.coderfaire.com/\" rel=\"nofollow\">conferences</a>. I\\'m also a reader for a philosophy publishing house, <a href=\"http://univocalpublishing.com/\" rel=\"nofollow\">Univocal</a>.</p>\\r\\n\\r\\n<p><strong>Play:</strong> I love moving -- walking, running, working out. I also love books and music (listening and writing -- I play guitar and am trying to learn piano.)</p>\\r\\n',\n",
       " '<p>Hey there. <br/>\\r\\n<br/>\\r\\nI\\'m in love with programming. My Projects are available at <a href=\"https://github.com/anvaka\" rel=\"nofollow\">github</a>. Yasiv is the one which I\\'m most proud of. It has multiple branches, including:</p>\\r\\n\\r\\n<p><a href=\"http://www.yasiv.com/#/Search?q=algorithms&amp;category=Books&amp;lang=US\" rel=\"nofollow\">Amazon visualization</a>, <a href=\"http://www.yasiv.com/facebook\" rel=\"nofollow\">Facebook visualization</a>, and <a href=\"http://www.yasiv.com/npm#view/browserify\" rel=\"nofollow\">npm visualization</a>.</p>\\r\\n\\r\\n<p>Feel free to drop me a line on my gmail mailbox. It\\'s named \"anvaka\" :).<br/>\\r\\n<br/>\\r\\nCheers!</p>\\r\\n',\n",
       " '<p>Just beginning with R, not programmer though!</p>\\r\\n',\n",
       " \"<p>It's time!</p>\\r\\n\\r\\n<p>I am a Microsoft employee on the app building team in Windows. The views I express on any Stack Exchange website are mine alone and not those of my employer.</p>\\r\\n\\r\\n<p>I'm also a C#, WPF, Silverlight, and WinRT enthusiast.</p>\\r\\n\",\n",
       " \"<p>Hello! When I'm not catching fishies I love to hack into the Antarctic research base's wifi, checking sexy profiles on Hatch.com and occasionally come here and answer questions.</p>\\r\\n\",\n",
       " '<p>Student in Quantitative Analysis in the Social Sciences</p>\\r\\n',\n",
       " '<p>I\\'m a former Stack Exchange employee. </p>\\r\\n\\r\\n<p>For all site support issues, please <a href=\"http://meta.stackoverflow.com/help\">contact the team</a>.</p>\\r\\n',\n",
       " '<p>Senior Survey Statistician, Abt SRBI</p>\\r\\n\\r\\n<p>Primary areas of expertise: survey statistics, structural equation and latent variable modeling, Stata programming, microeconometrics, resampling methods</p>\\r\\n\\r\\n<p>Given that I spent those freaking three minutes to fill in this profile information, I think that the users with generic <code>user0123456</code> lack respect for the community. My response to their questions is limited, and it is intentional.</p>\\r\\n\\r\\n<p>The views expressed are my own, independent of my employer.</p>\\r\\n',\n",
       " '<p>Here.</p>\\r\\n',\n",
       " '<p>I am an ecologist and tend to apply a lot of spatial stats for which I mainly use R.</p>\\r\\n',\n",
       " \"<p>I'm a Ph.D. student in Informatics Engineering @ University of Coimbra, enrolled in my third year. My research interests are mainly Failure Prediction and Fault Injection, extending to the field of Dependability, and passing through other fields as Virtualization.</p>\\r\\n\",\n",
       " '<p>Sociologist of religion at the University of Copenhagen, Institute for Regional and Cross-cultural Studies. </p>\\r\\n',\n",
       " '<p>There\\'s no such thing as \"beautiful code\"</p>\\r\\n\\r\\n<p>Beauty is in the design, the concept, the algorithm.</p>\\r\\n\\r\\n<p>The closer you approach the design, the concept, the algorithm -- the more beautiful the code, but the code itself is never beautiful.</p>\\r\\n',\n",
       " '<p>We once had a Nuclear Engineering proposal on Area 51, but it failed.  The space technology proposal might be interesting.</p>\\r\\n\\r\\n<p>My academic interests are in power plants of all types, shapes, and sizes, and how well they play with each other.  My hobbyist interest are kind of all over the place.  I like the maker movement, I\\'m most interested in the sensors and data, the \"internet of things\" kind of stuff.  In physics, I\\'m probably the most curious about general relativity topics, but my curiosities wind up all over the place.</p>\\r\\n',\n",
       " '\\\\\\\\n',\n",
       " '<p>With statistical machine learning background (PhD), also psychology background. :) Currently working in the industry. Other related interests: (functional) programming.</p>\\r\\n',\n",
       " '<p>&lt;3</p>\\r\\n',\n",
       " '<p>At work, I do a lot of python and shell scripting, and a bit of Fortran, usually involving file systems and numerical solvers.</p>\\r\\n\\r\\n<p>At home, I do a lot of C#. Usually some graphics are involved.</p>\\r\\n\\r\\n<p>I like working with python and C#, and I have more respect for Fortran than most people.</p>\\r\\n',\n",
       " '<p>Software Engineer at Google (StreetView/OCR)\\r\\n<p>Google+ <a href=\"https://plus.google.com/114601997315547801492\" rel=\"nofollow\">profile</a></p>\\r\\n',\n",
       " '<p>Coding something...</p>\\r\\n',\n",
       " '<p>I am a German <strong>computer science student</strong>, who also works as a part time web-developer with PHP and Zend Framework.</p>\\r\\n\\r\\n<p>I also blog about <strong>algorithms</strong> and other <strong>programming stuff</strong> at <a href=\"http://eliteinformatiker.de/\" rel=\"nofollow\">eliteinformatiker.de</a>. It’s mostly in German, but I also have a section with <a href=\"http://eliteinformatiker.de/category/english/\" rel=\"nofollow\">English articles</a> (which are usually those I think are especially interesting to international audience and cannot be found via Google that easy).</p>\\r\\n\\r\\n<p>At the moment I’m diving into <strong>games programming</strong> by developing a 2D Mario clone. Actually, I’m hoping that I will have a cool own 2D idea someday, but at the moment there only is one 3D idea.</p>\\r\\n',\n",
       " '<p>I am a student of psychology located in Heidelberg, Germany. I need statistics for psychological studies only on a very superficial level but I am very interested in the workings underneath. Unfortunately I struggle with mathmatical syntax a lot, which is why I will probably keep asking very basic questions for a while. Thank you for your patience.</p>\\r\\n',\n",
       " 'Comp Science Researcher',\n",
       " \"<p>Some days I'm a data scientist.  Some days I'm a software engineer.  Every day I'm a dad.</p>\\r\\n\",\n",
       " '<p>Nothing to say, I have</p>\\r\\n',\n",
       " '<p>Data Analysis Using R, SAS, SPSS</p>\\r\\n',\n",
       " '<p>@Test\\r\\n    public void me(){\\r\\n        assertEquals(me,PEOPLE.Programmer);\\r\\n    }</p>\\r\\n',\n",
       " \"<p>I'm an undergrad who's so willing to learn statistics. Help me please. Especially in Logistic regression.</p>\\r\\n\",\n",
       " '<p>Currently playing <a href=\"/questions/tagged/wfrp-3e\" class=\"post-tag\" title=\"show questions tagged \\'wfrp-3e\\'\" rel=\"tag\">wfrp-3e</a> with an amoral sell-sword. Possibly read too much Song of Ice and Fire recently.</p>\\r\\n\\r\\n<p>Looking to run a game set in Ankh-Morpork using <a href=\"/questions/tagged/fate\" class=\"post-tag\" title=\"show questions tagged \\'fate\\'\" rel=\"tag\">fate</a>.</p>\\r\\n\\r\\n<p>Got way too much time on his hands and too many opinions in his head.</p>\\r\\n',\n",
       " '<p>.Net Programmer in Mobile/Web/Desktop App.</p>\\r\\n\\r\\n<p>Dynamic person and strongly aimed in objectives achieving. \\r\\nExcellent priorities sensibility and time management. \\r\\nVery interested to increasing knowledge.</p>\\r\\n',\n",
       " \"<p>PhD in statistics from King's College Cambridge. Expertise in risk-adjusted healthcare monitoring and statistical quality control. 5yrs+ experience in applied statistics.</p>\\r\\n\",\n",
       " '<p>I am a PhD student in social psychology at the University of Colorado Boulder.</p>\\r\\n',\n",
       " '<p>I am a researcher studying communication in Twitter. \\r\\nSee <a href=\"https://sites.google.com/site/twitterresearch09/\" rel=\"nofollow\">https://sites.google.com/site/twitterresearch09/</a></p>\\r\\n',\n",
       " 'Graduate student in Physics',\n",
       " '<p>Graduate student in Computer Science at the University of Waterloo. </p>\\r\\n\\r\\n<p>Interests: Applications and experimental analysis of Machine Learning; Experimental analysis of AI techniques; Reasoning under uncertainty; Evolutionary approaches to machine learning.</p>\\r\\n',\n",
       " '<p>I live in Montreal, Quebec. I\\'m studying at l\\'ÉTS in Montréal. You can look at my <a href=\"http://careers.stackoverflow.com/olivierarteau\">careers profile</a> for more information.</p>\\r\\n\\r\\n<p>If you enjoy Javascript and you live in Montreal, join <a href=\"http://js-montreal.org/\" rel=\"nofollow\">JS Montréal</a> meetup !</p>\\r\\n\\r\\n<p><img src=\"http://projecteuler.net/profile/HoLyVieR.png\" alt=\"I also like math\"></p>\\r\\n',\n",
       " '<p>statsticsssss</p>\\r\\n',\n",
       " '<p>Msc. in Electrical Engineering.</p>\\r\\n\\r\\n<p>Interests:\\r\\nImage processing, machine learning, linear algebra</p>\\r\\n',\n",
       " '<p>A C# developer and a student in computer science in Lithuania, Vilnius</p>\\r\\n',\n",
       " '<p><code>*</code> denotes convolution, $\\\\\\\\cdot$ denotes multiplication</p>\\r\\n\\r\\n<p>I am the developer of the midasr R package: </p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://mpiktas.github.io/midasr/\" rel=\"nofollow\">http://mpiktas.github.io/midasr/</a></li>\\r\\n</ul>\\r\\n\\r\\n<p>and you cand find me on </p>\\r\\n\\r\\n<ul>\\r\\n<li><p>Google+: <a href=\"https://plus.google.com/112784314334842289710/\" rel=\"nofollow\">https://plus.google.com/112784314334842289710/</a></p></li>\\r\\n<li><p>Github: <a href=\"https://github.com/mpiktas\" rel=\"nofollow\">https://github.com/mpiktas</a> and <a href=\"https://github.com/vzemlys\" rel=\"nofollow\">https://github.com/vzemlys</a></p></li>\\r\\n</ul>\\r\\n',\n",
       " '<p>I am the left brain @ We Are Cube.³</p>\\r\\n\\r\\n<p>We create beautiful and intuitive interfaces for web and mobile.</p>\\r\\n\\r\\n<p><a href=\"http://www.wearecube.ch\" rel=\"nofollow\">http://www.wearecube.ch</a></p>\\r\\n',\n",
       " '<p>statistician at VAI</p>\\r\\n',\n",
       " '<p>I\\'m an elected moderator on <a href=\"http://scifi.stackexchange.com\">Science Fiction and Fantasy</a> and moderator pro tempore on <a href=\"http://outdoors.stackexchange.com\">The Great Outdoors</a>, and I\\'m also active on several other SE sites.   </p>\\r\\n\\r\\n<p><a href=\"http://stackexchange.com/users/484897/kevin\"><img src=\"http://stackexchange.com/users/flair/484897.png\" width=\"208\" height=\"58\" alt=\"profile for Kevin on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Kevin on Stack Exchange, a network of free, community-driven Q&amp;A sites\" /></a></p>\\r\\n\\r\\n<p>I am an iOS Software Engineer at <a href=\"http://www.fitbit.com/\" rel=\"nofollow\">Fitbit, Inc.</a> </p>\\r\\n',\n",
       " '<p>I have a PhD in economics. I mostly use Python, Maple, Matlab, Stata, and more recently R.</p>\\r\\n',\n",
       " '<p>I do bioinformatics and cancer research at University College London (UCL).</p>\\r\\n',\n",
       " '<p>Student :)</p>\\r\\n',\n",
       " '<p>I do research in theoretical and applied statistics and tackle applied problems as a consultant.</p>\\r\\n',\n",
       " '<p>Django Web Developer</p>\\r\\n',\n",
       " '<p>The economy is in fact over-expanded, particularly in railroad construction, and the weak link turns out to be the banking house of Jay Cooke and Company, which helped the U.S. Government finance the Civil War and also underwrote the construction of the Northern Pacific Railroad. Jay Cooke and Company, a large and respected banking house declares itself bankrupt, and announces its failure on September 18, 1873.. (The bank\\'s collapse precipitates the \"Panic of 1873\" and the ensuing three yea depression during which more than 10,000 businesses fail.\\xa0 \\r\\n\\xa0\\r\\n\\xa0The basic economic problems are overproduction, a declining market and deflation. Investors in Europe, where a depression is already underway, begin to call in American loans. The New York Stock Exchange closes its doors for 10 days; other businesses fail; and railroad construction is curtailed, with some railroads defaulting on their bonds. The unemployed begin to move about the country seeking jobs, and bread lines appear in the cities. The hard times drove numbers of laboring people and those in humble circumstances to the West and other portions of the country, to seek the rewards which the stagnation of business in the great commercial centre denied them.</p>\\r\\n',\n",
       " '<p>healthIT\\r\\nprotein folding</p>\\r\\n',\n",
       " '<p>Software developer specialized on image processing with a strong interest for machine learning and statistics</p>\\r\\n',\n",
       " '<p>Student at Technical University of Denamrk (DTU)</p>\\r\\n',\n",
       " '<p>SharePoint developer primarily working in the energy sector.</p>\\r\\n',\n",
       " \"I'm a data guy not a programmer. But sometimes I have to program my data into submission. I can be found on Twitter: @CMastication\",\n",
       " '<p>studying Electrical Engineering (and CS) in Columbia University.</p>\\r\\n',\n",
       " '<p>PhD researcher in Operations Research. Currently developping a VNS for computer aided composing.</p>\\r\\n',\n",
       " '<p>PhD Student in Statistics at the University of Bath. Working on statistical inference for dynamical processes. </p>\\r\\n',\n",
       " '<p>Pretty much new to everything but highly enjoy c# and asp. </p>\\r\\n',\n",
       " '<p>I am currently a Postdoctoral Fellow at the Center for Quantum Information and Control, University of New Mexico.</p>\\r\\n',\n",
       " '<p><a href=\"http://openlibrary.org/books/OL20544334M/De_cuando_en_cuando_Saturnina\" rel=\"nofollow\"><em>«Nosotros no estudiamos historia, sólo programación y tejidos andinos.»</em></a></p>\\r\\n\\r\\n<p>I sometimes write language learning software.</p>\\r\\n\\r\\n<p>I used to make <a href=\"http://ianmackinnon.co.uk/films\" rel=\"nofollow\">films</a>.</p>\\r\\n\\r\\n<p>I like to <a href=\"http://www.ravelry.com/people/ianmackinnon\" rel=\"nofollow\">knit</a>.</p>\\r\\n',\n",
       " '<p>I am a software engineer at FICO. </p>\\r\\n',\n",
       " '<p>I am a professional software engineer and a Math hon-ours student.</p>\\r\\n',\n",
       " '<p><img src=\"http://i.imgur.com/xiIeR.jpg\" alt=\"C++ DaVinci Code\"></p>\\r\\n',\n",
       " '<p>\"A brute-force solution that works is better than an elegant solution that doesn’t work.\" - Steve McConnell</p>\\r\\n',\n",
       " '<p>Evolutionary Biologist</p>\\r\\n',\n",
       " '<p>PhD candidate in computer science at Brigham Young University, studying programming languages for probabilistic modeling and inference. Current topics of interest: formal languages, foundations of mathematics, measure-theoretic probability, applied probability, topology, and computation in the limit.</p>\\r\\n\\r\\n<p>In my spare time, I learn piano arrangements of great music from video games. Current favorite composers: Yasunori Mitsuda (Chrono Trigger, Chrono Cross), and Koji Kondo and his protégés (Legend of Zelda, Super Mario). These guys are terrific.</p>\\r\\n',\n",
       " \"<p>I'm a software engineer (ruby, scala, java, hadoop, hive) at foursquare.</p>\\r\\n\",\n",
       " '<p>Master of Computer Science student</p>\\r\\n',\n",
       " '<p>Machine Learning, Computer Vision, Graphical Models, Matlab, Python, JavaScript, JQuery</p>\\r\\n',\n",
       " '<p>Graduate Student</p>\\r\\n',\n",
       " '<p>I love ice-cream.</p>\\r\\n',\n",
       " '<p>Laß die Zeit an dir ablaufen wie Wasser</p>\\r\\n',\n",
       " '<p>I am a Ph.D candidate at the Infectious Disease Epidemiology Unit, School of Population Health, University of Queensland. I am a frequent R user. I am also trying to learn Python to improve my programming skills.</p>\\r\\n',\n",
       " '<p>Currently an economics and applied math (statistics) undergraduate student. Hope to pursue a PhD in economics some day.</p>\\r\\n',\n",
       " \"<p>I'm currently working on my Masters at West Virginia University in Morgantown, WV.  My thesis-work is in Artificial Intelligence and Data Mining -- meaning I do most of my coding in Lisp, sometimes Python.</p>\\r\\n\\r\\n<p>My friends and I head up a Free Software organization at the University and spend a lot of our time staring at screens.</p>\\r\\n\",\n",
       " '<ol>\\r\\n<li>Mainly Interested in Machine Learning, Data Mining, Image Processing, Signal Processing and some interesting mathematical problems. </li>\\r\\n<li>MAC and Linux User</li>\\r\\n<li>LaTeX and XeLaTeX user</li>\\r\\n<li>Also, I play FOOTBALL!!!</li>\\r\\n</ol>\\r\\n',\n",
       " '<p>I am a final year student @ Imperial College London.</p>\\r\\n',\n",
       " '<p>Technology enthusiast, and pseudo early adopter, and takes-a-long-time-to-select-an-answer-er.</p>\\r\\n',\n",
       " 'Always happy, never blue.',\n",
       " '<p>now i am a student in UESTC, with the interest of studying image segmentation and libsvm classification .hope to be your friend!</p>\\r\\n',\n",
       " '<p>I am an economics student.</p>\\r\\n',\n",
       " '<p><a href=\"https://twitter.com/joyofdata\" rel=\"nofollow\">@joyofdata</a></p>\\r\\n\\r\\n<p><a href=\"http://www.joyofdata.de/\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/7b0yP.png\" alt=\"enter image description here\"></a></p>\\r\\n\\r\\n<p><a href=\"https://www.xing.com/profile/Raffael_Vogler\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/Q6DOD.gif\" alt=\"XING\"></a></p>\\r\\n',\n",
       " \"<p>I'm an Emeritus Prof. of Mathematics (Cal Poly Univ., Pomona) who's been researching  Seizure Prediction for several years. Right now I'm focusing on techniques involving phase-amplitude coupling of neuronal oscillations of different frequencies.</p>\\r\\n\",\n",
       " \"<p>I'm just a student who's seeking proficiency in mathematics. I'll mainly be asking question, since answering them is not within my capabilities yet.</p>\\r\\n\",\n",
       " '<p>2nd year PhD student at UT Austin</p>\\r\\n',\n",
       " '<p>When things get tough, the tough get going</p>\\r\\n',\n",
       " '<p>I am a Ph.D. student in the Electrical Engineering and Computer Science Department at MIT. </p>\\r\\n\\r\\n<p>My academic interests lie in the intersection between optimization and statistics. I am currently developing methods for data-driven decision making under uncertainty with applications in areas such as climate change, crime prediction, healthcare, revenue management and sports analytics.</p>\\r\\n\\r\\n<p>In my spare time, I play basketball, take pictures, and write reviews on Yelp.</p>\\r\\n',\n",
       " '<p>A shark on whiskey is mighty risky; a shark on beer is a beer engineer!</p>\\r\\n',\n",
       " '<p>semi-nocturnal, random dna unit</p>\\r\\n',\n",
       " '<p>Trying to adopt sane statistical strategies and use R to do it all.</p>\\r\\n',\n",
       " \"<p>I'm a dissertator specializing in social psychological interventions and quantitative methods at the University of Wisconsin-Madison Department of Psychology.</p>\\r\\n\",\n",
       " 'Chief Technical Officer at <a href=\"http://www.analyticsseo.com\" rel=\"nofollow\">Analytics SEO</a> - it\\'s a startup, which means I\\'m actually developer, architect, project manager, operations manager, tester, technical support, and everything else technical.\\\\\\\\n<br/><br/>\\\\\\\\nI specialise in Apache/MySQL/PHP development, especially using Drupal, but also have experience building high-performance high-load web applications on IIS/MSSQL/C#/ASP.Net\\\\\\\\n<br/><br/>\\\\\\\\nThere\\'s more at <a href=\"http://uk.linkedin.com/pub/mark-bennett/1/62b/3b5\" rel=\"nofollow\">LinkedIn</a>',\n",
       " '<p>about:me ;)</p>\\r\\n',\n",
       " '<p>Performing statistical genetics - mainly GWAs right now. In Nantes, France</p>\\r\\n',\n",
       " '<p>I am involved in designing and building ecommerce software.  Technology is typically from the Microsoft stack (.net, sql server etc.), but also a big fan of Lucene, ZeroMQ and other amazing tools that make my job so much easier!</p>\\r\\n',\n",
       " '<p>Work in Python, Numpy/SciPy, R, and others.</p>\\r\\n',\n",
       " \"<p>roses are red</p>\\r\\n\\r\\n<p>violets are blue</p>\\r\\n\\r\\n<p>functions don't have any color.</p>\\r\\n\",\n",
       " '<p>I am not a statistician but I am interested in learning Statistics.</p>\\r\\n',\n",
       " '<p>Contact:</p>\\r\\n\\r\\n<blockquote>\\r\\n  <p>Skype: naught101</p>\\r\\n  \\r\\n  <p>XMPP/Googletalk: naught101@jabber.org</p>\\r\\n</blockquote>\\r\\n',\n",
       " '<p>SAN and Unix admin.</p>\\r\\n\\r\\n<p>Hitachi AMS and USP systems, NetApp systems, Solaris and Linux.</p>\\r\\n',\n",
       " '<p>New to C++</p>\\r\\n',\n",
       " '<p>I own / operate Winwaed Software Technology LLC which writes and sells tools and add-ins for Microsoft MapPoint through the <a href=\"http://www.mapping-tools.com\" rel=\"nofollow\">Mapping-Tools.com</a> website. Other interests also include scientific software (especially geological/geophysical), optimization, and natural languages.</p>\\r\\n',\n",
       " '<p>Consumer of Statistics</p>\\r\\n',\n",
       " '<p>I am a 20 something Minnesota based programming enthusiast. My educational background is in mathematics, economcs and statistics, but I have always enjoyed programming and since entering the work world I have realized that you can get a lot of value from programming.</p>\\r\\n\\r\\n<p>I am pretty handy with R and VB.NET is my default \"time to program\" language. I know some C# and a few other things (reading Learn You A Haskell and doing Codeacademy), but really my programming approach is very stack overflow reliant, so I am never sure exactly how much I know vs. how much I am able to look up online in a reasonable time frame.</p>\\r\\n',\n",
       " \"<p>I'm a researcher in neurosciences</p>\\r\\n\\r\\n<p>I'm interested in all that regards pattern recognition, time series analysis, rhythms etc.</p>\\r\\n\",\n",
       " '<p>Evolutionary biologist by trade, with a sideline in teaching statistical analysis to undergraduates.</p>\\r\\n',\n",
       " '<p>I work at Quantopian, <a href=\"http://www.quantopian.com\" rel=\"nofollow\">www.quantopian.com</a>.  Quantopian is a community and a backtester for stock trading algorithms written in Python.</p>\\r\\n',\n",
       " '<blockquote>\\r\\n  <p>\"I would never die for my beliefs\\r\\n  because I might be wrong.\"</p>\\r\\n</blockquote>\\r\\n\\r\\n<p>-- Bertrand Russell</p>\\r\\n',\n",
       " '<p>Interests: Politics, data analysis, ethics, programming, and psychology as it relates to personal optimization.</p>\\r\\n\\r\\n<p>Specifics: (inter)National politics, statistics and econometrics, the proper role of a state, some Python with a preference for functional programming and less Java, and behavioral economics.</p>\\r\\n\\r\\n<p>Goals: Learn new things, save the world, and spend time with interesting people.</p>\\r\\n\\r\\n<p>Quotes:\\r\\n\"There are no limits. There are only plateaus, and you must not stay there, you must go beyond them.\" --Bruce Lee \"Cui bono?\" --Cicero</p>\\r\\n',\n",
       " '<p>Hello world!</p>\\r\\n',\n",
       " '<p>A programmer</p>\\r\\n',\n",
       " '<p><a href=\"http://gis.stackexchange.com/users/1270/dimitris\">\\r\\n<img src=\"http://gis.stackexchange.com/users/flair/1270.png\" width=\"208\" height=\"58\" alt=\"profile for dimitris at GIS, Q&amp;A for cartographers, geographers and GIS professionals\" title=\"profile for dimitris at GIS, Q&amp;A for cartographers, geographers and GIS professionals\">\\r\\n</a></p>\\r\\n',\n",
       " '<p>Student Marketing Research. Interests include: music, computers, social media (marketing), internet marketing, quantitative marketing, science, statistics and modeling, sports (table tennis), blogging, tech and mobile.</p>\\r\\n',\n",
       " '<p>Hello,</p>\\r\\n\\r\\n<p>My name is Mike Stumpf and I am a post-postgraduate living in Canada.  My main academic focus was on early modern drama, particularly Shakespeare and his contemporary playwrights, but most of my research has also been involved with the digital in one way or another.  My research blog <a href=\"http://allistrue.org/\" rel=\"nofollow\">All Is True</a> is one example.  Another is my project website, the <a href=\"http://earlymodernsandbox.com/\" rel=\"nofollow\">Early Modern Sandbox</a>, whose aim is to create a working environment to explore literature using digital tools.</p>\\r\\n\\r\\n<p>Although both of my degrees are in literature, I am searching for a position in Toronto, ON, in web development.  My passion for the digital aspect of my scholarship has evolved into a passion of its own.  I taught myself HTML, CSS, JavaScript, PHP, and MySQL during the second half of my time at university and, since graduating, I have focused more on Content Management Systems, particularly WordPress, Drupal, and Omeka.  If I had to choose one, I would say that my specialty is WordPress.</p>\\r\\n\\r\\n<p>The purpose of my <a href=\"http://mikestumpf.com/\" rel=\"nofollow\">personal website</a> is to create a personal portfolio of my work and my experiences while I am looking for a job.  I am open to contract or short-term work but I am ultimately seeking a permanent position.  If you are interested in my work or in hiring me, feel free to contact me using the \"Contact\" link on the menu above or this <a href=\"http://mikestumpf.com/contact/\" rel=\"nofollow\">link</a>.</p>\\r\\n\\r\\n<p>Cheers,</p>\\r\\n\\r\\n<p>Mike</p>\\r\\n',\n",
       " '<p>Professional geophysicst/astronomer who also writes the blog, \"<a href=\"http://pseudoastro.wordpress.com\" rel=\"nofollow\">Exposing PseudoAstronomy</a>,\" and runs the <a href=\"http://podcast.sjrdesign.net\" rel=\"nofollow\">podcast by that name</a>.</p>\\r\\n',\n",
       " '<p>master student \"biostatistics\";\\r\\nfields of interest:\\r\\n- regression modeling\\r\\n- survival analysis</p>\\r\\n',\n",
       " '<p>Engineering Manager, DevOps at <a href=\"http://conductor.com\" rel=\"nofollow\">Conductor, Inc.</a></p>\\r\\n',\n",
       " '<p>A fan of all things numerical...</p>\\r\\n',\n",
       " '<p>Quantitative finance student with background in software engineering, data bases and data mining technologies.</p>\\r\\n\\r\\n<p>Speak English, French and Russian.</p>\\r\\n',\n",
       " '<p>I am a software engineer in Pittsburgh, PA with a focus on architecture, design and development of solutions in both Java and .NET technologies.</p>\\r\\n\\r\\n<p>I recently helped a friend build a website for a business he is starting. <a href=\"http://www.championkitchens.com\" rel=\"nofollow\">http://www.championkitchens.com</a></p>\\r\\n',\n",
       " \"<p>I'm here to learn from others.</p>\\r\\n\",\n",
       " '<p>I’m currently a Lecturer in Economics in the Department of Management at King’s College London, which I joined in September 2007. I have an PhD in Econometrics from Imperial College London. I’m a Bayesian and my research is focused broadly in econometrics applied to public policy but with a particular interest in spatial econometrics. I use R and Python for my research and SPSS, Stata and WinBUGS for teaching methods.</p>\\r\\n',\n",
       " 'Mind the gap.',\n",
       " '<p>I am a web developer based in Sherbrooke, QC, Canada.  I have been doing Rails development since it\\'s 0.13 days, or 2005.</p>\\r\\n\\r\\n<p>I currently do big data analysis using <a href=\"http://www.postgresql.org/\" rel=\"nofollow\">PostgreSQL</a> for <a href=\"http://seevibes.com/\" rel=\"nofollow\">Seevibes</a>.</p>\\r\\n',\n",
       " '<p>Assistant professor at Eindhoven University of Technology and software evolution fan</p>\\r\\n',\n",
       " '<p>Yet another programmer...</p>\\r\\n',\n",
       " \"<p>I'm a polymath into engineering, software design, simulation, search and rescue, building things, cooking....</p>\\r\\n\",\n",
       " '<p>SSL isn\\'t good enough.  Your website can be hacked. <br>\\r\\nHelp solve the problem by advocating these RFCs:<br>\\r\\n<br>\\r\\n<a href=\"http://tools.ietf.org/html/rfc6698\" rel=\"nofollow\">TLSA (formerly DANE for DNS)</a> Fixes the hackable CA problem<br></p>\\r\\n\\r\\n<p><a href=\"http://www.browserauth.net/\" rel=\"nofollow\">TLS-OBC:</a> Fixes TLS, and the Related Domain Cookie Attack<br> </p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p><strong>About me</strong><br>\\r\\nI have no relation to the above sites; I am just an advocate<br>\\r\\n<br>\\r\\nWhy \"makerofthings7\"?  It\\'s a challenge to <i>\"make seven things in my life of significant quality and value\"</i>.   Who knows if those things will take the form of software, art, or people.  (I\\'m not married, no kids yet)\\r\\n<br>\\r\\n<br>See <a href=\"http://linkedin.com/in/makerofthings\" rel=\"nofollow\">...my LinkedIn profile</a></p>\\r\\n',\n",
       " '<p>I am a php developer, mostly using cakephp and mysql. have also played with many other languages</p>\\r\\n',\n",
       " '<p>Nothing out of the ordinary.</p>\\r\\n',\n",
       " '<p>Psychiatrist, Trickcyclist, Marathoner</p>\\r\\n',\n",
       " '<p>PSU Alumni!!  Woot.</p>\\r\\n',\n",
       " '<p>Statistics researcher</p>\\r\\n',\n",
       " '<p>I am a programmer.</p>\\r\\n',\n",
       " '<p>I am a software engineer from Télécom ParisTech, working in mobile/web apps and image processing. I speak Python, C, JavaScript and Scala, spiced up with a little bit of Vim for good measure!</p>\\r\\n',\n",
       " 'Wall street .net developer. Interested in WPF, PowerShell, LINQ, F#, MSBuild, and deployment',\n",
       " '<p>Trying to learn statistics, haphazardly.</p>\\r\\n',\n",
       " '<p>Principal - Game Analytics @ Activision</p>\\r\\n',\n",
       " \"<p>I have several years experience in business and analysis, using tools such as Excel, Access, SQL, as well as BI tools. I'm learning predictive modeling and how to use R. I begin about a year ago with both.  My goals are to more accurately predict who will retire or voluntarily resign from my organization. I am interested in both accuracy at the aggrregate level (total number of retirements or resignations by various demographic attrributes such as department and job type) and at the individual level (probability that a person will retire or resign).</p>\\r\\n\",\n",
       " '<p>I am interested in applied statistics and machine learning.</p>\\r\\n',\n",
       " '<p>Aspiring Renaissance man.</p>\\r\\n',\n",
       " '<p><img src=\"http://i.stack.imgur.com/goQvn.png\" alt=\"enter image description here\"></p>\\r\\n',\n",
       " '<p>ex-physicist turned data scientist in chicago.</p>\\r\\n',\n",
       " '<p>I am a PhD candidate at University of Cambridge &amp; the MRC Laboratory of Molecular Biology, and a Gates Cambridge scholar. My academic interest focuses on systems biology, molecular biology, data mining and biostatistics.</p>\\r\\n',\n",
       " 'unix sysadmin and general geek',\n",
       " '<p>Spring, Groovy, and JavaScript web applications for science data. </p>\\r\\n\\r\\n<p>Interested in NoSQL, Scala, interactive data, and mobile development.</p>\\r\\n',\n",
       " '<p><a href=\"http://twitter.com/yaaang\">@yaaang</a></p><p><a href=\"http://profiles.google.com/yaaang\">google</a></p><p><a href=\"http://www.linkedin.com/profile/view?id=5706061\">linkedin</a></p>',\n",
       " '<p>I\\'m a data scientist at <a href=\"https://www.highmark.com/\" rel=\"nofollow\">Highmark, Inc</a>, working as the manager of predictive analystics and data management within Operations. I earned my PhD doing computational neuroscience, working with decision making processes and investigating functional connectivity between different parts of the human brain. I\\'ve also worked as a financial quant, freelance web developer, and IT consultant. One of my favorite pastimes is <a href=\"http://www.playpoi.com/why-play-poi\" rel=\"nofollow\">poi spinning</a>.</p>\\r\\n\\r\\n<p>You can learn more about me on my <a href=\"http://www.linkedin.com/in/eykanal\" rel=\"nofollow\">LinkedIn page</a>. Feel free to contact me about anything and everything at eykanal at erikdev dot com.</p>\\r\\n',\n",
       " '<p>Undergrad major in physics, (switched to biophysics), medical school, family medicine, Masters in Public Health, and last 15 years in life insurance. Learned R on my own and have used it daily for last 6+years.</p>\\r\\n',\n",
       " '<p>Future neuroscientist:)</p>\\r\\n',\n",
       " '<p>founder and technical visionary of <a href=\"http://www.thenativeweb.io\" rel=\"nofollow\">@thenativeweb_io</a>. loves javascript, node.js, apples and raspberries. favors unix and the shell. spreads knowledge.</p>\\r\\n',\n",
       " '<p>UK computer networking student running several websites</p>\\r\\n\\r\\n<p><a href=\"http://www.facetimedirectory.net\" rel=\"nofollow\">Facetime Social Network</a></p>\\r\\n\\r\\n<p><a href=\"http://www.mintuz.co.uk\" rel=\"nofollow\">UK Web designer</a></p>\\r\\n\\r\\n<p>I am learning in my spare time objective-c cocoa touch and c#. Planning to make my own app dev company.</p>\\r\\n',\n",
       " '<p>BSc Statistics (Hons), First Class - 2007</p>\\r\\n\\r\\n<p>MSc Biostatistics &amp; Epidemiology - 2009</p>\\r\\n\\r\\n<p>I currently work for a Canadian university as a biostatistical consultant within the faculties of medicine and nursing. I very much enjoy all thinigs related to epidemiology and statistics (except power calculations).</p>\\r\\n',\n",
       " '<h1>Summary</h1>\\r\\n\\r\\n<p>From 2002 to 2012, I was part-owner of a startup ( Semantix S.A. ), in the role of the company\\'s Senior Software Engineer and Technical Lead. The company was acquired by Neuropublic S.A in June 2012.</p>\\r\\n\\r\\n<h1>Tech</h1>\\r\\n\\r\\n<ul>\\r\\n<li>I mostly code in <a href=\"http://users.softlab.ntua.gr/~ttsiod/pythonFlattenDictionaries.html\" rel=\"nofollow\">Python</a> and C/<a href=\"http://users.softlab.ntua.gr/~ttsiod/cpp.html\" rel=\"nofollow\">C++</a>, targeting Linux, Windows and <a href=\"http://users.softlab.ntua.gr/~ttsiod/stackusage.html\" rel=\"nofollow\">embedded</a> development. In some of my projects I had to optimize for speed using <a href=\"http://users.softlab.ntua.gr/~ttsiod/straylight.html\" rel=\"nofollow\">CUDA</a> and <a href=\"http://users.softlab.ntua.gr/~ttsiod/renderer.html\" rel=\"nofollow\">OpenMP/TBB</a>. I can also code in <a href=\"http://users.softlab.ntua.gr/~ttsiod/mandelSSE.html\" rel=\"nofollow\">x86/SSE asm</a> if necessary.</li>\\r\\n<li>Scripting: Python mostly ; <a href=\"http://users.softlab.ntua.gr/~ttsiod/gnuplotStreaming.html\" rel=\"nofollow\">Perl</a> in the past; daily one-liners with bash/awk/sed. </li>\\r\\n<li>SQL-wise: Oracle, MySQL and PostgreSQL (I\\'ve also written native apps over direct APIs: OCI for Oracle, psycopg2 for PostgreSQL).</li>\\r\\n<li>I begun my career 11 years ago, successfully coding Windows device drivers for 7 different FPGA designs that stress-tested Siemens 3G switches.</li>\\r\\n<li><a href=\"http://users.softlab.ntua.gr/~ttsiod/myvim.html\" rel=\"nofollow\">I mostly use VIM these days</a> but have no fear of IDEs (that\\'s where I begun). That being said, I prefer <a href=\"http://users.softlab.ntua.gr/~ttsiod/makefile.html\" rel=\"nofollow\">Makefiles</a> (recently, tup) to Eclipse-sized monsters.</li>\\r\\n<li>I value strong type systems and functional-style thinking (<a href=\"http://users.softlab.ntua.gr/~ttsiod/score4.html\" rel=\"nofollow\">OCaml</a>/F#). I am not an extremist in this regard, sometimes mutable state is the way to go (translation: I think <a href=\"http://stackoverflow.com/questions/3884121/haskell-function-application-and-currying\">Haskell</a> takes it too far).</li>\\r\\n<li>I love ZFS - in general, data checksums in the filesystem. When they are not there, I use <a href=\"http://users.softlab.ntua.gr/~ttsiod/rsbep.html\" rel=\"nofollow\">my own</a>.</li>\\r\\n<li>When my work in my startup demanded it, I skimmed over C#, Java and <a href=\"http://users.softlab.ntua.gr/~ttsiod/win32backup.html\" rel=\"nofollow\">Windows administration</a>.</li>\\r\\n</ul>\\r\\n\\r\\n<h1>Currently working on...</h1>\\r\\n\\r\\n<ul>\\r\\n<li>Frontend: AngularJS + TypeScript (coding my own Angular directives for CRUD apps for NeuroPublic)</li>\\r\\n<li>Backend: SVN/GIT + Hudson + custom Python scripting for clustered JBoss + Python/requests to stress test. Details about all in my CV.</li>\\r\\n</ul>\\r\\n\\r\\n<h1>My programming/admin blog (Slashdotted, Reddit-ed, HN-ed, etc)</h1>\\r\\n\\r\\n<p><a href=\"http://users.softlab.ntua.gr/~ttsiod/index.html\" rel=\"nofollow\">Here</a> ( mirrored on: <a href=\"http://ttsiodras.github.com/\" rel=\"nofollow\">http://ttsiodras.github.com/</a> ).</p>\\r\\n\\r\\n<p>My CV is here: <a href=\"http://users.softlab.ntua.gr/~ttsiod/cv.pdf\" rel=\"nofollow\">http://users.softlab.ntua.gr/~ttsiod/cv.pdf</a>\\r\\n( mirrored on: <a href=\"http://ttsiodras.github.com/cv.pdf\" rel=\"nofollow\">http://ttsiodras.github.com/cv.pdf</a> )</p>\\r\\n',\n",
       " '<p>Expert chess player</p>\\r\\n\\r\\n<p>Model UN president</p>\\r\\n\\r\\n<p>B.S./M.S. Computer Science, 2015</p>\\r\\n\\r\\n<p>Case Western Reserve University</p>\\r\\n\\r\\n<p><a href=\"http://stackexchange.com/users/954104\">\\r\\n<img src=\"http://stackexchange.com/users/flair/954104.png\" width=\"208\" height=\"58\" alt=\"profile for Andrew Latham on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Andrew Latham on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\\r\\n</a></p>\\r\\n',\n",
       " \"<p>I'm a analytics guy for a financial company.  Passionate about new technology.  Trying to simultaneously learn 5 different programming languages and failing at all of them...</p>\\r\\n\",\n",
       " '<p>Systems biologist.</p>\\r\\n',\n",
       " 'Erstwhile programmer turned experimental scientist. Now spend more time fiddling with cells and microscopes than coding, but still occasionally keep a hand in.',\n",
       " '<ul>\\r\\n<li>va san diego hsr&amp;d</li>\\r\\n<li>head of research > statigrafix &lt;</li>\\r\\n</ul>\\r\\n',\n",
       " '<p>I think the saying would go something like \"jack of all trades, master of none\". Have worked in PHP/Code Igniter, VB, C#, VBA, Silverlight, Javascript/jQuery, Azure, MySQL, SQL.... </p>\\r\\n\\r\\n<p>Former freelance dev, now w2 employed but with most of the same perks of freelancing (schedule, hours, work remote, etc). Work currently in primarily C#/ASP.NET, focus on front-end dev - html/css/javascript/jQuery. Pretty strong background in MVVM-based Silverlight.</p>\\r\\n',\n",
       " '<p>Statistical consultant</p>\\r\\n',\n",
       " '<p>Jack of a few trades, but (surely) master of none!</p>\\r\\n',\n",
       " 'Renewable Energy Analyst',\n",
       " '<p>I am a PhD student from Queensland University of Technology (QUT) at the beautiful Brisbane City, Australia. My research interest is on social media analytics and text mining. My personal interest on programming span across C#, Python, Ruby and HTML5, CSS3 web programming. I am also a Microsoft Certified Application Developer for .NET and Microsoft Office Expert World Champion on Microsoft Excel and National Champion for Microsoft Word.</p>\\r\\n',\n",
       " '<p>Computationally Intensive ...</p>\\r\\n',\n",
       " '<p>Web Developer at PetRelocation.com, specializing in PHP, mySQL, jQuery, javascript and the CodeIgniter framework. Currently polishing my skills at HTML5 and CSS3, and Android/Java development. @terrymatula on the Twitters.</p>\\r\\n',\n",
       " '<p>Formally I am educated within business and economics and I work with marketing as my main profession. I have also worked with WordPress development on and off since january 2012. </p>\\r\\n\\r\\n<p>I believe continued education within a diverse range of skills is important. New ideas and technologies emerge from cross disciplinary people. Especially marketing (in the business world) needs to be more innovative.</p>\\r\\n\\r\\n<p>Please support the new Marketing Q&amp;A site:<br>\\r\\n<a href=\"http://area51.stackexchange.com/proposals/51786/marketing?referrer=ZPyQhV_KHaLjvaTn2LelVw2\"><img src=\"http://area51.stackexchange.com/ads/proposal/51786.png\" width=\"220\" height=\"250\" alt=\"Stack Exchange Q&amp;A site proposal: Marketing\" /></a><br></p>\\r\\n',\n",
       " \"<p>I'm a scientist who, my necessity, had to learn how to program.</p>\\r\\n\",\n",
       " '<p>Statistician,Machine Learning,Data Mining,R,Perl.\\r\\nI maintain a blog listing out probability puzzles with the intention of spreading a simplistic understanding of the science of probability.</p>\\r\\n',\n",
       " '<p>William A. Yarberry, Jr., CPA, CISA, is President, ICCM Consulting LLC, based in Houston, Texas.  His practice is focused on IT governance, Sarbanes-Oxley compliance, security consulting and business analytics for cost management.  He was previously a senior manager with PricewaterhouseCoopers, responsible for telecom and network services in the Southwest region.  Yarberry has more than 30 years experience in a variety of IT-related services, including application development, internal audit management, outsourcing administration and Sarbanes-Oxley consulting.\\r\\nHis books include The Effective CIO (co-authored), Computer Telephony Integration and Telecommunications Cost Management.  A new book, The CIO Handbook, will be published in 2012. In addition he has written over 20 professional articles on topics ranging from wireless security to change management.  One of his articles, “Audit Rights in an Outsource Environment,” received the Institute of Internal Auditors Outstanding Contributor Award.  The Effective CIO was selected for read of the month by the South African chapter of the Institute of Directors.<br>\\r\\nPrior to joining PricewaterhouseCoopers, Yarberry was director of Telephony Services for Enron Corporation.  He was responsible for operations, planning, and architectural design for voice communications servers and related systems for more than 7,000 employees.  Yarberry graduated Phi Beta Kappa in Chemistry from the University of Tennessee and earned an MBA at the University of Memphis.  He enjoys reading history, swimming, occasional scuba diving and spending time with family.<br>\\r\\n![enter image description here][1]</p>\\r\\n',\n",
       " '<p>22 Aug 2013: A Level results........\\r\\nMathematics: A*,\\r\\nFurther Mathematics: A*, \\r\\nPhysics: A*, \\r\\nBiology: A*</p>\\r\\n\\r\\n<p><a href=\"http://stackexchange.com/users/1445301\">\\r\\n<img src=\"http://stackexchange.com/users/flair/1445301.png?\" width=\"208\" height=\"58\" alt=\"profile for Joe King on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Joe King on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\\r\\n</a></p>\\r\\n',\n",
       " '<p>Everyone needs a hobby.</p>\\r\\n',\n",
       " '<p>I am a financial and monetary economist.</p>\\r\\n',\n",
       " '<p>Co-founder: <a href=\"http://www.discourse.org\" rel=\"nofollow\">http://www.discourse.org</a> </p>\\r\\n\\r\\n<p>email: sam.saffron@gmail.com<br>\\r\\nblog: <a href=\"http://samsaffron.com\" rel=\"nofollow\">http://samsaffron.com</a><br>\\r\\ntwitter: <a href=\"https://twitter.com/@samsaffron\" rel=\"nofollow\">@samsaffron</a>   </p>\\r\\n\\r\\n<p>Ex <a href=\"http://blog.stackoverflow.com/2010/06/welcome-stack-overflow-valued-associate-00008/\">Stack Overflow Valued Associate #00008</a>, creator of <a href=\"http://data.stackexchange.com/\" rel=\"nofollow\">Stack Exchange Data Explorer</a></p>\\r\\n\\r\\n<p>Other projects: </p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://community.mediabrowser.tv\" rel=\"nofollow\">Community Tracker</a>,  a system inspired by Stack Overflow to manage your bugs, feature requests and customer support.</li>\\r\\n<li><a href=\"http://www.mediabrowser.tv/\" rel=\"nofollow\">Media Browser</a>, an open source plugin for Windows Media Center. I makes browsing through your media awesome. </li>\\r\\n</ul>\\r\\n\\r\\n<p>\\r\\nAll original source snippets I post on Stack Overflow are dedicated to the <a href=\"http://en.wikipedia.org/wiki/Public_Domain\" rel=\"nofollow\">public domain</a>. Do with them as you see fit. \\r\\n</p>\\r\\n',\n",
       " '<p>I like programming, language, and collecting hobbies.</p>\\r\\n\\r\\n<ul>\\r\\n<li><p><a href=\"http://evincarofautumn.blogspot.com/\" rel=\"nofollow\">My blog</a> is a pretty good blog.</p></li>\\r\\n<li><p>I am working on <a href=\"https://github.com/evincarofautumn/kitten\" rel=\"nofollow\">a programming language called Kitten</a> which you may like.</p></li>\\r\\n</ul>\\r\\n',\n",
       " '<ul>\\r\\n<li>PhD in CS/AI/Machine Learning/Cognitive Modeling from UIUC</li>\\r\\n<li>Post-doctoral work in Computational Cognitive Science at Columbia, UConn, NYU</li>\\r\\n<li>Statistical Data Scientist in private industry</li>\\r\\n</ul>\\r\\n\\r\\n<p>Twitter: @harlanh</p>\\r\\n',\n",
       " '<p>#samilosoi<BR></p>\\r\\n\\r\\n<p><a href=\"http://stackexchange.com/users/22329\">\\r\\n<img src=\"http://stackexchange.com/users/flair/22329.png\" width=\"208\" height=\"58\" alt=\"profile for Masi on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Masi on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\\r\\n</a></p>\\r\\n',\n",
       " \"<p>I'm a computer security researcher. I have programming skills in C/C++, Java, Objective-C and PHP.</p>\\r\\n\",\n",
       " '<p>I am a web developer using PHP/MySQL/jQuery as primary tools. Most projects I am realizing with the CMS Drupal, but I am also working with CodeIgniter and creating custom frameworks.</p>\\r\\n',\n",
       " \"<p>I'm a graduate student at New York University with interest in Machine Learning, Natural Language Processing, Deep Learning and I use python to do all my stuff.</p>\\r\\n\",\n",
       " '<p>I am an entomologist/ecologist studying at U.C.C. Ireland. I am currently completing my research masters on saproxylic Diptera.</p>\\r\\n',\n",
       " '<p>On one hand, my education is in Economics. On the other, I love programming.\\r\\nCombining both is beyond self-inspiring.</p>\\r\\n',\n",
       " \"<p>Hi, I'm Tim Jurka. I'm a researcher at the University of California, Davis, where I use methods from computer science, statistics, and population ecology to understand political systems.</p>\\r\\n\",\n",
       " '<p>Doctoral Candidate in Cognitive Psychology and Cognitive Science at Indiana University (Bloomington)</p>\\r\\n',\n",
       " \"<p>I'm a Post-Doctoral Fellow in atmospheric remote sensing at the University of Toronto in Toronto, Ontario, Canada.  I first came to Stack Exchange for practical reasons: Tex.SE has been of major help when I wrote my licentiate (midterm) thesis.  Since then, I have discovered the joy of many websites.  As my network profile will show, I'm interested in travel, outdoor, scientific skepticism, and as I work in academia, also in academia and LaTeX.</p>\\r\\n\",\n",
       " '<p>Professor in the Department of Biostatistics and Medical Informatics at the University of Wisconsin-Madison; research in statistical genetics.</p>\\r\\n',\n",
       " '<ul>\\r\\n<li><a href=\"http://stackoverflow.com/questions/7540227/strategies-for-simplifying-math-expressions/7542438#7542438\">Strategies for simplifying math expressions</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/7478322/why-is-air-nativeprocess-not-supported/7478416#7478416\">Why is AIR NativeProcess not supported?</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/6671405/flex-documentation-what-do-the-experts-use/6671462#6671462\">Flex documentation: what do the experts use?</a></li>\\r\\n</ul>\\r\\n\\r\\n<p>Member of OWASP. Join in too! <a href=\"https://www.owasp.org/\" rel=\"nofollow\">https://www.owasp.org/</a></p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://de.slideshare.net/ApEts/20130212-phpugsession\" rel=\"nofollow\">Alles in die Session, nix im Browser</a></li>\\r\\n</ul>\\r\\n\\r\\n<p>I declare</p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://www.codinghorror.com/blog/2012/07/new-programming-jargon.html\" rel=\"nofollow\">YODA comparisons</a> aren\\'t evil!</li>\\r\\n<li>Use assertions to permanently check the integrity of your code<a href=\"http://php.net/manual/en/function.assert.php\" rel=\"nofollow\"> [PHP</a>, <a href=\"http://docs.oracle.com/javase/1.4.2/docs/guide/lang/assert.html\" rel=\"nofollow\">Java</a> and <a href=\"http://php.net/manual/en/function.assert.php\" rel=\"nofollow\">ObjC]</a>!</li>\\r\\n<li>ZCE PHP 5 since 2009</li>\\r\\n</ul>\\r\\n',\n",
       " '<p>I just finished a Ph.D. in Applied Economics at Clemson University (May 2013 graduation). My primary skills are statistical modeling, machine learning, and programming. I\\'m excited about future potential in this area!</p>\\r\\n\\r\\n<p>I recently started an R video blog: <a href=\"http://vimeo.com/user19035707/videos\" rel=\"nofollow\">vimeo.com/user19035707/videos</a>. Check it out!</p>\\r\\n',\n",
       " '<p>Into biology, information sharing, and education...</p>\\r\\n',\n",
       " '<p>I am a zoologist interested in finding evolutionary relationships among organizational levels. Currently \"post-docking\" at the University of São Paulo.</p>\\r\\n',\n",
       " '<p>I’m a <a href=\"http://stackoverflow.com/q/104355/1968\">bioinformatics</a> PhD student at <a href=\"http://www.ebi.ac.uk/\" rel=\"nofollow\">EMBL-EBI</a> and the <a href=\"http://www.cam.ac.uk/\" rel=\"nofollow\">University of Cambridge</a> but I’m originally from <a href=\"http://en.wikipedia.org/wiki/Berlin\" rel=\"nofollow\">Berlin</a>.</p>\\r\\n\\r\\n<p>I’m mainly working on genomics using next-generation sequencing data.\\r\\nMy current thesis project is about the regulation of tRNA expression in mammals.</p>\\r\\n\\r\\n<p>Here’s my …</p>\\r\\n\\r\\n<p><strong><a href=\"http://twitter.com/klmr\" rel=\"nofollow\"><img src=\"http://dl.dropbox.com/u/6101039/profiles/twitter-pic.png\" alt=\"twitter-pic\"> Twitter</a></strong> account<br>\\r\\n<strong><a href=\"http://gplus.to/klmr\" rel=\"nofollow\"><img src=\"http://dl.dropbox.com/u/6101039/profiles/g%2B-pic.png\" alt=\"g+-pic\"> Google+</a></strong> account<br>\\r\\n<strong><a href=\"https://github.com/klmr\" rel=\"nofollow\"><img src=\"http://www.google.com/profiles/c/favicons?domain=github.com\" alt=\"github-pic\"> Github</a></strong> account<br>\\r\\n<strong><a href=\"http://careers.stackoverflow.com/klmr\"><img src=\"http://www.google.com/profiles/c/favicons?domain=careers.stackoverflow.com\" alt=\"cv-pic\"> Resume</a></strong>  </p>\\r\\n',\n",
       " '<p>Statistician,</p>\\r\\n',\n",
       " '<p>Top-50 in the Netflix Prize.</p>\\r\\n',\n",
       " 'Neuroscience PhD candidate.',\n",
       " '<p>Cognitive psychology and cognitive neuroscience.</p>\\r\\n',\n",
       " '<p>I\\'m a hobbyist programmer, part-time sysadmin, and full-time data center management, automation, and cloud computing architect and delivery engineer.</p>\\r\\n\\r\\n<p>I have experience with myriad editors, shells, and OSes. While I have my [strong] preferences, am pretty much beyond the religious wars of \"what\\'s best\" when it comes to platform, editor, or toolset: if it works for the problem at hand, and you know it, it\\'s [probably] worthwhile :)</p>\\r\\n',\n",
       " '<p>I am currently working towards a PhD in microbial genetics. Before that I spent a bit over a decade working as a programmer, with most of that time spent writing computer games. I have degrees in Mathematics (MMath), Life Sciences (BSc) and Molecular Genetics (MSc).</p>\\r\\n',\n",
       " '<p>Researcher in the biomedical field</p>\\r\\n',\n",
       " '<p>independent sales analyst</p>\\r\\n',\n",
       " '<p>PhD student (CS)  (machine learning, graph analysis, python, mysql, java) widh web development background (C#, Access, JavaScript, jQuery)</p>\\r\\n',\n",
       " '<p>mad scientist</p>\\r\\n',\n",
       " '<p>PhD structural biology. I do bioinformatics for Affymetrix in the SF bay area. Interested in systems / synthetic biology, expression analysis and machine learning.</p>\\r\\n\\r\\n<p>Working with a Biotech Accelerator in the East Bay.  <a href=\"http://www.meetup.com/Berkeley-Biolabs/\" rel=\"nofollow\">http://www.meetup.com/Berkeley-Biolabs/</a></p>\\r\\n\\r\\n<p>I believe that biology and other stackexchange sites should welcome newbies and encourage dialog about science, evolution, technology and etc.</p>\\r\\n',\n",
       " \"<p>A Master's student in Conservation Biology. Professionally a zookeeper, specialising in herptiles and birds, although have worked with a number of taxa. I am also a graphic designer and illustrator. </p>\\r\\n\",\n",
       " '<p>Astrophysics grad student</p>\\r\\n',\n",
       " '<p>Spoken Languages: English/Spanish/Japanese</p>\\r\\n',\n",
       " '<p>Java developer</p>\\r\\n\\r\\n<p><a href=\"https://github.com/isopov\" rel=\"nofollow\">On Github</a></p>\\r\\n',\n",
       " '<p>AI graduate student, university teaching assistant and programmer.<br>\\r\\nInterested in machine learning, multi-agent systems, computer vision, neuroinformatics.</p>\\r\\n\\r\\n<p>Languages: <a href=\"/questions/tagged/python\" class=\"post-tag\" title=\"show questions tagged \\'python\\'\" rel=\"tag\">python</a>, <a href=\"/questions/tagged/java\" class=\"post-tag\" title=\"show questions tagged \\'java\\'\" rel=\"tag\">java</a>, <a href=\"/questions/tagged/matlab\" class=\"post-tag\" title=\"show questions tagged \\'matlab\\'\" rel=\"tag\">matlab</a>, <a href=\"/questions/tagged/prolog\" class=\"post-tag\" title=\"show questions tagged \\'prolog\\'\" rel=\"tag\">prolog</a><br>\\r\\nLearning or want to learn: <a href=\"/questions/tagged/c%2b%2b\" class=\"post-tag\" title=\"show questions tagged \\'c++\\'\" rel=\"tag\">c++</a>, <a href=\"/questions/tagged/php\" class=\"post-tag\" title=\"show questions tagged \\'php\\'\" rel=\"tag\">php</a>, <a href=\"/questions/tagged/lua\" class=\"post-tag\" title=\"show questions tagged \\'lua\\'\" rel=\"tag\">lua</a>.</p>\\r\\n',\n",
       " '<p><a href=\"http://stackexchange.com/users/b54fca00a2634dd28fc53c5248dc9db6\">\\r\\n<img src=\"http://stackexchange.com/users/flair/b54fca00a2634dd28fc53c5248dc9db6.png?theme=clean\" width=\"208\" height=\"58\" alt=\"profile for troyaner on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for troyaner on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\\r\\n</a></p>\\r\\n',\n",
       " '<p>Doctoral Student, CISE, University of Florida. </p>\\r\\n',\n",
       " '<p>Solving problems for fun and for profit since the last century.</p>\\r\\n',\n",
       " '<p>Doing PhD in Electrical Engineering</p>\\r\\n',\n",
       " '<p>An economics Phd student dealing with empirical and statistical issues daily. </p>\\r\\n',\n",
       " '<p>Data Scientist</p>\\r\\n',\n",
       " \"<p>Whew, About me ? In this itty bitty box ?  That's a tough one !</p>\\r\\n\\r\\n<p>Let's see, Full time Husband and Dad to an Awesome and fabulous wife and two cool young daughters.  Part time computer geek.  On the geek side I'm currently obsessing about R, Sweave, Latex, Kubuntu, MythTV, Photography, Gimp, DigiKam among other things....</p>\\r\\n\",\n",
       " '<p>Work in analytical CRM / database marketing.</p>\\r\\n',\n",
       " \"<p>I love to learn more about Statistics! I'm an economist so my statistics background is not too well (though we have a LOT of statistics subjects back in the university) :)</p>\\r\\n\\r\\n<p>Nice to be here! </p>\\r\\n\",\n",
       " '<p>please delete me</p>\\r\\n',\n",
       " '<p>PhD student studying ecology and evolutionary biology. Use generalised linear mixed modeling in R.</p>\\r\\n',\n",
       " '<p>Researcher in Statistics and Machine Learning.</p>\\r\\n',\n",
       " '<p>I am a software engineer at Google.  I like machine learning, python, and algorithms.</p>\\r\\n\\r\\n<p>I am a sucker for analyzing game log data for games that I love.  I run <a href=\"http://rftgstats.com\" rel=\"nofollow\">RFTGStats.com</a> and <a href=\"http://councilroom.com\" rel=\"nofollow\">CouncilRoom.com: Dominion Statistics</a>.</p>\\r\\n\\r\\n<p><a href=\"http://www.linkedin.com/pub/robert-renaud/4/108/59\" rel=\"nofollow\">My LinkedIn profile</a>.</p>\\r\\n',\n",
       " '<p>Find me on Twitter @endy_tj</p>\\r\\n',\n",
       " '<p>Ph.D. Candidate in Computer Science, Brown University.</p>\\r\\n',\n",
       " '<ul>\\r\\n<li>Teacher and graduate student at University of Jyväskylä, Department of Mathematical Information Technology, with special interest in matters pertaining to programming languages.</li>\\r\\n<li>Experience in real-world programming with at least C, C++, Java, and Haskell.</li>\\r\\n<li>Debian Developer since 1999</li>\\r\\n</ul>\\r\\n',\n",
       " '<p>Software engineer.</p>\\r\\n',\n",
       " '<p>My name is Chew Kai Feng, currently working in a project, called <a href=\"http://cloudst.at\" rel=\"nofollow\">CloudStat</a>. CloudStat is a web-based statistical platform that leverage R Language to allow researchers to analyze data collaboratively in the cloud with high performance infrastructure. </p>\\r\\n',\n",
       " \"<p>I'm a Developer Relations Engineer @ Google, currently working with the Google BigQuery team (https://developers.google.com/bigquery/).</p>\\r\\n\\r\\n<p>Outside of work, I'm a technologist, traveler, writer, optimist and information addict. I've spent a big chunk of my career working on Internet media projects for non-profit organizations. I've also written for tech blog ProgrammableWeb.com, spent time in rural Uganda researching mobile phone use, and somehow found the time to complete a Masters degree in Information Management and Systems from UC Berkeley's School of Information.</p>\\r\\n\",\n",
       " '<p>I am having a challenge in creating a regression model for some data. I have a dataset with nominal(Both binary - gender and 3 valued) , ordinal and continuous variables. I want to set up a regression model to include all these variable. What is the best approach??</p>\\r\\n',\n",
       " '<p>I am a postdoctoral researcher and Habilitand at the Institute of Sociology and Social Psychology, University of Cologne. My research interests focus on methods of empirical research in sociology, family sociology, juvenile delinquency and educational sociology. Over the last years, I also developed an interest in medical sociology.</p>\\r\\n',\n",
       " \"<p>Ph. D. in Computer Science, interested in combinatorics and algorithmics applied to computational biology. If you think we may share common research interests, don't be shy and drop me a line! I'm always happy to work with others, and am looking forward to increase the number of publications and coauthors I have.</p>\\r\\n\",\n",
       " '<p>I make internets on a daily basis using Ruby on Rails.<br /><br /></p>\\r\\n\\r\\n<p>How about you follow me on twitter, I say things about web and such: <a href=\"http://twitter.com/marcgg\" rel=\"nofollow\">@marcgg</a></p>\\r\\n',\n",
       " '<p>-</p>\\r\\n',\n",
       " \"<p>Professionally, I do theory and simulation of scientific phenomena (C, Java, Scala, R, assorted other languages, on assorted platforms).</p>\\r\\n\\r\\n<p>Pseudo-professionally, I'm the architect/developer for a web-based human resources reporting system (Perl, mostly Catalyst+DBIx, and Javascript, mostly jQuery).</p>\\r\\n\",\n",
       " '<p>I currently work in the GIS field, but in my spare time I enjoy programming, design, reading, collecting colors, and writing really bad fiction.</p>\\r\\n',\n",
       " '<p>Ph.D. researcher in Anthropology and Education at Stanford University.\\r\\nTechie background with a B.S. in Computer Science and M.S. in Design Engineering.</p>\\r\\n',\n",
       " \"Research scientist finds himself dragged into software engineering-- at least it's interesting.\",\n",
       " 'A gentleman and a scholar.',\n",
       " '<p>Educational background in innovation management. All theory is nice, but I need tools to do innovation, which became computers. My main focus is on manipulating data so we can create innovations on the basis of those new insights. </p>\\r\\n',\n",
       " '<p>PhD researcher of GIS, spatial analysis and urban studies at the Centre for Advanced Spatial Analysis, University College London.</p>\\r\\n',\n",
       " '<p>Lecturer and PhD candidate in statistics at the University of Texas at San Antonio.</p>\\r\\n<p><a href=\"http://twitter.com/TheRandomTexan\">@TheRandomTexan</a></p>',\n",
       " '<p>See also <a href=\"http://www.linkedin.com/in/deepdevelopment\" rel=\"nofollow\">http://www.linkedin.com/in/deepdevelopment</a> as well as the Web site.</p>\\r\\n\\r\\n<p>Thanks.</p>\\r\\n',\n",
       " '<p>I work as an actuary and semantic coder at a Cape Town company doing amazing things in the cloud computing world. Also swimmer.</p>\\r\\n',\n",
       " '<p>I like to do stuff.</p>\\r\\n',\n",
       " '<p>Currently about to finish my master thesis focusing on ownership structute in corporate turnarounds.</p>\\r\\n',\n",
       " '<p>data scientist and survey research manager</p>\\r\\n',\n",
       " '<p><a href=\"http://www.linkedin.com/in/egbutter\" rel=\"nofollow\">http://www.linkedin.com/in/egbutter</a></p>\\r\\n',\n",
       " \"<p>A student of information technology for 7 years, with about a year's worth of working experience.</p>\\r\\n\\r\\n<p>Current pet project:\\r\\nPro-graduate paper on trend prediction and categorization with tags.</p>\\r\\n\",\n",
       " '<p>Postdoc at LMU Munich</p>\\r\\n',\n",
       " '<p>Student of Computational Linguistics / Language Technology</p>\\r\\n',\n",
       " '<p>Mathematics major, Statistics minor</p>\\r\\n',\n",
       " '<p>Independent consultant in statistical methods and programming.</p>\\r\\n',\n",
       " '<p>“Try again. Fail again. Fail better” (Beckett)</p>\\r\\n',\n",
       " \"<p>Today is a nice day, let's have some fun!</p>\\r\\n\",\n",
       " '<p>Discrete optimization.  Computational geometry.  Combinatorics.  Probabilistic analysis.  Bioinformatics.  Approximation algorithms.</p>\\r\\n',\n",
       " '<p>Biostatistician, NSW Health</p>\\r\\n',\n",
       " '<p>Actuary, love data analysis, with special interest in risk, financial mathematics, data mining, spacial data analysis, social networking analysis, supercomputing, HPC, CUDA, OpenCL, Hadoop, big data analysis and visualization.</p>\\r\\n',\n",
       " '<p>Doctoral Student in Criminal Justice.</p>\\r\\n',\n",
       " '<p>Student</p>\\r\\n',\n",
       " \"<p>I'm a developer from Takoma Park, MD.</p>\\r\\n\\r\\n<p>At the office, I work on MSSQL, C#, VB.NET, Javascript, ASP.NET.</p>\\r\\n\\r\\n<p>At home, I'm a vegan, a sci-fi fan, dabble in foreign languages and enjoy book clubs.</p>\\r\\n\\r\\n<p>I will try to up vote everyone who answers me and I thank everyone who has freely given me some of their time to answer my various questions.</p>\\r\\n\",\n",
       " '<p>Part programmer, part avid reader + part other things</p>\\r\\n',\n",
       " '<p>I am a 23 year old web developer living and working in the Cayman Islands.</p>\\r\\n\\r\\n<p>I work with a great team on a great product - DomainNameSales.com</p>\\r\\n\\r\\n<p>I have extensive knowledge of PHP and MySQL.\\r\\nI also have experience with Javascript, Java, Ruby, and Python.</p>\\r\\n\\r\\n<p>I like learning new things, and I like cool stuff.</p>\\r\\n',\n",
       " '<p><strong><code>M-A-K-E . S-T-U-F-F</code></strong></p>\\r\\n',\n",
       " '<p>Contact me at tomreilly@autobox.com.  Autobox was launched in 1975 as the first to market forecasting software using Box-Jenkins methodology.</p>\\r\\n',\n",
       " '<p>I study (not enough) maths, but mainly probability.</p>\\r\\n',\n",
       " \"<p>I'm JR Galia, a JEE programmer, a web developer, a certified PhilNITS IT professional and a FOSS advocate.</p>\\r\\n\",\n",
       " '<p>Professionally I am interested in finance and economics, and in maths and statistics relevant to those areas.  I have degrees in economics, philosophy and business administration, and have recently completed an MSc in Applied Environmental Economics.  I am also a qualified accountant (ACMA / CGMA).</p>\\r\\n\\r\\n<p>As a hobby, I am also interested in number theory and geometry.</p>\\r\\n\\r\\n<p>My website is a blog on environmental and natural resource economics.</p>\\r\\n',\n",
       " '<p>A geographer, demographer, and spatial statistician. I work on projects that help organizations understand policy impacts and demographic change.</p>\\r\\n',\n",
       " '<p>Greetings</p>\\r\\n\\r\\n<p>All my original contributions to StackExchange sites are hereby placed into the public domain. If this is not legally possible, then anyone receiving a copy of them by any means is granted a non-exclusive perpetual license to use, distribute and modify them, and to distribute modifications, under any license or none, with or without attribution. Please note that this license applies only to my original contributions - quoted material and edits by me to existing material on StackExchange sites are not my creations and I cannot grant rights in them.</p>\\r\\n',\n",
       " '<p>Rebel with rather too many causes.</p>\\r\\n',\n",
       " \"<p>I’ve got my Bs.C in Information Systems from the Technion at 2008 , while I was working as a student at Intel Research department in Israel, doing Machine learning oriented research .</p>\\r\\n\\r\\n<p>I then spent 2 amazing years working at Outbrain , a cool start up that changes the way you read on the web (you just don’t even know you've already used it), and now I’m working as an NLP software engineer at Clearforest , a Thomson Reuters company , where I get the change to work with leading expert in the NLP world  and learn a bunch of new stuff (Whoo!)</p>\\r\\n\\r\\n<p>I’ve always been fascinated by computer science – everything from machine learning algorithms via vision problems to the best way to manage concurrency in Java and debugging log files (that’s real programmers porn ). Currently I feel like StackOverflow is the best thing that happened since bread came sliced! </p>\\r\\n\\r\\n<p>As Plutarch said – “The mind is a fire to be kindled , not a vessel to be filled.” . So far , I’ve never been cold.</p>\\r\\n\",\n",
       " '<p>Software Engineer at OneMarketData</p>\\r\\n',\n",
       " '<p>Epitech student, Lyon, France.</p>\\r\\n',\n",
       " '<p>@aurimas</p>\\r\\n',\n",
       " 'PhD Student straddling the line between molecular biology and bioinformatics - I manage to cope by perennially breaking for coffee',\n",
       " '<p><img src=\"http://i.stack.imgur.com/CG296.png\" alt=\"\"></p>\\r\\n',\n",
       " '<p><a href=\"http://Aurametrix.com\" rel=\"nofollow\">Aurametrix</a> is designed for people who experience food sensitivities, allergies, metabolic disorders and those who want to improve their diet and exercise to maximize their health.</p>\\r\\n\\r\\n<p>Sites:</p>\\r\\n\\r\\n<p><li><a href=\"http://youtube.com/aurametrix\" rel=\"nofollow\">Aurametrix Youtube Channel</a></li>\\r\\n<li><a href=\"http://facebook.com/aurametrix\" rel=\"nofollow\">Aurametrix Facebook page</a></li>\\r\\n<li><a href=\"http://twitter.com/aurametrix\" rel=\"nofollow\">Aurametrix Twitter channel</a></li>\\r\\n<li><a href=\"https://plus.google.com/108995684691677708442/posts\" rel=\"nofollow\">Aurametrix G+ page</a></li>\\r\\n<li><a href=\"http://aurametrix.blogspot.com\" rel=\"nofollow\">Health Technologies Blog</a></li>\\r\\n<li><a href=\"http://ibs.aurametrix.com\" rel=\"nofollow\">IBS Blog</a></li>\\r\\n<li><a href=\"http://olfactics.aurametrix.com\" rel=\"nofollow\">Olfactics Blog</a></li>\\r\\n<li><a href=\"http://environment.aurametrix.com\" rel=\"nofollow\">Environmental Health Blog</a></li></p>\\r\\n',\n",
       " '<p>I am a PhD candidate at the faculty of Psychology at Bremen University, Germany. \\r\\nMore about me: <a href=\"http://www.markheckmann.de\" rel=\"nofollow\">www.markheckmann.de</a></p>\\r\\n',\n",
       " '<p>Statistical Bioinformatician with CSIRO Mathematics, Statistics and Informatics (CMIS).</p>\\r\\n',\n",
       " 'Web dev',\n",
       " '<p>MSCS student, almost finished!</p>\\r\\n\\r\\n<p>I enjoy Java and algorithms.</p>\\r\\n',\n",
       " '<p>Metallurgist with a statistician trapped inside</p>\\r\\n',\n",
       " '<p><strong>Software Engineer</strong><br>\\r\\n<strong>B.Pharm, M.Sc (Bioinformatics)</strong></p>\\r\\n\\r\\n<p><em><strong>BigData, MapReduce, NoSQL, Social Media Analytics</em></strong></p>\\r\\n\\r\\n<p>playing with -</p>\\r\\n\\r\\n<p><strong>1.</strong> Hadoop, Hive, MongoDb, Sqoop, Mahout, Pig, HBase<br>\\r\\n <strong>2.</strong> Java, R, JavaScript, Python, C, HTML5, CSS3, MySQL<br>\\r\\n <strong>3.</strong> git<br>\\r\\n <strong>4.</strong> Processing, Markdown<br>\\r\\n <strong>7.</strong> iReport-jasperserver  </p>\\r\\n',\n",
       " '<p>I am currently doing my Masters in Computer Science and I am very ambituous about learning new technologies and new concepts.I am well equipped with SQL programming,Dataware Housing fundamentals and looking forward for a best learning through this website.</p>\\r\\n',\n",
       " '<p>I work on problems that blend software development, data, and decision making. My background is a mixture of the computer science, engineering, and mathematical theory useful for working on interesting problems. I enjoy building great user experiences and data visualizations, creating automated and repeatable workflows, and helping people make complex decisions based on data.</p>\\r\\n\\r\\n<p><a href=\"http://github.com/adamgreenhall\" rel=\"nofollow\">github</a> | <a href=\"http://twitter.com/adamgreenhall\" rel=\"nofollow\">twitter</a> | hello@adamgreenhall.com</p>\\r\\n',\n",
       " \"I'm a graduate student in sociology at the University of California, San Diego. I'm interested in statistics, social network analysis, and R.\",\n",
       " '<p>matlab/stats/linux nerd, learning R slowly; proud, sleepless, new parent; HCSSiM alum, CMU grad, postdoc survivor; faking it as a quantitative analyst at a small quant fund in San Francisco; wants to be a statistician when he grows up.</p>\\r\\n<p><a href=\"http://twitter.com/shabbychef\">@shabbychef</a></p>',\n",
       " '<p>I work on the computer vision behind augmented reality applications for mobile phones at a startup called Ogmento in New York City.</p>\\r\\n',\n",
       " '<p>Former assistant professor teaching statistics and Ph.D. candidate in Hungary. Currently full-time founder of <a href=\"http://rapporter.net\" rel=\"nofollow\">rapporter.net</a>, a web application frontend to <a href=\"http://www.r-project.org/\" rel=\"nofollow\">R</a>-based data analysis and reports. Husband, father of two. <em>I will die before repeating any professional task: we code, then let the machines do the rest.</em></p>\\r\\n\\r\\n<p>My <strong><a href=\"http://www.r-project.org/\" rel=\"nofollow\">R</a> packages</strong>: <a href=\"http://rapporter.github.io/pander/\" rel=\"nofollow\">pander</a>, <a href=\"http://rapport-package.info/\" rel=\"nofollow\">rapport</a>, <a href=\"http://hackme.rapporter.net/\" rel=\"nofollow\">sandboxR</a>, <a href=\"http://cran.r-project.org/web/packages/migration.indices/\" rel=\"nofollow\">migration.indices</a>, <a href=\"http://cran.r-project.org/web/packages/saves/\" rel=\"nofollow\">saves</a></p>\\r\\n\\r\\n<p>Please feel free to <strong>get in touch</strong> (but please <em>do mention your SO username</em>):</p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://www.meetup.com/Budapest-Users-of-R-Network\" rel=\"nofollow\">Hungarian R User Group</a> and <a href=\"http://r-projekt.hu/welcome\" rel=\"nofollow\">r-projekt.hu</a></li>\\r\\n<li><a href=\"http://github.com/daroczig\" rel=\"nofollow\">GitHub</a></li>\\r\\n<li><a href=\"http://hu.linkedin.com/in/daroczig/\" rel=\"nofollow\">LinkedIn</a>, <a href=\"https://plus.google.com/104026536443441875944\" rel=\"nofollow\">Google Plus</a> and <a href=\"http://twitter.com/daroczig\" rel=\"nofollow\">Twitter</a></li>\\r\\n<li><a href=\"http://www.f6s.com/rapporter\" rel=\"nofollow\">F6S</a></li>\\r\\n</ul>\\r\\n',\n",
       " '<p>I am a resercher in statitics and I love data analysis of \"non standard\" data</p>\\r\\n',\n",
       " '<p>I am learning Fortran 95 , R and Mathematica</p>\\r\\n',\n",
       " \"<p>I'm a clinical data manager at a university. I'm interested in data management and analytics. I do most of my work in SAS and R, with some Python and Bash scripting thrown in there.</p>\\r\\n\",\n",
       " '<p>Aspiring Python / Django enthusiast...</p>\\r\\n',\n",
       " 'Ramnath Vaidyanathan is an Assistant Professor of Operations Management at the Desautels Faculty of Management, McGill University. He got his PhD from the Wharton School and worked at McKinsey & Co prior to that.',\n",
       " '<p>I am fond of thinking statistical and mathematical problems. More importantly, I suppose that communicating and discussing with others can help us gain a deeper understanding to concepts, theorems, etc in statistics and mathematics. Therefore, I would like to talk about and exchange ideas about all interesting issues in statistics and mathematics with anyone in the world.</p>\\r\\n',\n",
       " '<p>Software architect &amp; developer with about 20 years of experience. Primary interests - Functional programming (, Clojure, Erlang, Haskell), Emacs, Information Security, Machine Learning, Natural Language Processing, Big Data, etc.</p>\\r\\n\\r\\n<p>Also program in Scheme, Common Lisp, OCaml, C, C++, Java, and many other languages...</p>\\r\\n\\r\\n<p>I have 2 technological blogs: <a href=\"http://alexott.blogspot.com\" rel=\"nofollow\">English</a> and <a href=\"http://alexott-ru.blogspot.com\" rel=\"nofollow\">Russian</a>, and also 2 twitter accounts: <a href=\"http://twitter.com/alexott_en\" rel=\"nofollow\">@alexott_en</a> (English) and <a href=\"http://twitter.com/alexott\" rel=\"nofollow\">@alexott</a> (Russian)</p>\\r\\n',\n",
       " '<p>I am a graduate student at Oregon State University performing research on wireless high-definition video encoded with H.264 video compression technology and transmitted over 802.11 (WiFi) networks.</p>\\r\\n',\n",
       " '<p>I currently combine my scientific background with my software engineering skills, and apply them to high-end R&amp;D projects. My specialities are data analysis, software engineering, and clear communication of complex subjects. </p>\\r\\n\\r\\n<p>I am fluent in R and Python, and comfortable programming IDL, Fortran and Bash. In addition, I have some experience in C++ and JavaScript.</p>\\r\\n\\r\\n<p>For more details on my work, publications and software, please checkout <a href=\"http://www.numbertheory.nl\" rel=\"nofollow\">my website</a>. In addition, <a href=\"https://bitbucket.org/paulhiemstra\" rel=\"nofollow\">my bitbucket account</a> contains a number of software projects I worked on. For a more complete breakdown of my career path until now, I refer to my <a href=\"http://nl.linkedin.com/pub/paul-hiemstra/20/30b/770/\" rel=\"nofollow\">linkedin profile</a>.</p>\\r\\n',\n",
       " 'Web Developer & Geek',\n",
       " \"<p>For the past couple years I've been mostly developing web applications in Django.  Python has become my go-to language, but I also have years of experience with C/C++, Java, PHP and a few others.</p>\\r\\n\\r\\n<p>When I have some spare time, I like to mix things up and play around with Clojure.  I'm also interested in machine learning and statistical data mining, but mostly as a hobby right now.</p>\\r\\n\\r\\n<p>All of my personal systems run Linux (typically Ubuntu) as it tends to suit my tinkering nature more so than other operating systems.  I try hard not to be dogmatic about such things, however.</p>\\r\\n\\r\\n<p>I enjoy good discussions on programming, architecture and other software engineering topics.  I pride myself in being pragmatic, curious, skeptic, practitioner, craftsman.</p>\\r\\n\",\n",
       " '<p>Software developer, web miner and math person. </p>\\r\\n\\r\\n<p>Working at <a href=\"http://www.scrapinghub.com\" rel=\"nofollow\" title=\"ScrapingHub\">ScrapingHub</a>.</p>\\r\\n',\n",
       " '<p>Hello, World!</p>\\r\\n',\n",
       " '<p>Mechanical Engineering Student at Memorial University of Newfoundland and Labrador.</p>\\r\\n',\n",
       " '<p>Data scientist in training at the USF MS in Analytics program.</p>\\r\\n',\n",
       " '<p>Senior .NET developer. ASP.NET MVC. Windows Phone 8. Window 8. Windows Azure. SQL Server.</p>\\r\\n\\r\\n<p>Blogs<br>\\r\\n<a href=\"http://wp7agile.wordpress.com/\" rel=\"nofollow\">http://wp7agile.wordpress.com/</a><br>\\r\\n<a href=\"http://karpcom.blogspot.com/\" rel=\"nofollow\">http://karpcom.blogspot.com/</a><br>\\r\\n<a href=\"http://twitter.com/360AgileWeb\" rel=\"nofollow\">http://twitter.com/360AgileWeb</a><br>\\r\\n<a href=\"http://wp7agile.azurewebsites.net\" rel=\"nofollow\">http://wp7agile.azurewebsites.net</a></p>\\r\\n',\n",
       " \"<p>I love statistics and programming.  My primary languages are S/R, C/C++, Visual Basic.  I'm not sure if AutoHotKey is quite a programming language but there is nothing better for automating things in windows.</p>\\r\\n\",\n",
       " \"I'm a C#, VB6, ASP (Classic), and Java developer professionally.\\\\\\\\n\\\\\\\\nI do workflow systems, a little Activity Based Costing, and regular old business systems.\\\\\\\\n\\\\\\\\nIn my free time I play with php and java on linux.\",\n",
       " \"<p>I'm a teacher. My subjects are Mathematics and Economics and most of my teaching is at pre-university level (UK A levels). I also teach Management undergraduates courses in Economics and Mathematics.</p>\\r\\n\\r\\n<p>I studied Mathematics at university some twenty years ago and have forgotten much of it. Moreover, I didn't study Statistics which I have become very interested in and have taught. The trouble is, my knowledge is hapazard; I've picked it up as I've gone along.</p>\\r\\n\",\n",
       " '<p>PhD Candidate at the Department of Computer Science &amp; Engineering, Ohio State University.</p>\\r\\n',\n",
       " '<p>I use R for: quantitative financial data analysis/visualization, forecasting,  systematic/statistical trading strategies, multi-factor portfolio optimization, risk analysis, execution optimization.</p>\\r\\n',\n",
       " '<p>Data scientist with Pearson eCollege in Denver, Colorado. </p>\\r\\n',\n",
       " '<p>Data scientist at CQuotient</p>\\r\\n',\n",
       " \"<p>I'm an undegrad double majoring in quantitative biology and statistics. I write almost exclusively in R. I use R for school and for independent research projects. I've been coding for about a year now and I really like it. I'm on stack overflow and X validated to give and receive help with simple to intermediate problems and to avoid making the folks on the R help mailing list angry.</p>\\r\\n\",\n",
       " '<p>Finished with the PhD. Doing a post-doc in graph theory applied to social networks. Got too many things that are too heavy being juggle in my brain!!! ouchy</p>\\r\\n',\n",
       " \"<p>I'm a software engineer and distributed intelligent specialist.</p>\\r\\n\",\n",
       " '<p>Contract financial developer at a large bank in Charlotte, NC.</p>\\r\\n',\n",
       " '<p>Graduate student at Imperial College London</p>\\r\\n',\n",
       " \"<p>I'm employed as an analyst with an equal split in job duties between SAS programming and statistical consulting. The latter is actually a little embarrassing as I'm still learning but don't need a very high level of sophistication for the bulk of my work.</p>\\r\\n\",\n",
       " '<p><img src=\"http://i.stack.imgur.com/aMEZv.png\" alt=\"enter image description here\"></p>\\r\\n\\r\\n<p>Just Another Genome Hacker,</p>\\r\\n\\r\\n<p>Sadly, my most voted-up answer on Stackoverflow (312 votes) belonged to a question <a href=\"http://stackoverflow.hewgill.com/questions/242/068/9.html\" rel=\"nofollow\">that later got deleted.</a> The question came from a girl that asked for something cute that she could put on the icing of a birthday cake for her programmer boyfriend.</p>\\r\\n\\r\\n<p>I suggested that, if her boyfriend is somewhat mathematically inclined, she write the following:</p>\\r\\n\\r\\n<pre><code>    sin(t) √|cos(t)|\\r\\nr = __________________  - 2 sin(t) + 2\\r\\n    sin(t) + 7/5\\r\\n</code></pre>\\r\\n\\r\\n<p>If he doesn\\'t get it at first, she could tease him with saying that it is obvious that he should take the polar plot of it. If he still doesn\\'t get it, she should tell him to enter it in <a href=\"http://www.wolframalpha.com/input/?i=polar%20r=%28sin%28t%29%2asqrt%28abs%28cos%28t%29%29%29%29/%28sin%28t%29%20%2b%207/5%29%20-2%2asin%28t%29%20%2b%202\" rel=\"nofollow\">WolframAlpha</a>, that should yield the result:</p>\\r\\n\\r\\n<p><img src=\"http://i.stack.imgur.com/ol291.png\" alt=\"enter image description here\"></p>\\r\\n',\n",
       " '<p>I barely know anything about everything.</p>\\r\\n',\n",
       " '<p>aattard82 |at| gmail.com</p>\\r\\n',\n",
       " \"<p>I'm a Python programmer specializing in web scraping and web development.</p>\\r\\n\",\n",
       " '<p>Bioinformatician working in Cancer Genomics</p>\\r\\n',\n",
       " '<p>I am a medical resident. My research area is in the field of biomedical optics.</p>\\r\\n',\n",
       " '<p>Interested in NLP, ML.</p>\\r\\n',\n",
       " 'Co-founder, SU3 Analytics Ltd',\n",
       " '<p>Learning Java....  :)</p>\\r\\n',\n",
       " '<p>Mathematical statistician</p>\\r\\n',\n",
       " '<p>I am a statistical consultant and data scientist living in Ottawa, Canada, with over 10 years experience. I have yet to see a data set I did not like. I work primarily with R and SPSS.</p>\\r\\n',\n",
       " '<p>I like pie</p>\\r\\n',\n",
       " '<p>Cardiac anesthesiologist, programmer (R, SQL, perl, unix...)</p>\\r\\n',\n",
       " \"<p>I'm a psychology PhD student at Trinity College, Dublin doing my research on dangerous driving among young people. I'm a part time lecturer in statistics and research methods.</p>\\r\\n\",\n",
       " '<blockquote>\\r\\n  <p>[...] from this point of view, when you contemplate what to do about\\r\\n  outliers, you’re like Marlow, in “Heart of Darkness,” when he is\\r\\n  travelling up-river to find Kurtz. “Watching a coast as it slips by\\r\\n  the ship,” Conrad writes,</p>\\r\\n  \\r\\n  <blockquote>\\r\\n    <p>is like thinking about an enigma. There it is before you—smiling,\\r\\n    frowning, inviting, grand, mean, insipid, or savage, and always mute\\r\\n    with an air of whispering, ‘Come and find out.’</p>\\r\\n  </blockquote>\\r\\n</blockquote>\\r\\n',\n",
       " '<p>I have BSc(Hons) in biochemistry (1990) and have just graduated (2011) as Master of Biostatistics. I work for a government epidemiology department in Australia.</p>\\r\\n',\n",
       " '<p>I am a biology student that have a lot to learn about statistics.</p>\\r\\n',\n",
       " '<p>Computer scientist, mathematician, Go player, amateur musician. I obtained a PhD in Artificial Intelligence in 2009. Currently I work as a post-doc.</p>\\r\\n',\n",
       " '<p>Developer of the Dependency Structure Matrix AddIn for Visual Studio and Reflector:</p>\\r\\n\\r\\n<ul>\\r\\n<li><p>Analyse application architecture</p></li>\\r\\n<li><p>Untangle dependencies</p></li>\\r\\n</ul>\\r\\n',\n",
       " '<p>Software engineer in Cape Town, South Africa. I have been building commercial web applications professionally since 2003.</p>\\r\\n\\r\\n<ul>\\r\\n<li>My startup is <a href=\"http://mynames.co.za/\" rel=\"nofollow\" title=\"MyNames: Painless CO.ZA Domain Management\">MyNames</a>.</li>\\r\\n<li>I build repair shop software for <a href=\"http://ifix.co.za/\" rel=\"nofollow\" title=\"iFix Apple Repairs\">iFix Apple® Repairs</a> that runs eight retail stores across South Africa and processes millions in revenue.</li>\\r\\n<li>I built and ran <a href=\"http://rhythmmusicstore.com/\" rel=\"nofollow\" title=\"Rhythm shut down end 2012\">Rhythm Music Store</a> from 2007-2012, the biggest online music store in SA, with a catalogue of 85,000 DRM-free MP3s.</li>\\r\\n<li>I was involved with <a href=\"http://www.rooihub.co.za/\" rel=\"nofollow\" title=\"RooiHub\">peer-to-peer DC++</a> on campus.</li>\\r\\n<li>I graduated with an honours degree in Electronic Engineering with Computer Science from the <a href=\"http://sun.ac.za/\" rel=\"nofollow\">University of Stellenbosch</a>, in 2010.</li>\\r\\n<li>I blog on my <a href=\"http://petrustheron.com/\" rel=\"nofollow\">personal website</a>.</li>\\r\\n<li>You can find me as <a href=\"https://github.com/pate\" rel=\"nofollow\">pate on GitHub</a>.</li>\\r\\n</ul>\\r\\n',\n",
       " \"<p>I am an expert on Microsoft Access.  On PHP and MySQL, I have more experience than expertise.  For manipulating text files, Python is delightful; my attempts to use Python for web development are less so.<br/><br/>\\r\\nI'm still looking for the ultimate personal information manager.  TiddlyWiki is way out in front.  TiddlyWiki version 5 will extend this pre-eminence, I hope.\\r\\n<br/></p>\\r\\n\\r\\n<blockquote>\\r\\n  <p><em>The man who sets out to carry a cat by its tail learns something that will always be useful and which will never grow dim or doubtful.</em>  -- Mark Twain\\r\\n  <br/><br/>\\r\\n  <em>A rebuke goes deeper into a wise man than a hundred blows into a fool.</em> -- Solomon</p>\\r\\n</blockquote>\\r\\n\",\n",
       " '<p>A programmer, A post graduate in Signal and Image processing.<br>\\r\\nCurrently working in a start-up company in India.<br>\\r\\n<a href=\"http://www.linkedin.com/in/dipanmehta\" rel=\"nofollow\">http://www.linkedin.com/in/dipanmehta</a><br>\\r\\n<a href=\"http://dipanmehta.wordpress.com\" rel=\"nofollow\">http://dipanmehta.wordpress.com</a>  </p>\\r\\n\\r\\n<p>A new proposal for <a href=\"http://area51.stackexchange.com/proposals/36807/broadcast-and-media-technologies?referrer=lxeiaDBzsJIqpGpL-P5bMg2\">Broadcast and Media Technologies</a>. Please support.<br>\\r\\n<a href=\"http://area51.stackexchange.com/proposals/36807/broadcast-and-media-technologies?referrer=lxeiaDBzsJIqpGpL-P5bMg2\"><img src=\"http://area51.stackexchange.com/ads/proposal/36807.png\" width=\"155\" height=\"180\" alt=\"Stack Exchange Q&amp;A site proposal: Broadcast and Media Technologies\" /></a></p>\\r\\n',\n",
       " '<p>Computational biologists at European Molecular Biology Laboratory (EMBL), previously at Carnegie Mellon University and at the Institute for Molecular Medicine (University of Lisbon).</p>\\r\\n',\n",
       " '<p>Mathematician.</p>\\r\\n',\n",
       " '<p>Cognitive psychologist/magician.</p>\\r\\n',\n",
       " \"<p>I'm an associate professor of wildlife ecology at the University of Nebraska-Lincoln. I'm fascinated by social media, especially applications to nature conservaton and higher education.</p>\\r\\n\",\n",
       " '<p>Interested in statistics, modeling, and the uncertainty of measurements.</p>\\r\\n',\n",
       " '<ul>\\r\\n<li>Geographic Data Analysis</li>\\r\\n<li>Census data</li>\\r\\n<li>Web mapping</li>\\r\\n<li>Python and Javascript </li>\\r\\n</ul>\\r\\n',\n",
       " \"<p>I'm doing a PhD in Computer Science, University College London.</p>\\r\\n\",\n",
       " '<p>A psychologist interested in statistics.</p>\\r\\n',\n",
       " '<p>I am interested in Biology, Evolution, Genetics/Genomics, Machine Learning and Bayesian statistics.</p>\\r\\n',\n",
       " '<p>I am a PhD student in Social and Decision Sciences at Carnegie Mellon University</p>\\r\\n',\n",
       " \"<p>Webdeveloper that codes mostly in HTML,PHP and JavaScript(mostly jQuery) with some adventures in Java.</p>\\r\\n\\r\\n<p>Started my coding in VisualBasic back in the 90's, but migrated to building websites when i saw that it was easier doing the simple stuff I did. </p>\\r\\n\\r\\n<p>I love puzzlegames and thinking about numbers. I have some experience with higher level of math and physics.</p>\\r\\n\\r\\n<p>I like asking questions.</p>\\r\\n\",\n",
       " '<p>In the golden age of ancient Chinese philosophy, Gung-sun Lung Tsu was the one of the only thinkers, and certainly the most important, to discuss topics pertaining to logic and epistemology.</p>\\r\\n',\n",
       " '<p>I LOVE C/C++</p>\\r\\n',\n",
       " '<p>PhD candidate in Evolutionary Genetics of Aging</p>\\r\\n',\n",
       " '<p>Consulting analyst for the San Diego Padres, Houston Rockets.</p>\\r\\n\\r\\n<p>Twitter: <a href=\"https://twitter.com/octonion\" rel=\"nofollow\">https://twitter.com/octonion</a></p>\\r\\n\\r\\n<p>LinkedIn: <a href=\"http://www.linkedin.com/in/octonion\" rel=\"nofollow\">http://www.linkedin.com/in/octonion</a></p>\\r\\n',\n",
       " '<p>I am an embedded firmware engineer in Power Industry.\\r\\nI am also well versed in embedded design and C.</p>\\r\\n\\r\\n<p>I have a fair amount of interest in data analysis tools and am an intermediate to advanced user of MS excel.\\r\\nOf late, I have also dabbled in systems engineering.</p>\\r\\n',\n",
       " '<p>Coding junkie</p>\\r\\n',\n",
       " '<p>PhD student from italy</p>\\r\\n',\n",
       " '<p>I am an applied economics student at Montana State University and user of the following code: R, LaTeX, TikZ, Python, and Fortran.  I also dabble in GIS with QGIS, PostgreSQL, and PostGIS.</p>\\r\\n',\n",
       " \"<p>I'm a developer and project administrator for the Maxima project. I have a lot of experience in programming and some in mathematics. I have a special interest in Bayesian inference, among many other topics.</p>\\r\\n\",\n",
       " '<p>Husband, father, graduate student, aspiring scientist, avid programmer, amateur technophile, flipper of pancakes, and baker of biscuits.</p>\\r\\n',\n",
       " \"<p>I'm a Ph.D. student at CISE, University of Florida.</p>\\r\\n\",\n",
       " '<p>I am a biostatistician working in the Department of Surgery at Oregon Health and Science University in Portland, Oregon.</p>\\r\\n',\n",
       " \"<p>I'm a guy who likes to develop Mac and iPhone apps.</p>\\r\\n\",\n",
       " '<p>stats student</p>\\r\\n',\n",
       " '<p>PhD Student in the UK.</p>\\r\\n',\n",
       " '<p>Senior Developer for MicroQuant, LLC</p>\\r\\n\\r\\n<p>Loving Node JS, JavaScript, and PHP</p>\\r\\n',\n",
       " '<p>Asc. Prof. of CS/algorithms at NTNU. Author of \"Practical Python\"/\"Beginning Python\" and \"Python Algorithms\".</p>\\r\\n',\n",
       " '<p>I apply modern research in programming languages, static analysis, and software engineering to create practical software development tools. I studied Mathematics and Computer Science at Carnegie Mellon University, and am currently member of the 2012 class of Thiel Fellows.</p>\\r\\n',\n",
       " '<p>Just Getting started...</p>\\r\\n\\r\\n<p>Interest in: Machine Learning, Database Sys, and Bioinformatics.</p>\\r\\n\\r\\n<p>Know: Perl, Ruby on Rails, Java, AjAX, JS, CSS. </p>\\r\\n',\n",
       " 'Experienced software guy. Python, Web App, HTTP, etc.',\n",
       " '<p>Professor Emeritus of Marine Geology, Göteborg University, Sweden.</p>\\r\\n\\r\\n<p>Research fields: Quaternary and Cretacoeus paleoclimatology, marine micropaleontology, evolutionary patterns in microfossils, application of multivariate-statistical analysis and artificial neural networks to geological data.</p>\\r\\n',\n",
       " '<p>A doc interested in research and who smiles.</p>\\r\\n',\n",
       " \"<p>In the Spring of 2014, I graduated with a B.S. in Computer Engineering and a B.S. in Mathematics. I am trying to get into grad school for neuroscience with an application in Brain-Computer Interfacing. Job experience includes cryptanalysis, malware reverse engineering, and neural nets. I'll edit this as I think of something more interesting to put.</p>\\r\\n\",\n",
       " '<p>Just another geek</p>\\r\\n',\n",
       " '<p>Long time SPSS designer, developer, statistician and strategy person.  Python fan.  Serious recreational cyclist</p>\\r\\n',\n",
       " '<p>A student in Computer And Systems Engineering Dept. in Alexandria University in Egypt.</p>\\r\\n',\n",
       " '<p>physics undergrad</p>\\r\\n',\n",
       " '<pre><code>Hello World!!\\r\\nHello friends!!\\r\\n</code></pre>\\r\\n',\n",
       " '<p>Managment consultant w a background in Computer Science</p>\\r\\n',\n",
       " '<p>Software Developer in São Paulo, Brazil, currently working with Bioinformatics at Mendelics</p>\\r\\n',\n",
       " '<p><strong>I am currently looking for a Job in London</strong>. You can contact me at <code>bjoern dot pollex at googlemail dot com</code>.</p>\\r\\n\\r\\n<p>I believe that there is some answer to any question. It is not the task of the users here to lecture others or criticize them for laziness or stupidity. There is little educational value in sarcasm. </p>\\r\\n\\r\\n<p>If a question is badly formulated, one can give advice on how to do better. </p>\\r\\n\\r\\n<p>If important information is missing, one can kindly ask for it. </p>\\r\\n\\r\\n<p>Someone else being unfriendly or badly mannered is no justification for being so yourself.</p>\\r\\n\\r\\n<p>Before posting any question about a problem with your code, please read <a href=\"http://sscce.org/\" rel=\"nofollow\">this</a> (twice).</p>\\r\\n\\r\\n<h2>Favorite Answers</h2>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://stackoverflow.com/a/8614314/160206\">C++ - How to refer to recursive structs through pointers using vectors</a></li>\\r\\n</ul>\\r\\n',\n",
       " 'I am a developer for a media company in London.\\\\\\\\n\\\\\\\\nI am a Microsoft Certified Technology Specialist in .NET Framework 3.5, ASP.NET Applications.',\n",
       " '<p>I like math.</p>\\r\\n',\n",
       " '<ul>\\r\\n<li><a href=\"https://github.com/briatte\" rel=\"nofollow\">GitHub</a></li>\\r\\n<li><a href=\"https://twitter.com/phnk\" rel=\"nofollow\">Twitter</a></li>\\r\\n</ul>\\r\\n\\r\\n<p>Profile picture by <a href=\"http://hiddenmoves.deviantart.com/\" rel=\"nofollow\">Rhys Owens</a>.</p>\\r\\n',\n",
       " '<p>Junior academic respiratory paediatrician</p>\\r\\n',\n",
       " '<p>PhD student at <a href=\"http://en.wikipedia.org/wiki/University_of_S%C3%A3o_Paulo\" rel=\"nofollow\">University of São Paulo</a> at São Carlos, Brazil.</p>\\r\\n\\r\\n<p>My current MSc research project has as the main goal the study and development of state of art shape feature extraction methods and its applications to the development of Computer-Aided Diagnosis (CAD) systems supported by CBIR (Content-Based Image Retrieval) techniques.</p>\\r\\n',\n",
       " '<p>Algorithmic Artist</p>\\r\\n',\n",
       " '<p>grad student.\\r\\ninterested in arcgis, python, R and LaTeX.</p>\\r\\n',\n",
       " '<p>I am an independent ICT entrepeneur from the Netherlands. My main interests are PHP and Android development.</p>\\r\\n',\n",
       " '<p>I\\'m <a href=\"http://dorserg.com\" rel=\"nofollow\">Sergey Doroshenko</a>.</p>\\r\\n',\n",
       " '<p>An undergraduate CS student with interests in NLP and Software Engineering.</p>\\r\\n',\n",
       " '<p>Rails Developer, Health IT consultant</p>\\r\\n',\n",
       " '<p>I studied mathematics at the University of Potsdam and am now working as a statistician in a biotechnological enviroment.</p>\\r\\n',\n",
       " '<p>Hi! I work for Google as a software engineer in their Chicago office. I recently graduated from Northwestern University.</p>\\r\\n',\n",
       " '<p>background: pure maths phd.\\r\\nwork: I just began working for the government with the modeling of economic events (lots of stats involved)</p>\\r\\n',\n",
       " '<p>I am a postdoc in the Department of Molecular Medicine at the University of South Florida (Tampa) where I study gene regulation in Apicomplexa parasites (Toxoplasma and Plasmodium, the vector of malaria).\\r\\nI am a wet bench biologists trying to get more scripting experience. I love R but I am just at the baby steps level.</p>\\r\\n',\n",
       " '<p>Studying economics at Copenhagen University...\\r\\nUsing R when possible.</p>\\r\\n',\n",
       " \"<p>I am an undergraduate pursuing the degree of B.E.(Hons.) Computer Science from one of the prominent colleges of India. A true Geek by nature who loves computer science more than anything. Everything about computer science excites me.</p>\\r\\n\\r\\n<p>Recently i became more focused in my approach and now i'm a wannabe Machine Learning/ NLP expert.</p>\\r\\n\",\n",
       " \"I'm an assistant professor of Statistics at Rice University.\",\n",
       " '<p>I like turtles.</p>\\r\\n',\n",
       " '<p>Twitter: @glenn_mcdonald</p>\\r\\n\\r\\n<p>Work: the.echonest.com</p>\\r\\n\\r\\n<p>Defunct previous works: Interchange, eRoom, Needle.</p>\\r\\n',\n",
       " '<p>recursive: adjective, see recursive.</p>\\r\\n',\n",
       " '<p>I work at a small mechanical engineering company, where I develop software and image processing algorithms for camera-based inspection machines.</p>\\r\\n',\n",
       " '<p>Love computers, technology and Apple. Wanna be entrepreneur.</p>\\r\\n',\n",
       " 'Student / Administrator / Software Developer',\n",
       " \"<p>I am currently an independent consultant located in Berkeley, Ca.  Formerly I was a statistician at the now-defunct SolFocus, Inc., a concentrating photovoltaic technology company.  Lots of work around estimating how much energy we'd get out of a particular site!  And reliability work too, of course.  Prior to that, I worked for a number of years in Hewlett-Packard's Strategic Planning and Modeling (SPaM) group, an industry award-winning internal, and occasionally external, consulting organization, and as an independent consultant.</p>\\r\\n\",\n",
       " '<p>PhD in Computer Science, CTO at Shiftforward, Professor of CS at FEUP.</p>\\r\\n',\n",
       " '<p>Population scientist on Core Data Science @ FB. Using stats, surveys, and software (R, Python) to answer social science questions in the digital age.</p>\\r\\n',\n",
       " '<p>Epidemiology student </p>\\r\\n',\n",
       " '<p>Interests:Statistical Learning Theory, Kernel Methods, Robust Statistics,Nonparametric Probability Density Estimation</p>\\r\\n',\n",
       " '<p>R enthusiast, ecological statistics, mainly self-taught, still (always...) learning.</p>\\r\\n',\n",
       " '<p>Professor of Statistics, Monash University, Australia. Have used R and LaTeX for more than 20 years.</p>\\r\\n',\n",
       " '\\r\\n\\r\\n<p>I am a linux (Debian) user since long time ago, some of my actual fields of interest are currently improve my limited skills with LaTeX and R (at the same time thanks to Sweave).</p>\\r\\n',\n",
       " '<p>Ph.D. student, Psych. Dep. of St.Petersburg State Uni, Russia</p>\\r\\n',\n",
       " '<p>Research officer in glaciology at Swansea University.</p>\\r\\n',\n",
       " '<p>BBA Finance/Economics,\\r\\nMSc Statistics, and\\r\\ncurrently pursuing PhD in Finance</p>\\r\\n',\n",
       " 'Maths Teacher, Computer Programmer, Web Designer.',\n",
       " '<p>Im a psychologist who works with mathematical models of human cognition.</p>\\r\\n',\n",
       " '<p>Financial professional with expertise in financial modeling, risk management, and related quantitative methods including Monte Carlo simulation, linear and logit regression, and statistical hypotheses testing.</p>\\r\\n',\n",
       " '<p>Graduate student of Urban Planning. </p>\\r\\n',\n",
       " '<p>Ph.D. student.  Field: geotechnical engineering; geotechnical earthquake engineering.</p>\\r\\n',\n",
       " '<p>My formal education is in economics. My programming skills and computer knowledge are self-taught. I have an uncanny ability to find answers to questions outside my immediate expertise. I have a deep love of learning and I enjoy building / fixing all kinds of things -- software, computer hardware, engines, etc.</p>\\r\\n',\n",
       " \"<p>I'm a Master's student in Computer Science. I majorly work on C,C++ and Python. \\r\\nI know a fair amount of SQL,HTML and CSS as well. I have nothing against JAVA but it's not my preferred language for coding.</p>\\r\\n\",\n",
       " 'Quantitative modeling, simulation and statistical analysis.',\n",
       " '<p>Software Engineer @ Next Big Sound</p>\\r\\n',\n",
       " '<p>Yet another math enthusiast...</p>\\r\\n',\n",
       " \"<p>I'm an Assistant Professor of Economics at Dr. B.R. Ambedkar University, New Delhi. I received  my Ph.D. from Jawaharlal Nehru University, working on monetary economics.</p>\\r\\n\\r\\n<p>I fell in love with maths when I was required to take an elementary real analysis course as an undergraduate. I have spent the last 15 years learning calculus. I plan to spend the next 15 learning some geometry. </p>\\r\\n\",\n",
       " '<p>Biostatistician with diverse, multifaceted training in health research. Experienced with Phase I-III clinical trials, pharmaceuticals, health plan analytics, survey design and implementation, oncology, epidemiology, and genetics. Interested in positions in medical research. Sharp technical edge with all major statistical programming platforms. Eager and fast-paced learner and performer, flexible and seeking new dimensions of growth.</p>\\r\\n',\n",
       " \"<p>I'm an ABD statistician, and work fulltime in this capacity.  I have interest in Computational Statistics, Markov Chain Theory, and Sample Survey.  </p>\\r\\n\",\n",
       " '<p>i sometimes worry about the coming robot uprising.</p>\\r\\n',\n",
       " \"<p>I'm a programmer involved in developing a variety of programs.   One of these deals with forecasting retail products.   Another with forecasting manufactured food products.</p>\\r\\n\",\n",
       " '<p>A GNU/Linux user and Python developer<br>\\r\\nUse much PyGTK, PyQt, and some Django<br>\\r\\nInsterested in Math and specially Geometry  </p>\\r\\n\\r\\n<p>My Projects:<br>\\r\\n<a href=\"http://sourceforge.net/projects/starcal\" rel=\"nofollow\">http://sourceforge.net/projects/starcal</a><br>\\r\\n<a href=\"http://sourceforge.net/projects/pyglossary\" rel=\"nofollow\">http://sourceforge.net/projects/pyglossary</a></p>\\r\\n',\n",
       " '<p>Хаммага салом</p>\\r\\n',\n",
       " '<p>Formerly a webdev, then a failed postgrad, now shifting focus to more data and maths projects</p>\\r\\n',\n",
       " \"<p>Statistician / Mathematician learning to programming.</p>\\r\\n\\r\\n<p>Don't hate me because I'm lazy!</p>\\r\\n\",\n",
       " '<p>I work in bioformatics research with a strong emphasis on crowdsourcing.  I dabble in machine learning..</p>\\r\\n',\n",
       " \"<p>I'm a PhD student with engineering and biology background.</p>\\r\\n\",\n",
       " '<p>I am an associate professor at the Department of Mathematics, Ateneo de Naga University, Naga City, Camarines Sur, Philippines.</p>\\r\\n',\n",
       " '<p>PhD student in medical bioinformatics at Uppsala University.</p>\\r\\n',\n",
       " '<p>Student.</p>\\r\\n',\n",
       " '<p>Graduate Student at the University of Central Florida in Civil Engineering.</p>\\r\\n',\n",
       " '<p>~~<br>\\r\\n<strong>Green For Good Answers</strong><br>\\r\\n<a href=\"http://pm.stackexchange.com/questions/9072/how-to-sequence-tasks-given-time-estimates\">How to sequence tasks given time estimates?</a></p>\\r\\n\\r\\n<p>~~<br>\\r\\n<strong>Experiences on this site.....</strong></p>\\r\\n\\r\\n<p>::AND WHAT THE HELL IS WRONG WITH &lt;1% OF PPL CONTINUOUSLY PUTTING FAKE \\'HOMEWORK\\' TAGS!?! do these ppl never <strong>learn????????????</strong></p>\\r\\n\\r\\n<p>~ <strong>Case Easy Questions</strong> ~<br>\\r\\n<a href=\"http://www.ironpython.net/\" rel=\"nofollow\">http://www.ironpython.net/</a> answers one of my very few questions, like once in a whole month, (which i deleted) that NOBODY could help with!!</p>\\r\\n\\r\\n<p>~ <strong>Case Better Google</strong> ~<br>\\r\\nif you know the RIGHT terms, just google -- to solve simple, easy problems in minutes -- <a href=\"http://windowssecrets.com/forums/showthread.php/107596-Word-macro-needed-multiple-search-and-replace-(Word-2003)\" rel=\"nofollow\">http://windowssecrets.com/forums/showthread.php/107596-Word-macro-needed-multiple-search-and-replace-(Word-2003)</a> -- SO MUCH trouble... even when you gave them a very nice, detailed question -- stackoverflow.com/questions/13223561/find-replace-macro-long-and-detailed</p>\\r\\n\\r\\n<p>so <strong>disappointed</strong> at this site. </p>\\r\\n\\r\\n<p>~ <strong>Case Bad GIS</strong> ~<br>\\r\\nfound the great answer: <a href=\"http://spatialanalysis.co.uk/2011/12/2011/\" rel=\"nofollow\">http://spatialanalysis.co.uk/2011/12/2011/</a> -- no thanks to the ppl at gis stack <em>sigh</em></p>\\r\\n\\r\\n<p><strong>bad gis person:</strong> \"why should we spend our time answering something we already know?\" (SO GET LOST :D<br>\\r\\n<strong>actually smart people:</strong> \" <a href=\"http://www.youtube.com/watch?v=r9LCwI5iErE\" rel=\"nofollow\">http://www.youtube.com/watch?v=r9LCwI5iErE</a> \"<br>\\r\\n<strong>then</strong> an incrediably bad mod closed a good question out of merely not liking <a href=\"http://gis.stackexchange.com/questions/36733/good-maps-showing-findings-using-spatial-analysis\">http://gis.stackexchange.com/questions/36733/good-maps-showing-findings-using-spatial-analysis</a><br>\\r\\n<strong>but knows</strong> she reasonably can\\'t given similarly phrased questions asked by others -- <a href=\"http://gis.stackexchange.com/questions/36747/what-is-the-most-useful-spatial-analysis-trick-in-meeting-any-real-world-needs\">http://gis.stackexchange.com/questions/36747/what-is-the-most-useful-spatial-analysis-trick-in-meeting-any-real-world-needs</a><br>\\r\\n<strong>outcome:</strong> but since that gis mod can do whatever she wants, she banned my gis account for a month -- woho! </p>\\r\\n\\r\\n<p>~ <strong>Case Math &amp; Stat\\'s Inadequacy in knowing how to Communicate</strong> ~<br>\\r\\nGo Learn Stuff -- <a href=\"https://www.edx.org/\" rel=\"nofollow\">https://www.edx.org/</a> -- <a href=\"http://www.www.udacity.com\" rel=\"nofollow\">http://www.www.udacity.com</a> -- <a href=\"http://www.coursera.org\" rel=\"nofollow\">http://www.coursera.org</a> -- and get with the times and make significant, nonmarginal progress. also math &amp; stat ppl specifically really need to improve their answering skills so look towards <strong>quora on that respect</strong></p>\\r\\n',\n",
       " '<p>Finished a double degree Bachelor of Engineering (Robotics and Mechatronics), and Bachelor of Applied Science (Computer Science and Software Engineering) at Swinburne University in 2004, and a PhD, also at Swinburne on  \"Hard Real-Time Motion Planning for Autonomous Vehicles\"</p>\\r\\n\\r\\n<p>Currently employed as a research scientist specialising in modeling and simulation of physical systems. Working in Python, Mathematica and C/C++.</p>\\r\\n\\r\\n<p>My twitter handle is @walkera101</p>\\r\\n',\n",
       " '<p>Python and R</p>\\r\\n',\n",
       " \"<p>Software developer originally from New Zealand. I'm an ultra-runner and have completed at least 8 50km, a 50 mile and a 100km.</p>\\r\\n\",\n",
       " '<p>Just making good fresh codes :P</p>\\r\\n',\n",
       " '<p>I am a student who loves to learn more. i am always fascinated about our universe and how it works and because of this i have a love for physics (Astro). I have also interest in computers and programming which i does in my spare time.</p>\\r\\n',\n",
       " '<p>Computer Engineer; PhD candidate</p>\\r\\n',\n",
       " '<p>A great econometrics enthusiastic</p>\\r\\n',\n",
       " 'Todo',\n",
       " '<p>Swiss, 17, Web Developer.</p>\\r\\n',\n",
       " '<p>I\\'m a hydrogeologist, so I\\'m often swimming in data—either measured or modelled.</p>\\r\\n\\r\\n<p>I\\'ve been using R as my statistical computing tool since 2004.</p>\\r\\n\\r\\n<p>Several years ago I wrote an R package, <a href=\"http://cran.r-project.org/web/packages/seas/\" rel=\"nofollow\"><code>seas</code></a> (<a href=\"http://dx.doi.org/10.1016/j.cageo.2006.11.011\" rel=\"nofollow\">peer-reviewed article</a>), for deriving seasonal statistics from time series data, such as weather data.</p>\\r\\n',\n",
       " '<p>Student!</p>\\r\\n',\n",
       " '<p>Undergraduate student of Industrial Engineering at IIT Kharagpur, India</p>\\r\\n',\n",
       " '<p>Hi!</p>\\r\\n\\r\\n<p>sorry - I am C/C++ noobe, and I am reading a book<code>=)</code></p>\\r\\n\\r\\n<p>verse 4 rfc1925</p>\\r\\n\\r\\n<blockquote>\\r\\n  <p>Some things in life can never be fully appreciated nor\\r\\n          understood unless experienced firsthand. Some things in\\r\\n          networking can never be fully understood by someone who neither\\r\\n          builds commercial networking equipment nor runs an operational\\r\\n          network.</p>\\r\\n</blockquote>\\r\\n',\n",
       " '<p>Very courius guy!</p>\\r\\n',\n",
       " 'I\\'m a graduate student in computational biology working in the lab of Dr. David Baker. I develop code for <a href=\"http://boinc.bakerlab.org/\" rel=\"nofollow\">Rosetta@Home</a>, and I am interested in using computers to learn about the world. ',\n",
       " '<h2><em>Personality &amp; social psychologist</em></h2>\\r\\n\\r\\n<p><strong>Research interests:</strong></p>\\r\\n\\r\\n<ul>\\r\\n<li><em>Existential psychology</em> (meaning/purpose in life, uncertainty, existential problems and attitudes)</li>\\r\\n<li><em>Positive psychology</em> (psychological well-being, life satisfaction, affect)</li>\\r\\n<li><em>Motivation</em> (goals, values, motives, intrinsic &amp; extrinsic motivation, attainment, support &amp; conflict)</li>\\r\\n<li><em>Spirituality</em> (afterlife belief, religion, universalism)</li>\\r\\n<li><em>Traits</em> (the \"Big Five\", spirituality, intelligence, needs for cognition or closure, conservatism)</li>\\r\\n<li><em>Survey methods</em> (construction, validation, bias identification &amp; reduction)</li>\\r\\n<li><p><strong><em>Statistics</em></strong></p>\\r\\n\\r\\n<ol>\\r\\n<li>Latent factor structure</li>\\r\\n<li>Structural equation modeling</li>\\r\\n<li>Effect size estimation</li>\\r\\n<li>Longitudinal change</li>\\r\\n<li>Causality</li>\\r\\n<li>Nonparametric &amp; robust analysis</li>\\r\\n<li>Resampling methods</li>\\r\\n</ol></li>\\r\\n<li><em>Programming</em> (R, function writing, simulation testing)</li>\\r\\n</ul>\\r\\n\\r\\n<p><strong>Publications:</strong></p>\\r\\n\\r\\n<ul>\\r\\n<li>Me. (2013). <a href=\"http://escholarship.org/uc/item/3t34c68w\" rel=\"nofollow\"><em>Personal goals, psychological well-being, and meaning in life</em></a>. University of California, Riverside: Psychology.</li>\\r\\n<li>Steger, M. F., Pickering, N., Adams, E., Burnett, J., Shin, J. Y., Dik, B. J., &amp; Me. (2010). <a href=\"http://www.michaelfsteger.com/wp-content/uploads/2012/08/Steger-et-al.-PSR-2010.pdf\" rel=\"nofollow\">The quest for meaning: Religious affiliation differences in the correlates of religious quest and search for meaning in life</a>. <em>Psychology of Religion and Spirituality, 2</em>(4), 206-226.</li>\\r\\n</ul>\\r\\n\\r\\n<p><strong>Posters and presentations</strong> (PowerPoints available on <a href=\"http://www.slideshare.net/NickStauner\" rel=\"nofollow\">my SlideShare page</a>)</p>\\r\\n\\r\\n<ul>\\r\\n<li>Me &amp; Ozer, D. J. (2012). <em>Matching goals to values: Correlations follow semantic similarities.</em> 92nd WPA, San Francisco.</li>\\r\\n<li>Me. (2012). <em>Existential and psychological health as products of intrinsic goal attainment.</em> Presentation, April 19, UCR.</li>\\r\\n<li>Me, Selvam, T., Cheong, R., &amp; Ozer. (2011). <em>Religious differences in the value systems of meaningful (and meaningless) lives.</em> 2nd ARP, Riverside.</li>\\r\\n<li>Me &amp; Ozer. (2011). <em>Joint factors of spirituality and religiousness.</em> 91st WPA, Los Angeles.</li>\\r\\n<li>Me &amp; Ozer. (2011). <em>Spiritual predictors of the search for meaning in life.</em> 12th SPSP, San Antonio.</li>\\r\\n<li>Me. (2010). <em>Current research in existential psychology.</em> Presentation, November 4, UCR.</li>\\r\\n<li>Me, Boudreaux, M. J., &amp; Ozer. (2010). <em>Factor structure of the Values Q-Set.</em> 118th APA, San Diego.</li>\\r\\n<li>Me &amp; Ozer. (2010). <em>The motive content of meaningful (and meaningless) lives.</em> 15th EAPP, Brno, Czech Republic.</li>\\r\\n<li>Me, Stimson, T. S., &amp; Boudreaux. (2010). <em>The curve of the quest for a more meaningful life.</em> 11th SPSP, Las Vegas.</li>\\r\\n<li>Me. (2010). <em>The Values Q-Set.</em> Presentation, January 21, UCR.</li>\\r\\n<li>Me, Stimson, Boudreaux, &amp; Ozer. (2009). <em>When do personality traits predict personal goals?</em> 1st ARP, Evanston.</li>\\r\\n<li>Me. (2009). <em>The factor structure of personal goals.</em> Presentation, June 4, UCR.</li>\\r\\n<li>Me, Stimson, &amp; Ozer. (2009). <em>The factor structure of personal goals in an undergraduate population.</em> 10th SPSP, Tampa.</li>\\r\\n</ul>\\r\\n',\n",
       " '<p>I build robots.</p>\\r\\n',\n",
       " \"<p>I have been affected to computers ever since I first did get my hands on an old Windows 3.11 machine at the age of 8. Well, let's say starting then I always tried to expand my knowledge. While I still am not a typical wizard at some topic, I believe I have collected a fair amount of knowledge over various areas. </p>\\r\\n\\r\\n<p>These includes all of the three major desktop operating systems being used those days (with Windows being the least favored one and all Linux distributions counted as one single systems), as well as quite a lot of programming languages. </p>\\r\\n\\r\\n<p>In my later years I decided to study mathematics, which helped me to sharpen my logical reasoning and develop a toolset of problem solving methods.</p>\\r\\n\\r\\n<p>In 2011 I finished my M. Sc. in Maths and started working as a data- and process analyst at one of Germany's major companies. </p>\\r\\n\\r\\n<p>In parallel I am working part-time on my PhD, researching one practical application for B-Splines and the methods that can be used for that. </p>\\r\\n\\r\\n<p>Right now the languages and applications I use frequently are MATLAB, R and C++, as well as LaTeX, if one wants to call that a programming language (well, it's Turing-complete, isn't it?). Furthermore, I started getting used to Emacs and loving it - here, though, I'm only scratching on the surface.</p>\\r\\n\",\n",
       " \"<p>I'm BE Computer Engineering Student from Mumbai University.</p>\\r\\n\\r\\n<p>More interested in Learning New, powerful and expressive languages designed for different platform and with different intention.</p>\\r\\n\",\n",
       " '<p>I am a PhD Student at INRIA Rennes Bretagne Atlantique research center,  France. My PhD topic is on privacy preservation in user-centric peer to peer search environments. The field includes cryptography, differential privacy, data mining, secure multi-party computation and large scale peer to peer systems. I am also a staff member at the Computer Science department, Helwan University, Egypt. I currently hold a masters degree in Computer Science from University Rennes I, France.</p>\\r\\n\\r\\n<p>My interests include computational complexity and computability theory, linguistics and natural language processing, cryptographic protocols and quantum secure multi-party computation</p>\\r\\n',\n",
       " '<p>Researcher in meteorology and climatology in Fundación CEAM (<a href=\"http://www.ceam.es\" rel=\"nofollow\">http://www.ceam.es</a>) in Spain. Discovering R and getting things I never thought with meteo data.</p>\\r\\n',\n",
       " '<p>Professor of Stochastic Modelling, Newcastle University, UK.</p>\\r\\n',\n",
       " '<p>I work on data management and analysis.</p>\\r\\n',\n",
       " 'Just a plain old simple guy :)',\n",
       " '<p>Pursuing my PhD in statistics.</p>\\r\\n',\n",
       " \"<p>I'm a mathematics professor at New Mexico Tech.  </p>\\r\\n\",\n",
       " \"<p>Biochemist, new R user and 'American Distress' card user...</p>\\r\\n\",\n",
       " '<p>Developer: PHP, JavaScript, Debian / shell</p>\\r\\n\\r\\n<p>Current tools I actively use: MongoDB, MariaDB, Redis, PhalconPHP, CodeIgniter</p>\\r\\n\\r\\n<p>Projects I actively contribute to: <a href=\"http://trunkjs.com\" rel=\"nofollow\">TrunkJS</a>, <a href=\"https://github.com/mikegioia/debian-server\" rel=\"nofollow\">Debian-Server</a></p>\\r\\n',\n",
       " '<p>Thank you for visiting my profile page.</p>\\r\\n',\n",
       " \"<p>ex-quant doing data @shopify. We're hiring data analysts, scientists and statisticians! Screencaster at www.dataorigami.net</p>\\r\\n\",\n",
       " '<p>I want to be perfect in Perl, Linux, MySQL and R so that I can help others and pass my knowledge to next generation.</p>\\r\\n',\n",
       " \"<p>I'm a Senior Research Fellow and Consulting Biostatistician at the University of Otago, Wellington, in occasionally-sunny New Zealand. I work providing statistical consulting service for researchers at the Medical School here, as well as being a co-investigator on numerous health-related projects.</p>\\r\\n\\r\\n<p>My pre-biostats background (undergraduate and PhD) is in experimental psychology and cognitive neuroscience.</p>\\r\\n\\r\\n<p>When I'm not working, I'm doing other stuff :-)</p>\\r\\n\",\n",
       " 'I like turtles.',\n",
       " '<p>J2EE Developer and full time Ubuntu Linux user.\\r\\nWell versed and or certified in some of the following:\\r\\nAnt, Linux admin, Windows administration/deployments, Java, C, php, Apache, MySQL, javascript, HTML, XML</p>\\r\\n',\n",
       " '<p>I am a creative enthusiast who always seeks new ideas and technologies and aims to enhance work efficiency and increase my knowledge, I am able to balance competing priorities and tight deadlines with quality, I am able to develop and deliver plans, create new ideas and find market niches. I am a fast learner who cannot get enough of learning; I have excellent analytical skills and keen to work under pressure. I am always keen to work in different projects especially those who allow me the chance to learn.</p>\\r\\n',\n",
       " '<p>After about a year of being a member of the community I am now <a href=\"http://blog.stackoverflow.com/2010/06/server-fault-hiring-from-the-community/\">Stack Overflow Valued Associate #9 (Or Server Fault Valued Associate #1 if you like).</a><br></p>\\r\\n\\r\\n<p>Besides trying to keep these sites up I will also be writing posts for the [Server Fault Blog][2]. </p>\\r\\n\\r\\n<p>[2]: <a href=\"http://blog.serverfault.com/\">http://blog.serverfault.com/</a><p><a href=\"http://twitter.com/KyleMBrandt\" rel=\"nofollow\">@KyleMBrandt</a></p></p>\\r\\n',\n",
       " '<p>Commiter of Pig at The Apache Software Foundation</p>\\r\\n',\n",
       " '<p>I work in the market insights and strategies practice at Resource Systems Group. I specialize in discrete choice model estimation and application used to support the analysis of consumer behavior and value product drivers in a wide variety of settings for Fortune 500 clients.</p>\\r\\n',\n",
       " '<p>PostDoc</p>\\r\\n',\n",
       " '<p>united we stand</p>\\r\\n',\n",
       " '<p>Uhm, what about me? Just me.</p>\\r\\n',\n",
       " '<p>Here I am.</p>\\r\\n',\n",
       " '<p>NYU faculty.  Social science statistics.</p>\\r\\n',\n",
       " '<p>Interested lurker, occasional contributer, looking to increase my knowledge of statistics.</p>\\r\\n',\n",
       " '<p>Graduate student, University of Vermont</p>\\r\\n',\n",
       " '<p>Embedded systems programmer in Brazil.</p>\\r\\n',\n",
       " \"<p>I'm an analyst with an interest in public health and civic data hackery.</p>\\r\\n\",\n",
       " '<p>Psychologist, PhD student. </p>\\r\\n',\n",
       " '<p>My areas of expertise:</p>\\r\\n\\r\\n<ul>\\r\\n<li>C and C++ </li>\\r\\n<li>cross-platform development</li>\\r\\n<li>defensive coding practices</li>\\r\\n<li>critical code design</li>\\r\\n<li>Unix-like operating systems</li>\\r\\n<li>system scripting (bash, perl)</li>\\r\\n</ul>\\r\\n\\r\\n<p>Current employers:</p>\\r\\n\\r\\n<ul>\\r\\n<li>researcher for Cesnet (maintainer of the Czech NREN)</li>\\r\\n<li>C and C++ teacher at Faculty of Informatics at Masaryk University, Brno, Czech Republic</li>\\r\\n</ul>\\r\\n',\n",
       " '<p>student of bioinformatics. </p>\\r\\n\\r\\n<p>working at leibniz institute for crop plant reasearch and plant genetics. doing assemblys all day. :D </p>\\r\\n\\r\\n<p>interessted in programming languages. specially in scala. </p>\\r\\n',\n",
       " \"<p>I work as the main developer at Digital Creations AS, where I mainly develop the test-taking website Prove.no (mostly in Norwegian). </p>\\r\\n\\r\\n<p>In my spare time, I've written MaxTo, a Windows program that allows you partition your screen into regions that windows maximize to.</p>\\r\\n\\r\\n<p>I have a Master of Science in Computer Science from NTNU. I am also a Zend Certified PHP Engineer.</p>\\r\\n\",\n",
       " '<p>Studying Engineering Physics pursuing a Master\\'s degree in machine learning at KTH.</p>\\r\\n\\r\\n<p><a href=\"http://jim.pm\" rel=\"nofollow\">http://jim.pm</a> #portfolio</p>\\r\\n',\n",
       " '<blockquote>\\r\\n  <p><sup>The algorithm designer who does not run experiments risks becoming lost in abstraction. <em>—Sedgewick</em></sup></p>\\r\\n</blockquote>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://chat.stackexchange.com/rooms/9446/theory-salon\">cstheory salon</a> &amp; <a href=\"http://chat.stackexchange.com/rooms/2710/computer-science\">cs chat</a></li>\\r\\n<li><a href=\"http://vzn1.wordpress.com/2012/12/08/outline-for-a-np-vsppoly-proof-based-on-monotone-circuits-hypergraphs-and-factoring/\" rel=\"nofollow\">outline for a NP vs P/poly proof based on monotone circuits, hypergraphs, factoring, and slice functions</a></li>\\r\\n<li><a href=\"http://vzn1.wordpress.com/code/collatz-conjecture-experiments/\" rel=\"nofollow\">Collatz conjecture experiments</a></li>\\r\\n<li><a href=\"http://vzn1.wordpress.com/code/turing-machine-compiler/\" rel=\"nofollow\">Turing machine compiler in ruby</a></li>\\r\\n<li><a href=\"http://vzn1.wordpress.com/volunteer/\" rel=\"nofollow\">volunteer/collaborate in open science</a></li>\\r\\n</ul>\\r\\n',\n",
       " '<p>Interested in algorithms, graph theory, probability and statistics.</p>\\r\\n',\n",
       " '<p>Contact:\\r\\n<a href=\"http://www.linkedin.com/pub/kristjan-laane/40/9b0/698\" rel=\"nofollow\">http://www.linkedin.com/pub/kristjan-laane/40/9b0/698</a></p>\\r\\n',\n",
       " '<p>Github: <a href=\"https://github.com/michaelku\" rel=\"nofollow\">https://github.com/michaelku</a></p>\\r\\n',\n",
       " 'PhD student',\n",
       " '<p>Dreamer, Analyst, Engineer, Programmer, Photographer.</p>\\r\\n',\n",
       " '<p>Interested on statistics, mathematics and machine learning :)</p>\\r\\n',\n",
       " '<p>A Bioinformatics in a digging process</p>\\r\\n',\n",
       " '<p>I am a Senior Data Scientist at <a href=\"http://www.iovation.com/\" rel=\"nofollow\">iovation</a>. My professional interests lie in the application of open source analysis stacks to problems in various domains.</p>\\r\\n\\r\\n<p>Once upon a time I was a graduate student in Logic and Computation at Carnegie Mellon University, specializing in philosophy of science and formal epistemology (esp. computational epistemology). My undergraduate degrees are in philosophy and psychology (with a minor in computer information systems) from Humboldt State University.</p>\\r\\n\\r\\n<p>My interests are legion– they include statistics, complex systems theory, social psychology, visualization, machine learning, perception, cognitive psychology, political theory, the history of computing, and cognitive neuroscience.</p>\\r\\n',\n",
       " \"<p>I am the greatest programmer guy ever.\\r\\nOne time I put 6 semicolons at then end of my code.\\r\\nBecause I'm just that good.</p>\\r\\n\",\n",
       " '<p>I\\'m a computer science student at Princeton. My mission is to apply computer science to other fields - especially neuroscience, economics, and data mining. I\\'m also really into music and entrepreneurship.</p>\\r\\n\\r\\n<p>I wrote my first app (a VB6-powered word-search solver) when I was 7, and I\\'ve been hooked on programming ever since. I discovered and got involved in the Stack Overflow community in 2009 (when I was 12).</p>\\r\\n\\r\\n<p><a href=\"http://maximzaslavsky.com\" rel=\"nofollow\">Check out my website for more information or to get in touch</a>.</p>\\r\\n',\n",
       " '<p>Business Analyst, Furniture Builder, and Astronomer in Training.</p>\\r\\n',\n",
       " \"I'm a web developer/designer in Dallas, TX. I primarily work with PHP on the Symfony framework, along with MySQL, jQuery, Objective-C (for iPhone development), and of course XHTML/CSS.\",\n",
       " '<ul>\\r\\n<li>Masters degree in Math &amp; Computer Science, 2006–2011 — Moscow State University, Moscow, Russia.</li>\\r\\n<li>Masters degree in Management &amp; Human Resources, 2009–2011 — Moscow State University, Moscow, Russia.</li>\\r\\n<li>Ph.D. in Computer Science, 2011–... — Nanyang Technological University, Singapore.</li>\\r\\n</ul>\\r\\n',\n",
       " '<p>Recent Western Washington University graduate in computer science. I scooped up by Amazon and now am working at <a href=\"https://aws.amazon.com/marketplace\" rel=\"nofollow\">AWS Marketplace</a>.</p>\\r\\n\\r\\n<p>If you would like to befriend me on <a href=\"http://www.facebook.com/johnthecoolguy\" rel=\"nofollow\">the facebook</a>, feel free.</p>\\r\\n',\n",
       " '<p>I am a Research Analyst at WorldQuant. I enjoy photography and coding in my spare time.</p>\\r\\n',\n",
       " '<p>Professional CMS and TYPO3 developer. Feel free to write me an <a href=\"http://www.google.com/recaptcha/mailhide/d?k=01t1kGaxWdHhM2HBITlzL1WA==&amp;c=m8H_TE-gIy28sjV9rrCiaBhZk2iT0yRncIbFTMLrxIk=\" rel=\"nofollow\">e-mail</a>.</p>\\r\\n\\r\\n<p><a href=\"http://www.phpclasses.org/browse/author/441663.html\" rel=\"nofollow\">phpclasses</a> \\r\\n <a href=\"http://sourceforge.net/u/nano-math/profile/\" rel=\"nofollow\">Sourceforge</a>\\r\\n <a href=\"http://www.phpdevpad.de/\" rel=\"nofollow\">Phpdevpad</a>\\r\\n <a href=\"http://www.phpdevpad.de/geofence\" rel=\"nofollow\">Geofence</a><br>\\r\\n <a href=\"http://www.ozone3d.net/benchmarks/furmark_192_score.php?id=119383\" rel=\"nofollow\">Furmark-ID</a>\\r\\n <a href=\"http://www.3dmark.com/3dm11/6357413\" rel=\"nofollow\">3DMark</a>\\r\\n <a href=\"http://www.3dmark.com/3dmv/4654305\" rel=\"nofollow\">3DMark Vantage</a>\\r\\n <a href=\"http://www.3dmark.com/3dmv/4813519\" rel=\"nofollow\">3DMark Vantage 2</a></p>\\r\\n',\n",
       " '<p>Post Doc at Glostrup University Hospital, Denmark.</p>\\r\\n',\n",
       " 'Stock trader',\n",
       " '<ul>\\\\\\\\n<li><a href=\"http://mathiasbynens.be/\" rel=\"nofollow\">mathiasbynens.be</a></li>\\\\\\\\n<li><a href=\"http://twitter.com/mathias\" rel=\"nofollow\">twitter.com/mathias</a></li>\\\\\\\\n<li><a href=\"http://qiwi.be/\" rel=\"nofollow\">qiwi.be</a></li>\\\\\\\\n</ul>',\n",
       " '<p>I taught myself programming on a recycled Compaq 9071GL running Mandrake at the tender age of 12 and have been debating compilers ever since.</p>\\r\\n',\n",
       " '<p>computational linguist</p>\\r\\n',\n",
       " '<p>Consultant and Software programmer by day.</p>\\r\\n\\r\\n<p>Superhero by night. </p>\\r\\n\\r\\n<p>Speaks fluently Java. Works agile and test driven, or at least feels appropriately ashamed when failing to do so. Current job title is Eclipse specialist and java developer. Which basically means try keep the Eclipseboat afloat and working at optimum performance for some 400+ users.</p>\\r\\n',\n",
       " '<p>I like turtles. email == firstname@lastname.ca</p>\\r\\n',\n",
       " '<p>John Sjölander is the CTO of Burt, <a href=\"http://www.burtcorp.com/\" rel=\"nofollow\">www.burtcorp.com</a>. Creating tools for Creative Agencies and publishers to make better decisions in advertising.</p>\\r\\n',\n",
       " '<p>I study psychology and work in a research group. My main interests are evolution, individual differences, personality development and intelligence.</p>\\r\\n\\r\\n<p>I developed an open-source <a href=\"https://formr.org\" rel=\"nofollow\">survey framework called formr</a> as a summer side gig.</p>\\r\\n',\n",
       " '<p><sup><a href=\"http://web.archive.org/web/20080514205739/http://www.math.wustl.edu/~msh210/legal.html#sm\" rel=\"nofollow\"><em>msh210</em> is a service mark.</a></sup></p>\\r\\n',\n",
       " 'Independent developer',\n",
       " '<p>Coder. </p>\\r\\n',\n",
       " '<p>Developing features for the Casebook 2, an innovative Rails-based case management and social analytics web-app that assists social workers for the State of Indiana\\'s Department of Child Services (<a href=\"http://www.in.gov/dcs\" rel=\"nofollow\">http://www.in.gov/dcs</a>) in tracking, assessing, and managing their child welfare cases. My peers and I are working out of Pivotal Labs NYC and leveraging Agile/XP and Rails to ensure better outcomes for children in foster care through better tools for Indiana DCS. Working on multidiscipline teams of Designers, Rails Developers, and DevOps pairs from Case Commons and Pivotal Labs to continuously refine our XP process and extend our large scale agile development capabilities.</p>\\r\\n',\n",
       " '<p>Student of MSc Econometrics at Vilnius University and MSc Economic Analysis at Universidad Carlos III de Madrid.</p>\\r\\n',\n",
       " '<p>Merge keep</p>\\r\\n',\n",
       " '<p>I do a bunch of stuff.  Lately a lot of Scala, Play Framework, Hadoop/Hbase/Pig/, Redshift, and some python.</p>\\r\\n',\n",
       " \"<p>Moved to ubuntu a while ago, and haven't regretted it! Its fantastic and goes along well with me wanting to help others when they in trouble :)</p>\\r\\n\\r\\n<p>Profession wise, mechanical engineer and i do my day to day work in ubuntu :) got most of the software i need!</p>\\r\\n\",\n",
       " \"<p>Currently, I'm a student and CV lurker getting up on my statistics.</p>\\r\\n\",\n",
       " \"<p>I'm new here.</p>\\r\\n\\r\\n<p>Some things I'm into (listed chronologically of when they entered my love-life) - math, music, my wife, bike polo.</p>\\r\\n\",\n",
       " '<p>Actuary for a life insurance company.</p>\\r\\n',\n",
       " 'C# software developer, generally specialising in financial markets.',\n",
       " '<p>I am an oceanographer who enjoys studying floating body hydrodynamics and surface waves.  I am also interested in the design of electrical generators.  My day-to-day work includes programming in Matlab and Simulink and statistical analysis of SQL databases. Am seeking to learn Simscape and improve Simulink skills. Thanks Stack Overflow for so many helpful answers to programming dilemmas!</p>\\r\\n',\n",
       " '<p>My name is Chris Lasher. I use computers to study biology. I enjoy revision control, unit testing, and long coding sessions in Python.</p>\\r\\n',\n",
       " 'Graduate student at UC Irvine.',\n",
       " \"<p>Name intended to be a vocal exercise :)</p>\\r\\n\\r\\n<p>email: i'll put a tor redirect eventually</p>\\r\\n\",\n",
       " '<p>Post-doctoral scholar at UC Berkeley, currently visiting at Duke</p>\\r\\n',\n",
       " \"GIS, Transportation, Civil Engineer in New Brunswick Canada. I'm interested in using FOSS, python, QGIS, PostGIS, GRASS, and QT to develop open source engineering applications\",\n",
       " \"<p>I'm a freelance consultant and Microsoft Certified Trainer, and an MVP for Dynamics CRM. </p>\\r\\n\\r\\n<p>I've been working in IT in all sorts of roles for over 20 years including programming, teaching and system management, and specialising in Dynamics CRM for more than 6.</p>\\r\\n\",\n",
       " \"<p>Intermediate statistics, mostly self taught so don't take me <em>too</em> seriously.</p>\\r\\n\",\n",
       " '<p>I am a researcher in theoretical ecology and evolution. My work focuses on regime shifts and draws from many disciplines, including mathematics, computer science, economics, and physics.  I\\'m particularly interested in open science, active learning, big data, web tools, R, high performance computing.  Visit my <a href=\"http://www.carlboettiger.info/lab-notebook.html\" rel=\"nofollow\">open lab notebook</a> for more information on what I do.  </p>\\r\\n',\n",
       " \"<p>I'm a PhD student in the stats department at Stanford.</p>\\r\\n\",\n",
       " '<p>Sociologist - economist\\r\\nNot really into maths\\r\\nQuite good with Stata</p>\\r\\n',\n",
       " '<p>astrophysics student at University of Sydney</p>\\r\\n',\n",
       " 'Hardware engineer turned programmer',\n",
       " \"<p>I design and build Grant and Program management web apps for SAIC by day and I design and build the best place to find US dressage scores  online for horse shows by night.  The first computers I programmed on were an IBM 8088 and an Apple 2.  I graduated in 1986 to a Mac-Plus... </p>\\r\\n\\r\\n<blockquote>\\r\\n  <p><em>(What's the most powerful computer in the world?  The one people use.)</em></p>\\r\\n</blockquote>\\r\\n\\r\\n<p>... then I upgraded to dual floppy drives (Chuck Yeager Flight Simulator required it), then upgraded to a 40 MB HD (all my MIDI programming demanded it).  I was then seduced by the PC-dark side </p>\\r\\n\\r\\n<blockquote>\\r\\n  <p><em>(What's the most powerful computer in the world?  The one that has software to run on it.)</em></p>\\r\\n</blockquote>\\r\\n\\r\\n<p>In 1990, I discovered SAS, SPSS and Statistics.  So while I'm a .NET geekarchitect today, I'm also a kick-butt SAS-Ninja-Shadow-Warrior as well.</p>\\r\\n\",\n",
       " '<p>A software developer.</p>\\r\\n',\n",
       " '<p><a href=\"http://stackexchange.com/users/54832\">\\r\\n<img src=\"http://stackexchange.com/users/flair/54832.png\" width=\"208\" height=\"58\" alt=\"profile for hhh on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for hhh on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\\r\\n</a></p>\\r\\n\\r\\n<p><strong>Great!</strong></p>\\r\\n\\r\\n<blockquote>\\r\\n  <ul>\\r\\n  <li><p><em>Ungeduld ist Angst.</em></p></li>\\r\\n  <li><p><a href=\"http://www.youtube.com/watch?v=JCQVnSOFqfM\" rel=\"nofollow\"><em>It\\'s not time to make a change. Just relax, take it easy. You\\'re still young, that\\'s your fault. There\\'s so much you have to know.</em></a></p></li>\\r\\n  <li><p><em>It is nice to be important but more important to be nice.</em> ~Roger Federer</p></li>\\r\\n  <li><p><em>Be yourself; everyone else is already taken.</em> - Oscar Wilde</p></li>\\r\\n  <li><p><em>I can\\'t control the wind but I can adjust the sail.</em> - Ricky Skaggs</p></li>\\r\\n  <li><p><em>If you want your children to be intelligent, read them fairy tales. If you want them to be more intelligent, read them more fairy tales.</em> - Albert Einstein</p></li>\\r\\n  <li><p><em>You swing your best when you have the fewest things to think about.</em> - Bobby Jones</p></li>\\r\\n  <li><p><em>It is the mark of an educated mind to be able to entertain a thought without accepting it.</em> - Aristotle</p></li>\\r\\n  <li><p><em>We can do better. We have to do better.</em> - Teemu Selänne</p></li>\\r\\n  <li><p><em>Real artists ship.</em> - Steve Jobs</p></li>\\r\\n  <li><p><em>The only thing you can control is your attitude toward the next shot.</em> - Mark McCumber</p></li>\\r\\n  <li><p><em>Simplicity is not the goal. It is the by-product of a good idea and modest expectations.</em> -Paul Rand</p></li>\\r\\n  <li><p><em><a href=\"http://www.inc.com/jeff-haden/the-8-qualities-of-remarkable-employees.html\" rel=\"nofollow\">\"Self-motivation often springs from a desire to show that doubters are wrong.\"</a></em></p></li>\\r\\n  <li><p><em>An overflow of good converts to Awesome.</em> ~William Shakespeare</p></li>\\r\\n  <li><p><em>I skate to where the puck is going to be, not where it has been.</em> ~Gretzky</p></li>\\r\\n  <li><p><em>In mathematics the art of asking questions is more valuable than solving problems.</em> ~Cantor</p></li>\\r\\n  <li><p><em>Test fast, fail fast, adjust fast.</em> ~Tom Peters</p></li>\\r\\n  <li><p><a href=\"http://www.khoslaventures.com/\" rel=\"nofollow\"><em>\"Our willingness to fail gives us the ability and opportunity to succeed where others may fear to tread.\"</em></a></p></li>\\r\\n  <li><p><em>I\\'ve failed over and over and over again -- that is why I succeed.</em> —Michael Jordan</p></li>\\r\\n  <li><p><a href=\"http://chat.stackexchange.com/transcript/message/6016840#6016840\"><em>To keep pace with the growth of mathematics, one would have to read about fifteen papers a day</em></a>.</p></li>\\r\\n  <li><p><a href=\"http://apple.stackexchange.com/users/1346/jason-salaz\"><em>The best protagonists rarely say anything.</em></a>.</p></li>\\r\\n  <li><p><em>Don\\'t give advices and don\\'t listen advices.</em></p></li>\\r\\n  <li><p><em>The less you say the stronger the strength of your words is.</em></p></li>\\r\\n  </ul>\\r\\n  \\r\\n  <p><strong>3 Principles of Success</strong> (Harford)</p>\\r\\n  \\r\\n  <blockquote>\\r\\n    <ol>\\r\\n    <li><p>Seek out and try new things.</p></li>\\r\\n    <li><p>When trying something new, do it on a scale where failure is survivable.</p></li>\\r\\n    <li><p>Seek out feedback (to determine your level of success) and learn from your\\r\\n    mistakes as you go along.</p></li>\\r\\n    </ol>\\r\\n  </blockquote>\\r\\n</blockquote>\\r\\n\\r\\n<p><em>P.s. <a href=\"http://www.freerangefactory.org/site/uploads/Main/article1.pdf\" rel=\"nofollow\">6 ways to kill creativity</a>, contact in forename@surname.com.</em></p>\\r\\n',\n",
       " \"<p>I'm a Master's student at Colorado School of Mines pursuing my degree in Computer Science.</p>\\r\\n\",\n",
       " '<p>Developer, designer from Cork, Ireland working for <a href=\"http://www.teamwork.com\" rel=\"nofollow\">Teamwork</a>.</p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://adamlynch.com\" rel=\"nofollow\">adamlynch.com</a></li>\\r\\n<li><a href=\"http://github.com/adam-lynch\" rel=\"nofollow\">github.com/adam-lynch</a></li>\\r\\n<li><a href=\"http://twitter.com/lynchy010\" rel=\"nofollow\">@lynchy010</a></li>\\r\\n</ul>\\r\\n',\n",
       " '<p>Student of Computational Linguistics B.Sc. at the University of Potsdam, Germany</p>\\r\\n\\r\\n<p>Programmer at a small backup software company (Visual C++, Python, some VB6 and PHP)</p>\\r\\n',\n",
       " '<p>MSc in Mathematics, former software developer/systems architect (mostly MS technology), currently working on a PhD in Statistics (bioscience-engineering) and teaching maths (calculus and algebra) and informatics (java, LaTeX and Excel) to future Bachelors in bioscience Engineering at UGent.</p>\\r\\n',\n",
       " '<p>Contract developer, worked on everything from MIPS assembly to financial J2EE systems. </p>\\r\\n\\r\\n<p>I wish to go work on Mars. Or Australia.</p>\\r\\n',\n",
       " '<p>Bioinformaticist studying the evolution of ant societies. \\r\\nSome recent stuff: </p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://www.antgenomes.org\" rel=\"nofollow\">ant genomes database</a></li>\\r\\n<li>software for <a href=\"http://www.sequenceserver.com\" rel=\"nofollow\">custom blast servers</a></li>\\r\\n</ul>\\r\\n\\r\\n<p>hacking with ruby/bioruby, R/bioconductor and a bunch of shell scripts</p>\\r\\n',\n",
       " \"<p>I received my m.s. in math and statistics about 10 years ago.  Unfortunately, due to things somewhat out of my control, I haven't been able to do much stats until recently. I'm trying to 'get back in the game'.  </p>\\r\\n\",\n",
       " \"<p>I'm senior-year psychology student interested in statistical analysis, nonverbal psychology and web-development.</p>\\r\\n\",\n",
       " '<p>I\\'m a chemical oceanographer working at the Microbiology Department of the University of Tennessee, Knoxville.  You can find more about my work at my <a href=\"http://web.utk.edu/~asteen1\" rel=\"nofollow\">professional website</a>.</p>\\r\\n',\n",
       " \"<p>I'm an Italian boy. Actually I'm studying Computer Science Engineering (Software Engineering) at the University of Catania.</p>\\r\\n\\r\\n<h2>Main interests</h2>\\r\\n\\r\\n<p>I really like programming. I am very experienced in Microsoft ASP.NET web applications development but also in other languages and application types:</p>\\r\\n\\r\\n<ul>\\r\\n<li>Microsoft ASP.NET for web applications development (C#, VB.NET, F#)</li>\\r\\n<li>Microsoft .NET Framework for software development (C#, VB.NET, F#)</li>\\r\\n<li>Microsoft WCF and WPF applications</li>\\r\\n<li>C++ with Boost library integration</li>\\r\\n<li>C</li>\\r\\n<li>Java (SE and EE) for applications development</li>\\r\\n</ul>\\r\\n\\r\\n<p>I also like creating graphics, animations and presentation using the most popular applications ni the Adobe families (I saw Flash growing since when it was still a baby, 5.0, and it was still Macromedia).</p>\\r\\n\\r\\n<h2>Languages</h2>\\r\\n\\r\\n<p>I like travelling a lot, and the best part of travelling is experiencing new languages, new sounds new people and culture. I actually love Asia. I lived in Japan for one year and worked there. Made also a lot of friends and gonna return there soon (I hope).</p>\\r\\n\\r\\n<h2>Sports</h2>\\r\\n\\r\\n<p>I also like sports. I practiced many but now I almost focus on <strong>human swimming</strong> and <strong>open water human swimming</strong>.</p>\\r\\n\\r\\n<h2>Other</h2>\\r\\n\\r\\n<p>I laso like photography, food (strange food) and music.</p>\\r\\n\",\n",
       " '<p>Software engineer with interests in Distributed Systems, Programming Languages and Statistics &amp; Probability.</p>\\r\\n',\n",
       " '<p>x</p>\\r\\n',\n",
       " \"<p>Student, Ubuntu 10.04 User.</p>\\r\\n\\r\\n<p>Uses Windows XP sometimes. Hates Vista with a passion. Interested in Windows 7, though haven't tried it much.</p>\\r\\n\\r\\n<p>Programs PIC microcontrollers in C and assembly. Favours dsPICs at the moment, but hasn't tried any other microcontroller yet.</p>\\r\\n\\r\\n<p>Prefers Python any day!</p>\\r\\n\",\n",
       " 'Only a little special.',\n",
       " '<p>I am a UC Berkeley Undergraduate majoring in applied mathematics.</p>\\r\\n',\n",
       " \"<p>I'm an analyst trained in frequentist statistics looking to learn more about ever-present Bayesian statistics - and to catch hold of the coat tails of machine learning that's come speeding out of nowhere and looks set to eclipse everything I yawned my way through at university. </p>\\r\\n\\r\\n<p>I've worked on poverty data for the UK government and clinical trials for pharmaceutical's.  Am currently working alongside epidemiologists on large, medical, observational data sets.</p>\\r\\n\",\n",
       " '<p>You know what is reuptation? Is people talking, is gossip. I also have reputation; not so pleasant, I think you know.</p>\\r\\n',\n",
       " 'Prometheus in the making.',\n",
       " '<p>MSc in Artificial Intelligence the University of Edinburgh</p>\\r\\n\\r\\n<p>BSc, Masters in Electrical and Computer Engineering, National Technical University of Athens</p>\\r\\n',\n",
       " '<p>Professor (emeritus) Political Science and International Relations, University of Delaware</p>\\r\\n',\n",
       " \"<p>I love to build things that people love to use.<br>\\r\\nI like scientific data analysis.<br>\\r\\nI've spent most of my life in medicine and neuroscience!<br>\\r\\nI like: vim, zsh, golang, debian, python\\r\\nI am addicted to writing code! and learning new things!!</p>\\r\\n\",\n",
       " \"<p>I'm a SCJP, currently studying CS. </p>\\r\\n\",\n",
       " '<p>I am in my third year of studies in mathematics at University of Montreal. My current interests are number theory, analysis, measure theory and abstract algebra.</p>\\r\\n',\n",
       " '<p>I am a recent electrical engineering graduate from Rutgers University, and a employee at <a href=\"http://pulsetor.com/\" rel=\"nofollow\">PulseTor</a> doing mostly FPGA programming.</p>\\r\\n\\r\\n<p>My advice for R programmers: <a href=\"https://svn.r-project.org/R/trunk/\" rel=\"nofollow\">look under the hood</a>. Most of the R interpreter source makes a lot of sense and you can learn a lot about how the language works.</p>\\r\\n\\r\\n<p><a href=\"http://www.facebook.com/ellbur\" rel=\"nofollow\">facebook</a></p>\\r\\n',\n",
       " \"<p>Academic in the Dept of Communication &amp; Systems at The Open University. I'm not a statistician, but I am a public open data junkie...</p>\\r\\n\",\n",
       " \"I'm a recent graduate of UC Davis in Economics, Political Science, and Statistics. I currently work at the Bioinformatics Core at UC Davis building web and database applications, and doing statistical programming in R.\\\\\\\\n\\\\\\\\nFavorites: \\\\\\\\nEmacs, Git, C, Python, R, and any nifty open source *nix tool.\",\n",
       " '<p>I am currently working on machine learning and signal processing technologies to deliver innovative, low-cost, energy efficient smart home solutions to the market. My work includes constant interaction and problem solving with top-notch machine learning and data mining algorithms as well as big data structures and streaming methodologies that make machine learning possible for real-time data.</p>\\r\\n\\r\\n<ul>\\r\\n<li>Professional experience of Java and Python software/algorithm development with Scrum methodologies, specializing on signal processing, audio processing, data mining and machine learning</li>\\r\\n<li>Theoretical and practical hands-on experience on audio fingerprinting, end-to-end mobile application development in Android</li>\\r\\n<li>Practical experience of project management and development during internships at high-tech companies professional in their fields</li>\\r\\n</ul>\\r\\n\\r\\n<p>Please refer to My Android app portfolio on Google Play using my full name.</p>\\r\\n',\n",
       " '<p>I am an Assistant Professor of Biology at Longwood University.  My main interests are how biodiversity affects ecosystem function in aquatic sediments.  You can find out more about what my lab is doing here: <a href=\"http://longwood.edu/staff/fortinok\" rel=\"nofollow\">http://longwood.edu/staff/fortinok</a></p>\\r\\n',\n",
       " '<p>Passionate about Machine Learning, Analytics, Information Extraction/Retrieval and Search.</p>\\r\\n',\n",
       " '<p>Interested primarily in econometric analysis of large data sets and in dimensionality reduction methods. Making a living with equity factor models and portfolio optimization. B.S./M.S. in Physics (University of Rome), M.S. Statistics, M.S./Ph.D. Management Science (Stanford). </p>\\r\\n',\n",
       " '<p>Curious &amp; Evolving...</p>\\r\\n',\n",
       " '<p>i build Systems &amp; Tools for Analysis, Prediction, Visualization, &amp; Simulation.</p>\\r\\n\\r\\n<p>i also design, code, and deploy complete Machine Learning-based applications (e.g., anti-fraud filter, recommendation engine, monitoring/anomaly detectors), usually designed as modular internal thrift services decoupled from the main app.</p>\\r\\n\\r\\n<p><b>Techniques:</b> </p>\\r\\n\\r\\n<ul>\\r\\n<li><p><strong><em>Machine Learning</em></strong>: in particular,\\r\\nrecursive descent parser (CART/C4.5), Multi-layer Perceptron,\\r\\nSVM/SVR, Kernel Machines, kNN/kdtree, Probabilistic Graphical Models (eg, Markov Random Field)</p></li>\\r\\n<li><p><strong><em>Dimension Reduction Techniques</em></strong>: spectral decomposition (PCA &amp; kPCA, kLDA), Kohonen Map (self-organizing map)</p></li>\\r\\n<li><p><strong><em>Social Network Analysis &amp; Visualization</em></strong>: using graph theoretic techniques for e.g., community detection, id of members essential for network health/growth; identify nascent sub-communities; (particular fluency <em>GraphViz</em>, the premiere tool for graph layout/visualization, <em>NetworkX</em>, the primary network analysis library for python, and <em>d3</em>). </p></li>\\r\\n<li><p><strong><em>Analysis &amp; Modeling of Time-Dependent Data</em></strong></p></li>\\r\\n<li><p><strong><em>Optimization</em></strong>: Combinatorial Optimization and Constraint-Satisfaction Programming</p></li>\\r\\n<li><p><strong><em>Numerical Methods</em></strong>: e.g., matrix decomposition, Monte Carlo techniques, Gaussian quadrature, finite difference methods </p></li>\\r\\n<li><p><strong><em>Data Modeling</em></strong> for \"Non-Relational\" systems (in particularly <strong>Redis</strong> and MongoDB) and for relational (ROLAP), multi-dimensional (MOLAP), and hybrid (HOLAP) Data Warehouse systems using conventional relational/SQL servers.</p></li>\\r\\n</ul>\\r\\n\\r\\n<hr/>\\r\\n\\r\\n<p><b>toolchain:</b> </p>\\r\\n\\r\\n<ul>\\r\\n<li>python</li>\\r\\n<li>C</li>\\r\\n<li>cython</li>\\r\\n<li>SciPy + NumPy + Matplotlib</li>\\r\\n<li>javascript</li>\\r\\n<li>R</li>\\r\\n<li>redis</li>\\r\\n<li>riak</li>\\r\\n<li>HDF5 (&amp; pytables, h5py)</li>\\r\\n<li>graphviz</li>\\r\\n<li>node.js</li>\\r\\n<li>flask/werkzeug (python web framework)</li>\\r\\n<li>storm (distributed processing)</li>\\r\\n<li>d3.js (svg template primitives for rendering plots in the browser)</li>\\r\\n<li>git (&amp; gitHub)</li>\\r\\n<li>vagrant</li>\\r\\n</ul>\\r\\n\\r\\n<p>No recruiters</p>\\r\\n',\n",
       " \"<p>I'm a graduate student interested in comparative physiology, animal behavior, biomechanics, and myrmecology.</p>\\r\\n\",\n",
       " '<p>Physicist, Photonics Engineer, cellist, R-evangelist, pinball fanatic, xkcd follower. And owner of \"Hells Doggies,\"  2 wonderful Papillons and one incredibly sweet motley mutt.</p>\\r\\n\\r\\n<p>New!  <a href=\"https://github.com/cellocgw\" rel=\"nofollow\">https://github.com/cellocgw</a>  for various R- code tools and toys.</p>\\r\\n',\n",
       " '<p>Its me :), \\r\\nMohamed cherif DANI, 25yo, technologies fan :) , ;) </p>\\r\\n',\n",
       " '<p>Risk Manager at Raiffeisen Capital Management</p>\\r\\n\\r\\n<p>External Lecturer at Vienna University of Technology</p>\\r\\n',\n",
       " '<p>I am a Lecturer in the School of Psychology at Deakin University bridging I/O psychology and statistics.</p>\\r\\n\\r\\n<p>I\\'m quite active on the <a href=\"http://cogsci.stackexchange.com/users/52/jeromy-anglim\">Cognitive Sciences</a> and <a href=\"http://stats.stackexchange.com/users/183/jeromy-anglim\">Statistics</a> Stack Exchanges.</p>\\r\\n\\r\\n<p>You can find me also on:</p>\\r\\n\\r\\n<ul>\\r\\n<li>Twitter: <a href=\"https://twitter.com/#!/JeromyAnglim\" rel=\"nofollow\">@JeromyAnglim</a></li>\\r\\n<li>My blog on psychology and statistics: <a href=\"http://jeromyanglim.blogspot.com\" rel=\"nofollow\">http://jeromyanglim.blogspot.com</a></li>\\r\\n</ul>\\r\\n',\n",
       " \"<p>The work bio redux...</p>\\r\\n\\r\\n<p>Work is divided between digital strategy consulting and business development for online publishers. With a career started in engineering and development, Mike brings a knowledgeable, pragmatic approach to digital business. Outside of work Mike is WordPress core contributor and contributed plugin developer.</p>\\r\\n\\r\\n<p>Mike also has a background in engineering and applied mathematics from Northwestern University and The University of Chicago, he's not afraid to get technical and craft detailed answers. Combining thorough analysis with industry knowledge, ranging from commercial construction to manufacturing, he has solved development challenges and built repeatable processes for reliable software. Leading and building teams for companies like Lexus, Google's Android/Nexus One, The Amgen Foundation, and LucasArts he's also helped future generations of programmers to work smart and build well.</p>\\r\\n\\r\\n<p>When not staring at a screen and keyboard, which isn't often, Mike is a runner with over a dozen marathons, triathlons, &amp; a recent ultra completed. When it's raining in Portland, he claims to only run to keep his malamute in shape and so his kids won't get too far ahead.</p>\\r\\n\",\n",
       " '<p>epidemiology</p>\\r\\n',\n",
       " '<p>merge keep</p>\\r\\n',\n",
       " \"<p>data_stuff &lt;- paste('data', c('scientist', 'munger', 'learner', 'geek')</p>\\r\\n\\r\\n<p>r_stuff &lt;- paste('R', c('enthusiast', 'community builder')</p>\\r\\n\",\n",
       " '<p>I\\'m a PhD student in  Computer Science at <a href=\"http://bit.ly/1lugQ1M\" rel=\"nofollow\">ETH Zurich</a> doing research on Bitcoin.</p>\\r\\n\\r\\n<p><strong>Publications</strong></p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://bit.ly/1emqRvE\" rel=\"nofollow\">Information Propagation in the Bitcoin Network</a> (<a href=\"http://bit.ly/18UcYqu\" rel=\"nofollow\">bibtex</a>)</li>\\r\\n<li><a href=\"http://bit.ly/1pc9GQZ\" rel=\"nofollow\">Bitcoin Transaction Malleability and MtGox</a> (<a href=\"http://bit.ly/YACGNc\" rel=\"nofollow\">bibtex</a>)</li>\\r\\n<li><a href=\"http://bit.ly/18Ud1Ts\" rel=\"nofollow\">Have a Snack, Pay with Bitcoins</a> (<a href=\"http://bit.ly/1kWdp6F\" rel=\"nofollow\">bibtex</a>)</li>\\r\\n<li><a href=\"http://bit.ly/1tCz9Hl\" rel=\"nofollow\">BlueWallet: The secure Bitcoin Wallet</a> (<a href=\"http://bit.ly/1xP5kts\" rel=\"nofollow\">bibtex</a>)</li>\\r\\n</ul>\\r\\n',\n",
       " '<p>\\r\\nI am ... if you really want to know, check my website ;-)\\r\\n</p>\\r\\n',\n",
       " '<p>student of bioinformatics and sociology</p>\\r\\n',\n",
       " '<p>Half network, half mathematician, half programmer, half computer scientist, technicaly half of evrything .\\r\\nR, java fan, open source lover, soft computing hacker.</p>\\r\\n',\n",
       " '<p>Yet another physicist pretending to be a statistician. Mortgages, housing, economic forecasts, and trading strategies, oh my.</p>\\r\\n',\n",
       " '<p>Postdoctoral Research Fellow in Computational Linguistics at the University of Oslo, Norway. Research on dialogue modelling for spoken dialogue systems and statistical machine translation.</p>\\r\\n',\n",
       " '<p>Interested in learning as much as possible about TeX and co.</p>\\r\\n',\n",
       " 'A python, matlab and django programmer in a bioinformatics PhD program',\n",
       " '<p>I work with animals and like it that way.</p>\\r\\n',\n",
       " \"<p>Three years ago I start scripting - since then I've spent endless hours learning. I don't think I've had a single day in those three years that I haven't learnt something new about computers. And of course, there is no better way to learn than to do - probably why I'd happily rewrite the wheel to find out how it works. Of course my personal favourite is maths. Studying Further Maths for my AS levels gives me plenty to do implementing various theories, particularly in decision maths.</p>\\r\\n\",\n",
       " '<p>Research methods, stats, psychometrics, and everything related to R.</p>\\r\\n',\n",
       " '<p>Newbie to computer hacking. Long time math geek. PhD in Machine Learning. Interested in classification and big data on *nix platform using open source technologies.</p>\\r\\n',\n",
       " '<p>“Daughter of the Night, she walks again. The ancient war, she yet fights. Her new lover she seeks, who shall serve her and die, yet serve still. Who shall stand against her coming? The Shining Walls shall kneel. Blood feeds blood. Blood calls blood. Blood is, and blood was, and blood shall ever be.”</p>\\r\\n',\n",
       " '<p>I am the head of a research team in genomics. My academic interests include satistics, computer sciences, biotechnologies and evolution. You can have a look at our activities on the web site of the lab\\r\\n<a href=\"http://lab.thegrandlocus.com\" rel=\"nofollow\">http://lab.thegrandlocus.com</a></p>\\r\\n\\r\\n<p>I maintain a blog on statistics, their use, and their misuse at <a href=\"http://blog.thegrandlocus.com\" rel=\"nofollow\">http://blog.thegrandlocus.com</a></p>\\r\\n',\n",
       " '<p>Professor of Financial Econometrics at Warwick Business School and advisor to London Select Fund and Old Mutual Asset Managers in London. Interested in all areas of Stats, Econometrics and Finance.</p>\\r\\n',\n",
       " '<p>C# and all round .NET Enthusiast</p>\\r\\n\\r\\n<p>Closet Linux Fan (Vim).</p>\\r\\n',\n",
       " '<p>I am a biology student, wanting to learn all I can about statistics in order to better inform my choices for my experiments. I am extremely interested in the fate of our oceans and all aspects of science which intersect with that topic. Currently I am involved in a globally comparable research project investigating the stress tolerance of invasive species.</p>\\r\\n',\n",
       " '<ul>\\r\\n<li>Graduated in Economics at the University of Vienna</li>\\r\\n<li>Research Assistant at the Vienna University of Economics and Business</li>\\r\\n</ul>\\r\\n',\n",
       " '<p>After studying algebraic geometry, algebraic topology and category theory, \\r\\nafter teaching  mathematics and computer science \\r\\nin university and high school, \\r\\nafter studying bioinformatics and implementing image analysis algorithms in a biotech start-up, \\r\\nI have since been working in finance, in London, Tōkyō and Hong Kong.</p>\\r\\n\\r\\n<p>My preferred programming languages are R and Perl, \\r\\nbut I try to use the best language and tools \\r\\nto solve the problem at hand.\\r\\nI also like to learn more marginal languages, such as Haskell or Oz/Mozart. I am interested in human languages, too.</p>\\r\\n\\r\\n<p>Here is a selection of my answers.</p>\\r\\n\\r\\n<ul>\\r\\n<li><p>Computing a probability density function \\r\\nfrom its \\r\\n<a href=\"http://stackoverflow.com/questions/10029956/calculating-a-density-from-the-characteristic-function-using-fft-in-r/10038141#10038141\">characteristic function</a>\\r\\nor by \\r\\n<a href=\"http://stats.stackexchange.com/questions/26598/maximum-entropy-sampler/26605#26605\">maximizing its entropy</a>;</p></li>\\r\\n<li><p>Quadratic programming \\r\\n<a href=\"http://stackoverflow.com/questions/9817001/optimization-with-constraints/9817442#9817442\">to ensure a sequence is increasing</a>\\r\\nor to <a href=\"http://stackoverflow.com/questions/9971268/plot-an-item-map-based-on-difficulties/9971741#9971741\">position labels on a plot</a></p></li>\\r\\n<li><p>Reparametrizing \\r\\n<a href=\"http://stackoverflow.com/questions/9666120/constrain-optimisation-problems-in-r/9667097#9667097\">optimization problems</a>\\r\\nto make them \\r\\n<a href=\"http://stackoverflow.com/questions/9592369/in-r-how-do-i-find-the-optimal-variable-to-maximize-or-minimize-correlation-bet/9593809#9593809\">unconstrained</a>,\\r\\nfor instance \\r\\n<a href=\"http://stats.stackexchange.com/questions/25007/fitting-the-parameters-of-a-stable-distribution\">to fit stable distribution</a>;\\r\\npenalizing them to restrict them to a <a href=\"http://stackoverflow.com/questions/11110848/how-to-optimize-for-integer-parameters-and-other-discontinuous-parameter-space/11111069#11111069\">discrete search space</a></p></li>\\r\\n<li><p>Speeding up computations in R\\r\\n<a href=\"http://stackoverflow.com/questions/9526691/efficiently-compute-histogram-of-pairwise-differences-in-a-large-vector-in-r/9527046#9527046\">using C</a>\\r\\nor by \\r\\n<a href=\"http://stackoverflow.com/questions/10011425/vectorize-function-to-avoid-loop/10011954#10011954\">expanding</a>\\r\\nsome of the computations</p></li>\\r\\n<li><p><a href=\"http://stackoverflow.com/questions/9106401/select-rows-of-a-data-frame-based-on-column-properties/9106581#9106581\">Computing the skyline</a></p></li>\\r\\n<li><p><a href=\"http://stats.stackexchange.com/questions/16631/what-are-essential-rules-for-designing-and-producing-plots/22010#22010\">On the use of colour in statistical plots</a></p></li>\\r\\n<li><p><a href=\"http://stackoverflow.com/questions/9747426/how-can-i-produce-plots-like-this/9748213#9748213\">Plotting the result of a time series clustering</a></p></li>\\r\\n<li><p><a href=\"http://stackoverflow.com/questions/10150161/ordering-117-by-perfect-square-pairs/10150797#10150797\">Solving puzzles with graph theory</a></p></li>\\r\\n</ul>\\r\\n',\n",
       " '<p>My <a href=\"http://www.linkedin.com/pub/jeff-hansberry/58/351/a77\" rel=\"nofollow\">LinkedIn</a> profile. I work on diagnostics for cancer and other diseases. Any views expressed are my own. They should not be interpreted as reflecting the views of my employers.</p>\\r\\n',\n",
       " '<p>Professional DBA working on many different types of accounts. Interested in learning about predictive analysis and other types of machine learning.</p>\\r\\n',\n",
       " '<p>Professional UX designer and research; Author of Salstat, open source software for friendlier statistical analysis (<a href=\"https://github.com/salmoni/Salstat\" rel=\"nofollow\">https://github.com/salmoni/Salstat</a>); psychologist (Ph.D. in human-computer interaction) and statistician; and freelance business owner. </p>\\r\\n\\r\\n<p>I enjoy research, coding in Python (particularly stats: I wrote Salstat before stats got trendy!) and designing incredible experiences.</p>\\r\\n\\r\\n<p>In terms of work, I\\'m ideally looking for remote work that combines research, programming and design. If anyone has a need for that sweet spot, let me know. </p>\\r\\n',\n",
       " '<p>neverending student ;)</p>\\r\\n',\n",
       " '<p>Flash and web developer with experience in online advertising and marketing.</p>\\r\\n',\n",
       " '<p>Statistician currently based in the UK.</p>\\r\\n',\n",
       " '<p>Daniel Graziotin is a PhD student in Computer Science at the Free University of Bozen-Bolzano. His research interests include human aspects in empirical software engineering with psychological measurements, Web engineering, and Open Science. He is Editorial Associate at the <a href=\"http://openresearchsoftware.metajnl.com\" rel=\"nofollow\">Journal of Open Research Software</a> and the local coordinator of the <a href=\"http://okfn.org\" rel=\"nofollow\">OKF</a> <a href=\"http://openscience.it\" rel=\"nofollow\">Open science Italia</a> group. He is a member of the ACM, SIGSOFT, IEEE, and the IEEE Computer Society.</p>\\r\\n\\r\\n<p><a href=\"http://task3.cc\" rel=\"nofollow\">More info</a>.</p>\\r\\n',\n",
       " '<p>GIS, data analyst and more</p>\\r\\n',\n",
       " 'Web developer.',\n",
       " '<p>Statistician &amp; SAS Programmer</p>\\r\\n',\n",
       " '<p>Postdoctoral researcher, Institut für Astrophysik Göttingen.</p>\\r\\n',\n",
       " '<p>I would like to have <code>git</code> for my life.</p>\\r\\n',\n",
       " '<p>PhD student in zooplankton ecology and oceanography, based in Scotland</p>\\r\\n',\n",
       " 'I mostly code in R, MATLAB and C#.',\n",
       " \"<p>I'm currently working on a PhD in Public Policy at the Sanford School of Public Policy at Duke University, where I'm focusing primarily on the relationship between the nonprofit sector and governance development in the Middle East. Life rocks.</p>\\r\\n\",\n",
       " '<p>I\\'m a mix of things, a physicist by education, I earn my living as a mathematician and programmer working at <a href=\"http://www.wolfram.com/\" rel=\"nofollow\">Wolfram Research</a>. I thoroughly enjoy learning and discovery process.</p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<blockquote>\\r\\n“Technical skill is mastery of complexity, while creativity is mastery of simplicity”<br/>\\r\\n&mdash; Erik Christopher Zeeman\\r\\n</blockquote>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p><img src=\"http://projecteuler.net/profile/confluential.png\" alt=\"Project Euler\"></p>\\r\\n',\n",
       " '<ul>\\r\\n<li><a href=\"http://www.google.com/recaptcha/mailhide/d?k=01plreGSjSrQWP1LVwn3XDlQ==&amp;c=ZQDmBLMld4WLmCRdUWqaHU9Q4M9V_bo8tzXePJUsQpQ=\" rel=\"nofollow\"><strong>E-mail</strong></a></li>\\r\\n</ul>\\r\\n',\n",
       " \"<p>I'm a software engineer based in Glasgow, Scotland mostly writing software in C# and a little Java.</p>\\\\\\\\n<br>\\\\\\\\n<p>I'm interested in a variety of topics from best practice and coding standards and UI to nitty gritty programming details.</p>\",\n",
       " '<p><a href=\"http://careers.stackoverflow.com/gctrullinger\"><strong>Careers 2.0 Profile</strong></a></p>\\r\\n',\n",
       " '<p>Student of Computer Science Engineering at the Tecnhical University of Denmark (DTU). Passionate about scripting and data analyisis, currently looking for a student job.</p>\\r\\n',\n",
       " \"<p>I'm a statistical consultant to graduate students and researchers in fields including the behavioral and health sciences. I've assisted with review of articles, and with preparation of grants, dissertations and papers. </p>\\r\\n\",\n",
       " '<p>I am a full time computer programmer and software developer working for a small business called <a href=\"http://www.highplainstech.com/\" rel=\"nofollow\">High Plains Technolgies</a>. I love what I do and am always excited for new and interesting projects. I plan on enjoying my work, enjoying my life, helping and hanging out with others, and working for Jesus Christ as He has called me.</p>\\r\\n',\n",
       " '<p>As one of my idols once did, I use the name Elvis Jagger Abdul-Jabbar when I am too shy to tell my real name.</p>\\r\\n\\r\\n<p>I am a former algebraist who made a strategic move to applied research (successful move, now I have a position). I’m a full autodidact in statistics, still learning and struggling to enhance my level.</p>\\r\\n\\r\\n<p>My portrait as Homer has been drawn in R by one of my students!</p>\\r\\n',\n",
       " '<p>Corey Yanofsky, Ph.D.</p>\\r\\n\\r\\n<p>I\\'m a biostatistician with an interest in financial statistics located in Ottawa, Canada. Philosophically, I\\'m a Bayesian of the <a href=\"http://en.wikipedia.org/wiki/Cox%27s_theorem\" rel=\"nofollow\">Cox</a>-<a href=\"http://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes\" rel=\"nofollow\">Jaynes</a> school. In practice I\\'m a statistical ecumenist whose role models are <a href=\"http://www.cs.berkeley.edu/~jordan/\" rel=\"nofollow\">Michael I. Jordan</a> and <a href=\"http://www.stat.columbia.edu/~gelman/\" rel=\"nofollow\">Andrew Gelman</a>. </p>\\r\\n',\n",
       " '<p>I am a neuroscientist trying to have a solid statistical buttress for interpretations of experimental data</p>\\r\\n',\n",
       " '<p>My name is Chris, and I have general interests in some things.</p>\\r\\n',\n",
       " '<p>Biologist, interested in R language, Latex, Ecology and Evolution</p>\\r\\n',\n",
       " '<p>Developer at IBM Toronto</p>\\r\\n',\n",
       " '<p>In 2007 I got my bachelor degree in applied informatics in Belgium. Immediately after, I started working at the company where I did my internship, <a href=\"http://aimproductions.be/\" rel=\"nofollow\">AIM Productions</a>. I liked the work and colleagues at the company too much to give up entirely for further studies, so I decided to combine the two. In 2009 I started studying for my master in Game and Media Technology at the <a href=\"http://www.uu.nl/EN/Pages/default.aspx\" rel=\"nofollow\">University of Utrecht</a> in the Netherlands and I graduated in 2012. At the moment I work as a research assistant at the <a href=\"http://www.itu.dk/\" rel=\"nofollow\">IT University of Copenhagen</a>, at <a href=\"http://itu.dk/pit/\" rel=\"nofollow\">the PIT lab</a> working on <a href=\"http://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction\" rel=\"nofollow\">Human-Computer Interaction</a>.</p>\\r\\n\\r\\n<p>I mainly develop in C#, but occasionally do C++, PHP, Flash/actionscript 3 and Java projects. I care a lot about proper code design, and some people tend to say I overdo it.</p>\\r\\n',\n",
       " '<p>Phd candidate in Industrial engineering</p>\\r\\n',\n",
       " '<p>I am a soil ecologist and R user.</p>\\r\\n',\n",
       " '<p>Currently doing things with data at ScraperWiki, UK.</p>\\r\\n',\n",
       " '<p>I love JavaScript, Ruby and Web Development.</p>\\r\\n\\r\\n<p>I work at Etsy.</p>\\r\\n',\n",
       " '<p>I am a project trainee working in a reputed software concern.</p>\\r\\n',\n",
       " '<p>Medical student in Halifax, with an interest in statistical inference and modeling.</p>\\r\\n',\n",
       " '<p>I\\'m <strong>an applied mathematician &amp; engineer</strong>, living &amp; working just outside the London metro area.</p>\\r\\n\\r\\n<p>Over the years and on both sides of the Atlantic, I\\'ve led a variety of quantitative &amp; operational challenges across intelligent systems &amp; software engineering, sonar, defense, environmental technology, robotics, and most recently the complex world of multi-channel retail.</p>\\r\\n\\r\\n<p>The common thread that links passion, profession, and play is applying technology, good design, and quantitative modelling and simulation, to build better products, enable better decisions, and optimise performance.</p>\\r\\n\\r\\n<p><em>I\\'m always experimenting and tinkering, so feel free to get in touch with ideas.</em></p>\\r\\n\\r\\n<p>My Google+ <strong><a href=\"https://plus.google.com/u/0/+AssadEbrahim/about\" rel=\"nofollow\">profile</a></strong>, Google+ <strong><a href=\"https://plus.google.com/u/0/+AssadEbrahim/posts\" rel=\"nofollow\">posts</a></strong> (active), and <strong><a href=\"http://www.mathscitech.org/articles\" rel=\"nofollow\">blog</a></strong> (older)</p>\\r\\n',\n",
       " '<p>Interests: SQL, C#, ASP.NET, Java</p>\\r\\n\\r\\n<p>--- Trying to answer questions of others helps you learn as well ---</p>\\r\\n',\n",
       " '<p>MSc AI with a focus on Machine Learning and Data Mining</p>\\r\\n',\n",
       " \"<p>25 years programming experience in Java, Python, C, C++, C#, Forth, BBC BASIC, macro assembler, bash, PHP and MATLAB. I don't really do Lisp or Perl though.</p>\\r\\n\",\n",
       " '<p>Author of \"<a href=\"http://marketingforhackers.com/\" rel=\"nofollow\">Marketing for Hackers</a>\" and \"<a href=\"http://blog.hartleybrody.com/guide-to-web-scraping/\" rel=\"nofollow\">The Ultimate Guide to Web Scraping</a>\". Builder at <a href=\"http://burstworks.com/\" rel=\"nofollow\">Burstworks</a>.</p>\\r\\n',\n",
       " '<p>Applied Mathematician<br/>\\r\\nAustrian Institute of Technology</p>\\r\\n\\r\\n<p>Interests:<br/></p>\\r\\n\\r\\n<ul>\\r\\n<li>Math, [Computational] Statistics, Machine Learning, HPC</li>\\r\\n<li>Data Visualization (ggplot2, D3.js, ...), GIS (grass, qgis, osm, ...)</li>\\r\\n<li>R, Python, C[++], js, LaTeX, Vim, zsh, mercurial, ...</li>\\r\\n</ul>\\r\\n\\r\\n<hr/>\\r\\n\\r\\n<p><a href=\"http://stackexchange.com/users/57496\">\\r\\n<img src=\"http://stackexchange.com/users/flair/57496.png\" width=\"208\" height=\"58\" alt=\"profile for rcs on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for rcs on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\\r\\n</a></p>\\r\\n',\n",
       " '<p>Researcher</p>\\r\\n',\n",
       " '<p>I am statistician developer interested in R, Python, behavioral finance, and conveying complex statistical models through visualization. </p>\\r\\n\\r\\n<p><a href=\"http://kldavenport.com\" rel=\"nofollow\">http://kldavenport.com</a></p>\\r\\n',\n",
       " \"<h1>Developer</h1>\\r\\n\\r\\n<pre>def Lover(Programming):\\r\\n    return ['Love']*len(Programming)\\r\\n\\r\\nprint Lover(['Python','Fortran','VB','Matlab',[]])</pre>\\r\\n\",\n",
       " '<p>I\\'m broadly interested in <a href=\"http://www.johndcook.com/veryappliedmath.html\" rel=\"nofollow\">very applied math</a>. I try to apply ideas from mathematics, statistics, machine learning, formal systems and computer science to solve real-world problems. Mostly in applied finance/quantitative trading, but in other areas if the mood takes me.</p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p>The following is for my own use, but feel free to borrow it -</p>\\r\\n\\r\\n<p>Hi. It looks like you are new here. We are generally very willing to help, but we like users to show the work that they\\'ve done towards solving the problem on their own first. If you can edit your question to show the code you\\'ve written so far, and where you are stuck, then you will get a much better response.</p>\\r\\n',\n",
       " '<p>I mainly use the statistical programming language <code>R</code> to solve problems, typically of a spatial, biological, and geographical nature.  I have been using R since it was S-plus (back in the late 90s).  Despite this I have an unfortunate tendency to resort to the <code>for</code> loop, which is not exactly favoured in R practice, probably because I learned to program using Turbo Pascal version 5.5!</p>\\r\\n',\n",
       " '<p>UX Architect in London</p>\\r\\n',\n",
       " '<p>As a student in South Asian languages, culture and history, specialising in comparative cultures and cognitive science, I take special interest in the influence of cultural paradigms in the perception and interaction with the world in terms of development.\\r\\nI am also a polyglot with a strong passion for learning languages, including programming languages such as Python. Also I am firm supporter of unix and free software.</p>\\r\\n',\n",
       " '<p><a href=\"https://plus.google.com/101646675254020690108/\">google</a></p><p><a href=\"http://www.facebook.com/david.d.jensen\">facebook</a></p>',\n",
       " \"Computer scientist and linguist. I'd like to work with reasearch in computational linguistics, but while waiting for that I do whatever programming jobs I can get. I like Perl, Prolog, C and Haskell (now I just have to learn how to use it). I can stand Java, and loathe PHP.\",\n",
       " '<p>Econometrics</p>\\r\\n',\n",
       " '<p>Software Developer at Epic Systems</p>\\r\\n',\n",
       " '<p>I am a geographer working as a city planner. I see cities as fascinating laboratories human interaction and dynamoes of innovation.  </p>\\r\\n',\n",
       " '<p>C++ C# Java Android Python Django</p>\\r\\n',\n",
       " '<p>My background is in theoretical/mathematical physics [MPhys/PhD], I was a Senior Computational Fluid Dynamics Analyst in the nuclear/defense industry before selling my sole and becoming a full-time commercial software developer around 2011.</p>\\r\\n',\n",
       " '<p>Currently a software engineer at Google. Recently graduated from University of California, Berkeley with B.S. degrees in Electrical Engineering and Computer Science (EECS) and Engineering Math and Statistics (EMS).</p>\\r\\n\\r\\n<p>Personal Website: <a href=\"http://www.chenyuzhao.net\" rel=\"nofollow\">http://www.chenyuzhao.net</a></p>\\r\\n\\r\\n<p>LinkedIn: <a href=\"http://www.linkedin.com/in/chenyuzhao\" rel=\"nofollow\">http://www.linkedin.com/in/chenyuzhao</a></p>\\r\\n\\r\\n<p>Careers profile: <a href=\"http://careers.stackoverflow.com/chenyuzhao\">http://careers.stackoverflow.com/chenyuzhao</a></p>\\r\\n',\n",
       " '<p>A Postgraduate research student of Computer Science at Strathclyde University.</p>\\r\\n',\n",
       " \"My name is Sergio Díaz. I am known as Seichleon(a.k.a. Seich) on the internet. I am a young-self taught Honduran Web Developer and Software designer. I've been working with code for around 5 years and I've been in love with computers since I was 4 and I love it.\",\n",
       " '<p>Walks to work.</p>\\r\\n',\n",
       " \"Canadian expat living in Kyoto. I need to learn MATLAB scripting for numerical analysis for economics. I took a C+ class a long time ago and tried to learn a little HTML, but otherwise I'm a total coding noob.\\\\\\\\n\\\\\\\\nIn Japan we'd say 'yoroshiku onegaishimasu' - thanks in advance for taking care of me.\",\n",
       " '<p><a href=\"http://stackexchange.com/users/9ed9aac6bf454e0b94afd129f78ecf6b\">\\r\\n<img src=\"http://stackexchange.com/users/flair/9ed9aac6bf454e0b94afd129f78ecf6b.png\" width=\"208\" height=\"58\" alt=\"profile for Seamus on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Seamus on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\\r\\n</a></p>\\r\\n\\r\\n<p>I am a postdoc in philosophy.</p>\\r\\n\\r\\n<p>I contribute to a blog for philosophers who use LaTeX. If you fall in to that niche, check it out: <a href=\"http://www.charlietanksley.net/philtex/\" rel=\"nofollow\">PhilTeX</a>. (The blog will be of use to all kind of humanities scholars using LaTeX, I imagine, but it was started by, and is run by philosophers...) The blog is currently defunct, but may be resurrected soon.</p>\\r\\n\\r\\n<p>I made this <a href=\"https://github.com/scmbradley/Beamer-colour-change-project\" rel=\"nofollow\">beamer colour change package</a> that slowly changes the colour of structure elements of beamer presentations. Feedback welcome.\\r\\nI also made this <a href=\"https://github.com/scmbradley/moreenum\" rel=\"nofollow\"><code>moreenum</code></a> package which adds new enumeration options.\\r\\nThe <a href=\"http://www.seamusbradley.net/tex.html\" rel=\"nofollow\">TeX goodies</a> page of my website includes some other bits and bobs I\\'ve done.</p>\\r\\n',\n",
       " '<p>Grad student in machine learning.</p>\\r\\n',\n",
       " \"<p>I'm a software engineer from Edinburgh, UK.  Married with three children.</p>\\r\\n\",\n",
       " '<p>I am a PhD student at BYU in CS.</p>\\r\\n',\n",
       " '<p>Assistant Professor\\r\\nDepartment of Computer Science\\r\\nUniversity of Colorado, Boulder, and\\r\\nExternal Faculty at the Santa Fe Institute</p>\\r\\n',\n",
       " '<p><strong>Website:</strong> <a href=\"http://www.netconstructor.com\" rel=\"nofollow\" title=\"http://netconstructor.com\">http://www.netconstructor.com</a> <br />\\r\\n<strong>Linkedin:</strong> <a href=\"http://linkedin.com/in/netconstructor\" rel=\"nofollow\" title=\"Christian Hochfilzer\">Christian Hochfilzer</a>  <br />\\r\\n<strong>Dribbble:</strong> <a href=\"http://www.netconstructor.com\" rel=\"nofollow\" title=\"San Diego web development\">Christian Hochfilzer</a> <br />\\r\\n<strong>Twitter:</strong> <a href=\"http://linkedin.com/in/netconstructor\" rel=\"nofollow\" title=\"Christian Hochfilzer\">Christian Hochfilzer</a> <br /></p>\\r\\n\\r\\n<p><strong><a href=\"http://www.netconstructor.com\" rel=\"nofollow\" title=\"http://netconstructor.com\">NetConstructor.com</a></strong> is a <a href=\"http://www.netconstructor.com\" rel=\"nofollow\" title=\"San Diego web development\">San Diego web development</a> and marketing firm founded by <a href=\"http://linkedin.com/in/netconstructor\" rel=\"nofollow\" title=\"Christian Hochfilzer\">Christian Hochfilzer</a>, Leo Baghdassarian and Kris Fredrickson and is composed of a tightly-knit group of highly talented and experienced UX Designers, Usability Experts and BigData Development Pros.</p>\\r\\n\\r\\n<p>At the heart of NetConstructor is the simple belief that our results speak for themselves. We understand that regardless of how aesthetically pleasing any website, application or marketing campaign maybe, or how much theoretical sense the technique makes, unless one is able to deliver measurable results and meet customer expectations it\\'s failed.</p>\\r\\n\\r\\n<p>The majority of our efforts are spent supporting and delivering results for the clients of some high profile design agencies but whenever possible we enjoy working directly with firms. Generally speaking, we are called up on when clients are looking to either:</p>\\r\\n\\r\\n<ol>\\r\\n<li><strong>Increasing Sales, Business Intelligence &amp; Online Positioning</strong></li>\\r\\n<li><strong>Reducing Operational, Marketing &amp; Support Costs</strong></li>\\r\\n</ol>\\r\\n\\r\\n<p>Our teams experience includes such firms as ACE Parking, Arkeia.com, Bakbone.com, Breach.com, Gateway.com, Cooking.com, Kingston.com, SK-Sanctuary.com, Swarco.com.com and various others.</p>\\r\\n\\r\\n<p>What sets NetConstructor apart remains our high attention to detail, our tightly knit team of gurus, and our focused on delivering quantifiable results focused on current/future growth.</p>\\r\\n\\r\\n<p><strong>Behance:</strong> <a href=\"http://www.behance.net/netconstructor\" rel=\"nofollow\" title=\"Behance Chistian Hochfilzer\">Chistian Hochfilzer</a>  <br />\\r\\n<strong>CrunchBase:</strong> <a href=\"http://www.crunchbase.com/company/netconstructor-com\" rel=\"nofollow\" title=\"NetConstructor\">NetConstructor</a>  <br />\\r\\n<strong>CrunchBase:</strong> <a href=\"http://www.worky.com/christian-hochfilzer\" rel=\"nofollow\" title=\"Crunchbase Profile: Christian Hochfilzer\">Christian Hochfilzer</a> <br />\\r\\n<strong>Worky:</strong> <a href=\"http://www.worky.com/christian-hochfilzer\" rel=\"nofollow\" title=\"Christian Hochfilzer\">Christian Hochfilzer</a>  <br /></p>\\r\\n',\n",
       " '<p>Another geek still trying to decipher the meaning of “42”.</p>\\r\\n\\r\\n<p>It seems that amount his main interest are:</p>\\r\\n\\r\\n<ul>\\r\\n<li>online communities of practice and the way they evolve in time</li>\\r\\n<li>product design, simplicity in design and accessibility</li>\\r\\n<li>productivity and the way the IT solutions are impacting it</li>\\r\\n</ul>\\r\\n',\n",
       " '<p>I am a mathematician from Finland. In maths, my main interest are in algebra and number theory. Nowadays I mostly read mathematics on my own and look for an interesting math projects.</p>\\r\\n',\n",
       " '<p>loves python, cooking, yoga, math and loud electrical guitars.</p>\\r\\n',\n",
       " '<p>Plain and simple, math is awesome!</p>\\r\\n',\n",
       " \"<p>Studying cyber security (theory of cryptographic protocols) for M.Sc. in the Applied Logic And Security (ALAS) lab at Worcester Polytechnic Institute. I am learning how to do research mathematics.</p>\\r\\n\\r\\n<p>I love to take real-world problems and model them with formal structures and computer programs.</p>\\r\\n\\r\\n<p>I'm practiced with picking up new languages quickly for one-off projects.</p>\\r\\n\\r\\n<p>Hobbies: problem solving, critical reasoning, rhetoric, math (esp. graph theory!), economics, cognitive science, political science, and theoretical physics.</p>\\r\\n\\r\\n<p>Interested in machine learning, especially fields with practical applications that can make a difference in the world such as economics, knowledge sharing, healthcare, bioinformatics, and ecoinformatics.</p>\\r\\n\\r\\n<p>Deep relationship with Python;\\r\\nPretty skillful with C++, PHP, Javascript, MySQL, Java, Haskell, Matlab.</p>\\r\\n\",\n",
       " '<p>I am a software developer... I am certified (MCP) in WindowsForms.NET, ASP.NET and ADO.NET (with C#) and I really like .NET (I specially love LINQ)... </p>\\r\\n\\r\\n<p>I sometimes <a href=\"http://www.javamexico.org/blogs/luxspes\" rel=\"nofollow\">blog at Javamexico</a> or at my <a href=\"http://luxspes.blogspot.mx/\" rel=\"nofollow\">personal blogspot</a>. I also <a href=\"https://twitter.com/lux_spes\" rel=\"nofollow\">tweet ocasionally</a></p>\\r\\n\\r\\n<p>I also know how to code in Java (I love Hibernate,and I love Spring-Mvc but I think for the web the future is in JSF with Seam... and for SmartClients XAML... but perhaps Flex will give a good fight) , I know a little of Delphi... and I have worked with Oracle and SQLServer...</p>\\r\\n\\r\\n<p>The best web technology that I have used is WebObjects (sadly IMHO Apple just doesn\\'t seem to know its value) I think SQL is full of flaws... I used to believe the only way to go is to use an ORM and I really disliked plain ADO.NET and JDBC, I used to think that to build the business rules of a software system.. there was nothing better than Domain Model pattern... built with Apple\\'s EOF... or Hibernate (or NHibernate or LINQ &amp; EntityFramework). But lately, after reading <a href=\"http://www.thethirdmanifesto.com/\" rel=\"nofollow\">TheThirdManifesto</a>, I have been thinking that relational is not limited, it is underexploited, and perhaps Alphora <a href=\"http://dataphor.org\" rel=\"nofollow\">Dataphor</a> or <a href=\"http://dbappbuilder.sourceforge.net/Rel.php\" rel=\"nofollow\">Rel</a> will finally exploit its power.</p>\\r\\n\\r\\n<p>I am also certified in TFS 2010, and I know my way around CVS, SVN, Mercurial, Bazaar and a little Git</p>\\r\\n',\n",
       " '<p>a research student</p>\\r\\n',\n",
       " \"<p>Electronic Engineer and self-employed consultant, Physics degree, working in VHDL, Verilog, and C++, primarily in FPGA and ASIC design. I've written two compilers, including one which generates Verilog (9-pass, about 50K lines of C++). I'll add VHDL output if/when I get some spare time. I also have experience in SystemC and Specman/'e'.</p>\\r\\n\\r\\n<p>I occasionally write JavaScript, and odd bits of HTML/PHP/Ajax, when I've got nothing better to do, primarily to generate and display SVG images.</p>\\r\\n\\r\\n<p>Always looking for new opportunities - mail me if you want to discuss anything; I'm on <code>sa212+stackoverflow</code> at <code>cyconix dotcom</code>.</p>\\r\\n\",\n",
       " '<ul>\\r\\n<li><p>Likes: MATLAB, R, C++, Regression models and Linear Algebra. Spatial statistics are pretty cool too.</p></li>\\r\\n<li><p>Dislikes: Perl, Prolog and Asymptotic Theory (probably cause I am bad in all three of them).</p></li>\\r\\n</ul>\\r\\n',\n",
       " '<p>Rookie coder with a bit of C++. Currently brushing up my java.</p>\\r\\n',\n",
       " '<p>My first interest is the Kingdom of GOD.\\r\\nMy second interest in programming: Java and R.\\r\\nI am an economics major.</p>\\r\\n',\n",
       " '<p>Mechanical engineer, part time MATLAB developer. Interested in physics, software development and statistics. Master of no trades.</p>\\r\\n',\n",
       " '<p>Full time J2EE &amp; Jave EE developer / Part time PhD candidate<br>\\r\\nI am really keen on <strong>computational linguistics</strong>.</p>\\r\\n',\n",
       " '\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n',\n",
       " '<p>Graduate Student at Osaka University</p>\\r\\n',\n",
       " \"<p>I noticed that this profile receives views.  Let me make your click a little more worthwhile, and explain Iterator's <em>raison d'être</em>...</p>\\r\\n\\r\\n<p>I do some things again and again, or my code does (almost) the same thing again and again.  Iterating isn't tedious <em>per se</em>, but it helps to iterate rapidly.  I am mastering that.</p>\\r\\n\\r\\n<p>I work on topics in many areas, usually solving stimulating problems with stimulating data.  Sometimes there is a stimulating amount of stimulating data.</p>\\r\\n\\r\\n<p>To contact me, look for me in the SO R chat room, where I am often learning from R experts, or leave a message there or in a separate chat (@Iterator) - SO will alert me.</p>\\r\\n\",\n",
       " '<p>I am very interested in learning mysteries of nature and find all the underlying cause. I wish to learn technologies so as to make my study of nature easier and effective.</p>\\r\\n',\n",
       " '<p>Researcher based in Brisbane, Australia, mostly dealing with public health / survey data.</p>\\r\\n',\n",
       " \"<p>I'm a graduate in computational linguistics and received my master's degree from Saarland University, Saarbrücken, Germany. </p>\\r\\n\\r\\n<p>I'm interested in answering questions related to Python programming and web development, preferably using Django and other Python-based technologies. Linguistics is always nice as well, of course.</p>\\r\\n\\r\\n<p>Jobwise, I've been working with Adobe ColdFusion very intensively, so I might ask or answer questions related to ColdFusion as well.</p>\\r\\n\",\n",
       " '<p>School of Public Health - The Chinese University of Hong Kong</p>\\r\\n\\r\\n<p>If you are interested in Public Health and/or Epidemiology, please follow <a href=\"http://area51.stackexchange.com/proposals/34565/public-health-epidemiology\">this proposal</a>, submit some sample questions, and vote (or downvote) those that exist already...</p>\\r\\n',\n",
       " '<p>i mess with computers as a distraction to doing real work. </p>\\r\\n',\n",
       " 'third continent and counting...\\\\\\\\n<p>\\\\\\\\n<br>\\\\\\\\n<img src=\"http://unicornify.appspot.com/avatar/203842398470284375023465237405928345?s=128\"> ',\n",
       " \"I'm a programmer. I still have a lot to learn.\",\n",
       " '<p>SQL, Perl, C, ITK, VTK, Bash shell. </p>\\r\\n',\n",
       " '<p>Ex-physicist.</p>\\r\\n',\n",
       " '<p>No description.</p>\\r\\n',\n",
       " '<p>MS UC Berkeley</p>\\r\\n',\n",
       " '<pre><code>\\u3000\\r\\n\\u3000\\r\\n\\u3000\\r\\n                         (__)\\r\\n                         (--) ... ( *&gt;YAWN&lt;* )\\r\\n                   /------\\\\\\\\/\\r\\n                  /|     ||\\r\\n                 * ||----||\\r\\n                   ~~    ~~\\r\\n\\u3000\\r\\n\\u3000\\r\\n\\u3000\\r\\n\\u3000\\r\\n</code></pre>\\r\\n\\r\\n\\r\\n',\n",
       " \"<p>I'm currently doing my masters thesis in geography. Therefore I'm working with R to perform cluster analysis of diffrent land-use types. I#m new to R and I don't have former expirience in programming.</p>\\r\\n\",\n",
       " '<p><img src=\"http://i.stack.imgur.com/aIWUa.gif\" alt=\"enter image description here\"> \\r\\nSome answers I like:</p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://stackoverflow.com/questions/2135116/how-can-i-determine-distance-from-an-object-in-a-video/2152097#2152097\">How can I determine distance from an object in a video?</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/2691021/problem-with-precision-floating-point-operation-in-c/2691079#2691079\">Problem with Precision floating point operation in C</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/1768197/bounding-ellipse/1768440#1768440\">Bounding ellipse</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/1689839/transforming-captured-co-ordinates-into-screen-co-ordinates/1689899#1689899\">Transforming captured co-ordinates into screen co-ordinates</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/1500498/how-to-use-sift-algorithm-to-compute-how-similiar-two-images-are/1501903#1501903\">How to use SIFT algorithm to compute how similiar two images are?</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/1521958/shortest-path-to-transform-one-word-into-another/1521973#1521973\">Shortest path to transform one word into another</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/2446494/skewing-an-image-using-perspective-transforms/2448069#2448069\">Skewing an image using Perspective Transforms</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/3365487/code-golf-build-me-an-arc/3369332#3369332\">Code Golf: Build Me an Arc</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/2770155/plotting-bessel-function-in-matlab/2770272#2770272\">Plotting Bessel function in MATLAB</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/a/12765147/71131\">Incrementing values in a sparse matrix takes very long</a></li>\\r\\n</ul>\\r\\n\\r\\n<p>NB: The first person to get a <a href=\"http://stackoverflow.com/help/badges/993/computer-vision\"><code>computer-vision</code></a> &amp; <a href=\"http://stackoverflow.com/help/badges/1247/opencv\"><code>opencv</code></a> badge :)</p>\\r\\n',\n",
       " '<p>Feel free to contact me on my email to chat about our common interests / topics.</p>\\r\\n',\n",
       " '<p>Computer Science double BSc, master-level student. Bitcoin enthusiast.</p>\\r\\n\\r\\n<p>My Google-powered Bitcoin Calculator:</p>\\r\\n\\r\\n<p><a href=\"http://tpbitcalc.appspot.com/\" rel=\"nofollow\">http://tpbitcalc.appspot.com/</a></p>\\r\\n\\r\\n<p>My tweets:</p>\\r\\n\\r\\n<p><a href=\"http://twitter.com/#!/ThePiachu\" rel=\"nofollow\">http://twitter.com/#!/ThePiachu</a></p>\\r\\n\\r\\n<p>If you have an iPhone and need some dice, check out my apps:</p>\\r\\n\\r\\n<p><a href=\"http://tiny.cc/TPiDev\" rel=\"nofollow\">http://tiny.cc/TPiDev</a></p>\\r\\n',\n",
       " '<p>I do security for the Python and Django projects.</p>\\r\\n',\n",
       " '<p>I want to understand (biomedical) statistics and I have a growing number of questions... :)</p>\\r\\n',\n",
       " \"<p>I'm a PhD student at the Vrije Universiteit in Amsterdam. \\r\\nMy research is on the genetics of mental disorders and population genetics. \\r\\nSometimes I use Perl, R or bash to program something, and usually when I get stuck, the friendly users of stackoverflow are able to help me out :-)</p>\\r\\n\",\n",
       " '<p>Researcher working in the field of environmental science. My work focusses on the use of earth observation data for hydrological applications.</p>\\r\\n',\n",
       " '<p>Interested in data mining, machine learning, quant. finance, operations management, customer insight/marketing analytics, customer behaviour, statistics for psychology, computational neuroscience, brain dynamics.</p>\\r\\n\\r\\n<p>Currently focusing on learning about copula methods, fourier analysis for time series, kernel methods.</p>\\r\\n',\n",
       " 'Undergraduate studying Environmental Engineering and Applied Mathematics.',\n",
       " '<p><a href=\"http://www.mendeley.com/profiles/kay-cichini/\" rel=\"nofollow\">My Profile at Mendeley</a></p>\\r\\n',\n",
       " 'pi3B20MIL84VaBX1\\\\\\\\nFront End developer and UI designer on <a href=\"http://www.genesreunited.co.uk\" rel=\"nofollow\">Genes Reunited</a>',\n",
       " '<p>Nothing.</p>\\r\\n',\n",
       " '<p>Hacker, game developer, CG artist and machine learning entrepreneur</p>\\r\\n',\n",
       " 'PhD student in Norway',\n",
       " '<p>Quantitative developer, writer, instructor, private trader, so-so musician</p>\\r\\n',\n",
       " '<p>Statistician</p>\\r\\n',\n",
       " \"<p>I'm a PhD student in economics.</p>\\r\\n\",\n",
       " '<p>I\\'ve got a BS in Physics from the University of Rochester, and I\\'m over half-way through my masters in optical engineering. I work at the Laboratory for Laser Energetics, where I develop and maintain technology for the Omega EP laser.</p>\\r\\n\\r\\n<p><a href=\"http://en.wikipedia.org/wiki/Laboratory_for_Laser_Energetics\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Laboratory_for_Laser_Energetics</a></p>\\r\\n',\n",
       " '<p>Currently a phd student in machine learning at Unicamp, Brazil.</p>\\r\\n',\n",
       " '<p>Software Engineering PhD Student at the University of Illinois at Urbana-Champaign</p>\\r\\n',\n",
       " '<p>Here is a sample of my work.  I hope that it will prove useful...\\r\\n\"Nothing has been accomplished if something remains to be done.\"\\r\\n(The Works of Gauss vol. 5, quot. from Worbs, 1955,  p. 43)</p>\\r\\n',\n",
       " '<p>When not fighting crime in riverine environments and avoiding having my head detached from my body by public health and animal control authorities, I am a mild-mannered economic geographer. I teach a mandatory introductory level statistics course to groups of 60 to 90 largely unwilling students, as well as other kinds of descriptive and explanatory quantitative techniques in applied regional analysis.</p>\\r\\n',\n",
       " '<p>Consultant (environmental and spatial stats a specialty), expert witness, and teacher.  I can be reached through (outdated but still valid) links posted <a href=\"http://www.quantdec.com/quals/quals.htm\" rel=\"nofollow\">on my web site</a>.</p>\\r\\n\\r\\n<p>Twitter: @WilliamAHuber  // ASA-P website: <a href=\"http://amstatphilly.org/\" rel=\"nofollow\">http://amstatphilly.org/</a></p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<blockquote>\\r\\n  <p>Why waste time learning, when ignorance is instantaneous?</p>\\r\\n</blockquote>\\r\\n\\r\\n<p><em>--T(iger) Hobbes.</em></p>\\r\\n\\r\\n<blockquote>\\r\\n  <p>For any complex problem there is a simple solution.  And it\\'s always wrong.</p>\\r\\n</blockquote>\\r\\n\\r\\n<p>--[Mis?]attributed to H.L. Mencken by Dava Sobel, <em>Longitude</em>.</p>\\r\\n',\n",
       " '<p>I am a programmer working out of San Francisco in a mobile and web start-up company.  I have worked in C, Java, Labview, Assembly, Perl, Javascript/jQuery, MySQL, and mostly PHP.  I attended Princeton University and the projects I work in normally focus in AI, Machine Learning, Natural Language Processing, or Data Mining.</p>\\r\\n',\n",
       " '<p>I like statistical modeling and its applications. I am good in R.</p>\\r\\n\\r\\n<p><img src=\"http://i.stack.imgur.com/WDsug.jpg\" alt=\"enter image description here\"></p>\\r\\n',\n",
       " '<blockquote>\\r\\n  <p>\"Live a good life. If there are gods and they are just, they will not\\r\\n  care how devout you have been, but will welcome you based on the\\r\\n  values you have lived by. If there are gods, but unjust, then you\\r\\n  should not want to worship them. If there are no gods, then you will\\r\\n  be gone, but will have lived a noble life that will live on in the\\r\\n  memory of your loved ones. I am not afraid.\"</p>\\r\\n</blockquote>\\r\\n\\r\\n<ul>\\r\\n<li>Unknown</li>\\r\\n</ul>\\r\\n',\n",
       " '<p>Multi-disciplinary Systems Engineer</p>\\r\\n',\n",
       " '<p>French R and linux user.</p>\\r\\n',\n",
       " '<p>A poor ignorant who humbly seeks to learn.</p>\\r\\n',\n",
       " '<p>I\\'m an entrepreneur and computer scientist, with a particular interest in Artificial Intelligence and Peer-to-Peer.  My two most notable projects are <a href=\"http://freenetproject.org/\" rel=\"nofollow\">Freenet</a> and <a href=\"http://revver.com/\" rel=\"nofollow\">Revver</a> (I\\'m founder and co-founder respectively).  My current projects are a predictive analytics system called <a href=\"http://sensearray.com\" rel=\"nofollow\">SenseArray</a>, and a new approach to distributed computation called <a href=\"http://code.google.com/p/swarm-dpl/\" rel=\"nofollow\">Swarm</a>.  You can find my personal blog <a href=\"http://blog.locut.us/\" rel=\"nofollow\"> here</a>.\\\\\\\\n</p>\\\\\\\\n<p>\\\\\\\\nWhile I\\'ve used C, C++, ML, Haskell, Prolog, Python, even Perl in the past, these days I do most of my programming in Java.\\\\\\\\n\\\\\\\\nI am gaining experience with Scala though and expect to become my primary language as it and its tools mature.  I was honored to be asked by Scala\\'s creator to be on the program committee for the first Scala workshop.  \\\\\\\\n</p>',\n",
       " '<p>Economics student. </p>\\r\\n',\n",
       " '<p>Old-ish IT Geezer, young at heart, memoir fanboy</p>\\r\\n',\n",
       " \"<p>I am a biostatistician, in my 4th year of work (2011). I have worked largely on candidate gene studies, and am moving into large scale population epidemiology, as well as some clinical trial work in the area of pedatric diabetes.</p>\\r\\n\\r\\n<p>My blog is literally a dumping ground for 'notes to self' and code I plan to reuse. If even a single post gets found by someone in google and helps them then it will have been worth doing it as a blog rather than it's original form (.txt file).</p>\\r\\n\\r\\n<p>Always eager to learn and happy to help. Will happily chat, sports first stats second, I don't make the rules the alphamabet does.</p>\\r\\n\\r\\n<p>Matt</p>\\r\\n\",\n",
       " '<p>CTO and Co-founder of mobile messaging startup <a href=\"http://touchgr.am\" rel=\"nofollow\">touchgr.am</a>.</p>\\r\\n\\r\\n<p>Graduate at the <a href=\"http://fi.co\" rel=\"nofollow\">Founder Institute</a>, from the 2014 semester in Perth Western Australia.</p>\\r\\n\\r\\n<p>Occasional freelance multi-platform software developer currently busy on iOS <a href=\"http://www.lingopal.com/\" rel=\"nofollow\">Lingopal</a>. </p>\\r\\n\\r\\n<p>Author of \"<a href=\"http://www.packtpub.com/getting-started-with-leveldb/book\" rel=\"nofollow\">Getting Started with LevelDB</a>\" from Packt.</p>\\r\\n\\r\\n<ul><li>non-computing interests: \\r\\n<ul><li>Chow Gar Kung Fu and Yang-style Tai Chi, \\r\\n<li>reading and (so-far unpublished) writing SF\\r\\n<li>snow skiing\\r\\n<li>scuba diving, \\r\\n<li>solar housing, \\r\\n<li>carpentry</ul>\\r\\n\\r\\n<li>computing interests: \\r\\n<ul><li>Functional programming and tools\\r\\n<li>OOD, OO languages and frameworks\\r\\n<li>Usability, \\r\\n<li>cross-platform development, \\r\\n<li>code generation</ul>\\r\\n</ul>\\r\\n',\n",
       " '<p>Currently an MSc(IT) student...........................................</p>\\r\\n',\n",
       " '<p>PhD Student facing preliminary defense issues...</p>\\r\\n',\n",
       " '<p>I am a software developer.</p>\\r\\n\\r\\n<p>I like search engines, machine learning and natural language processing.</p>\\r\\n\\r\\n<p>Currently working on something new.</p>\\r\\n',\n",
       " '<p>Perpetually confused.</p>\\r\\n',\n",
       " '<p>I am a PhD student in enegineering department and I am trying to improve my mathematical skills. I am always grateful to any of your comments.</p>\\r\\n',\n",
       " \"<p>Ask if you'd like to know more, I don't bite</p>\\r\\n\",\n",
       " '<p>Research-level student, Scala fanboy, trying to wrap my head around Python, Clojure and Dart</p>\\r\\n\\r\\n<p>I had a broad range of interests: from bioinformatics and computer vision to data mining, functional programming and rock-climbing.</p>\\r\\n\\r\\n<p>Working at <a href=\"http://company.yandex.com/\" rel=\"nofollow\">Yandex</a> on a variety of tasks</p>\\r\\n\\r\\n<p><a href=\"https://twitter.com/#!/ya_kostya\" rel=\"nofollow\">Twitter</a><br>\\r\\n<a href=\"http://www.linkedin.com/profile/view?id=71861540&amp;locale=en_US&amp;trk=tab_pro\" rel=\"nofollow\">LinkedIn profile</a>  </p>\\r\\n',\n",
       " '<p>Data visualization, C++, Forth</p>\\r\\n\\r\\n<p>I work for SAS Institute on <a href=\"http://www.jmp.com/\" rel=\"nofollow\">JMP</a>.</p>\\r\\n\\r\\n<p>Twitter: <a href=\"https://twitter.com/xangregg\" rel=\"nofollow\">@xangregg</a></p>\\r\\n\\r\\n<p><a href=\"http://stackexchange.com/users/37021\">\\r\\n<img src=\"http://stackexchange.com/users/flair/37021.png\" width=\"208\" height=\"58\" alt=\"profile for xan on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for xan on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\\r\\n</a></p>\\r\\n',\n",
       " '<p>I conduct research into energy system modelling - using large linear programming optimisation models to simulate investment decisions under uncertainty.</p>\\r\\n',\n",
       " \"<p>I'm a statistical analyst and social researcher with approximately ten years experience who works with a wide range of data in the broad field of social research: criminology, child protection, public administration and nonprofit organisational theory.</p>\\r\\n\\r\\n<p>Every project I work on has a different methodological challenge which means I am a generalist with regards to statistical theory, however over the years I have developed more specific experience in research design, survey sampling and survey analysis.</p>\\r\\n\",\n",
       " '<p>Applied mathematician and database designer</p>\\r\\n',\n",
       " ' - I love Algorithms, Programming, AI and machine learning\\\\\\\\n<br/>\\\\\\\\n - In my spare time, I like playing real time strategy games, chess, watching movies.',\n",
       " '<p>Economist and Professor at the Federal Institute of Education, Science and Technology Minas Gerais - Campus Formiga</p>\\r\\n',\n",
       " '<p>I am a professor of Statistics at Université Paris Dauphine, Paris, France, and at University of Warwick, Coventry, United Kingdom, specialised in Bayesian statistics, numerical probability applied to simulation methods, and statistical inference with a interest mostly in genetics and astronomy, plus, on the side, a definitely unhealthy but so far not fatal) fascination for mountains and (easy) climbing, an almost-daily run, and a reading list mainly centred at fantasy books… Hence the categories on <a href=\"http://xianblog.wordpress.com\" rel=\"nofollow\">a blog</a> I maintain since 2008... </p>\\r\\n',\n",
       " '<p>App Spec</p>\\r\\n',\n",
       " '<p>My professional interest is developing visualization methods and software to help people to explorer and understand high dimensional data. I am especially interested in geometrical data modelling which is often also referred to as multidimensional scaling (MDS) or non-linear dimensionality reduction. </p>\\r\\n',\n",
       " '<p>I am a social researcher employed by a consultancy firm that specialises in market research. </p>\\r\\n\\r\\n<p>I mostly use correlation and regression models. And I often get confused about them (like my non-statistical clients)!</p>\\r\\n\\r\\n<p>I am not statistically trained so learning as fast as I can. Perhaps one day, I may be able to offer some answers myself when I feel I have gained enough statistical knowledge.</p>\\r\\n',\n",
       " \"<p>I'm a marine biologist PhD student interested in the analysis of time-series. I'm currently working on a time-series of environmental data from the pelagic ecosystem of the NE Atlantic Ocean, containing physical, chemical and biological variables. I'm mainly focus on the changes in the zooplankton and phytoplankton communities and the principal drivers of their variations through time.</p>\\r\\n\",\n",
       " \"<p>I'm a PhD student in ecology at Tulane University. </p>\\r\\n\\r\\n<p>I've been known to do some math, some statistics, some programming, some photography, and some home improvement.  In all those areas I'm mostly self-taught (on an as-needed basis), and so hardly expert. I'll probably do a lot more asking than answering. </p>\\r\\n\\r\\n<p>If anyone has any questions about insects or plants, though, I'm your guy. Also, I speak and write the heck out of the English language, so I'm excited to participate in that site!</p>\\r\\n\",\n",
       " '<p>Student at Texas A &amp; M Univeristy</p>\\r\\n',\n",
       " '<p>I\\'m an astrophysicist and heavy numpy / scipy user.</p>\\r\\n<p><a href=\"http://twitter.com/keflavich\">@keflavich</a></p>',\n",
       " '<h3>Deeptechtons</h3>\\r\\n\\r\\n<p>Programmer, Developer and Enthusiast always looking for chances to learn interesting technology. Meet awesome people and enhance life of people</p>\\r\\n',\n",
       " '<p><strong>Experience</strong></p>\\r\\n\\r\\n<p>I work as a quantitative researcher on the buy side, finding new sources of alpha and designing relative value models and trading strategies around them.  I\\'ve dealt with practically all the major asset classes.\\r\\n<br>\\r\\nPresently at <a href=\"http://www.lordabbett.com/\" rel=\"nofollow\">Lord Abbett</a>, one of the oldest asset management firms to still actively innovate and push the envelope.\\r\\n<br>\\r\\nPreviously at Parkcentral Capital Management, the hedge fund division of Perot Investments.</p>\\r\\n\\r\\n<p><strong>Education</strong></p>\\r\\n\\r\\n<p>PhD in Economics from Princeton University.\\r\\n<br>\\r\\nSB in Economics from MIT.</p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p>I live in New York City with my wife and daughter.</p>\\r\\n\\r\\n<p>To stalk me further, you can check out my <a href=\"http://www.linkedin.com/in/tfishman\" rel=\"nofollow\">LinkedIn profile</a> and twitter: @TalQuant.</p>\\r\\n',\n",
       " '<p>Urban Planner / Population Demographer / GIS Analyst by training.</p>\\r\\n\\r\\n<p>Data Mining / Sales Statistics by trade.</p>\\r\\n\\r\\n<p>Loves anything to do with science, logic, math, and skepticism; and ALWAYS ready to learn somethign new.</p>\\r\\n',\n",
       " '<p>Software Engineering student at the Rochester Institute of Technology.\\r\\nI dig functional programming and simple code.</p>\\r\\n',\n",
       " '<p>I am currently a Ph.D. candidate in the labs of Dr. Graham Bell and Dr. Gregor Fussmann, Biology Department, McGill University.  I am studying the evolutionary and ecological response of phytoplankton to rising atmospheric CO2.\\r\\nI have experience working with R, teaching R and using R to teach statistics. <a href=\"http://www.meetup.com/Montreal-R-User-Group/\" rel=\"nofollow\">Co founder of the Montreal R User group</a>.\\r\\n<a href=\"http://ca.linkedin.com/in/etiennedecarie\" rel=\"nofollow\">I can be found on LinkedIn.</a></p>\\r\\n',\n",
       " '<p>Algorithm Eigeneer</p>\\r\\n',\n",
       " '<p>Interested in statistics, machine learning, applied algebraic topology.  Pure math is also interesting too, particularly topology and geometry. </p>\\r\\n',\n",
       " '<p>entrepreneur; graduate in mathematics / theoretical computer science / theoretical physics; polymath-in-training</p>\\r\\n\\r\\n<p>based in <em>London, United Kingdom</em></p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://about.me/alexreg\" rel=\"nofollow\">About Me</a></li>\\r\\n<li><a href=\"http://blog.noldorin.com/\" rel=\"nofollow\">Website</a></li>\\r\\n<li><a href=\"http://blog.noldorin.com/\" rel=\"nofollow\">Blog</a></li>\\r\\n<li><a href=\"http://twitter.com/alexreg90\" rel=\"nofollow\">Twitter</a></li>\\r\\n<li><a href=\"https://www.google.com/+AlexRegueiro90\" rel=\"nofollow\">Google+</a></li>\\r\\n<li><a href=\"http://www.linkedin.com/in/alexreg\" rel=\"nofollow\">LinkedIn</a></li>\\r\\n</ul>\\r\\n',\n",
       " '<p>scala, java, linux</p>\\r\\n\\r\\n<p>searching job in Berlin.</p>\\r\\n',\n",
       " '<p>see <a href=\"http://NealWalters.com\" rel=\"nofollow\">http://NealWalters.com</a>, \\r\\nMy main expertise is Microsoft BizTalk (http://BizTalk-Training.com), but also delve into WCF and general C# issues quite often. </p>\\r\\n\\r\\n<p>I\\'m also building a system on Google App Engine in Python and starting to use the Dojo/Dijit JavaScript library for Olexe.com. </p>\\r\\n',\n",
       " '<p><strong>argonaut:</strong> A person who is engaged in a dangerous but potentially rewarding quest.</p>\\r\\n',\n",
       " '<p><a href=\"http://meta.stackexchange.com/questions/2950/should-hi-thanks-and-taglines-and-salutations-be-removed-from-posts\">\"Hi\" and \"thanks\" are for the RW, not SO...</a></p>\\r\\n\\r\\n<p>Obligatory social media links:</p>\\r\\n\\r\\n<p>Twitter: <a href=\"http://twitter.com/casperOne\" rel=\"nofollow\">@casperOne</a></p>\\r\\n',\n",
       " \"<p>I'm a phd student at UC Riverside.</p>\\r\\n\",\n",
       " '<p>Statistics student.</p>\\r\\n',\n",
       " '<p>I have my Bachelors of Science in Electrical Engineering and am currently working on my Masters of Science in EE while working for a major telecom company.</p>\\r\\n\\r\\n<p>I specialize in wireless communications and am interested in the psychological aspects of images, audio, and video.</p>\\r\\n',\n",
       " '<p>I am a Computer Scientist and Open Scholar: Web, OLAP, Databases, Time Series, Collaborative Filtering, Information Retrieval, e-Learning.</p>\\r\\n',\n",
       " '<p><img src=\"http://i.stack.imgur.com/6yami.png\" width=\"450\"></p>\\r\\n',\n",
       " '<p>I\\'m an astrophysicist working with the <a href=\"http://en.wikipedia.org/wiki/Pan-STARRS\" rel=\"nofollow\">PS1</a>, <a href=\"http://en.wikipedia.org/wiki/Gaia_%28spacecraft%29\" rel=\"nofollow\">GAIA</a>, and <a href=\"http://en.wikipedia.org/wiki/LAMOST\" rel=\"nofollow\">LAMOST</a> projects.</p>\\r\\n',\n",
       " '<p>Biostatistician\\r\\nBiostatistics Unit\\r\\nFaculty of Health and Environmental Sciences\\r\\nAUT University</p>\\r\\n',\n",
       " '<p>I am an Adjunct Professor of Biology at the University of New Mexico, in Albuquerque, where I sometimes teach a course in Biological Statistics. I also own a small data analysis company. My background includes basic biology and ecology, computer programming, and modeling. Research areas include forest ecology and management, invasive species, and ecological networks. Professional interests include environmental impact studies, ecosystem restoration, and spatial analysis, and visualization. I have taught courses in Biological Statistics, Ecological Modeling, and Field Methods in Biodiversity in the US and Canada.</p>\\r\\n',\n",
       " '<p>I am not actually a rabbit I am a person.</p>\\r\\n',\n",
       " '<p>Statistical Machine Learning, PhD Candidate, Computational Science &amp; Engineering, Georgia Institute of Technology</p>\\r\\n',\n",
       " '<p>I do professionally program in Java. I do also study Physics and Philosophy. (But i dont do these three things at the same time). \\r\\n9Zq4bu42S3wr0K12</p>\\r\\n',\n",
       " '<p>Student at Swinburne University, Applied Statistics </p>\\r\\n',\n",
       " '<p>www.vanheusden.com</p>\\r\\n',\n",
       " \"<p>Hobbes: What's the point of attaching a number to everything you do ?<br>\\r\\nCalvin: If your numbers go up, it means you're having more fun.</p>\\r\\n\",\n",
       " '<p>Hello.</p>\\r\\n',\n",
       " '<p>Python / C++ programmer. working in Bioinformatics. see my <a href=\"http://github.com/tanghaibao\" rel=\"nofollow\">GITHUB</a>.</p>\\r\\n',\n",
       " '<p>merge delete</p>\\r\\n',\n",
       " \"<p>I'm a Computer Science undergraduate and the University of California, Irvine. My interests lie in Machine Learning and Artificial Intelligence. I was introduced to ML in my sophomore year here at UCI during a summer program when I was working on an shape recognition project. After taking my introduction to AI course I was convinced that this was what I wanted to study, and was further reassured after taking my first course in ML. </p>\\r\\n\\r\\n<p>My other interests are in conservation and sustainability. I would really like to see people start making changes in their lives in order to help reverse the environmental damage that our society is creating. I think that people have a lot to learn from nature and that we should pay closer attention to the processes that it has created to solve the same issues that we as a people are struggling with.</p>\\r\\n\",\n",
       " '<p>Econ grad student at MIT. Interested in development economics and labor, have lived in various places in South America and enjoys contributing to public goods if they involve knowledge (ie, SE or Quora).</p>\\r\\n',\n",
       " '<p>a psychometrician by title, i consider myself a quantitative social scientist. and lover of coffee, no mere coffee drinker.</p>\\r\\n',\n",
       " '<p>PLEASE DELETE ME</p>\\r\\n',\n",
       " '<p>I am interested in examining face and object recognition as well as the functional organisation of the ventral temporal lobes with masked priming techniques and functional magnetic resonance imaging coupled with multivariate pattern analysis.</p>\\r\\n',\n",
       " '<p>I\\'m a reluctant SO user, truth be known. I\\'ve had 30 years programming experience, the last 20 of which have been almost exclusively C++. I prefer to spend my idle time anywhere than on internet forums, so I only come here when I have unusual or perplexing questions. But that leads to a low reputation, and perhaps people assume that means I\\'m a novice, or don\\'t understand the basics. The reality is I usually find myself asking questions from a viewpoint of \"this doesn\\'t make sense - something is missing - there is this factor and that factor and another factor... and yet those are the trees, and I can\\'t see the forest for them.\" And then I roll my eyes in despair when people give me answers about trees, and wonder whether SO is worth the pain of trying to pose questions about forests.</p>\\r\\n',\n",
       " '<p>M.Sc student in Tel-Aviv University.</p>\\r\\n',\n",
       " '<p>Database and Java developer</p>\\r\\n',\n",
       " '<p>That\\'s \"Steven\" (with the \"n\" at the end)</p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p><em>\"The whole problem with the world is that fools and fanatics are always so certain of themselves, but wiser people so full of doubts.\"</em>  &mdash; Bertrand Russell  </p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p>Product designer, consumer electronics: audio (with Philips), home automation.<br>\\r\\nDone computer science in a previous life too.  </p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p>Belbin team roles: <a href=\"http://en.wikipedia.org/wiki/Team_Role_Inventories#Plant\" rel=\"nofollow\">Plant</a> and <a href=\"http://en.wikipedia.org/wiki/Team_Role_Inventories#Resource_Investigator\" rel=\"nofollow\">Resource Investigator</a></p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p>Personal values: <strong>respect</strong>, <strong>honesty</strong>, <strong>pride</strong>, <strong>modesty</strong>, <strong>fairness</strong></p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p>I yell because I care  </p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p><a href=\"http://nenya1.net/img/ritter.jpg\" rel=\"nofollow\">favorite candy</a></p>\\r\\n',\n",
       " '<p>a new user of WinBUGS</p>\\r\\n',\n",
       " '<p><a href=\"https://github.com/sabof\" rel=\"nofollow\">github, mostly emacs stuff</a></p>\\r\\n',\n",
       " '<p>Deputy Head of the <a href=\"http://www.csb.gov.lv/en/par-mums/mathematical-support-division-30805.html\" rel=\"nofollow\">Mathematical Support Division</a> at the <a href=\"http://www.csb.gov.lv/en\" rel=\"nofollow\">Central Statistical Bureau of Latvia</a>. More details are available at my <a href=\"http://home.lu.lv/~pm90015/work/CV/CV_Martins_Liberts_EN.pdf\" rel=\"nofollow\">CV (PDF)</a>.</p>\\r\\n',\n",
       " '<p>loves social media technologies, data analyst and researcher in Social Network Analysis!</p>\\r\\n',\n",
       " \"<p>Software developer. I work primarily in C#, Silverlight, SQL, WPF, and ASP[.NET] at work and home, but am trying to find a new practical language to learn that isn't quite so C-based. Right now I'm refreshing my abilities in functional programming with some F#. Also somewhat experienced with C++, PHP, Java, LaTeX, HTML/CSS, and a touch of Scheme.</p>\\r\\n\",\n",
       " \"<p>enough about me, let's talk about you</p>\\r\\n\\r\\n<p>I am a dev advocate at Red Hat's OpenShift. I have been a dev advocate at deCarta, LinkedIn and ESRI.</p>\\r\\n\\r\\n<p>I know some stuff about Java, Spatial Stuff, databases, and general code problem</p>\\r\\n\\r\\n<hr>\\r\\n\",\n",
       " '<p>Blog: <a href=\"http://www.parasdoshi.com\" rel=\"nofollow\">http://www.parasdoshi.com</a></p>\\r\\n\\r\\n<p>MSDN profile: <a href=\"http://social.msdn.microsoft.com/profile/paras%20doshi/?type=forum\" rel=\"nofollow\">http://social.msdn.microsoft.com/profile/paras%20doshi/?type=forum</a> (Microsoft Community Contributor 2011 Award Recipient)</p>\\r\\n',\n",
       " '<p><a href=\"http://davidshen84.appspot.com/blog\" rel=\"nofollow\">My Blog</a></p>\\r\\n',\n",
       " '<p>Machine Learner!</p>\\r\\n',\n",
       " \"<p>I'm an enthusiast programmer since I'm 13 :-) Lost that passion a bit in later years while being an entrepreneur. Right now, I'm working hard to create a symbiosis of these two passions and enjoying it a lot ;-)</p>\\r\\n\",\n",
       " '<p>I\\'m a chemoinformatician. I\\'m keen on data mining and machine learning.</p>\\r\\n\\r\\n<p><a href=\"https://github.com/DrrDom\" rel=\"nofollow\">https://github.com/DrrDom</a></p>\\r\\n',\n",
       " '<p>A data geek and Python enthusiast.</p>\\r\\n',\n",
       " '<p>Data Scientist, Alt-Ac, Technoprogressive, Policy Wonk</p>\\r\\n',\n",
       " '<ul>\\r\\n<li>Director of Data Science, <a href=\"http://cloudera.com\" rel=\"nofollow\">Cloudera</a></li>\\r\\n<li><a href=\"http://github.com/cloudera/oryx\" rel=\"nofollow\">Oryx</a> project creator (formerly Myrrix)</li>\\r\\n<li>Erstwhile VP/PMC/committer for <a href=\"http://mahout.apache.org\" rel=\"nofollow\">Apache Mahout</a></li>\\r\\n<li>Primary author, <a href=\"http://github.com/zxing/zxing\" rel=\"nofollow\">ZXing</a></li>\\r\\n</ul>\\r\\n',\n",
       " 'twitter.com/kjhealy',\n",
       " '<p>I am PhD neuroscientist and data scientist.</p>\\r\\n\\r\\n<p>More at my <a href=\"https://plus.google.com/u/0/101266444458532133630/posts\" rel=\"nofollow\">Google+</a> and <a href=\"http://www.linkedin.com/in/mattbaggott\" rel=\"nofollow\">LinkedIn</a> profiles</p>\\r\\n',\n",
       " '<p>Retired Professor of Statistics; served in a bona-fide University in India for 35 years; currently doing some freelance programmes in Statistics for users; started using computers from First Generation computers in 1972.</p>\\r\\n',\n",
       " '<p>The era of data has yet to come...</p>\\r\\n',\n",
       " '<p>I have a PhD in Thermohydraulics, and I do maths as a hobby. Being free from any constraints, I like to favour areas not covered in my specialty, like Differential Geometry and Category Theory. I also spend some time learning Machine Learning and Artificial Intelligence, due to the sustained growth of data analysis applications.</p>\\r\\n',\n",
       " '<p>I am a young programmer, which trys to get a scholar-ship for studying game design at the Quantm SAE Institute.\\r\\nFormerly studying   business information technology in munich.\\r\\nCurrently studying information technology at heidelberg university.</p>\\r\\n',\n",
       " 'fields of interest: Open Source Software, Design Patterns, Real-Time and Embedded Systems, Software Architecture, Advanced Object-Oriented Design and Analysis, Modern C++ Design and Programming, Data Mining, Machine Learning, Safety Critical Systems, TDD, Agile Software Development, RIA Development\\\\\\\\n',\n",
       " '<p><p><a href=\"http://twitter.com/ceptional\" rel=\"nofollow\">@ceptional</a></p><p><a href=\"http://au.linkedin.com/in/alexholcombe\" rel=\"nofollow\">linkedin</a></p></p>\\r\\n',\n",
       " '<p>Visit <a href=\"http://mortoray.com/\" rel=\"nofollow\">my blog</a> about programming, software development and language design.</p>\\r\\n\\r\\n<p>If it\\'s programming related, the I\\'ve done it. A plethora of languages. Many application domains. A lot of companies. Several countries.</p>\\r\\n\\r\\n<p>I now spend my time working on the Leaf programming language.</p>\\r\\n',\n",
       " \"<p>I predict US bank financials and generate the world's largest body of US bank analytics, worldwide.</p>\\r\\n\",\n",
       " '<p>Political scientist trying to learn python and R. My blog <a href=\"http://polstat.org/blog\" rel=\"nofollow\">polstat.org/blog</a></p>\\r\\n',\n",
       " '<p>PhD candidate at Vrije Universiteit Brussel</p>\\r\\n',\n",
       " \"<p>I'm interested in recommender engines and underlying theoretic base like statistics, probability and machine learning.<br>\\r\\nCurrently I'm working as a web developer.</p>\\r\\n\",\n",
       " '<p>PhD student in Theoretical Physics</p>\\r\\n\\r\\n<p>Research interests include Quantum Optomechanics and Quantum Optics</p>\\r\\n',\n",
       " '<p>Ph.D. student in Mathematics at the University of Chicago</p>\\r\\n',\n",
       " '<p>I am a CS PhD student and a data geek. I have experience in data (and\\r\\nnetwork) mining, analysis, visualization and predictive modeling as it\\r\\nrelates to very large volumes of data (mostly whats on web and what\\r\\nrelates to social media). I also like to build systems to collect\\r\\ndata. I use various database technologies and distributed platforms in\\r\\nmy day to day research.\\r\\nI am currently studying Images on social media and my thesis is about\\r\\nunderstanding the role of multimedia content in information spread.\\r\\nI love to write code, drink good coffee and read good books.</p>\\r\\n',\n",
       " '<p>Mathematician, MSc in Machine Learning and Data Mining, working in Financial Services</p>\\r\\n',\n",
       " '<p>Dr Paul Brewer</p>\\r\\n\\r\\n<p>Owner: Economic and Financial Technology Consulting LLC\\r\\n<a href=\"http://eaftc.com\" rel=\"nofollow\">eaftc.com</a></p>\\r\\n\\r\\n<p>Open Source Projects:</p>\\r\\n\\r\\n<p><a href=\"http://www.armdisarm.com\" rel=\"nofollow\">armdisarm.com</a> Unofficial HAMP Loan Modification Calculator</p>\\r\\n\\r\\n<p><a href=\"http://github.com/DrPaulBrewer/html5csv\" rel=\"nofollow\">html5csv.js</a> Javascript/JQuery lib for manipulating tabular data</p>\\r\\n\\r\\n<p>Hobbies:</p>\\r\\n\\r\\n<p>Amateur Radio Callsign <a href=\"http://www.qrz.com/db/KI6CQ\" rel=\"nofollow\">KI6CQ</a></p>\\r\\n',\n",
       " '<p>A GIS professional who works with web mapping. My main skills are with ESRI Technology, .NET, FLEX and ORACLE.</p>\\r\\n',\n",
       " '<p>Master student in computational Neuroscience...</p>\\r\\n',\n",
       " '<p>I founded and run <a href=\"https://circleci.com\" rel=\"nofollow\">Circle</a>, where we make <a href=\"https://circleci.com\" rel=\"nofollow\">easy-to-use, hosted continuous integration</a>.</p>\\r\\n\\r\\n<p>In past lives, I\\'ve worked on the Javascript engine at Mozilla, done a PhD in compiler optimizations, static analyses and dynamic languages, and started a YCombinator startup.</p>\\r\\n',\n",
       " '<p>Hello! My concentration is statistical genetics. I also like statistical computing (mainly) using R and other open-source software packages.</p>\\r\\n',\n",
       " '<p>Mostly harmless.</p>\\r\\n',\n",
       " '<p>I am a math professor at the University of Puget Sound.  My background is in operations research, and I teach typical OR courses such as optimization, modeling, and probability, as well as calculus, statistics, and differential equations.</p>\\r\\n\\r\\n<p>My math blog, <em><a href=\"http://mikespivey.wordpress.com/\" rel=\"nofollow\">A Narrow Margin</a></em>, includes (among other things) discussion of some of my favorite posts - of mine and of others - from math.SE. </p>\\r\\n',\n",
       " '<p>As <a href=\"http://algo.inria.fr/flajolet/\" rel=\"nofollow\">somebody</a> used to say: </p>\\r\\n\\r\\n<p><em>Does research. Smokes. Battles administration. Smokes. Wishes he could stop battling administration so that he could have more time to do research. Smokes some more.</em> </p>\\r\\n\\r\\n<p>The same. Except I do not smoke.</p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p>This paragraph is for my personal use but freely available:</p>\\r\\n\\r\\n<p>Welcome to Math.SE! Please, consider updating your question to include what you have tried and where you are getting stuck. That way, people on this site will know exactly what help you need.</p>\\r\\n',\n",
       " \"<p>Scientist, programmer, and a digger of all things tech nestled snugly in the Columbia River Gorge (i.e. Heaven). When not paying my mortgage, I'm building Hydrasi, an environmental data company that is trying its best to drag the environmental sciences kicking and screaming into the 21st century.</p>\\r\\n\",\n",
       " '<p>Statistician and R programmer at the faculty of Bio-Engineering, university of Ghent</p>\\r\\n\\r\\n<p>Co-author of \\'<a href=\"http://eu.wiley.com/WileyCDA/WileyTitle/productCd-1119962846.html\" rel=\"nofollow\">R for Dummies</a>\\' (out in july 2012 )</p>\\r\\n\\r\\n<p>contact : Joris - dot - Meys - at - Ugent - dot - be</p>\\r\\n',\n",
       " '<p>PhD student in econometric theory at UCSD</p>\\r\\n',\n",
       " '<p>vegetation ecologist and remote sensing scientist from Hamburg, Germany</p>\\r\\n',\n",
       " '<p>hello world</p>\\r\\n',\n",
       " '<p>Assistant prof in Industrial and Systems Engineering at the University of Minnesota</p>\\r\\n',\n",
       " \"<p>ah, Carl, while you are not safe I am not safe, and \\r\\nnow you're really in the total animal soup of time</p>\\r\\n\",\n",
       " '<p>@neuinfo</p>\\r\\n\\r\\n<p>@jcachat</p>\\r\\n',\n",
       " '<p>Mad Skillz: </p>\\r\\n\\r\\n<pre><code> - Programming\\r\\n - System Administration\\r\\n - L2/L3/L4 network knowledge\\r\\n - FreeBSD and Linux experience\\r\\n - Highload passion\\r\\n - HPC(GPFS/Infiniband) user\\r\\n - IPv6 evangelist\\r\\n - nginx fan\\r\\n - p2p lover\\r\\n</code></pre>\\r\\n',\n",
       " '<p>While I have degrees in mathematics and theoretical computer science, I have since become a quantitative social science researcher. My day-to-day programming experience is largely with applied statistical analyses and simulations, although I maintain an amateur interest in abstract theoretical CS issues.</p>\\r\\n\\r\\n<p>My first major programming experience was with Mathematica. I have at various times programmed in (or at least dabbled with) C++, SAS, SPSS, Javascript, R, and Haskell. On the whole, I tend to use multi-paradigm languages in a functional style. I enjoy Haskell the most, and am trying to shift most of my work there, but still consider myself a beginner.</p>\\r\\n',\n",
       " '<p>PhD-student and anesthesiologist</p>\\r\\n',\n",
       " '<p>Tech Enthusiast, Traveller, Gastronome, Love movies, Youtube addict, ...and many more :)</p>\\r\\n',\n",
       " '<p>Working in JavaScript, Google Analytics, Python, PHP, MySQL, HTML/CSS, jQuery, Google App Engine, Git, NodeJS, Heroku, EC2.</p>\\r\\n\\r\\n<p>Find me on <a href=\"http://twitter.com/yahel\" rel=\"nofollow\">Twitter</a>.</p>\\r\\n',\n",
       " '<p>Graduate (M.Sc.) Student at State University of Campinas.\\r\\nMachine Learning, CBIR, High Dimensional Indexing Methods and kNN Search in HighD.</p>\\r\\n',\n",
       " '<p>Just a regular Norwegian guy. Nothing more to see here, move on =)</p>\\r\\n',\n",
       " '<p>Contractor specialising in charity/not-for-profit sector<br/>\\r\\nMCTS, Asp.Net 3.5<br/>\\r\\nOpen-source SSRS Front end: <a href=\"http://stackoverflow.com/questions/5272753/alternative-ssrs-front-ends/6779738#6779738\">CrissCross</a><br/>\\r\\nBlog: <a href=\"http://www.codeulike.com\" rel=\"nofollow\">www.codeulike.com</a></p>\\r\\n',\n",
       " '<p>IBM Software Engineer.</p>\\r\\n',\n",
       " '<p>Will master C/C++</p>\\r\\n',\n",
       " 'http://www.quora.com/Dimitry-Lukashov',\n",
       " '<p>I\\'m Senior Researcher at the <a href=\"http://www.mzes.uni-mannheim.de\" rel=\"nofollow\" title=\"The Mannheim Centre for European Social Research\">MZES</a> and a member of the <a href=\"http://reforms.uni-mannheim.de\" rel=\"nofollow\" title=\"SFB 884: The Political Economy of Reforms\">SFB 884</a>, both at the <a href=\"http://www.uni-mannheim.de/1/\" rel=\"nofollow\">University of Mannheim</a> in Germany.</p>\\r\\n',\n",
       " '<p>Digital Technology Nerd, Music fanatic, Health Foodie, Political News Junkie &amp; Lover of ALL things Apple.</p>\\r\\n\\r\\n<p>University of Texas at Austin</p>\\r\\n\\r\\n<p>B.A. Economics</p>\\r\\n',\n",
       " 'I just want a badge.',\n",
       " '<p>Computer programmer, etc.  Interests: AI, game programming</p>\\r\\n',\n",
       " '<p>Developer</p>\\r\\n',\n",
       " '<p>I\\'m specialized in <strong>iOS Dev</strong> and also in <strong>MATLAB, Machine Learning, Image Processing</strong></p>\\r\\n\\r\\n<p><a href=\"https://github.com/guveniscan\" rel=\"nofollow\">https://github.com/guveniscan</a></p>\\r\\n',\n",
       " '<p>I work under the propulsion directorate in the Air Force Research Laboratory aka (AFRL).  Currently working on an iPad app, just picked up Xcode and Objective-C so this is one of my more challenging projects.  </p>\\r\\n',\n",
       " '<blockquote>\\r\\n  <p><em>That is not dead which can eternal lie.</em></p>\\r\\n  \\r\\n  <p><em>And with strange aeons even death may die.</em></p>\\r\\n</blockquote>\\r\\n',\n",
       " '<p>I\\'m finishing up my Ph.D. in Human Genetics at Vanderbilt University, and will be job-hunting near the end of the year. Find me on my <a href=\"http://www.stephenturner.us/\" rel=\"nofollow\">website</a>, <a href=\"http://gettinggeneticsdone.blogspot.com/\" rel=\"nofollow\">blog</a>, or <a href=\"http://twitter.com/Genetics_Blog\" rel=\"nofollow\">twitter</a>.</p>\\r\\n',\n",
       " '<p>At various times a user of python (esp. django, tornado, and numpy/scipy), javascript (esp. jquery), Cognos, and statistical analysis packages similar to the programming language R.  Also did a few years in a mainframe gig.</p>\\r\\n',\n",
       " \"<p>I don't really know yet.</p>\\r\\n\",\n",
       " \"<p>I'm a chemist researcher, working now for a french company making high-performance materials.</p>\\r\\n\",\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " '<blockquote>\\n  <p><strong>Possible Duplicate:</strong><br>\\n  <a href=\"http://stats.stackexchange.com/questions/4713/binomial-confidence-interval-estimation-why-is-it-not-symmetric\">Binomial confidence interval estimation - why is it not symmetric?</a>  </p>\\n</blockquote>\\n\\n\\n\\n<p>Are confidence intervals always symmetrical around the point estimate?</p>\\n',\n",
       " '<p>I view PCA as a geometric tool. If you are given a bunch of points in 3-space which are pretty much all on a straight line, and you want to figure out the equation of that line, you get it via PCA (take the first component). If you have a bunch of points in 3-space which are mostly planar, and want to discover the equation of that plane, do it via PCA (take the least significant component vector and that should be normal to the plane). </p>\\n',\n",
       " \"<p>I'm sorry as I'm not sure I completely understand your problem, but is your sample size staying at n=100 and you're just looping it a different number of times and calculating sample standard deviations?  If so, I don't think you should expect the larger k to produce a smaller sample standard deviation.</p>\\n\\n<p>I have noticed that some things computationally grow complex faster than linear where I would expect linear.  </p>\\n\\n<p>Just some thoughts.</p>\\n\",\n",
       " '<p>Here\\'s an example from the ggplot2 homepage: <a href=\"http://had.co.nz/ggplot2/geom_errorbar.html\" rel=\"nofollow\">http://had.co.nz/ggplot2/geom_errorbar.html</a> as others have mentioned in the comments, you have to calculate SE on your own and append this information to the data.frame</p>\\n\\n<pre><code>df &lt;- data.frame( \\n  trt = factor(c(1, 1, 2, 2)), \\n  resp = c(1, 5, 3, 4), \\n  group = factor(c(1, 2, 1, 2)), \\n  se = c(0.1, 0.3, 0.3, 0.2) \\n ) \\ndf2 &lt;- df[c(1,3),]\\np &lt;- ggplot(df, aes(fill=group, y=resp, x=trt)) \\np + geom_bar(position=\"dodge\", stat=\"identity\") \\ndodge &lt;- position_dodge(width=0.9) \\np + geom_bar(position=dodge) + geom_errorbar(aes(ymax = resp + se, ymin=resp - se), position=dodge, width=0.25) \\n</code></pre>\\n\\n<p>As a pointer, SE @ 95% CI usually looks something like this: </p>\\n\\n<pre><code>df$se &lt;- 1.96*(sd(your_data, na.rm=T)/sqrt(your_n))\\n</code></pre>\\n\\n<p>Your upper and lower CI bounds will just be df$se +/- the response (as shown in the aes() for geom_errorbar(), above)</p>\\n',\n",
       " '<p>The essential test in regression models is the Full-Reduced test.  This is where you are comparing 2 regression models, the Full model has all the terms in it and the Reduced test has a subset of those terms (the Reduced model needs to be nested in the Full model).  The test then tests the null hypothesis that the reduced model fits just as well as the full model and any difference is due to chance.</p>\\n\\n<p>Common printouts from statistical software include an overall F test, this is just the Full-Reduced test where the reduced test is an intercept only model.  They also often print a p-value for each individual predictor, this is just a series of Full-Reduced model tests, in each one the reduced model does not include that specific term.  There are many ways to use these tests to answer questions of interest.  In fact pretty much every test taught in a introductory stats course can be computed using regression models and the Full-Reduced test and the results will be identical in many cases and a very close approximation in the few others.</p>\\n',\n",
       " '<p>If the correct result is obtained by T=1 and L=-1, then I would consider using T=1,N=-1,A=-1.</p>\\n',\n",
       " '<p>In a regression, the interaction term wipes out both related direct effects. Do I drop the interaction or report the outcome? The interaction was not part of the original hypothesis. </p>\\n',\n",
       " '<p><em>Updated</em></p>\\n\\n<p>The question asks us to choose between Pearson\\'s and Spearman\\'s method when <em>normality</em> is questioned.  Restricted to this concern, I think the following paper should inform anyone\\'s decision:</p>\\n\\n<ul>\\n<li><a href=\"http://www.jstor.org/pss/2346598\" rel=\"nofollow\">On the Effects of Non-Normality on the Distribution of the Sample Product-Moment\\nCorrelation Coefficient</a> (Kowalski, 1975)</li>\\n</ul>\\n\\n<p>It\\'s quite nice and provides a survey of the considerable literature, spanning decades, on this topic -- starting from Pearson\\'s \"mutilated and distorted surfaces\" and robustness of distribution of $r$.  At least part of the contradictory nature of the \"facts\" is that much of this work was done before the advent of computing power -- which complicated things because the type of non-normality had to be considered and was hard to examine without simulations.</p>\\n\\n<p>Kowalski\\'s analysis concludes that the distribution of $r$ is <em>not</em> robust in the presence of non-normality and recommends alternative procedures.  The entire paper is quite informative and recommended reading, but skip to the very short conclusion at the end of the paper for a summary.</p>\\n\\n<p>If asked to choose between one of Spearman and Pearson when normality is violated, the distribution free alternative is worth advocating, i.e. Spearman\\'s method.  </p>\\n\\n<hr>\\n\\n<p><em>Previously</em> ..</p>\\n\\n<p>Spearman\\'s correlation is a rank based correlation measure; it\\'s non-parametric and does not rest upon an assumption of normality.  </p>\\n\\n<p>The sampling distribution for Pearson\\'s correlation does assume normality; in particular this means that although you can compute it, conclusions based on significance testing may not be sound.  </p>\\n\\n<p>As Rob points out in the comments, with large sample this is not an issue.  With small samples though, where normality is violated, Spearman\\'s correlation should be preferred.</p>\\n\\n<p><em>Update</em> Mulling over the comments and the answers, it seems to me that this boils down to the usual non-parametric vs. parametric tests debate.  Much of the literature, e.g. in biostatistics, doesn\\'t deal with large samples.  I\\'m generally not cavalier with relying on asymptotics.  Perhaps it\\'s justified in this case, but that\\'s not readily apparent to me.</p>\\n',\n",
       " '<p>Your wording is implying causality, which is not what the R^2 represents. \"Does price (x) determine square footage (y)?\" is implying causality which is not what is captured through a correlation. \"Price explains X% of the variation in square footage\" describes that there is a relationship between price and square footage, but not a causal one. This only implies that these variables vary together, not that price causes square footage. Its more akin to saying \"In general, When price goes up X amount square footage goes up Y amount\"</p>\\n',\n",
       " \"<p>I'm trying to simulate this scenario: 10 different algorithms are solving a number of problems. All 10 are run on each problem instance, which means if a particular problem instance is hard, I expect all algorithms to perform quite bad and vice versa. </p>\\n\\n<p>Now, to simulate this, I created 10 different normal distributions from which I draw random numbers. These numbers represent the algorithms performance for some problem instance; however, since the distributions are independent, I'm not capturing the effect of the problem instance on the drawn numbers. Any idea how I can do this (i.e. correlated the random numbers that are drawn)?</p>\\n\\n<p>Regards,  </p>\\n\",\n",
       " \"<p>Both functions are great for comparing, for example, survival models. The first especially for computing Harrell's C-index, the second for NRI and IDI.</p>\\n\\n<p>However, it looks like <code>rcorr.cens</code> takes survival probabilities and <code>improveProb</code> takes event probabilites, that is, 1 - survival probabilities, as arguments. Could somebody confirm this? It just feels weird, and I haven't been able to find information on it in the documentation.</p>\\n\",\n",
       " '<p>I wanted to compare how two different (independent) models perform under winbugs, So I created code like:</p>\\n\\n<pre><code># likelihood\\nfor (i in 1:sites) {\\n    M[i] ~ dpois(lambda) # model 1\\n    M2[i] ~ dpois(lambda2) # model 2\\n\\n    for (j in 1:sample) {\\n        obs[i, j] &lt;- sum(y[i, j,])\\n\\n        # model 1\\n        y[i, j, 1] ~ dbin(p, M[i])\\n        obs[i, j] ~ dbin(p_komb, M[i])\\n\\n        # model 2\\n        obs[i, j] ~ dbin(p2, M2[i])\\n    }\\n} \\n</code></pre>\\n\\n<p>But it leads to wrong results in model 2! But if I separated the model 2:</p>\\n\\n<pre><code># likelihood\\nfor (i in 1:sites) {\\n    M2[i] ~ dpois(lambda2)\\n    for (j in 1:sample) {\\n        obs[i, j] &lt;- sum(y[i, j,])\\n        obs[i, j] ~ dbin(p2, M2[i])\\n    }\\n}\\n</code></pre>\\n\\n<p>then it performs well and returns good result! I can\\'t really find out why the first example should return different result. The two models combined in the example seems to be completely independent. <strong>WinBUGS bug? Any ideas?</strong></p>\\n\\n<p>(It doesn\\'t matter if I separate the models in two for loops \\n- result is the same.)</p>\\n\\n<p>I have WinBUGS 1.4.3 (August 2007) with immortality patch installed.\\nHere is complete <strong>reproducible code for R and package R2WinBUGS (with data generation)</strong>:</p>\\n\\n<pre><code>################## \\n# data generation\\nrequire(vcd)\\n\\nsites &lt;- 120 # 60\\n\\nmean_M &lt;- 16\\n\\nM &lt;- rpois(sites, mean_M) \\n\\np &lt;- 0.4 #0.64\\n\\nsample &lt;- 2 # 3\\n\\ny = rep(NA, sites * sample * 2)\\ndim(y) = c(sites, sample, 2)\\n\\nfor (i in 1:sites) {\\n#   obs[i,] = rbinom(sample, M[i], p)\\n    for (j in 1:sample) {\\n        y[i,j,1] = rbinom(1, M[i], p)\\n        y[i,j,2] = rbinom(1, M[i] - y[i,j,1], p)\\n    }\\n}\\n\\ny_sample_total = apply(y, c(1, 2), sum)\\n\\n############################################\\n# two models together - simple model with complex model together \\n\\n\\nsink(\"tmp_bugs/model.txt\")\\ncat(\"\\n\\nmodel {\\n\\n# likelihood\\nfor (i in 1:sites) {\\n    M[i] ~ dpois(lambda) # model 1\\n    M2[i] ~ dpois(lambda2) # model 2\\n\\n    for (j in 1:sample) {\\n        obs[i, j] &lt;- sum(y[i, j,])\\n\\n        # model 1\\n        y[i, j, 1] ~ dbin(p, M[i])\\n        obs[i, j] ~ dbin(p_komb, M[i])\\n\\n        # model 2 (simple)\\n        obs[i, j] ~ dbin(p2, M2[i])\\n    }\\n}\\n\\n# derived parameters\\nMtot &lt;- sum(M[])\\nM2tot &lt;- sum(M2[])\\n\\n# priors\\n\\ntau &lt;- 1/(4 * 4)\\n\\np &lt;- 1/(1+exp(-logit_p))\\nlogit_p ~ dnorm(0, tau)\\n\\np2 &lt;- 1/(1+exp(-logit_p2))\\nlogit_p2 ~ dnorm(0, tau)\\n\\np_komb &lt;- p + (1 - p) * p\\n\\n\\nlambda ~ dunif(0, 100)\\nlambda2 ~ dunif(0, 100)\\n\\n}\\n\\n\\n\")\\nsink()\\n\\n\\nwin.data = list(y = y, sample = sample, sites = sites)\\n\\ninits = function () { list(\\n    M = apply(y_sample_total, 1, max), \\n    M2 = apply(y_sample_total, 1, max), \\n    logit_p = rnorm(1, 0, 4),\\n    logit_p2 = rnorm(1, 0, 4)\\n) }\\n\\nparams = c(\"M\", \"M2\", \"p\", \"p2\", \"Mtot\", \"M2tot\", \"lambda\", \"lambda2\")\\n#params = c(\"M2\", \"p2\", \"M2tot\", \"lambda2\")\\n\\nni &lt;- 2500\\nnt &lt;- 16\\nnb &lt;- 1000\\nnc &lt;- 3\\n\\ndate()\\nout1 &lt;- bugs(win.data, inits, params, \"model.txt\",\\n    nc, ni, nb, nt, bugs.directory = \"C:/Program Files/WinBUGS14/\", \\n    working.directory = paste(getwd(), \"/tmp_bugs/\", sep = \"\"),\\n    debug = TRUE\\n)\\ndate()\\n\\n\\n#############################\\n# simpler model itself\\n\\nsink(\"tmp_bugs/model.txt\")\\ncat(\"\\n\\nmodel {\\n\\n# likelihood\\nfor (i in 1:sites) {\\n    M2[i] ~ dpois(lambda2)\\n    for (j in 1:sample) {\\n        obs[i, j] &lt;- sum(y[i, j,])\\n        obs[i, j] ~ dbin(p2, M2[i])\\n    }\\n}\\n\\n# derived parameters\\nM2tot &lt;- sum(M2[])\\n\\n# priors\\n\\ntau &lt;- 1/(4 * 4)\\n\\np2 &lt;- 1/(1+exp(-logit_p2))\\nlogit_p2 ~ dnorm(0, tau)\\n\\nlambda2 ~ dunif(0, 100)\\n\\n}\\n\\n\\n\")\\nsink()\\n\\n\\nwin.data = list(y = y, sample = sample, sites = sites)\\n\\ninits = function () { list(\\n#   M = apply(y_sample_total, 1, max), \\n    M2 = apply(y_sample_total, 1, max), \\n#   N = y_sample_total, \\n#   after_removal = y[,,2],\\n#   logit_p = rnorm(1, 0, 4),\\n#   logit_q = rnorm(1, 0, 4), \\n    logit_p2 = rnorm(1, 0, 4)\\n) }\\n\\n\\n#params = c(\"M\", \"M2\", \"p\", \"p2\", \"Mtot\", \"M2tot\", \"lambda\", \"lambda2\")\\nparams = c(\"M2\", \"p2\", \"M2tot\", \"lambda2\")\\n\\nni &lt;- 2500\\nnt &lt;- 16\\nnb &lt;- 1000\\nnc &lt;- 3\\n\\ndate()\\nout2 &lt;- bugs(win.data, inits, params, \"model.txt\",\\n    nc, ni, nb, nt, bugs.directory = \"C:/Program Files/WinBUGS14/\", \\n    working.directory = paste(getwd(), \"/tmp_bugs/\", sep = \"\"),\\n    debug = TRUE\\n)\\ndate()\\n\\n\\n\\n#############################\\n# summary - results of simple model (M2, M2tot) differ depending on whether the \\n# model was evaluated along with the more complex model or alone!!!\\n# Shouldn\\'t be!!!\\n\\n#print(out, dig = 3)\\n\\npar(mfrow = c(2, 2))\\n\\nout &lt;- out1\\n\\nhist(out$sims.list$M2tot, breaks = 100)\\nabline(v = out$mean$M2tot, col = \"red\", lwd = 2)\\nabline(v = sum(M), col = \"green\", lwd = 2)\\n#lines(quantile(out$sims.list$M2tot, c(0.025, 0.975)), rep(sum(par(\"usr\")[3:4]*c(0.9,0.1)), 2), lwd = 2)\\nlines(quantile(out$sims.list$M2tot, c(0.025, 0.975)), rep(par(\"usr\")[3], 2), lwd = 4)\\nlegend(\"topright\", c(\"estimated total M\", \"real total M\", \"95% credible int.\"), col = c(\"red\", \"green\", \"black\"), lty = 1, box.lty = 0, cex = 0.7)\\n\\nhist(out$sims.list$lambda2)\\n\\nout &lt;- out2\\n\\nhist(out$sims.list$M2tot, breaks = 100)\\nabline(v = out$mean$M2tot, col = \"red\", lwd = 2)\\nabline(v = sum(M), col = \"green\", lwd = 2)\\n#lines(quantile(out$sims.list$M2tot, c(0.025, 0.975)), rep(sum(par(\"usr\")[3:4]*c(0.9,0.1)), 2), lwd = 2)\\nlines(quantile(out$sims.list$M2tot, c(0.025, 0.975)), rep(par(\"usr\")[3], 2), lwd = 4)\\nlegend(\"topright\", c(\"estimated total M\", \"real total M\", \"95% credible int.\"), col = c(\"red\", \"green\", \"black\"), lty = 1, box.lty = 0, cex = 0.7)\\n\\nhist(out$sims.list$lambda2)\\n</code></pre>\\n',\n",
       " '<p>I\\'m interested in the process of testing or validating a particular implementation of a statistical method, and what datasets and/or published analysis exist that could be used to do this in practice.</p>\\n\\n<p>For instance, if I write an algorithm to implement a simple linear regression, I might feed in some numbers and check the result looks good, or I might feed numbers into my code and some other system and compare.  In some cases, people seem to have already done this and then publish the numbers and results which could be defined as reference data.</p>\\n\\n<p>To start off, the best one I know is the <a href=\"http://www.itl.nist.gov/div898/strd/index.html\" rel=\"nofollow\">NIST Statistical Reference Datasets</a> page that publishes a wide range of datasets and calculations that covers areas such as Analysis of Variance, Linaear Regression, Markov Chain and Monte Carlo simulation and Non-Linear regression.</p>\\n\\n<p>Are there any other good / notable ones out there.</p>\\n\\n<p>Edit: I reworded to make clear that I\\'m not just looking for open datasets, but I\\'m interested in datasets and solutions to specific statistical problems that could be used to test an implementation of a technique.</p>\\n',\n",
       " '<p>Everything sounds pretty OK except the ending of Q2. When you do regress y on the first ten PCs, w1,...,w10 are unknown weights to be estimated from your data using Ordinary Least Squares for example.</p>\\n',\n",
       " '<p>If you throw out observations outside 3 sigma from the mean the distribution you end up with is a truncated normal.  You just divide the normal density by the norming constant that makes the integral over the restricted region equal 1.  Then you can write down the likelihood function for your data and compute the maximum likelihood estimate.  I am addressing you specific example and not the generalconditional distribution question.</p>\\n',\n",
       " '<ul>\\n<li>68% of all people lie within 1 standard deviation of the mean (<code>2.26 - 3.34</code>):</li>\\n</ul>\\n\\n<p><img src=\"http://i.stack.imgur.com/V9Qli.png\" alt=\"enter image description here\"></p>\\n\\n<ul>\\n<li>95% of all people lie within 2 standard deviations of the mean (<code>1.72 - 3.88</code>):</li>\\n</ul>\\n\\n<p><img src=\"http://i.stack.imgur.com/eaTEe.png\" alt=\"enter image description here\"></p>\\n\\n<p>It tells you how \"spread out\" your numbers are.</p>\\n',\n",
       " '<p>I don\\'t think the Fligner-Killeen test (nor the Brown-Forsythe) test is appropriate since you don\\'t know the median in the published data (if you do have it and simply didn\\'t mention it then never mind).</p>\\n\\n<p>I wouldn\\'t suggest simulation of the data either unless you\\'re sure the samples follow a specific distribution.</p>\\n\\n<p>Since you don\\'t have the median and the distribution is uncertain <a href=\"http://en.wikipedia.org/wiki/Levene%27s_test\" rel=\"nofollow\">Levene\\'s Test</a> would be appropriate. I\\'ve never ran the test in R before, but there is a description of it <a href=\"http://hosho.ees.hokudai.ac.jp/~kubo/Rdoc/library/car/html/levene.test.html\" rel=\"nofollow\">here</a>. If you\\'re having a lot of trouble getting the R code to work though I\\'d just compute it by hand given the summary statistics from the literature and your own data. As the wikipedia indicates that statistic is F distributed so <a href=\"http://www.statsoft.com/textbook/distribution-tables/#f\" rel=\"nofollow\">you\\'ll need a table</a> if you don\\'t have one.</p>\\n',\n",
       " '<p>If the assumption of homoskedasticity is truly valid, the simple estimator of the VCE is more efficient than the robust sandwich version. That means it has smaller variance, so your estimates are less uncertain.</p>\\n\\n<p>Of course, you can always do a heteroskedasticity test first and estimate accordingly.  </p>\\n',\n",
       " '<p>Ritualized Statistics.</p>\\n\\n<p>This \"sin\" is when you apply whatever thing you were taught, regardless of its appropriateness, because it\\'s how things are done. It\\'s statistics by rote, one level above letting the machine choose your statistics for you.</p>\\n\\n<p>Examples are Intro to Statistics-level students trying to make everything fit into their modest t-test and ANOVA toolkit, or any time one finds oneself going \"Oh, I have categorical data, I should use X\" without ever stopping to look at the data, or consider the question being asked.</p>\\n\\n<p>A variation on this sin involves using code you don\\'t understand to produce output you only kind of understand, but know \"the fifth column, about 8 rows down\" or whatever is the answer you\\'re supposed to be looking for.</p>\\n',\n",
       " \"<p>Suppose you have a binary outcome $Y_i$ and a single predictor $X_i$ and fit a logistic regression model: </p>\\n\\n<p>$$ \\\\\\\\log \\\\\\\\left( \\\\\\\\frac{ P(Y_i = 1 | X_i) }{ P(Y_i = 0 | X_i) } \\\\\\\\right) = \\\\\\\\beta_0 + \\\\\\\\beta_1 X_i$$</p>\\n\\n<p>Then, $e^{\\\\\\\\beta_1}$ is the multiplicative change in odds that $Y_i=1$ associated with a 1-unit increase in the predictor. For example, $e^{\\\\\\\\beta_1} = 1.03$ then a one unit increase in $X_i$ leads to $3\\\\\\\\%$ increase in the odds that $Y_i = 1$. </p>\\n\\n<p>Now, if the natural scale of $X_i$ is not of interest to you, then you can re-scale so that $e^{\\\\\\\\beta_1}$ is the percentage change of interest. For example, if you're truly interested in the effect of a $10$-unit change in $X_i$, as you appear to be here, then $e^{10\\\\\\\\beta_1}$ is the quantity of interest. In general, $e^{c \\\\\\\\cdot \\\\\\\\beta_1}$ is the change in odds associated with a $c$ unit increase in the predictor. </p>\\n\\n<p>You can also, equivalently, scale your predictor variable before the analysis to get the same result. That is, if you instead used the predictor $X^{\\\\\\\\star}_i = c \\\\\\\\cdot X_i$ then the resulting regression coefficient would be interpreted as the effect for a $c$ unit increase in the original predictor, $X_i$. </p>\\n\",\n",
       " '<p>This is a great example to illustrate the difference between frequentist and Bayesian approaches to inference.</p>\\n\\n<p><strong>My first, simplistic frequentist response:</strong>\\nIf you\\'ve already assumed the distribution of strikes is binomial you don\\'t need to know anything about the other 1000 players (other than perhaps you could use them to check your binomial assumption).</p>\\n\\n<p>Once you have the binomial assumption clear, your estimate is very straightforward: 3/10.  The variance of this estimate is the usual p(1-p)/n = 0.021.  </p>\\n\\n<p>Basically, the 1000 other players are irrelevant unless you think there is something interesting and non binomial about the strike distribution (eg people get better as they play more games).  </p>\\n\\n<p><strong>A more considered Bayesian way of looking at it:</strong>\\nAlternatively, if you are interested in applying the prior knowledge you have from other players and you think that the new player is basically a new sample from that same population, you should think of it in <a href=\"http://en.wikipedia.org/wiki/Bayesian_inference\" rel=\"nofollow\">Bayesian terms</a>.  </p>\\n\\n<p>Estimate a prior distribution of players.  To do this, you need to look at your 1000 data points - the 1000 players who have already been observed, for each of whom you have an estimate of their probability of a strike.  Each of these 1000 points can take only one of 21 values (from zero to twenty strikes out of twenty) and you will see a distribution over the whole field.  If you convert these scores to proportions (ie between zero and one) this distribution can probably be approximated reasonably well by a probability distribution of a random variable with a <a href=\"http://en.wikipedia.org/wiki/Beta_distribution\" rel=\"nofollow\">Beta distribution</a>.  A beta distribution is fully characterised by just two parameters - lets say a and b - but because these parameters are not really to do with the distribution you have asked us about (the particular player\\'s own probability of a strike) but a higher level distribution we call them hyperparameters.  You can develop estimates of these hyperparameters from your 1000 data points in one of a number of ways which aren\\'t really relevant to the main point of your question.</p>\\n\\n<p>Before you have any information on your player at all, your best guess as to his/her proportion of scoring a strike (lets call it p) would just be the most likely value of p from that Beta distribution we just fitted.  </p>\\n\\n<p>However, we have data on our own player, not just the general population!  <em>In God we trust, all others must bring data</em> (I\\'d attribute this quote if I could remember where I found it, sorry).  Each time we observe our player play a game and get a strike or not, we have a new piece of information to precisify our estimate of his proportion.  </p>\\n\\n<p>One of the neat things about the beta distribution as a probability distribution for a proportion is that as we gather new information from data and create a new, improved estimate of the proportion, probability theory can show that the new, improved estimate is also a beta distribution - just a more concentrated version.  This is because the beta distribution is what is referred to as a <a href=\"http://en.wikipedia.org/wiki/Conjugate_prior\" rel=\"nofollow\">conjugate prior</a> when trying to make estimates about a binomial model.</p>\\n\\n<p>That is, if we observe z out of n successful events (games with strikes in this case); and the prior distribution was beta(a,b); the posterior distribution (are estimate of the probability distribution of p given both the original 1000 data points and are new observation of ten games) is beta(a+z, b+n-z) or (in our case) beta(a+3, b+7).  As you can see, the more data you get the less important a and b are.  The mathematics of this is reasonably straightforward and in many texts but not that interesting (to me, anyway).</p>\\n\\n<p>If you have R you can see an example by running the code below (and if you don\\'t have R you should get it - it\\'s free and it\\'s awesome for helping think through this sort of problem).  This assumes the prior distribution of players can be modelled by beta(2,5) - this was just made up by me.  In reality, there are ways you can estimate figures for a and b better than just making up 2 and 5 because I think the curve looks ok.  </p>\\n\\n<p>As you will see if you run this stylized example, the point estimate of the player\\'s probability of scoring a strike, given a prior distribution of beta(2,5), is 0.29 rather than 0.30.  Also, we can create a credibility interval, which is frankly more intuitive and easier to explain than a confidence interval (see many questions and discussions on the internet of the difference between the two, including on CrossValidated).</p>\\n\\n<pre><code>plot(0:100/100,dbeta(0:100/100,2,5), type=\"l\", ylim=c(0,4), bty=\"l\")\\nlines(0:100/100,dbeta(0:100/100,2+3,5+7), type=\"l\", lty=2)\\nlegend(0.6,3.5,c(\"Posterior distribution\", \"Prior distribution\"), \\n    lty=2:1, bty=\"n\")\\nqbeta(c(0.025, 0.975), 2, 5) # credibility interval prior to any new data\\nqbeta(c(0.025, 0.975), 2+3, 5+7) # credibility interval posterior to data\\nqbeta(0.5, 2+3, 5+7) # point estimate of p, posterior to data\\n</code></pre>\\n\\n<p>Then observe your new player; and calculate a new posterior distribution for the new player.  Effectively this says \"given what we just observed, where in the distribution of players do we think this person is most likely to be?\"</p>\\n',\n",
       " \"<p>The difference is that the first example is a test of all main effects, two-way, and three-way interactions of a three factor ANOVA whereas the second through fourth examples are tests of a single factor.</p>\\n\\n<p>Specifically, the first test will answer all of the following:</p>\\n\\n<ol>\\n<li>Main effects:\\n<ul>\\n<li>is there an effect of col1 on base? </li>\\n<li>is there an effect of col2 on base?</li>\\n<li>is there an effect of col3 on base?</li>\\n</ul></li>\\n<li>Two-way interactions:\\n<ul>\\n<li>does the effect of col1 on base depend on the level of col2? </li>\\n<li>does the effect of col2 on base depend on the level of col3? </li>\\n<li>does the effect of col1 on base depend on the level of col3? </li>\\n</ul></li>\\n<li>Three-way interaction:\\n<ul>\\n<li>if you don't know how to interpret this, it should not be in the model</li>\\n</ul></li>\\n</ol>\\n\\n<p>The second through fourth tests will answer the three questions under main effects, but it is probably more appropriate to test these at the same time:</p>\\n\\n<pre><code>aov.ex2 &lt;- aov(base~col1 + col2 + col3, data = data1)\\n</code></pre>\\n\",\n",
       " '<p>Do it right, spend some money and work out the bugs. I would suggest <a href=\"http://www.infinityqs.com\" rel=\"nofollow\">Infinity QS</a> from Chantilly, VA. We use the package and it has worked well to do all the things you have described above.</p>\\n',\n",
       " \"<p>I have a data frame like this</p>\\n\\n<pre><code>User     OS\\nA        Windows\\nA        Linux\\nB        MacOS\\nC        Linux\\nC        FreeBSD\\nC        Windows\\nD        Windows  \\n</code></pre>\\n\\n<p>What I want to do is plot two types of statistics.</p>\\n\\n<ol>\\n<li>The share of users with different number of OSs. So, it would be the fraction of the users with 1 OS, with OS etc.</li>\\n<li>The share of users with only one OS and more than one OS.</li>\\n</ol>\\n\\n<p>For this I tried using <code>tapply(OS, User, unique)</code>. But don't know how to go about plotting the results.</p>\\n\\n<p>I was wondering if this is the right way and what more would I need to do to to get the plots I wanted.</p>\\n\",\n",
       " '<p>The first question is actually from geometry. If you have two lines of the form:</p>\\n\\n<p>$$y=a_1x+b_1$$\\n$$y=a_2x+b_2$$</p>\\n\\n<p>then they are parallel if $a_1=a_2$. So if the slopes are equal then then the lines are parallel. </p>\\n\\n<p>For the second question, use the fact that $\\\\\\\\tan \\\\\\\\alpha=a_1$, where $\\\\\\\\alpha$ is the angle the line makes with $x$-axis, and $a_1$ is the slope of the line. So </p>\\n\\n<p>$$\\\\\\\\alpha=\\\\\\\\arctan a_1$$</p>\\n\\n<p>and to convert to degrees, recall that $2\\\\\\\\pi=360$. So the answer in the degrees will be</p>\\n\\n<p>$$\\\\\\\\alpha=\\\\\\\\arctan a_1\\\\\\\\cdot \\\\\\\\frac{360}{2\\\\\\\\pi}.$$</p>\\n\\n<p>The R function for $\\\\\\\\arctan$ is called <code>atan</code>. </p>\\n\\n<p>Sample R code:</p>\\n\\n<pre><code>&gt; x&lt;-rnorm(100)\\n&gt; y&lt;-x+1+rnorm(100)/2\\n&gt; mod&lt;-lm(y~x)\\n&gt; mod$coef\\n    (Intercept)           x \\n      0.9416175   0.9850303 \\n    &gt; mod$coef[2]\\n        x \\n0.9850303 \\n&gt; atan(mod$coef[2])*360/2/pi\\n       x \\n44.56792 \\n</code></pre>\\n\\n<p>The last line is the degrees.</p>\\n\\n<p><strong>Update.</strong> For the negative slope values conversion to degrees should follow different rule. Note that the angle with the x-axis can get values from 0 to 180, since we assume that the angle is above the x-axis. So for negative values of $a_1$, the formula is:</p>\\n\\n<p>$$\\\\\\\\alpha=180-\\\\\\\\arctan a_1\\\\\\\\cdot \\\\\\\\frac{360}{2\\\\\\\\pi}.$$</p>\\n\\n<p><strong>Note.</strong> While it was fun for me to recall high-school trigonometry, the really useful answer is the one given by Gavin Simpson. Since the slopes of regression lines are random variables, to compare them statistical hypothesis framework should be used.</p>\\n',\n",
       " '<p>What methods are there for measuring the strength of arbitrary, highly non-linear relationships between two paired variables? By highly non-linear, I mean relationships that can\\'t sensibly or reliably be modelled by regression to a known model. I\\'m particularly interested in time-series, but I imagine any thing that works for bi-variate data would work here (if we treat the two time-series as a set of pair data points)</p>\\n\\n<p>Two that I am aware of are Mean Square Difference (ie. <a href=\"http://en.wikipedia.org/wiki/Mean_square_error\" rel=\"nofollow\">mean square error</a>, treating one time-series as the \"expected\" value, and one as the observed), as and <a href=\"https://en.wikipedia.org/wiki/Distance_correlation\" rel=\"nofollow\">Distance Covariance</a>. What others are there?</p>\\n\\n<p><strong>Clarification:</strong> I\\'m basically asking about dependence between series, where linear correlation or simple non-linear correlation (after log, exp, trig, other simple analytic transformations) doesn\\'t really mean much.</p>\\n',\n",
       " '<p>I think you need additional assumptions to draw this conclusion.</p>\\n\\n<p>Imagine this: the cars on the road are originally unpainted, but just before you observe them a demon decides what color they will become: red or some other color (of which there could be many).  The demon does not have a clock (to ensure that whatever he does is independent of time), but he does have a fair coin.  Ordinarily he flips the coin to determine whether the car will be red or some other color.  However, his can of red paint can color ten cars and needs to be used as quickly as possible.  So, if the coin decides red, the demon just goes ahead and paints the next ten cars red.  Once the paint is exhausted, he goes back to flipping the coin.  Other colors are painted in the same fashion.</p>\\n\\n<p>Obviously the car count per unit time is unaffected by this procedure, but I hope it\\'s just as obvious that the count of red cars per unit time is not a Poisson process, because it will be overdispersed: there will be too many high counts, due to the temporal clumping of red cars, and too many low counts, due to the temporal clumping of non-red cars.</p>\\n\\n<p>It could be that this scenario violates your sense of \"the chance of red being independent of the color of the other cars,\" but it\\'s difficult to know exactly what this statement means, since it\\'s open to various interpretations.  In the present case, the probability that the next car to be observed is red, given that the previous car was non-red (\"the other\" colors), is independent of the color of the previous car.</p>\\n',\n",
       " '<p>Well, @Srikant already gave you the right answer since the rotation (or loadings) matrix contains eigenvectors arranged column-wise, so that you just have to multiply (using <code>%*%</code>) your vector or matrix of new data with e.g. <code>prcomp(X)$rotation</code>. Be careful, however, with any extra centering or scaling parameters that were applied when computing PCA EVs.</p>\\n\\n<p>In R, you may also find useful the <code>predict()</code> function, see <code>?predict.prcomp</code>. BTW, you can check how projection of new data is implemented by simply entering:</p>\\n\\n<pre><code>getS3method(\"predict\", \"prcomp\")\\n</code></pre>\\n',\n",
       " '<p><a href=\"http://snap.stanford.edu/data/\" rel=\"nofollow\">Stanford Large Network Dataset Collection</a>. Which is part of the Stanford Network Analysis Project, SNAP.</p>\\n',\n",
       " 'Structured data files in any format, collected together with the documentation that explains their production or use.',\n",
       " '<p>I\\'m currently having a similar problem. I also believe that the approach to calculate the total effect of w is correct. I believe this can be tested via</p>\\n\\n<p>h0: b2 + b3 * mean(x) = 0;\\nha: b2 + b3 * mean(x) != 0</p>\\n\\n<p>However, I stumbled upon a paper by Ai/Norton, who claim that \"the magnitude of the interaction effet in nonlinear models does not equal the marginal effect of the interaction term, can be of opposite sign, and its statistical significance is not calculated by standard software.\" (2003, p. 123)</p>\\n\\n<p>So perhaps you should try to apply their formulas. (And if you understand how to do that, please tell me.)</p>\\n\\n<p>PS. This seems to resemble the chow-test for logistic regressions. Alfred DeMaris (2004, p. 283) describes a test for this. </p>\\n\\n<p>References:</p>\\n\\n<p>Ai, Chunrong / Norton, Edward (2003): Interaction terms in logit and probit models, Economic Letters 80, p. 123–129</p>\\n\\n<p>DeMaris, Alfred (2004): Regression with social data: modeling continuous and limited response variables. John Wiley &amp; Sons, Inc., Hoboken NJ</p>\\n',\n",
       " '<p>The common way of evaluating a similarity measure seems to be by using the similarity for a particular task, e.g. information retrieval or kNN classification, and then computing precision$@k$ or the area under the roc curve (ROC AUC).</p>\\n\\n<p>There is also some literature on <em>calibrating</em> such scores. One should however note that a well calibrated score is not necessarily better; for example a weather prediction that constantly gives the year average as chance of rain is well calibrated, will actually be correct quite often, but is not very useful on a particular day (only in the year average).</p>\\n\\n<p>In my opinion, there is nothing wrong with evaluating a similarity/distance measure for a single, particular task. There are tons of examples for such measures that - just like feature extraction - work well for one task, but not for another. Dynamic time warping distance, for example, just fails badly when you look at sine curves of different frequencies.</p>\\n\\n<p>Have a look at for example:</p>\\n\\n<ul>\\n<li><p>Houle, Kriegel, Kröger, Schubert, Zimek<br>\"Can Shared-Neighbor Distances Defeat the Curse of Dimensionality?\" Scientific and Statistical Database Management SSDBM 2010.</p></li>\\n<li><p>Bernecker, Houle, Kriegel, Kröger, Renz, Schubert, Zimek<br>\"Quality of Similarity Rankings in Time Series\". Advances in Spatial and Temporal Databases SSTD 2011.</p></li>\\n</ul>\\n\\n<p>This pair of publications seem to do a similar task as you are doing: evaluating whether or not shared-neighbor similarities improve over existing distance measures. IIRC correctly, they also look at the <em>contrast</em> provided by the distance measures, but I\\'m not sure if they measured this contrast numerically.</p>\\n\\n<p>If you cannot select the treshold yet, a classic ROC curve might do the trick for you. The area under the ROC curve gives you a good estimation of how well a treshold can work. There is a probabilistic interpretation of ROC curves that is as follows: taken two examples, one positive and one negative: the AUC value is the likelyhood that the two examples will be ranked in the correct order. An AUC of 100% means that they are always right; a value of 0% is they are always wrong, and 50% is the outcome of a randomized ranking. (I do however not have a reference or proof for this interpretation, and it might be numerically off. I do remember having read this somewhere, though, that the ROC AUC equals this statistic)</p>\\n\\n<p>On the other hand, the ROC AUC does <em>not</em> take the actual values into account, only the ranking. So it does not tell you whether or not it is easy to select the threshold. The publications by Houle et al. above had some pretty nice histogram plots for this, where you could visually tell that for the SNN distances they could just choose 0.5 as threshold, while for other distances there was just no working threshold at all (and the distances were numerically essentially unbounded, while SNN was 0 to 1)</p>\\n',\n",
       " '<blockquote>\\n  <p>Statisticians, like artists, have the bad habit of falling in love with their models.</p>\\n</blockquote>\\n\\n<p>-- George Box</p>\\n',\n",
       " '<p>In the spirit of <a href=\"http://stats.stackexchange.com/questions/114/what-statistical-blogs-would-you-recommend\">What statistical blogs would you recommend?</a> I propose a collection of machine learning blogs worth reading.</p>\\n',\n",
       " '<p>I believe it follows from <a href=\"http://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality#Probability_measure\" rel=\"nofollow\">Hölder\\'s inequality</a>:</p>\\n',\n",
       " '<p>Intuitive explanation is such: multiplying by constant does not change information content of X and Y, so also their mutual information -- and thus it is invariant to scaling. Still Srikant gave you a strict proof of this fact.</p>\\n',\n",
       " '<p>Be wary of procedures that can be referred to as \"Regression Without Ethics\" or procedures that believe the data rather than challenging it for consistenct/regularity . Outliers/level Shifts.SEasonal Pulses/Pulses should not be treated as regular observations but need to be isolated.  Transfer Function Identification will suggest the appropriate lag structure.</p>\\n',\n",
       " '<p>Increasing the training data always adds information and should improve the fit.  The difficulty comes if you then evaluate the performance of the classifier only on the training data that was used for the fit.  This produces optimistically biased assessments and is the reason why leave-one-out cross validation or bootstrap are used instead.</p>\\n',\n",
       " '<p>Suppose I am in the business of building mathematical models for clients and a client gives me the outputs from his \"random number generator\" that has generated a million random numbers $x_1$, $x_2, \\\\\\\\ldots, x_{10^6}$\\nall between $0$ and $1$, and all different.  He would like to have a model \\nfor the output of this random number generator as a random variable $X$.</p>\\n\\n<p><strong>First attempt:</strong> $X$ is a discrete random variable taking on values $x_1$, $x_2, \\\\\\\\ldots, x_{10^6}$ with equal probability.  That is indeed very satisfactory.  It matches the data perfectly, the client is happy, and so am I as I pocket my fee.</p>\\n\\n<p>But the next day, the client is back because he has run his random number\\ngenerator some more and <em>none</em> of the new outputs $x_{10^6+1}$, \\n$x_{10^6+2}, \\\\\\\\ldots$, match what the model is predicting, viz., the\\noutput will be one of $x_1$, $x_2, \\\\\\\\ldots, x_{10^6}$ (with equal probability).\\nSo, now I have to think a bit.</p>\\n\\n<p><strong>Second attempt:</strong> I look at the histogram of the numbers generated\\nby the random number generator and it looks pretty flat all the way across\\n$(0,1)$.  It looks like $X$ can take on <em>any</em> real number value between\\n$0$ and $1$ and so I model $X$ as a <em>continuous</em> random variable.\\nBut what value should I assign to, say, $P\\\\\\\\{X = 0.2173333605\\\\\\\\}$? It\\nappears just once in the list and thus has relative frequency $10^{-6}$\\non the first million outcomes but looking at $x_{10^6+1}$, \\n$x_{10^6+2}, \\\\\\\\ldots$, I see that the relative frequency is decaying away \\ntowards $0$.  I look at $2\\\\\\\\times 10^6$ outputs and the relative frequency\\nof the value  $0.2173333605$ has halved, while the relative frequency of\\n$0.92387634504$ is stuck at $0$ because it is not among the first two\\nmillion trials.</p>\\n\\n<p>So since probabilities in the model should be similar to observed \\nrelative frequencies in real life, I say $P\\\\\\\\{X = x\\\\\\\\} = 0$ for \\nevery real  number $x$, $0 &lt; x &lt; 1$.  Wait a minute!  Where did\\nthe probability disappear?  Well, the probability is hiding in the\\n<em>intervals</em>.  Roughly $500,000$ of $x_1, \\\\\\\\ldots, x_{10^6}$ are in\\nthe <em>interval</em> $(0, 0.5)$ while roughly $1,000,000$ of\\n$x_1, \\\\\\\\ldots, x_{2\\\\\\\\times 10^6}$ are in the interval $(0, 0.5)$.\\nIn other words, the relative frequencies of the <em>intervals</em> are\\npretty stable but the relative frequency of a <em>specific number</em>\\nis either $0$ or decaying away towards $0$.  Similar remarks\\napply to other intervals.  So the model for\\nthis continuous random variable $X$ is that </p>\\n\\n<ul>\\n<li>For any $a, b$ such that $0 \\\\\\\\leq a &lt; b \\\\\\\\leq 1$, $P\\\\\\\\{a &lt; X &lt; b\\\\\\\\} = b-a$.</li>\\n<li>For any $a, b$ such that $a \\\\\\\\leq 0 &lt; b \\\\\\\\leq 1$, $P\\\\\\\\{a &lt; X &lt; b\\\\\\\\} = b$.</li>\\n<li>For any $a, b$ such that $0 \\\\\\\\leq a &lt; 1 \\\\\\\\leq b$, $P\\\\\\\\{a &lt; X &lt; b\\\\\\\\} = 1-a$.</li>\\n<li>For any $a, b$ such that $a \\\\\\\\leq 0 &lt; 1 \\\\\\\\leq b$, $P\\\\\\\\{a &lt; X &lt; b\\\\\\\\} = 1$.</li>\\n</ul>\\n\\n<p>More succinctly, for $a &lt; b$, $P\\\\\\\\{a &lt; X &lt; b\\\\\\\\} = \\\\\\\\min\\\\\\\\{b, 1\\\\\\\\} - \\\\\\\\max\\\\\\\\{a, 0\\\\\\\\}$,\\nand $X$ is said to be <em>uniformly distributed</em> on $(0, 1)$.  Its \\n<em>probability density function</em> is \\n$$f_X(x) = \\\\\\\\begin{cases} 1, &amp; 0 &lt; x &lt; 1,\\\\\\\\\\\\\\\\0, &amp; \\\\\\\\text{otherwise}.\\\\\\\\end{cases}$$ \\nNote that the function is nonnegative and the area under the curve is $1$.  Also\\n$$P\\\\\\\\{a &lt; X &lt; b\\\\\\\\} = \\\\\\\\int_a^b f_X(x) dx.$$\\nMore generally, a continuous random variable $Y$ has a probability density\\nfunction $f_Y(y)$ that is nonnegative and has area $1$, and\\n$$P\\\\\\\\{a &lt; Y &lt; b\\\\\\\\} = \\\\\\\\int_a^b f_Y(y) dy.$$</p>\\n\\n<p>As @NickSabbe and StasK have told you, for continuous random variables,\\nprobabilities are obtained via integrals not via sums.  Indeed, it is possible\\nthat $f_Y(y) &gt; 1$ for some values of $y$.  As long as\\n$$P\\\\\\\\{-\\\\\\\\infty &lt; Y &lt; \\\\\\\\infty\\\\\\\\} = \\\\\\\\int_{-\\\\\\\\infty}^{\\\\\\\\infty} f_Y(y) dy = 1$$\\nit is perfectly OK to have $f_Y(y) &gt; 1$ for some values of $y$.  The\\n<em>density</em> of the probability can exceed $1$, but the <em>total</em> probability\\nis $1$.  Note that $f_Y(y) &gt; 1$ will definitely break the sums that\\nyou want to use. </p>\\n',\n",
       " \"<p>What's the similarities and differences beetween this 3 methods: bagging, boosting, stacking?</p>\\n\\n<p>Which is the best one? And why?</p>\\n\\n<p>Can you give me an example for each?</p>\\n\",\n",
       " '<p>In text classification problems where the number of features >> number of documents, is it useful to perform feature selection with filters (e.g. Information Gain) when using Naive Bayes. However, what about SVM and Maximum Entropy classifiers ?</p>\\n',\n",
       " '<blockquote>\\n  <p><strong>Possible Duplicate:</strong><br>\\n  <a href=\"http://stats.stackexchange.com/questions/39134/not-sure-how-to-work-out-ssg-and-sse-on-anova-table\">Not sure how to work out SSG and SSE on ANOVA table</a>  </p>\\n</blockquote>\\n\\n\\n\\n<p>I have a table that is:</p>\\n\\n<p>$$\\n\\\\\\\\begin{matrix} &amp; \\\\\\\\mathrm{Tyre\\\\\\\\, A} &amp; \\\\\\\\mathrm{Tyre\\\\\\\\, B} &amp; \\\\\\\\mathrm{Tyre\\\\\\\\, C} &amp; \\\\\\\\mathrm{Total} \\\\\\\\\\\\\\\\ \\\\\\\\mathrm{number\\\\\\\\, of\\\\\\\\, tyres\\\\\\\\, tested} &amp; 25 &amp; 32 &amp; 40 &amp; 97 \\\\\\\\\\\\\\\\ \\\\\\\\mathrm{mean\\\\\\\\, response*} &amp; 18.46 &amp; 19.56 &amp; 16.47 &amp; 18.00 \\\\\\\\\\\\\\\\ \\\\\\\\mathrm{sample\\\\\\\\, variance**} &amp; 14.42 &amp; 13.66 &amp; 26.77 &amp; 20.73 \\\\\\\\end{matrix} \\n$$</p>\\n\\n<p>*mean was calculated as $bar{y} = \\\\\\\\frac{1}{n} \\\\\\\\sum_{i=1}^{n}y_i$.\\n**variance was calculated as $ \\\\\\\\frac{1}{n-1} \\\\\\\\sum_{i = 1}^{n} (y_i - \\\\\\\\bar{y})^2 $</p>\\n\\n<p>Using this information, I need to create an ANOVA table. The degrees of freedom were easy (I got 2, 94 and 96). But I am now stuck on the SS columns of the table. For SST I did:</p>\\n\\n<p>$(N - 1)S_T^2 $ and used 20.73 as the total variance and this gave me SST = 1990.08. Is that correct?</p>\\n\\n<p>Also, I am stuck on how to work out SSG or SSE. I know if I can work one out, then I\\'ll have the other but how do I do this?</p>\\n',\n",
       " '<p>I wanted to focus on volatility forecasting, so instead of asking R to compute a GARCH where it would compute the errors on the returns, I wanted to model the volatility as an ARMA and add an external regressor using the argument xreg in the arima function.</p>\\n\\n<p>I have two questions:</p>\\n\\n<ul>\\n<li><p>Is it exactly equivalent to compute an ARMA(p,q) on the volatility with external regressors as the squared returns and to compute a GARCH (for the volatility forecast)</p></li>\\n<li><p>Is it the correct way to do it in R ?</p></li>\\n</ul>\\n\\n<p>Tony</p>\\n',\n",
       " '<p>This is the fundamental point of weighting a sample to population.  You weight each individual in your sample based on known demographic features of the population such that the weights of each demographic group in the sample add up to population totals.</p>\\n\\n<p>See any book on sampling theory and practice - no, it\\'s not reject inference.</p>\\n\\n<p>I recommend Thomas Lumley\\'s <a href=\"http://faculty.washington.edu/tlumley/survey/\" rel=\"nofollow\">survey package in R</a> and the accompanying book on complex surveys.  Even if you don\\'t use R it is a great, clear introduction.</p>\\n',\n",
       " '<p>In my experience, the rationale for using \\'panel econometrics\\' is that the panel \\'fixed effects\\' estimators can be used to control for various forms of omitted variable bias. </p>\\n\\n<p>However, it is possible to perform this type of estimation within a multilevel model using a <a href=\"http://www.jstor.org/discover/10.2307/1913646?uid=3738232&amp;uid=2129&amp;uid=2&amp;uid=70&amp;uid=4&amp;sid=21100993975113\" rel=\"nofollow\">Mundlak</a> type approach, i.e. including the group means as extra regressors. This approach removes the correlation between the error term and potential group level omitted factors, revealing the \\'within\\' coefficient. However, for a reason unknown to me this is not typically done in applied research. These <a href=\"http://www.stats.ox.ac.uk/~snijders/MLB_new_S.pdf\" rel=\"nofollow\">slides</a> and this <a href=\"http://polmeth.wustl.edu/media/Paper/FixedversusRandom_1_2.pdf\" rel=\"nofollow\">document</a> provide an elaboration.</p>\\n',\n",
       " '<p>The example in the package reference on pages 5-6 is pretty straightforward. If your original data frame is named <code>GPPANDDRIVER</code>, one could make new dataframes to pass to <code>hier.part</code> as follows;</p>\\n\\n<pre><code>GPP &lt;- GPPANDDRIVER$GPP\\nIND_VARS &lt;- subset(GPPANDDRIVER, select = -GPP)\\n</code></pre>\\n\\n<p>The making of the <code>IND_VARS</code> can be done many ways, see <a href=\"http://stackoverflow.com/q/4605206/604456\">the answers to this question</a> on stackoverflow for a few examples of how it can be done. For instance, if you do not want all of the variables minus GPP in the <code>IND_VARS</code> data frame you can specify exactly what columns you do want.</p>\\n\\n<p>Then to be clear in what we are doing, lets be explicit in assigning our arguments to specific objects in the call to <code>hier.part</code>.</p>\\n\\n<pre><code>hier.part(y = GPP, xcan = IND_VARS, family = \"gaussian\", gof = \"RMSPE\", barplot = TRUE)\\n</code></pre>\\n\\n<p>If that isn\\'t clear (or doesn\\'t work) please provide an entirely reproducible example.</p>\\n',\n",
       " '<p>You can always <em>force</em> the $\\\\\\\\beta$s to be between 0 and 1 by rewriting the likelihood in terms of $\\\\\\\\beta_j = f(\\\\\\\\beta^*)$ for some function $f$, e.g. $1/(1+\\\\\\\\exp(-\\\\\\\\beta^*))$. Then optimise the $\\\\\\\\beta^*$s but report the $\\\\\\\\beta$s.  However that won\\'t automatically get you an interpretation in terms of the probability that $\\\\\\\\beta$ is relevant to the regression.\\nSo I think the first thing to do is  <em>separate</em>, conceptually speaking, the question of the probability that each $\\\\\\\\beta_j$ is equal to zero i.e. not included/relevant from the value it should take if it is relevant.</p>\\n\\n<p>As for models, the keywords are probably <em>Bayesian</em> and  <em>automatic relevance determination</em>.  Assume a Bayesian model with a hierarchical prior such that $p(\\\\\\\\beta \\\\\\\\mid \\\\\\\\alpha) = \\\\\\\\prod_j N(0, \\\\\\\\alpha_j^{-1})$ and maybe a gamma prior on the $\\\\\\\\alpha$s if you\\'re going to sample.  As $\\\\\\\\alpha_j$ goes to infinity, $\\\\\\\\beta_j$ is more probably equal to zero.\\nThen integrate out (or optimise) the (ir)relevance parameters $\\\\\\\\alpha$ during posterior inference to get predictions for $y$ and separately examine the $\\\\\\\\alpha$s to give you an idea which $\\\\\\\\beta$s are irrelevant.  For more details on these methods, the references I have to hand are <a href=\"http://www.inference.phy.cam.ac.uk/mackay/pred.pdf\" rel=\"nofollow\">MacKay (ms)</a> or <a href=\"http://www.miketipping.com/papers.htm\" rel=\"nofollow\">Tipping\\'s work</a>, and a tech report by <a href=\"http://affect.media.mit.edu/pdfs/04.qi-minka-picard-ghahramani.pdf\" rel=\"nofollow\">Minka et al.</a> discussing fitting stategies.  </p>\\n\\n<p>I\\'ve no idea what is your $m$ parameter(s), so I may have missed something to do with that.</p>\\n',\n",
       " '<p>If a variable $X$ follows a gamma distribution with shape parameter $k &gt; 0$ and scale parameter $\\\\\\\\theta$, then it has probability density function </p>\\n\\n<p>$$ p(x) = \\\\\\\\frac{1}{\\\\\\\\theta^k}\\\\\\\\frac{1}{\\\\\\\\Gamma(k)}x^{k-1}e^{-\\\\\\\\frac{x}{\\\\\\\\theta}} $$ </p>\\n\\n<p>It follows that $E(X) = k \\\\\\\\theta$ and ${\\\\\\\\rm var}(X) = k \\\\\\\\theta^2$. In some texts, the gamma distribution is parameterized instead by the <em>rate</em> parameter, which is the reciprocal of the scale parameter: $\\\\\\\\beta = 1/\\\\\\\\theta$. </p>\\n\\n<p>The <a href=\"http://en.wikipedia.org/wiki/Exponential_distribution\" rel=\"nofollow\">exponential distribution</a> and the <a href=\"http://en.wikipedia.org/wiki/Chi-squared_distribution\" rel=\"nofollow\">$\\\\\\\\chi^2$ distribution</a> are both special cases of the gamma distribution. </p>\\n',\n",
       " '<p>I think that dynamic pricing algorithms (used in aviation and ticketing industry) is very statistical based, anyone here has experience with those algorithms with references for it?</p>\\n',\n",
       " '<p>I ran a simple psychology experiment that included 4 conditions, each containing 8 blocks of training. There were different participants in each condition. Hence, condition and block and subject and block are crossed, but subject is nested in condition. I\\'m trying to do a basic repeated measures anova to test for effects of block and condition. I have an unbalanced design, and a mixed effects model. My approach was to use the lme4 package in conjunction with the car package. I am running R version 2.14 on mac os x lion.</p>\\n\\n<p>Here is what I\\'ve done:</p>\\n\\n<pre><code>library(nlme)\\nlibrary(car)\\n\\nrm( list = ls() )\\n\\ndata &lt;- read.table(\"anova_data\", header = TRUE)\\n\\ncondition &lt;- factor(data$condition)\\nblock &lt;- factor(data$block)\\nsubject &lt;- factor(data$subject)\\naccuracy &lt;- data$accuracy\\n\\nfm1 &lt;- lmer( accuracy ~ block*condition + (1|subject %in% condition) )\\n\\nAnova(fm1)\\n</code></pre>\\n\\n<p>My problem is that this returns a summary table without F values, like such:</p>\\n\\n<pre><code>Analysis of Deviance Table (Type II tests)\\n\\nResponse: accuracy\\n\\n                 Chisq Df Pr(&gt;Chisq)\\n     block           17.169  7    0.01634 *  \\n\\ncondition       68.294  3  9.897e-15 ***\\n\\nblock:condition 26.481 21    0.18869\\n</code></pre>\\n\\n<p>Any help is greatly appreciated.</p>\\n',\n",
       " '<p>I\\'d have two two-panel plots, both have the xy plot on the left, and a histogram on the right. In the first plot, a horizontal line is placed at the mean of y and lines extend from this to each point, representing the residuals of y values from the mean. The histogram with this simply plots these residuals. Then in the next pair, the xy plot contains a line representing the linear fit and again vertical lines representing the residuals, which are represented in a histogram to the right. Keep x axis of the histograms constant to highlight the shift to lower values in the linear fit relative to the mean \"fit\".</p>\\n',\n",
       " '<p>This is an ugly, ugly solution, but if you\\'re locked into e1071, you can pass the \\'raw\\' option to predict, which will give you the probabilities of each class. </p>\\n\\n<p>You could then \"correct\" these by dividing out the \"empirical\" prior (i.e. the one from your sample) and multiplying them by the true population prior.</p>\\n',\n",
       " '<p>I agree with the answers that have appeared, but would like to add that perhaps the question could be redirected.  Whether to test a hypothesis or not is a research question that ought, at least in general, be independent of how much data one has.  If you really need to test a hypothesis, do so, and don\\'t be afraid of your ability to detect small effects.  But first ask whether that\\'s part of your research objectives.</p>\\n\\n<p>Now for some quibbles:</p>\\n\\n<ul>\\n<li><p>Some null hypotheses are absolutely true by construction.  When you\\'re testing a pseudorandom number generator for equidistribution, for instance, and that PRG is truly equidistributed (which would be a mathematical theorem), then the null holds.  Probably most of you can think of more interesting real-world examples arising from randomization in experiments where the treatment really does have no effect.  (I would hold out the entire literature on esp as an example. ;-)</p></li>\\n<li><p>In a situation where a \"simple\" null is tested against a \"compound\" alternative, as in classic t-tests or z-tests, it typically takes a sample size proportional to $1/\\\\\\\\epsilon^2$ to detect an effect size of $\\\\\\\\epsilon$.  There\\'s a practical upper bound to this in any study, implying there\\'s a practical lower bound on a detectable effect size.  So, as a theoretical matter der Laan and Rose are correct, but we should take care in applying their conclusion.</p></li>\\n</ul>\\n',\n",
       " \"<p>I have 60 apps.  For each app, I look at 10 review of that app, and count how many of the reviews say the app is unreliable.  Let $X_i$ denote the number of such reviews, for the $i$th app.  I want to check whether the reviews seem to be pretty consistent for each app.  For instance, if one review says the app is unreliable, does this make it more likely that the other reviews for that app also mention unreliability?</p>\\n\\n<p>So, one model for the situation is that the reviews don't reflect anything about the apps: there is a parameter $p$, and each of the 600 reviews is an iid Bernoulli($p$) random variable.  In this model, $X_i \\\\\\\\sim \\\\\\\\text{Binomial}(10, p)$ for each $i$.  The value of $p$ is not known a priori.</p>\\n\\n<p>An alternative model is that each app has some inherent degree of reliability, modeled by a parameter $p_i$ for the $i$th app, and each of the 10 reviews of the $i$th app is an iid Bernoulli($p_i$) random variable.  In this model, $X_i \\\\\\\\sim \\\\\\\\text{Binomial}(10, p_i)$ for each $i$.  The value of the $p_i$'s is not known a priori.</p>\\n\\n<ol>\\n<li><p>If I observe the values of $X_1,\\\\\\\\dots,X_{60}$, is there any way I can determine which model seems to better reflect the data?  Is there a good way to construct a hypothesis test to compute a $p$-value for the null hypothesis, that $X_i \\\\\\\\sim \\\\\\\\text{Binomial}(10, p)$ for all $i$?</p></li>\\n<li><p>Let's say that I reject the null hypothesis.  Can you suggest a meaningful way to measure the degree of internal consistency across reviews of the same app?  What would be a good measure of this sort of consistency/correlation?  Ideally, something that is amenable to a relatively natural interpretation?</p>\\n\\n<p>(For instance, maybe I compute the sample standard deviation $\\\\\\\\hat\\\\\\\\sigma$ of $X_1,\\\\\\\\dots,X_{60}$; I compute the expected standard deviation $\\\\\\\\sigma_0$ under the null hypothesis, $\\\\\\\\sigma_0 = \\\\\\\\sqrt{10 \\\\\\\\hat{p} (1 - \\\\\\\\hat{p})}$, where $\\\\\\\\hat{p} = (X_1+\\\\\\\\dots+X_{60})/600$; and I look at the ratio $\\\\\\\\hat{\\\\\\\\sigma}/\\\\\\\\sigma_0$?)</p></li>\\n</ol>\\n\",\n",
       " '<p>Have you considered this paper \"Subsampling methods for genomic inference\" ( <a href=\"http://arxiv.org/pdf/1101.0947.pdf\" rel=\"nofollow\">http://arxiv.org/pdf/1101.0947.pdf</a> ) from the ENCODE project.  They have associated software.  There are a few approaches such as Cooccur package for R.  A more complete discussion can be found here: <a href=\"http://www.biostars.org/p/5484/\" rel=\"nofollow\">http://www.biostars.org/p/5484/</a></p>\\n\\n<p>EDIT:  I got a message saying my answer was a little too short, so I\\'m elaborating on why I provided it. The reason I\\'m mentioning this solution is that overlap of genomic features is a well-researched topic in bioinformatics with a lot of associated literature.  There are lots of aspects of genomes that might not be anticipated by an approach from first principles.  ENCODE is a large research project for aggregating genomic data in humans, and the method I mentioned (which was developed within the ENCODE project) is thus quite widely used.</p>\\n\\n<p>I have actually spent a lot of my time over the last few weeks reading and rereading this \\'Subsampling\\' paper.  It\\'s very involved. They model the genome as a \"piecewise stationary ergodic random process\".</p>\\n',\n",
       " '<p>For a gentle introduction, I like examples using 2x2 contingency tables.  The diagnostic testing example as mentioned above, where the Probability of a positive test result given disease is not equal to the Probability of disease given a positive test result.  Also, one can use designs with different sampling schemes, such as the cohort study vs. the case-control study, to illustrate how that affects what probabilities can be estimated.</p>\\n',\n",
       " \"<p>Here is the procedure to be followed in order to compute Pearson's Chi-squared statistic.</p>\\n\\n<hr>\\n\\n<p>Assume this is your <em>observed</em> 2x2 contingency table:</p>\\n\\n<p>$$\\\\\\\\begin{array}{c|cc|c}\\\\\\\\n\\\\\\\\hline\\\\\\\\n X/Y               &amp; \\\\\\\\textrm{cat 1}  &amp; \\\\\\\\textrm{cat 2}  &amp; \\\\\\\\textrm{tot}      \\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\hline\\\\\\\\n\\\\\\\\textrm{cat 1}  &amp; O_{11}          &amp; O_{12}          &amp; O_{1.}\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\textrm{cat 2}  &amp; O_{21}          &amp; O_{22}          &amp; O_{2.}\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\hline\\\\\\\\n\\\\\\\\textrm{tot}    &amp; O_{.1}          &amp; O_{.2}          &amp; O_{..}\\\\\\\\n\\\\\\\\end{array}$$</p>\\n\\n<ul>\\n<li>there are $O_{11}$ observations that fall within the first category of $X$ $\\\\\\\\underline{\\\\\\\\textrm{and}}$ within the first category of $Y$;</li>\\n<li><p>There are $0_{2.}$ observations that fall within the second category of $X$;</p></li>\\n<li><p>the total sample size is $O_{..}$.</p></li>\\n</ul>\\n\\n<p><strong>1st step</strong>: Compute the <em>expected</em> contingency table</p>\\n\\n<p>$$\\\\\\\\begin{array}{cc}\\\\\\\\n\\\\\\\\hline\\\\\\\\nE_{11} &amp; E_{12} \\\\\\\\\\\\\\\\\\\\\\\\nE_{21} &amp; E_{22}\\\\\\\\n\\\\\\\\end{array}$$</p>\\n\\n<p>where </p>\\n\\n<p>$$E_{ij} = \\\\\\\\frac{O_{i.} O_{.j}}{O_{..}} \\\\\\\\cdot$$</p>\\n\\n<p><strong>2nd step</strong> Compute Pearson's statistic:</p>\\n\\n<p>$$\\\\\\\\chi^2 = \\\\\\\\sum_{ij} \\\\\\\\frac{(O_{ij} - E_{ij})^2}{E_{ij}} \\\\\\\\cdot$$</p>\\n\",\n",
       " '<p>Srikant asks for a \"more elegant approach.\"  Perhaps the following will respond to this challenge.</p>\\n\\n<p>Let the argument of the exponential be $f(x)$ (so its power series coefficients are the gammas) and let the right hand side be $g(x)$ (with deltas as its coefficients), so that by definition</p>\\n\\n<p>$$g(x) = \\\\\\\\exp(f(x)).$$</p>\\n\\n<p>Differentiating both sides and replacing $\\\\\\\\exp(f)$ with $g$ yields</p>\\n\\n<p>$$g\\' = f\\' * g.$$</p>\\n\\n<p>Writing this out as power series gives the desired result: the delta comes from $g\\'$ while the convolution of the gammas and deltas comes from $f\\' * g$.</p>\\n\\n<p>You don\\'t have to worry about convergence (and the whole machinery of Taylor series), by the way: all these calculations can be performed in the ring of formal power series.</p>\\n',\n",
       " \"<p>I have to run repeated correlations. If one of them reach the significant value (as given in the Pearson table), how can I be sure that it isn't actually a false positive?</p>\\n\\n<p>I'm pretty new in the statistical analysis, so I apologize if my question seems naive.</p>\\n\\n<p>Thanks for any answers.</p>\\n\",\n",
       " \"<p>I'm pretty sure you are using wrong tools here.</p>\\n\\n<p>ML methods are created for interpolation (like predicting time series A from time series B and C); for extrapolations we have Markov chains and friends.</p>\\n\\n<p>The problem with your approach is that it is terribly easy to overfit the model in this conditions and, what's worse, it is hard to spot this (normal cross-validation will fail, so it is very hard to fit parameters the proper way, etc.).<br>\\nAdding explicit time to predictors is also a bad idea -- I have seen models fitted <em>only</em> on time and decision with 90% accuracy on cross-validation and random guessing on post-training-data tests. If you need time, it is better to include it as a series of cycle descriptors like day of week or seconds past midnight, obviously never exceeding or even going near the length of your training series.</p>\\n\",\n",
       " '<p>I haven\\'t looked into it much, but this article on <a href=\"http://www.thebroth.com/blog/118/bayesian-rating\" rel=\"nofollow\">Bayesian rating systems</a> looks interesting.</p>\\n',\n",
       " '<p>I have had enough courses on statistics during my school years and at the university. I have a fair understanding of the concepts, such as, CI, p-values, interpreting statistical significance, multiple testing, correlation, simple linear regression (with least squares) (general linear models),  and all tests of hypothesis. I had been introduced to it much of the earlier days mostly mathematically. And lately, with the help of the book <a href=\"http://rads.stackoverflow.com/amzn/click/0199730067\">Intuitive Biostatistics</a> I have grasped and unprecedented understanding towards the actual conceptual theory, I believe.</p>\\n\\n<p>Now, what I find I lack is understanding of fitting models (estimating parameters to model) and the like. In particular, concepts such as maximum likelihood estimation, <strong>generalized</strong> linear models, bayesian approaches to inferential statistics always seem foreign to me. There aren\\'t enough examples or tutorials or conceptually sound ones, as one would find on simple probabilistic models or on other (basic) topics on the internet. </p>\\n\\n<p>I am a bioinformatician and I work on RNA-Seq data which deals with raw read counts towards finding, lets say, gene expression (or differential gene expression). From my background, even if I am not familiar with statistical models, I am able to grasp the reason for a poisson distribution assumption and negative binomials and so on.. But some papers deal with generalized linear models and estimate a MLE etc.. which I believe I have the necessary background to understand. </p>\\n\\n<p>I guess what I am asking for is an approach some experts among you deem useful and (a) book(s) which helps me grasp these concepts in a more intuitive way (not just rigorous math, but theory backed with math). As I am mostly going to apply them, I would be satisfied (at the moment) with understanding what is what and later, I can go back to rigorous mathematical proofs... Does anyone have any recommendations? I don\\'t mind buying more than 1 book if the topics I asked for are indeed scattered to be covered in a book.</p>\\n\\n<p>Thank you very much!</p>\\n',\n",
       " '<p>The term \"regression\" was used by Francis Galton in his 1886 paper \"Regression towards mediocrity in hereditary stature\". To my knowledge he only used the term in the context of <a href=\"http://en.wikipedia.org/wiki/Regression_toward_the_mean\">regression toward the mean</a>. The term was then adopted by others to get more or less the meaning it has today as a general statistical method. </p>\\n',\n",
       " '<p>Not sure if this is what you\\'re looking for, but you might want to look at <a href=\"http://en.wikipedia.org/wiki/Katz%27s_back-off_model\" rel=\"nofollow\">Katz backoff</a>. This entails training vanilla <em>n</em>-gram models for $1 \\\\\\\\le n \\\\\\\\le N$, then estimating probabilities for large <em>n</em> by \"backing off\" to an (<em>n</em>-1)-gram model when the <em>n</em>-gram in question was not observed more often than some frequency threshold.</p>\\n',\n",
       " '<p>One thing which may be difficult to determine using sampling theory is $P(data|\\\\\\\\rho&gt;0.5)$.  I don\\'t know of a sampling theory based decision problem which evaluates this directly (that is not to say it is impossible, I may just be ignorant).</p>\\n\\n<p>You usually have to take a \"minimax\" approach.  That is, find the \"most favorable\" $\\\\\\\\rho_1$ within the range $\\\\\\\\rho&gt;0.5$, and do a <em>specific</em> test of $H_0:\\\\\\\\rho=0.5$ vs $H_1:\\\\\\\\rho=\\\\\\\\rho_1$.  Usually the \"most favourable\" value is the MLE of $\\\\\\\\rho$, which is the sample correlation, so you would test against $H_1:\\\\\\\\rho=r$.  I believe this is called a \"severe test\"  because if you reject $H_1$ in favor of $H_0$, then you will also reject all the other possibilities in favor of $H_0$ (because $H_1$ was the \"best\" of the alternatives).</p>\\n\\n<p>But note that this hypothesis test is all under the assumption that the model/structure is true.  If the correlation were to enter the model in a different functional form, then the test may be different.  It is very easy to think that the test is \"absolute\" because usually you don\\'t explicitly specify a class of alternatives.  But the class of alternatives is always there, whether you specify it implicitly or explicitly.  So if you were being pedantic about it, you should really include the full model specification and any additional assumptions in both $H_0$ and $H_1$.  This will help you understand exactly what the test has and has not achieved.</p>\\n',\n",
       " \"<p>Yes, that's right.  We usually talk about categorical variables as those having distinct categories as possible values, and split these into ordinal variables, where the possible categories can be ordered (like low, medium, high), and nominal variables, whose categories don't have an ordering (like plate, cup, spoon, fork).</p>\\n\",\n",
       " '<p>$X$ is the total number of events observed. See <a href=\"http://en.wikipedia.org/wiki/Poisson_distribution#Confidence_interval\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Poisson_distribution#Confidence_interval</a> .</p>\\n',\n",
       " \"<p>I'm a novice to data mining and started to read about it. What's the exact difference between experimental data and observation data? Both are obviously data; and many say observation data can lead to errors. But I guess it's not possible to do an experiment for all data sets. I'm really confused, explain me what is experimental data and observation data and say when these should be used?</p>\\n\\n<p>Thanks in advance. </p>\\n\",\n",
       " \"<p>You want to test the hypothesis that probability of a result depends on the matchup. $H_0$, then, is that every game is essentially a coin flip.</p>\\n\\n<p>A simple test for this would be calculating the proportion of times the player with more previous games played will win, and comparing that to the binomial cumulative distribution function. That should show the existence of some kind of effect.</p>\\n\\n<p>If you're interested about the quality of the Elo rating system for your game, a simple method would be to run a 10-fold crossvalidation on the predictive performance of the Elo model (which actually assumes outcomes aren't iid, but I'll ignore that) and comparing that to a coin flip.</p>\\n\",\n",
       " '<p>There is no particular reason why this should <em>not</em> happen. Multiple regression asks a different question from simple regression. In particular, multiple regression (in this case, multiple logistic regression) asks about the relationship between the dependent variables and the independent variables, controlling for the other independent variables. Simple regression asks about the relationship between a dependent variable and a (single) independent variable.</p>\\n\\n<p>If you add the context of your study (e.g. what are these variables?) it may be possible to give more specific responses. Also, given that all three variables in your case are dichotomies, you could present us with the data pretty easily....there are only 8 lines needed to summarize this:</p>\\n\\n<p>\\\\\\\\begin{table}\\nDV   IV1    IV2    Count\\nA     A      A      10\\nA     A      B      20\\n\\\\\\\\end{table}</p>\\n\\n<p>etc.      </p>\\n',\n",
       " '<p>I have written a document that is freely available at my website and on CRAN. See the linked page:</p>\\n\\n<p><a href=\"http://www.ms.unimelb.edu.au/~andrewpr/r-users/\">icebreakeR</a></p>\\n\\n<p>The datasets that are used in the document are also linked from that page.  Feedback is welcome and appreciated!</p>\\n\\n<p>Andrew</p>\\n',\n",
       " \"<p>Typically APCA gets used when there are lots of series but very few samples. I wouldn't describe APCA as better or worse than PCA, because of the equivalence you noted. They do, however, differ in when the tools are applicable. That is the insight of the paper: you can flip the dimension if it's more convenient! So in the application you mentioned, there are a lot of assets so you would need a long time series to compute a covariance matrix, but now you can use APCA. That said, I don't think APCA gets applied very often because you could try to reduce the dimensionality using other techniques (like factor analysis).</p>\\n\",\n",
       " '<p>Here\\'s what I was suggesting in R code. I don\\'t know what software you\\'re working with, but at the very least you can download R for free and run the script pretty easily just to see what I was talking about and then create your own version. If you were in R, a lot of the loops could be replaced with the \"rollapply\" function in the \"zoo\" package. But this way the code is self contained.</p>\\n\\n<p>What I\\'ve done is created three time series:\\n1. Simple signal, where the next value at point (i+1) correlates with the preceding value (i).\\n2. Signal based on the first so they should correlate highly but have different amplitudes\\n3. A random signal made with Gaussian noise</p>\\n\\n<p>What this does leave out is phase shifts, that is, if a person a starts moving and then person b starts moving after but in time with, this method will underestimate the correlation. This can be rectified by including a number of time shifts. Or, possibly by increasing the length of time for the rolling average (acts like a low pass filter).</p>\\n\\n<p>Of course there are other methods which might be more suitable, but those are based on oscillating signals, e.g., you could use wavelets for a time-frequency decomposition and then calculate a between person phase-locking across a number of frequencies. Then create phase and coherence maps. If you think this might be more what you\\'re after, I have scripts for that too, but you may want to look into dedicated packages in matlab or R.</p>\\n\\n<p>Before applying, you\\'d probably need to take a couple of random samples from your videos, or perhaps even a \"training\" video and see what parameters give you the information that you\\'re looking for. Then apply this to your actual samples. E.g., changing the length of the rolling average, playing with phase shifts, adjusting the weighting parameter. You could even get boostrapped CIs if you wanted. </p>\\n\\n<p>Here\\'s the R script:  </p>\\n\\n<pre><code>#highly correlated series\\nrl&lt;-20                  #rolling average length\\nx&lt;-5                    #Just a starting value\\nxvec&lt;-3000\\n#1st time series, made so that the next value correlates with preceding value\\nfor(i in 1:(xvec-1)) { x[i+1] &lt;- x[i] +rnorm(1, 0, 0.2) }\\ny&lt;-x+rnorm(xvec, 0, 0.3) #Second series based on 1st series for high correlation\\nxy&lt;-(x*y)/max(x*y)      #For weighting\\n\\n#Calculating rolling correlation with 20 values either side\\ncxy&lt;-sapply((rl+1):(xvec-rl+1), function(i) cor(x[(i-rl):(i+rl)], y[(i-rl):(i+rl)]))\\n#Smoothed rolling correlation by rolling average\\ncxym&lt;-sapply((rl+1):(xvec-3*rl+1), function(i) mean(cxy[(i-rl):(i+rl)]))\\n#Smoothed weighting\\nxym&lt;-sapply((2*rl+2):(xvec-2*rl+2), function(i) mean(xy[(i-rl):(i+rl)]))\\n\\npar(mfcol = c(2,2))     #Create plot so that there are 4 figures per plot space\\nplot(1:xvec, x, type=\"l\"); lines(1:xvec, y, col=2)  #plot 1st and 2nd time series\\n\\n#Plot correlations\\nplot((rl+1):(xvec-rl+1), cxy, type=\"l\", xlim=c(0, xvec), ylim=c(-1,1))\\nlines((2*rl+2):(xvec-2*rl+2), cxym, col=2)      #Smoothed rolling correlation\\nlines((2*rl+2):(xvec-2*rl+2), cxym*xym, col=3)  #Smoothed weighted correlation\\n\\n#No correlation between series and plot\\ny&lt;-rnorm(xvec, 5, 1)\\nxy&lt;-(x*y)/max(x*y)\\ncxy&lt;-sapply((rl+1):(xvec-rl+1), function(i) cor(x[(i-rl):(i+rl)], y[(i-rl):(i+rl)]))\\ncxym&lt;-sapply((rl+1):(xvec-3*rl+1), function(i) mean(cxy[(i-rl):(i+rl)]))\\nxym&lt;-sapply((2*rl+2):(xvec-2*rl+2), function(i) mean(xy[(i-rl):(i+rl)]))\\nplot(1:xvec, x, type=\"l\"); lines(1:xvec, y, col=2)\\nplot((rl+1):(xvec-rl+1), cxy, type=\"l\", xlim=c(0, xvec), ylim=c(-1,1))\\nlines((2*rl+2):(xvec-2*rl+2), cxym, col=2)\\nlines((2*rl+2):(xvec-2*rl+2), cxym*xym, col=3)\\n</code></pre>\\n',\n",
       " '<p>An alternative way to find the ML estimator is to note that the constraint $p_1=p_2$ reduces the number of relevant outcomes to two: decided and undecided. The vector of probabilities is thus $(2p_1,p_3)$. We have $$(X_1+X_2,X_3)\\\\\\\\sim\\\\\\\\mbox{Multinomial}(n,(2p_1,p_3))$$ which is equivalent to $$X_1+X_2\\\\\\\\sim\\\\\\\\mbox{Binomial}(n,2p_1).$$\\nThis transforms the problem into a simpler one. <a href=\"http://en.wikipedia.org/wiki/Maximum_likelihood#Discrete_distribution.2C_continuous_parameter_space\" rel=\"nofollow\">Deriving the MLE of $p$ in the binomial distribution</a> is perhaps <em>the</em> standard example for ML estimation in discrete distributions: denoting $2p_1=r$ one finds that $$\\\\\\\\hat{r}=\\\\\\\\frac{X_1+X_2}{n}=\\\\\\\\frac{X_1+X_2}{X_1+X_2+X_3}.$$\\nBy the <a href=\"http://en.wikipedia.org/wiki/Maximum_likelihood#Functional_invariance\" rel=\"nofollow\">functional invariance</a> of maximum likelihood estimators, $$\\\\\\\\hat{a}=\\\\\\\\hat{p}_1=\\\\\\\\hat{p}_2=\\\\\\\\frac{1}{2}\\\\\\\\hat{r}=\\\\\\\\frac{X_1+X_2}{2(X_1+X_2+X_3)}$$\\nand\\n$$\\\\\\\\hat{p}_3=1-\\\\\\\\hat{r}=\\\\\\\\frac{X_3}{X_1+X_2+X_3}.$$</p>\\n',\n",
       " '<p><strong>The Problem:</strong></p>\\n\\n<p>Retailer Y uses (for client operation purposes) a network of about 3000 devices. The whole network has 10 different models (from distinct manufacturers), aged from 0 years old \\n(purchased in the current year) to 12 years old.</p>\\n\\n<p>Each device may suffer from malfunctions for which immediate repair is requested. The total time of unavailability for each device is accounted at the end of month and we call it downtime.</p>\\n\\n<p>The same device may have no malfunction during that month period or may have one or more unavailability periods during that month period.</p>\\n\\n<p>Common sense says that older equipment should have higher downtimes.</p>\\n\\n<p><strong>The Question:</strong></p>\\n\\n<p>(a)  How can I check whether the Age variable has effective impact over the \\nDowntime variable? Which statistical model should I use?</p>\\n\\n<p>(b)  How can I account for the fact that each type of equipment should behave\\ndifferently to aging regarding downtime?</p>\\n\\n<p>Thanks,\\nEduardo</p>\\n',\n",
       " '<p>Let $S_k$ be the wealth after $k$ plays of this game, where we assume $S_0 = 1.$ The temptation here is to take $X_k = \\\\\\\\log{S_k}$, and study $X_k$ as a symmetric random walk, with innovations of size $\\\\\\\\pm \\\\\\\\log{2}$. This, as it turns out, will be fine for the second question, but not the first. A bit of work will show that, asymptotically we have $X_k \\\\\\\\sim \\\\\\\\mathcal{N}(0,k(\\\\\\\\log{2})^2)$. From this you <strong>cannot</strong> conclude that $S_k$ is asymptotically log normally distributed with $\\\\\\\\mu = 0, \\\\\\\\sigma = \\\\\\\\log{2}\\\\\\\\sqrt{k}.$ The log operation does not commute with the limit.  If it did, you would get the expected value of $S_k$ as $\\\\\\\\exp{(k \\\\\\\\log{2}\\\\\\\\log{2}/2)}$, which is nearly correct, but not quite.</p>\\n\\n<p>However, this method is just fine for finding quantiles of $S_k$, and other questions of probability, like question (2). We have $S_k \\\\\\\\ge (\\\\\\\\frac{5}{4})^k \\\\\\\\Leftrightarrow X_k \\\\\\\\ge k \\\\\\\\log{(5/4)} \\\\\\\\Leftrightarrow X_k / \\\\\\\\sqrt{k}\\\\\\\\log{2} \\\\\\\\ge \\\\\\\\sqrt{k}\\\\\\\\log{(5/4})/\\\\\\\\log{2}.$ The quantity on the lefthand side of the last inequality is, asymptotically, a standard normal, and so the probability that $S_k$ exceeds its mean approaches $1 - \\\\\\\\Phi{(\\\\\\\\sqrt{k}\\\\\\\\log{(5/4)}/\\\\\\\\log{2})},$ where $\\\\\\\\Phi$ is the CDF of the standard normal. This approaches zero fairly quickly.</p>\\n\\n<p>Matlab code to check this:</p>\\n\\n<pre><code>top_k = 512;\\nnsamps = 8192;\\ninnovs = log(2) * cumsum(sign(randn(top_k,nsamps)),1);\\ns_k = exp(innovs);\\nk_vals = (1:top_k)\\';\\nmean_v = (5/4) .^ k_vals;\\nexceed = bsxfun(@ge,s_k,mean_v);\\nprob_g = mean(double(exceed),2);\\n\\n%theoretical value\\n%(can you believe matlab doesn\\'t come with normal cdf function!?)\\nnrmcdf = @(x)((1 + erf(x / sqrt(2)))/2);\\np_thry = 1 - nrmcdf(sqrt(k_vals) * log(5/4) / log(2));\\n\\nloglog(k_vals,prob_g,\\'b-\\',k_vals,p_thry,\\'r-\\');\\nlegend(\\'empirical probability\\',\\'theoretical probability\\');\\n</code></pre>\\n\\n<p>the graph produced:\\n<img src=\"http://i.stack.imgur.com/r2U31.jpg\" alt=\"alt text\"></p>\\n',\n",
       " '<p><em>\"it is 88 times more probable that one of the two films will lead through all 20,000 customers than it is that, say, the lead continuously seesaws\"</em></p>\\n\\n<p>In plain English: one of the movies gets an early lead. It has to, as the first customer has to go to A or B. That movie is then just as likely to keep its lead as lose it.</p>\\n\\n<p><em>88 times more likely</em> sounds, well, unlikely, until you remember that perfect seesawing is very improbable. The chart in <a href=\"http://stats.stackexchange.com/a/35707/5503\">MansT\\'s answer</a>, showing this graphically, is fascinating isn\\'t it.</p>\\n\\n<p>ASIDE: Personally, I think it\\'ll be more than 88 times - due to <code>&lt;buzzword-alert&gt;</code> viral marketing <code>&lt;/buzzword-alert&gt;</code>. Each person will ask other people what they saw, and are more likely to visit the same movie. They\\'ll even do this subconsciously: people are more likely to join a long queue to go an see something. I.e. as soon as randomness amongst the first few customers has created a leader, human psychology will keep it as a leader :-).</p>\\n',\n",
       " '<p>I have a signal, of varying amplitude, and after a certain point in time, I expect to see the period of this signal lengthen. I am looking for a way to measure the lengthening of this period.</p>\\n\\n<p>Is this a way I can calculate the instantaneous period of the signal below, at any given point? For example, at index 220 I would expect the period to be roughly 40 units, whereas at index 50 I expect it to be 24 units.</p>\\n\\n<p>I imagine the solution relates to either Fourier Transforms or Match Filtering, but I have little practical experience of either of these, so would appreciate any guidance. I am currently using R, but willing to consider other tools if they can help solve this problem.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/zFxbz.png\" alt=\"Signal\"></p>\\n',\n",
       " '<p>The other thing to think about are the underlying assumptions when we talk about means and standard deviations, and correlations. </p>\\n\\n<p>If we are talking about a data sample, one common assumption is that the data is (at least approximately) normally distributed, or can be transformed such that it is (e.g. via a log transform). If you observe a standard deviation of zero, there are two scenarios: either the standard deviation is in fact nonzero, but very small, and therefore the dataset you have has samples that are all on the mean value (this could, for example, happen if you are measuring data at a coarse level of precision); or the model is misspecified.</p>\\n\\n<p>In this second scenario, the standard deviation, and consequently the correlation, is a meaningless measure.</p>\\n\\n<p>More generally, the underlying distributions must both have finite second moments, and therefore non-zero standard deviations, for the correlation to be a valid concept.</p>\\n',\n",
       " '<p>This is a problem from a machine learning pset that I\\'m self-learning from <a href=\"http://www.seas.harvard.edu/courses/cs281/assignment-1.pdf\" rel=\"nofollow\">http://www.seas.harvard.edu/courses/cs281/assignment-1.pdf</a>.</p>\\n\\n<blockquote>\\n  <p>Suppose we are provided with a hierarchy of three distributions:\\n  $p(\\\\\\\\alpha)$, $p(\\\\\\\\theta | \\\\\\\\alpha)$ and $p(y | \\\\\\\\theta, \\\\\\\\alpha)$.\\n  Write expressions to compute the following related distributions:</p>\\n  \\n  <p>$p(y|\\\\\\\\theta)$,  $p(y|\\\\\\\\alpha)$,  $p(y)$,  $p(\\\\\\\\theta|y,\\\\\\\\alpha)$, \\n  $p(\\\\\\\\alpha|y,\\\\\\\\theta)$, $p(\\\\\\\\theta)$, $p(\\\\\\\\theta|y)$, $p(\\\\\\\\alpha|y)$</p>\\n</blockquote>\\n\\n<p>I see that I integrate over the joint distribution to get the marginal probability</p>\\n\\n<p>$$p(y|\\\\\\\\theta) = \\\\\\\\int p(y|\\\\\\\\theta,\\\\\\\\alpha)p(\\\\\\\\alpha)d\\\\\\\\alpha$$</p>\\n\\n<p>This gives me the first expression in terms of the givens.</p>\\n\\n<p>However, I don\\'t know how to get the others. I know I can set up various equations using Bayes\\' Theorem</p>\\n\\n<p>$$p(\\\\\\\\theta|\\\\\\\\alpha) = p(\\\\\\\\alpha|\\\\\\\\theta) * p(\\\\\\\\theta)/p(\\\\\\\\alpha)$$</p>\\n\\n<p>But the above equation has two unknowns ($p(\\\\\\\\theta)$ and $p(\\\\\\\\alpha|\\\\\\\\theta)$). Is there another property that I can exploit besides marginalizing the joint distribution and utilizing Bayes\\' Theorem?</p>\\n',\n",
       " '<p>I have a question on the term used for the type of the item that I would like to use and how it is \"usually\" analysed.\\nLet\\'s assume that we have two items and the one is linked to the other, namely, the one is the answer in a multiple choice question and the second on is quantifies an attribute, for example, \"did you find the lesson interesting\" then a LIKERT scale is provided and then another item that asks whether finding the lesson interesting was important for the respondent (something like classification/priotirisation). So an item attaches somehow an additional quantity to the first one. These kinds of \"pair of items\" are used in customer satisfaction research usually...\\nDo you know how they are called?\\nHow are they analysed?\\nWould Kullback-Leibler divergence be needed?</p>\\n',\n",
       " \"<p>I was wondering how statistics and decision theory are related?</p>\\n\\n<p>It looks to me all the statistics problems/tasks can be formulated in decision theory. Also problems in decision theory can be formulated in statistics/probability problems, or in deterministic way.  So is statistics just a part of the problems studied in decision theory? </p>\\n\\n<p>Or are they just two theories with overlapping and neither falls completely inside the other?</p>\\n\\n<p>But I have to admit that I don't have a systematic big picture of what topics statistics theory and decision theory are covered respectively, and would like to here some of your point of view.</p>\\n\\n<p>Thanks and regards!</p>\\n\",\n",
       " \"<p>I'm currently working on my thesis where I performed a multiple linear regression. The analysis is basically about the impact of projects. Among other independent variables I have the <strong>dummy variable</strong> <em>team composition</em> which is divided into functional(0) and cross-functional(1).</p>\\n\\n<pre><code>Independent Variable                       N\\n---------------------------------------------\\n....\\nTeam composition                          77    \\n     Functional teams (0)                 70\\n     Cross-Functional teams (1)            7\\n</code></pre>\\n\\n<p>As can be seen the number of observations (N) is quite different between those two. From the regression analysis it turned out that the <em>team composition</em> is <strong>not</strong> statistically significant which was contrary to what I actually expected.</p>\\n\\n<p>Now I'm trying to figure out what could be the reasoning behind such a result. <strong>My question is</strong> on whether the huge difference in the number of observations between <em>functional</em> and <em>cross-functional</em> could be <em>one</em> of the influencing factors for the missing statistical significance in the regression analysis?? </p>\\n\\n<p>thx</p>\\n\",\n",
       " '<p>I have a static panel data model with small T (T=5) that makes it impossible for me to use granger causality as it requires a long time span.</p>\\n\\n<p>So my question: </p>\\n\\n<ul>\\n<li>Is there any alternative solution to test for causation even in a  small T context?</li>\\n</ul>\\n\\n<p>Any hint will be highely appreciated!</p>\\n',\n",
       " '<p>When you conduct a 1 sample t-test why does minitab only give an upper bound or a lower bound for a confidence interval? So if $H_0: \\\\\\\\mu = \\\\\\\\mu_0$ and we test $H_a: \\\\\\\\mu &lt; \\\\\\\\mu_0$ or $H_a: \\\\\\\\mu &gt; \\\\\\\\mu_0$ why is only an upper bound or lower bound of a confidence interval returned respectively? Whereas testing $H_a: \\\\\\\\mu \\\\\\\\neq \\\\\\\\mu_0$ returns both a lower and upper bound.</p>\\n',\n",
       " '<p>I\\'m fitting a neural network to this example data I found online:\\n<a href=\"http://archive.ics.uci.edu/ml/datasets/Skin+Segmentation\">Machine Learning Repository</a></p>\\n\\n<p>I am cross validating 1 to 10 hidden units (in only 1 layer), and I have the minimum error with 10 hidden units. However, I\\'m somehow thinking of linearly dependent design matrices when introducing 10 hidden units for only 3 input variables (the level of Red, Green and Blue).</p>\\n\\n<p>Is this concern justified, or can I just use 10 hidden units here? Maybe the (sigmoid) transformation does something to avoid the linear dependency?  </p>\\n',\n",
       " '<ol>\\n<li>delete the leading zeroes as they can inflate the autocorrelation function</li>\\n<li>a visual suggest possibly a level shift and then a slight upward trend</li>\\n<li>a few anomalies , maybe just one , (pulses)</li>\\n<li>no apparent seasonal structure.</li>\\n</ol>\\n\\n<p>An ARIMA model would be good just as long as the reflections above were considered.</p>\\n\\n<p>If you want to post the data , I will be more specific as to the applicability of ARIMA. </p>\\n\\n<p>The 114 values you posted are quite different from your original plot. The actual-fit-forecast is<img src=\"http://i.stack.imgur.com/HLd3H.jpg\" alt=\"enter image description here\">. The acf of the original series shows little structure <img src=\"http://i.stack.imgur.com/HiT7s.jpg\" alt=\"enter image description here\"> . The \"best model\" contains no ARIMA structure but evidences a few unusual data points and three distinct means or GROUPS [1-32 ; 33-69 ; 70-114 ] <img src=\"http://i.stack.imgur.com/JnIPd.jpg\" alt=\"enter image description here\"> with outliers <img src=\"http://i.stack.imgur.com/9AS5V.jpg\" alt=\"enter image description here] .[4] . The acf of the residuals suggests randomness![enter image description here\"> . What we have here are three arima models of the form (0,0,0)(0,0,0) with three different means or regimes XBAR1=8.0 ; XBAR2=14.826 and XBAR3=10.8572. One could consider this single-dimension cluster analysis (see <a href=\"http://stats.stackexchange.com/questions/30361/univariate-clustering-of-time-series/32279#32279\">Univariate clustering of time series </a> )</p>\\n',\n",
       " '<p>How about something like this?\\n<img src=\"http://i.stack.imgur.com/OL0HP.png\" alt=\"alt text\"></p>\\n\\n<p>Following Crawley (2005). Statistics. An introduction using R: Wiley.</p>\\n',\n",
       " '<p>I suppose I should probably give a full answer, so here it is.</p>\\n\\n<p>I don\\'t use Lyx, I use vanilla LaTeX. I tried using Lyx, but it confused me, and I actually found plain latex to be much more understandable. For my money, the interface to Lyx was too much like a word processor and hid the code from me (although I didn\\'t use it for that long, so there may be ways around it). \\nI use Emacs with Emacs speaks statistics and Auctex for my sweave and latex files. This has the benefits of extremely good documentation, cross-platform support and syntax highlighting for both LaTeX and R within an Rnw file. As stated in my comment, it also makes it much easier to go from interactive analysis in the R buffer (an emacs term) to the Sweave file that I intend to use for my report/thesis/paper. </p>\\n\\n<p>I keep absolutely everything I\\'ve done in the Sweave file (as bitter experience tells me that the one thing you don\\'t put in the file will be what you have a problem with). </p>\\n\\n<p>I normally have one folder per project/paper, and this folder contains all of the input data files, the sweave files and all the output graphics/data files. This folder tends to become quite crowded over time, so I normally create a subfolder for my final document and rerun the analysis from there. </p>\\n\\n<p>I put everything in the sweave file as stated above, to ensure that all the analysis can be rerun. I typically call rm(list=ls()) on my R buffer before regenerating a sweave file, just to be sure. Rda files are dangerous, as the objects within my not be fully reproducible. That being said, if you have a computation that takes a really long time, then after you\\'re sure that it works, you could change the sweave chunk to eval=FALSE and then load that object from an Rda file. Thats a last resort though (<a href=\"http://andrewgelman.com/2011/09/reproducibility-in-practice/\" rel=\"nofollow\">this advice shamelessly stolen from andrew gelman</a>).</p>\\n\\n<p>I try to keep the same strategy for all my analyses, but that may change if I start doing more Bayesian work (due to the large amounts of computation involved).</p>\\n\\n<p>The packages I use most for LaTeX is xtable, which suits my needs. I don\\'t tend to use cacheSweave, as I have a morbid fear that I won\\'t be able to replicate my analyses afterwards. </p>\\n\\n<p>Recommended links:\\n<a href=\"http://www.stat.uni-muenchen.de/~leisch/Sweave/Sweave-manual.pdf\" rel=\"nofollow\">Sweave manual</a>\\n<a href=\"http://vgoulet.act.ulaval.ca/en/emacs/\" rel=\"nofollow\">Emacs for windows with ess and auctex</a>\\n<a href=\"http://www.gnu.org/s/teximpatient/\" rel=\"nofollow\">TeX for the impatient</a></p>\\n\\n<p>To be honest, I found that the best way to learn LaTeX is to start hacking away at it. There are many Sweave templates floating around the net, get one and start coding. Googling of error messages will probably sort you out, unless you want to do fancy stuff. There\\'s a guide to <a href=\"http://tug.org/pracjourn/2008-1/zahn/zahn.pdf\" rel=\"nofollow\">sweave from a psychologist</a> also floating around which is also pretty useful.</p>\\n\\n<p>Finally, LaTeX in general works way, way better on either a Mac or Linux (Ubuntu is always nice), and Emacs is much easier to extend on Linux. If you do need to stay on Windows, install both R and LaTeX (Miktex is good, select the install packages on the fly option) into a location with no spaces in the path (for example, c:\\\\\\\\bin rather than c:\\\\\\\\Program Files). This will save you a lot of hassle in the long run (but really install linux, r is faster, latex works better and the command line tools are to die for). </p>\\n',\n",
       " '<p>This sounds like just a stacked bar chart.  I don\\'t see how you handle the situation when the \"contribution\" made by a predictor and its coefficient is negative.  But you might get something like the not very elegant but workable below.  It returns warnings for when it is trying to plot something negative.  Perhaps in your case this doesn\\'t happen.</p>\\n\\n<p>The code uses ggplot2 0.8.9 - I think melt() changes in the latest implementation but I don\\'t have it installed at work:</p>\\n\\n<pre><code>library(ggplot2)    \\nX1 &lt;- rnorm(100)\\nX2 &lt;- rnorm(100,5,3)\\nY &lt;- 4 + 5*X1 + 3*X2 + rnorm(100)\\nmod &lt;- lm(Y ~ X1 + X2)\\ntmp &lt;- data.frame(t(coef(mod) * t(cbind(1, X1, X2))))\\nnames(tmp) &lt;- c(\"Intercept\", \"X1\", \"X2\")\\nqplot(x=as.factor(1:100), fill=variable, weight=value, geom=\"bar\", data=melt(tmp)) +\\n    geom_point(aes(x=1:100, y=predict(mod))\\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/GQzKa.png\" alt=\"enter image description here\"></p>\\n',\n",
       " '<p>Taking a simple alpha-stable distribution, the Normal Inverse Gaussian distribution for example, how would one derive the likelihood function <em>provided non-i.i.d. data</em>, e.g. a price series? </p>\\n',\n",
       " '<p>As far as I know, there is no reason to stop you from applying constrained optimization to an unconstrainted problem. However, this may not be a great idea in terms of computational complexity and convergence. For example, fitting a logistic regression model can done efficiently with the Newton-Raphson approach (or the Fisher scoring variant). I am not sure if there is much to gain with the interior point approach in this particular case.</p>\\n',\n",
       " '<p>I suggest three different functions:</p>\\n\\n<blockquote>\\n  <p>stats:::arima</p>\\n  \\n  <p>forecast:::Arima</p>\\n  \\n  <p>forecast:::auto.arima</p>\\n</blockquote>\\n\\n<p>forecast:::auto.arima will automatically seelct the p and q lags for you.</p>\\n',\n",
       " '<p><strong>Edited Question:</strong></p>\\n\\n<p><em>As I promised I\\'ve edited this question.  The previous version was written with the intention of simplifying the real question, but it ended in losing the real significance. Now I\\'m posting the \"whole story\". ;)</em></p>\\n\\n<p>My purpose is to calculate $n$ players\\' equity in a poker tournament (their probability of ending the tournament) in every $j$ place (1st, 2nd, and so on).</p>\\n\\n<blockquote>\\n  <p>I\\'ve previously solved this problem in 2 different ways you can find\\n  here:</p>\\n  \\n  <p>For the Maths:</p>\\n  \\n  <p><a href=\"http://math.stackexchange.com/questions/92942/applying-a-math-formula-in-a-more-elegant-way-maybe-a-recursive-call-would-do-t\">http://math.stackexchange.com/questions/92942/applying-a-math-formula-in-a-more-elegant-way-maybe-a-recursive-call-would-do-t</a></p>\\n  \\n  <p>And for the code:</p>\\n  \\n  <p><a href=\"http://stackoverflow.com/questions/8605183/how-to-translate-this-math-formula-in-php\">http://stackoverflow.com/questions/8605183/how-to-translate-this-math-formula-in-php</a></p>\\n</blockquote>\\n\\n<p>So, when I know every players\\' number of chips, I can easily apply those formulas and get their equity.</p>\\n\\n<hr>\\n\\n<p>There are 2 problems involved that hopefully can be solved with a statistical method. (I\\'m not a mathematician so I\\'m not sure it will be feasible).</p>\\n\\n<ol>\\n<li>First problem, even if I know everyone\\'s stack, when the number of players is high, the code is too slow to be implemented;</li>\\n<li>Second problem, this code should work by knowing only a limited number of stacks, belonging to the players of the analyzed table.</li>\\n</ol>\\n\\n<p>Optimistically these 2 problems can both be solved with some kind of approximations.</p>\\n\\n<p>In particular the formulas mentioned above should be applicable to scenarios with 27,45,90 players who are distributed in tables of 9.</p>\\n\\n<p>For example in the case of 27 players there would be 3 tables: when there are 18 players left they will be redistributed in 2 tables and when there are only 9 left the final table will be opened.\\nIt\\'s not important to take into account the players\\' skill since it\\'s a high variance game where its influence is reduced to the minimum, and mostly there are coin flips that eliminate players.</p>\\n\\n<p>So I\\'m in a situation where I know:</p>\\n\\n<ul>\\n<li>My number of chips.</li>\\n<li>The number of chips of the other 8 players of my table.</li>\\n<li>The total number of chips.</li>\\n<li>The average number of chips.</li>\\n<li>The maximum and minimum number of chips.</li>\\n</ul>\\n\\n<p>As I suggested in the previous question, this seems to me (from my humbles math skills) to be a gaussian curve, with a maximum, a minimum and an average number of chips.</p>\\n\\n<p><em>I think that\\'s all. If you need additional details please leave a comment, and I will add them as soon as possible.\\nI wanna thank you for your interest, and for all the previous comments and answers. I hope your statistics can help me solve this. :D</em> </p>\\n\\n<p>Best Regards,</p>\\n\\n<p>Giorgio.</p>\\n\\n<hr>\\n\\n<hr>\\n\\n<p><strong>Old Question:</strong> </p>\\n\\n<p>I\\'d like to calculate or approximate the probability that I have to win a tournament where every player has a determined amount of chips.</p>\\n\\n<p>Let\\'s consider a scenario where there are 9 players and I know everyone\\'s number of chips, to calculate my probability of winning I would do: my chips/(tot chips - my chips).</p>\\n\\n<p>Now imagine those 9 players are put on 3 different tables of 3 players each, and I know the chips only of the 2 players of my table and mine obviously. I also know the total number of chips, the max and min amount chips the players have and the average stack.</p>\\n\\n<p>Is it possible to make an approximation of my probability of winning?</p>\\n\\n<p>I have only basic math skills but I think players\\' stack could be approximated to a gaussian curve,  then use some \"statistical trick\" to calculate my probability.</p>\\n\\n<p>Thanks in advance for any hint!</p>\\n\\n<p>Best regards,\\nGiorgio</p>\\n',\n",
       " '<p>I am trying to forecast customer LTV using an exponential gamma distribution suggested in a Journal of Forecasting article (<a href=\"http://citeseer.uark.edu:8080/citeseerx/showciting;jsessionid=1A05CEB8A0BAE4B576E4C1699C1EE8E2?cid=2799268\" rel=\"nofollow\">Empirical Comparison of New Product Trial Forecasting Models</a>; authored by Bruce Hardie, Peter Fader, and Michael Wisniewski).  </p>\\n\\n<p>The specific formula that I am looking to solve is $P(t)=1-(α/(α+t))^r$, where $t$ is the period, $P(t)$ is the probability of a customer still being a customer at time $t$, $α$ is the scale parameter, and $r$ is the shape parameter.  </p>\\n\\n<p>I have some initial data on the per period attrition / retention of customers over 12 periods but I don\\'t know how to use this data to calculate $α$ or $r$ to enable me to forecast future periods attrition/retention and ultimately LTV.  </p>\\n\\n<p><strong>Can anybody explain how I can use the data I have to calculate $α$ and $r$?</strong></p>\\n',\n",
       " \"<p>For nearly equal sample sizes you can translate <em>Tukey's HSD</em> (Google it) into a set of individual CIs.  For unequal sample sizes your approach may be doomed, because all pairwise comparisons cannot be reduced to pairwise comparisons of intervals: check out the literature on the <em>Tukey-Kramer Method</em> for details.  (I know Stata and SAS both do these computations; contributed package DTK does it in R.)</p>\\n\",\n",
       " '<p>Can you explain what is a <a href=\"http://www.originlab.com/www/helponline/origin/en/UserGuide/Fit_Exponential.html#Second_order_exponential_decay_.28Exponential_Decay.2C_2_term.26ldots.3B.29\" rel=\"nofollow\">second order exponential decay function</a>:\\n$$\\ny(x) = y_0+A_{1}e^{-\\\\\\\\frac{x}{t_1}}+A_{2}e^{-\\\\\\\\frac{x}{t_2}}\\n$$</p>\\n\\n<p>(the $t_i$, $A_i$, and $y_0$ are constants and, presumably, the \"decay constants\" $t_i$ are positive)? </p>\\n\\n<ul>\\n<li><p>Qualitatively, what is the difference between \"first order\" and \"second order\"?   (A first order exponential function has the form $y(t)=y_0 + A_1 e^{-\\\\\\\\frac{x}{t}}$.)</p></li>\\n<li><p>How can we estimate $t_1$ and $t_2$ from data?</p></li>\\n</ul>\\n',\n",
       " \"<p>I want to perform a two-sample T-test to test for a difference between two independent samples which each sample abides by the assumptions of the T-test (each distribution can be assumed to be independent and identically distributed as Normal with equal variance). The only complication from the basic two-sample T-test is that the data is weighted. I am using weighted means and standard deviations, but weighted N's will artificially inflate the size of the sample, hence bias the result. Is it simply a case of replacing the weighted Ns with the unweighted Ns?</p>\\n\",\n",
       " '<p>Note that \\'<code>fitdistr</code> does nothing but maximum likelihood estimation. That is to say, you can do it by yourself by writing down the likelihood. Below is an example for the poisson distribution in R. It can be adapted to upweight/downweight the contribution to the likelihood of each data.</p>\\n\\n<hr>\\n\\n<p><strong>density (as in R)</strong> $$f(x; \\\\\\\\lambda) = \\\\\\\\lambda^x \\\\\\\\frac{\\\\\\\\exp(-\\\\\\\\lambda)}{x!}, \\\\\\\\qquad \\\\\\\\lambda &gt; 0$$</p>\\n\\n<p><strong>likelihood</strong> $$L(\\\\\\\\lambda; \\\\\\\\mathbf{x}) = \\\\\\\\prod_{i=1}^n \\\\\\\\left\\\\\\\\{ \\\\\\\\lambda^{x_i} \\\\\\\\frac{\\\\\\\\exp(-\\\\\\\\lambda)}{x_i!} \\\\\\\\right\\\\\\\\} $$</p>\\n\\n<p><strong>log-likelihood</strong></p>\\n\\n<p>$$\\\\\\\\ell(\\\\\\\\lambda; \\\\\\\\mathbf{x}) = \\\\\\\\sum_{i=1}^n \\\\\\\\left\\\\\\\\{ x_i \\\\\\\\log(\\\\\\\\lambda) - \\\\\\\\lambda - \\\\\\\\log(x_i!) \\\\\\\\right\\\\\\\\}$$</p>\\n\\n<p><strong>2nd derivatie of $\\\\\\\\ell$</strong>:</p>\\n\\n<p>$$ \\\\\\\\frac{d^2 \\\\\\\\ell}{d\\\\\\\\lambda^2}(\\\\\\\\lambda; \\\\\\\\mathbf{x}) = \\\\\\\\sum_{i=1}^n - \\\\\\\\frac{x_i}{\\\\\\\\lambda^2} = -\\\\\\\\frac{1}{\\\\\\\\lambda^2} n\\\\\\\\bar{x}$$</p>\\n\\n<hr>\\n\\n<pre><code>#------data------\\nset.seed(730)\\nsample &lt;- rpois(1000, 10)\\n#----------------\\n\\n ################################################################################\\n # Using \\'fitdistr\\'                                                             #\\n ################################################################################\\n\\n library(MASS)\\n fitdistr(x=sample, densfun=\"Poisson\")\\n lambda  \\n   10.1240000 \\n   ( 0.1006181)\\n\\n ################################################################################\\n # writing down the log-likelihood explicitly                                   #\\n ################################################################################\\n\\n #------minus log-likelihood------\\n mloglik &lt;- function(lambda2, sample) #lambda2 = log(lambda) in (-\\\\\\\\infty, \\\\\\\\infty)\\n {\\n  - sum(sample * lambda2 - exp(lambda2) - log(factorial(sample)))\\n }\\n #--------------------------------\\n\\n #------optimisation------\\n res &lt;- nlm(f=mloglik, p=1, sample=sample)\\n #------------------------\\n\\n #------recover lambda------\\n lambda &lt;- exp(res$estimate)\\n round(lambda, 7)\\n [1] 10.12399\\n #--------------------------\\n\\n #------standard error------\\n #square root of negative inverse second derivative of the log-likelihood\\n se &lt;- lambda / sqrt(length(sample) * mean(sample))\\n round(se, 7)\\n [1] 0.100618\\n #--------------------------\\n</code></pre>\\n',\n",
       " '<p>I am roughly running an OLS regression of prices and the number of network providers in cities in the UK.  Let $P$ stand for prices and $net$ stand for the number of network providers.  The frequency of $net$ is:  </p>\\n\\n<pre><code>net   frequency\\n0        100\\n1        140\\n2        100\\n3        40\\n4        10\\n5        10\\n6        1\\n7        1\\n</code></pre>\\n\\n<p>After generating dummies d0, ..., d7, for each state of $net$ (for example, d0 is equal to 0 when there are at least one company on the market and zero otherwise), I ran a regression of $P$ over d0,...,d6.  My question is not about this model per se, but about if it is correct in general to keep the dummies d6 and d7 given that they are equal to 1 only once.  Should I group d5, d6 and d7?  (that is consider d5+, a dummy for the case where there are 5 or more companies on the market), or the correct thing to do is to keep the given dummies as it is?   On the general model I am considering, the outcomes change a bit when I work with d5+ instead of d5,d6,d7.  Would you know a reference dealing with this situation? </p>\\n',\n",
       " '<p>hopefully you can help me with the meaning of the following, I don\\'t really understand the terminology:\\n\"regression of a vector of ones on the matrix $W$\",\\nwhere $W$ is something like $(W_t)&#39; = (w_{1t},w_{2t},w_{3t}, w_{4t})$.</p>\\n\\n<p>I don\\'t understand, which regression I actually have to compute. If it is of help for you, I\\'m trying to understand the Godfrey/Wickens test according to this article: Ghosh, S., Gilbert, C. L., &amp; Hallett, A. J. H. (1983). <em>Empirical Economics</em>, 8, 63–69. <a href=\"http://dx.doi.org/10.1007/BF01973190\" rel=\"nofollow\">DOI: 10.1007/BF01973190</a></p>\\n\\n<p>Greeting, Julius</p>\\n',\n",
       " '<p>This is the solution I came up with for this problem, based on some research and the answers so far. Sorry in advance for the length. I wanted to provide an in-depth explanation of my approach so I can be sure I didn\\'t miss anything important.</p>\\n\\n<p>I\\'ll start with a little more detail about my specific situation. I\\'ve got a database that contains records of jobs, clients, and workers. My goal is to see if a given worker has a lower than average score when it comes to converting first-time jobs into recurring jobs.</p>\\n\\n<p>I do this by taking all first-time jobs that a given worker has been assigned to, and I then check to see if any subsequent jobs exist for the same client. If there were subsequent jobs the worker gets a 1, otherwise they get a 0. This gives me something that looks like this: </p>\\n\\n<pre><code>Worker 1 [48 total records (first-time jobs)]\\n0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,\\n--\\nWorker 2 [56 total records (first-time jobs)]\\n1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,\\n</code></pre>\\n\\n<p>I then take this data and calculate the mean. I do this by counting the total number of first-time jobs in my system (2,925), as well as the sum of the 1\\'s and 0\\'s. This gives me a mean of 0.38 (so 38% of all first-time jobs typically become recurring jobs). I then calculate the standard deviation, which in this case is 0.13.</p>\\n\\n<p>I then look at each worker who has completed a minimum of 30 first-time jobs. This is so that I only analyze workers with a sufficiently large sample size, in order to increase confidence in the results.</p>\\n\\n<p>I then come up with a mean score for each of these workers (the sum of the 1\\'s and 0\\'s, divided by the total number of first-time jobs). Finally, I convert this into a z score, which I do by subtracting the mean (for all the data) from the worker\\'s mean score. I then divide the result by the standard deviation to obtain the individual z score. Finally, I put the results in a bar chart, which looks like this: </p>\\n\\n<p><img src=\"http://i.stack.imgur.com/YE9E5.gif\" alt=\"Bar chart demo\"> </p>\\n\\n<p>The dark bars are for any result with a z-score above 1 or below -1.</p>\\n\\n<p>That\\'s pretty much it. I guess my questions are as follows:</p>\\n\\n<ol>\\n<li>Is this the correct approach, given my goals?</li>\\n<li>Does it make sense to limit this analysis to workers with a minimum of 30 records? I know that generally the larger sample size the better, but would it work to apply this analysis to workers with only 20 records, for example? </li>\\n<li>Finally, what z-scores should I consider significant? Right now I am focusing on anything greater than 1, but is that too low?</li>\\n</ol>\\n\\n<p>I greatly appreciate any input. Thank you.</p>\\n',\n",
       " '<p>Here is one made specially for you.  Note that the density of a distribution symmetric about $0$ is the same for positive and negative values.</p>\\n\\n<pre><code>          density      cumprob\\n-3.5 0.0008726827 0.0002326291\\n-3.4 0.0012322192 0.0003369293\\n-3.3 0.0017225689 0.0004834241\\n-3.2 0.0023840882 0.0006871379\\n-3.1 0.0032668191 0.0009676032\\n-3   0.0044318484 0.0013498980\\n-2.9 0.0059525324 0.0018658133\\n-2.8 0.0079154516 0.0025551303\\n-2.7 0.0104209348 0.0034669738\\n-2.6 0.0135829692 0.0046611880\\n-2.5 0.0175283005 0.0062096653\\n-2.4 0.0223945303 0.0081975359\\n-2.3 0.0283270377 0.0107241100\\n-2.2 0.0354745928 0.0139034475\\n-2.1 0.0439835960 0.0178644206\\n-2   0.0539909665 0.0227501319\\n-1.9 0.0656158148 0.0287165598\\n-1.8 0.0789501583 0.0359303191\\n-1.7 0.0940490774 0.0445654628\\n-1.6 0.1109208347 0.0547992917\\n-1.5 0.1295175957 0.0668072013\\n-1.4 0.1497274656 0.0807566592\\n-1.3 0.1713685920 0.0968004846\\n-1.2 0.1941860550 0.1150696702\\n-1.1 0.2178521770 0.1356660609\\n-1   0.2419707245 0.1586552539\\n-0.9 0.2660852499 0.1840601253\\n-0.8 0.2896915528 0.2118553986\\n-0.7 0.3122539334 0.2419636522\\n-0.6 0.3332246029 0.2742531178\\n-0.5 0.3520653268 0.3085375387\\n-0.4 0.3682701403 0.3445782584\\n-0.3 0.3813878155 0.3820885778\\n-0.2 0.3910426940 0.4207402906\\n-0.1 0.3969525475 0.4601721627\\n0    0.3989422804 0.5000000000\\n0.1  0.3969525475 0.5398278373\\n0.2  0.3910426940 0.5792597094\\n0.3  0.3813878155 0.6179114222\\n0.4  0.3682701403 0.6554217416\\n0.5  0.3520653268 0.6914624613\\n0.6  0.3332246029 0.7257468822\\n0.7  0.3122539334 0.7580363478\\n0.8  0.2896915528 0.7881446014\\n0.9  0.2660852499 0.8159398747\\n1    0.2419707245 0.8413447461\\n1.1  0.2178521770 0.8643339391\\n1.2  0.1941860550 0.8849303298\\n1.3  0.1713685920 0.9031995154\\n1.4  0.1497274656 0.9192433408\\n1.5  0.1295175957 0.9331927987\\n1.6  0.1109208347 0.9452007083\\n1.7  0.0940490774 0.9554345372\\n1.8  0.0789501583 0.9640696809\\n1.9  0.0656158148 0.9712834402\\n2    0.0539909665 0.9772498681\\n2.1  0.0439835960 0.9821355794\\n2.2  0.0354745928 0.9860965525\\n2.3  0.0283270377 0.9892758900\\n2.4  0.0223945303 0.9918024641\\n2.5  0.0175283005 0.9937903347\\n2.6  0.0135829692 0.9953388120\\n2.7  0.0104209348 0.9965330262\\n2.8  0.0079154516 0.9974448697\\n2.9  0.0059525324 0.9981341867\\n3    0.0044318484 0.9986501020\\n3.1  0.0032668191 0.9990323968\\n3.2  0.0023840882 0.9993128621\\n3.3  0.0017225689 0.9995165759\\n3.4  0.0012322192 0.9996630707\\n3.5  0.0008726827 0.9997673709\\n</code></pre>\\n',\n",
       " '<p>Suppose one has a deck of cards. If one reshuffles it using a discrete probability model (based on combinatorics) one will still have the same probability of getting a particular card on each draw.</p>\\n\\n<p>This seems to be a little bit naïve for real cards, since reshuffling is somewhat non-random. Suppose you know the ordering of the cards in the deck before reshuffling.</p>\\n\\n<p>Is it possible to tackle non-randomness of reshuffling to get conditional (on original ordering) probabilities of getting a particular card from the deck?</p>\\n',\n",
       " '<p>If you objective is purely optimization of VI then without a lot of knowledge on the structure of your data you can\\'t expect to do much better than what standard combinatorial algorithms offer. You can try things like simulated annealing to move across the feature space \"intelligently\", perhaps. </p>\\n\\n<p>Typically however you want a statistical method to determine which subset of features yield the most informative clustering. The terminology for this problem is unsupervised feature selection. Some popular methods include: </p>\\n\\n<ol>\\n<li><p>Regularization on parameters <a href=\"http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.ejs/1262617415\" rel=\"nofollow\">Zhou, Pan and Shen (2009)</a>, which assumes a Gaussian mixture model. </p></li>\\n<li><p>Regularization on feature weights <a href=\"http://www.tandfonline.com/doi/abs/10.1198/jasa.2010.tm09415#preview\" rel=\"nofollow\">Witten and Tibshirani (2010)</a>, which express clustering criteria as additive in features then regularize the weights assigned to each summand.</p></li>\\n</ol>\\n\\n<p>If your clustering algorithm in already known (and/or the number of clusters) then I worked on a method that is based on <a href=\"http://www.stat.washington.edu/wxs/Stat592-w2011/Literature/tibshirani-walther-prediction-strength-2005.pdf\" rel=\"nofollow\">Prediction Strength</a>. The idea is to split your data into two halves by rows (observations), call them $X$ (training data) and $Y$ (test data). </p>\\n\\n<p>Then calculate $A^X_{i} = \\\\\\\\{x:C_X(x) = i\\\\\\\\}$ and $B_{ij} = A^X_i \\\\\\\\cap A^Y_j, \\\\\\\\: i,j = 1,\\\\\\\\ldots,k$, where $C_X(x)$ is the cluster index the algorithm would assign to a data point $x$ when fit on the data-set $X$. </p>\\n\\n<p>The $A_i^X$ represent how the training data partitions the feature space space. Therefore $B_{ij}$ represents how much overlap there is between the training set and a test set clustering. If the clustering is extremely stable the overlap will be large, so you search for features that maximize the statistic</p>\\n\\n<p>$\\\\\\\\begin{align*}\\nps(K) &amp;= \\\\\\\\mathbf{P}(x_1, x_2 \\\\\\\\: \\\\\\\\text{in same training cluster} \\\\\\\\: | \\\\\\\\: x_1,x_2 \\\\\\\\:\\\\\\\\text{in same test cluster}) \\\\\\\\\\\\\\\\\\n&amp;=\\\\\\\\frac{1}{K}\\\\\\\\sum_{i=1}^K\\\\\\\\frac{\\\\\\\\sum_{j=1}^K |B_{ij}|\\\\\\\\left(|B_{ij}| - 1\\\\\\\\right)}{|A_i^X|\\\\\\\\left(|A_i^X| - 1\\\\\\\\right)} + o_p(1)\\n\\\\\\\\end{align*}$</p>\\n\\n<p>given the number of clusters $K$.</p>\\n\\n<p>In practice I sample feature subsets randomly then average to see which features yield the best prediction strength.</p>\\n',\n",
       " '<p>Yeah, how about a nonlinear regression model. There are a few models that are used all of the time in biostats. I used the Michaelis-Menten model for somebody one time: <a href=\"http://en.wikipedia.org/wiki/Michaelis%E2%80%93Menten_kinetics\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Michaelis%E2%80%93Menten_kinetics</a>. </p>\\n\\n<p>Looking at the plot in your script, this model would be a great fit. Would this be accepted in public health?</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/EiSM4.jpg\" alt=\"enter image description here\"></p>\\n\\n<p>The function is smooth - it looks piecewise because I only plotted it at the fitted values. I used the <code>nls</code> function in <code>R</code> to estimate this model:</p>\\n\\n<pre><code>  model1 = nls(DATA$R ~ ((a * DATA$CONC) / (b + DATA$CONC)), start = list(a = 100, b = -1), algorithm = \"port\")\\n  plot(DATA$CONC, DATA$R)\\n  lines(DATA$CONC, fitted.values(model1), lty = 1)\\n</code></pre>\\n',\n",
       " '<p>In R I would like to solve a system of linear equations with constraints to preserve monotonicity.  I can do this easily with no constraints on the coefficients.  Here is an example:</p>\\n\\n<pre><code>       [,1]      [,2]        [,3]      [,4]       [,5]      [,6]      [,7]\\n[1,] 0.6945405 0.1157702 0.004973632 0.0000000 0.00000000 0.0000000 0.1625076\\n[2,] 0.9212828 0.3055870 0.026655560 0.0000000 0.00000000 0.0000000 0.3916894\\n[3,] 1.0000000 0.9987081 0.835572186 0.1767705 0.00000000 0.0000000 6.6993305\\n[4,] 1.0000000 1.0000000 0.992828243 0.5758778 0.02530867 0.0000000 7.6371723\\n[5,] 1.0000000 1.0000000 0.997171672 0.6412910 0.04668548 0.0000000 7.6628770\\n[6,] 1.0000000 1.0000000 1.000000000 0.9970624 0.90614523 0.4305434 7.6796152\\n</code></pre>\\n\\n<p>Columns 1-6 are the coefficients and column 7 is the right side of the linear system.  This can easily be solved by:</p>\\n\\n<pre><code>solve(mat[,-ncol(mat)],mat[,ncol(mat)])\\n</code></pre>\\n\\n<p>However if I want to put constraints where all coefficients must be greater than 0 is there an easy way to do this in R?  If the solve function returns a negative coefficient does this indicate no solution exists where all the coefficients are positive?</p>\\n',\n",
       " '<p><strong>Context</strong></p>\\n\\n<p>I have a regression framework and two sets of data. Using leave-one-out cross-validation, the first set gives very good performance and the second set gives rather poor performance. I need to explain the reason for this difference in performance.</p>\\n\\n<p>Having looked at the data, it is clear that the first set is a much easier test case. The data in the first set does not vary as much as in the second set. In addition, the test and training data of the first set are strikingly similar, which is not the case in the second dataset.</p>\\n\\n<p>To explain the reason for the performance difference, I can show these differences in data qualitatively using pictures of the physical phenomena being modeled. However, I would also like to quantify these dataset differences and relate it to the performance discrepancy.</p>\\n\\n<p><strong>Questions</strong></p>\\n\\n<p>What metrics can I use to show that the first dataset is a trivial test case and that the second set is more challenging?</p>\\n\\n<p>As a statistician, what else would you want to know in order to be confident that the performance discrepancy was caused by these differences in data and not something else?</p>\\n',\n",
       " '<p>Typical measures of autocorrelation, such as Moran\\'s I, are global estimates of clumpiness and could be masked by a trend or by \"averaging\" of clumpiness. There are two ways you could handle this:\\n1) Use a local measure of autocorrelation - but the drawback is you don\\'t get a single number for clumpiness.  An example of this would be Local Moran\\'s I*\\nHere is a document (from a google search) that at least introduces the terms and gives some derivations\\n<a href=\"http://onlinelibrary.wiley.com/doi/10.1111/0022-4146.00224/abstract\" rel=\"nofollow\">http://onlinelibrary.wiley.com/doi/10.1111/0022-4146.00224/abstract</a></p>\\n\\n<p>2) Use a statistic specifically geared towards point distributions and their clumpieness at various spatial scales, such as Ripley\\'s K\\n<a href=\"http://scholar.google.com/scholar?q=Ripley%27s+K&amp;hl=en&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart\" rel=\"nofollow\">http://scholar.google.com/scholar?q=Ripley%27s+K&amp;hl=en&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart</a></p>\\n',\n",
       " '<p>This is a statistical version of my <a href=\"http://math.stackexchange.com/questions/30731/will-two-convex-hulls-overlap\" >Math.SE </a> post.</p>\\n\\n<p>Given natural numbers $b$ and $r$, uniformly randomly choose $b+r$ points within a unit square. Call the $b$ points the <i>blue points</i> and the $r$ points <i> red points</i>. Let $p(b,r)$ be the probability that $H_r$, the convex hull of red points, overlaps with $H_b$, that of the blue points. How do I efficiently estimate $p(b,r)$ via Monte Carlo (or perhaps better) methods?</p>\\n\\n<p>I can think of averaging the probability over a large number of randomly chosen test cases, but how can I sample the search space in a way that gives me error bounds?  </p>\\n',\n",
       " '<p>As suggested by @whuber, you can \"abstract\" the scale effect by working with a standardized version of your data. If you\\'re willing to accept that an interval scale is the support of each of your item (i.e. the distance between every two response categories would have the same meaning for every respondents), then linear correlations are fine. But you can also compute <a href=\"http://en.wikipedia.org/wiki/Polychoric_correlation\" rel=\"nofollow\">polychoric correlation</a> to better account for the discretization of a latent variable (see the R package <a href=\"http://cran.r-project.org/web/packages/polycor/index.html\" rel=\"nofollow\">polycor</a>). Of note, it\\'s a largely more computer-intensive job, but it works quite well in R.</p>\\n\\n<p>Another possibility is to combine optimal scaling within your PCA, as implemented in the <a href=\"http://cran.r-project.org/web/packages/homals/index.html\" rel=\"nofollow\">homals</a> package. The idea is to find a suitable non-linear transformation of each scale, and this is very nicely described by Jan de Leeuw in the accompagnying vignette or the JSS article, <a href=\"http://www.jstatsoft.org/v31/i04/paper\" rel=\"nofollow\">Gifi Methods for Optimal Scaling in R: The Package homals</a>. There are several examples included.</p>\\n\\n<p>For a more thorough understanding of this approach with any factorial method, see the work of <a href=\"http://takane.brinkster.net/Yoshio/\" rel=\"nofollow\">Yoshio Takane</a> in the 80s.</p>\\n\\n<p>Similar points were raised by @Jeromy and @mbq on related questions, <a href=\"http://stats.stackexchange.com/questions/539/does-it-ever-make-sense-to-treat-categorical-data-as-continuous\">Does it ever make sense to treat categorical data as continuous?</a>, <a href=\"http://stats.stackexchange.com/questions/548/how-can-i-use-optimal-scaling-to-scale-an-ordinal-categorical-variable\">How can I use optimal scaling to scale an ordinal categorical variable?</a></p>\\n',\n",
       " \"<p>I am using the Holt-Winters' exponential smoothing technique to forecast expenditure data 2 years into the furture. The monthly data has an increasing trend and annual seasonality.</p>\\n\\n<p>I'm using MS Excel with the Solver add-in to calculate the optimal values of $\\\\\\\\alpha$, $\\\\\\\\beta$ and $\\\\\\\\gamma$ to give the smallest MSE for the forecasts. The optimal values found for $\\\\\\\\alpha$ and $\\\\\\\\beta$ lie in (0,1) and $\\\\\\\\gamma$ is found to be 1.</p>\\n\\n<p>I am able to calculate the forecasts for the next year (season) because the seasonals from the previous year exist. However, the forecasts for the second year are calculated to be zero because the seasonals do not exist (because <em>m</em> is greater than 12). </p>\\n\\n<p>I have discovered that if $\\\\\\\\gamma$ is zero, then the seasonals will be periodic, and so could be replicated after the last observed values. Is this the best way to forecast beyond one season after the last observed values? Any advice would be appreciated.</p>\\n\\n<p>Example data is below. Forecasts are needed for every month up to December 2011. I cannot see how this is possible unless $\\\\\\\\gamma$ is zero.</p>\\n\\n<p>Numbers of Tourists<br>\\nPeriod  Month   No. Tourists (Yt)<br>\\n1   Jan-99  500<br>\\n2   Feb-99  543<br>\\n3   Mar-99  899<br>\\n4   Apr-99  835<br>\\n5   May-99  900<br>\\n6   Jun-99  881<br>\\n7   Jul-99  1154<br>\\n8   Aug-99  1586<br>\\n9   Sep-99  743<br>\\n10  Oct-99  1104<br>\\n11  Nov-99  799<br>\\n12  Dec-99  560<br>\\n13  Jan-00  514<br>\\n14  Feb-00  665<br>\\n15  Mar-00  949<br>\\n16  Apr-00  975<br>\\n17  May-00  924<br>\\n18  Jun-00  724<br>\\n19  Jul-00  1155<br>\\n20  Aug-00  1541<br>\\n21  Sep-00  746<br>\\n22  Oct-00  944<br>\\n23  Nov-00  786<br>\\n24  Dec-00  652<br>\\n25  Jan-01  479.4<br>\\n26  Feb-01  644.4<br>\\n27  Mar-01  815.8<br>\\n28  Apr-01  1035.4<br>\\n29  May-01  1000.9<br>\\n30  Jun-01  793.8<br>\\n31  Jul-01  1347.3<br>\\n32  Aug-01  1378<br>\\n33  Sep-01  798.1<br>\\n34  Oct-01  1070.5<br>\\n35  Nov-01  625.3<br>\\n36  Dec-01  654<br>\\n37  Jan-02  477.5<br>\\n38  Feb-02  656.2<br>\\n39  Mar-02  888.7<br>\\n40  Apr-02  926.6<br>\\n41  May-02  1000.1<br>\\n42  Jun-02  1030.8<br>\\n43  Jul-02  1123<br>\\n44  Aug-02  1473.5<br>\\n45  Sep-02  717.8<br>\\n46  Oct-02  974.7<br>\\n47  Nov-02  761.2<br>\\n48  Dec-02  641.5<br>\\n49  Jan-03  501.6<br>\\n50  Feb-03  588.3<br>\\n51  Mar-03  917.6<br>\\n52  Apr-03  990<br>\\n53  May-03  1051<br>\\n54  Jun-03  764.4<br>\\n55  Jul-03  1014.2<br>\\n56  Aug-03  1313.6<br>\\n57  Sep-03  736.3<br>\\n58  Oct-03  1042.9<br>\\n59  Nov-03  685.9<br>\\n60  Dec-03  621.5<br>\\n61  Jan-04  492.8<br>\\n62  Feb-04  722<br>\\n63  Mar-04  869.9<br>\\n64  Apr-04  927.9<br>\\n65  May-04  1028.1<br>\\n66  Jun-04  883<br>\\n67  Jul-04  1097.4<br>\\n68  Aug-04  1398.9<br>\\n69  Sep-04  834.4<br>\\n70  Oct-04  1072.3<br>\\n71  Nov-04  801.9<br>\\n72  Dec-04  711.2<br>\\n73  Jan-05  616.1<br>\\n74  Feb-05  774<br>\\n75  Mar-05  1088.5<br>\\n76  Apr-05  956.2<br>\\n77  May-05  1175.6<br>\\n78  Jun-05  949.5<br>\\n79  Jul-05  1120.8<br>\\n80  Aug-05  1426.2<br>\\n81  Sep-05  841.5<br>\\n82  Oct-05  996.6<br>\\n83  Nov-05  908<br>\\n84  Dec-05  696.7<br>\\n85  Jan-06  606.4<br>\\n86  Feb-06  771.6<br>\\n87  Mar-06  967.1<br>\\n88  Apr-06  1235<br>\\n89  May-06  1216.1<br>\\n90  Jun-06  945.1<br>\\n91  Jul-06  1194.4<br>\\n92  Aug-06  1433.4<br>\\n93  Sep-06  830.6<br>\\n94  Oct-06  984.7<br>\\n95  Nov-06  880.2<br>\\n96  Dec-06  668.3<br>\\n97  Jan-07  644.9<br>\\n98  Feb-07  808<br>\\n99  Mar-07  998.2<br>\\n100 Apr-07  1283.9<br>\\n101 May-07  1080.9<br>\\n102 Jun-07  989.9<br>\\n103 Jul-07  1167<br>\\n104 Aug-07  1568.9<br>\\n105 Sep-07  951.7<br>\\n106 Oct-07  1121.4<br>\\n107 Nov-07  859<br>\\n108 Dec-07  660.9<br>\\n109 Jan-08  647.9<br>\\n110 Feb-08  911.1<br>\\n111 Mar-08  1201.2<br>\\n112 Apr-08  1258.1<br>\\n113 May-08  1177.8<br>\\n114 Jun-08  1067.6<br>\\n115 Jul-08  1349.4<br>\\n116 Aug-08  1702.1<br>\\n117 Sep-08  982.8<br>\\n118 Oct-08  1116.5<br>\\n119 Nov-08  904.7<br>\\n120 Dec-08  655.9<br>\\n121 Jan-09  733.75<br>\\n122 Feb-09  852.67<br>\\n123 Mar-09  1049.88<br>\\n124 Apr-09  1377.11<br>\\n125 May-09  1344.05<br>\\n126 Jun-09  1030.95<br>\\n127 Jul-09  1242.56<br>\\n128 Aug-09  1542.24<br>\\n129 Sep-09  1016.42<br>\\n130 Oct-09  2301.41<br>\\n131 Nov-09  1138.9<br>\\n132 Dec-09  1032.87  </p>\\n\",\n",
       " \"<p>I think you're idea of a regression predicting X with A, B, and C, is a good place to start.  Make sure to distinguish between a statistically insignificant effect and a precisely estimated effect of zero (or close to zero).</p>\\n\\n<p>Also, consider the possibility of non-linear relationships.  Throw squared or even cubed terms into the regression equation. </p>\\n\",\n",
       " '<p>I have a VAR model, which shows very low $R^2$ values (below 0.05). Does this mean that my model is very bad in explaining? </p>\\n\\n<p><img src=\"http://i.stack.imgur.com/cEDIA.png\" alt=\"enter image description here\"></p>\\n',\n",
       " \"<p>I have a data set with my target gene and more than thousand transcriptional factors somehow correlated with this gene. There is data of  these factors in more than  70 variable conditions. What I'm looking to do is use CV to find most useful variables which correlate with my target gene. How could I do that in R please?</p>\\n\",\n",
       " \"<p>I am working on a problem which looks like this:</p>\\n\\n<p>Input Variables</p>\\n\\n<ul>\\n<li><p>Categorical</p>\\n\\n<ul>\\n<li>a</li>\\n<li>b</li>\\n<li>c</li>\\n<li>d</li>\\n</ul></li>\\n<li><p>Continuous</p>\\n\\n<ul>\\n<li>e</li>\\n</ul></li>\\n</ul>\\n\\n<p>Output Variables</p>\\n\\n<ul>\\n<li><p>Discrete(Integers)</p>\\n\\n<ul>\\n<li>v</li>\\n<li>x</li>\\n<li>y</li>\\n</ul></li>\\n<li><p>Continuous</p>\\n\\n<ul>\\n<li>z</li>\\n</ul></li>\\n</ul>\\n\\n<p>The major issue that I am facing is that Output Variables are not totally independent of each other and there is no relation that can be established between them. That is,  there is a dependence but not due to the causality (one value being high doesn't imply that the other will be high too but the chances of other being higher will improve)</p>\\n\\n<p>An Example would be:</p>\\n\\n<p>v - Number of Ad Impressions</p>\\n\\n<p>x - Number of Ad Clicks</p>\\n\\n<p>y - Number of Conversions</p>\\n\\n<p>z - Revenue</p>\\n\\n<p>Now, for an Ad to be clicked, it has to first appear on a search, so Click is somewhat dependent on Impression.</p>\\n\\n<p>Again, for an Ad to be Converted, it has to be first clicked, so again Conversion is somewhat dependent on Click.</p>\\n\\n<p>So running 4 instances of the problem predicting each of the output variables doesn't make sense to me. Infact there should be some way to predict all 4 together taking care of their implicit dependencies.</p>\\n\\n<p>But as you can see, there won't be a direct relation, infact there would be a probability that is involved but which can't be worked out manually.</p>\\n\\n<p>Plus the output variables are not Categorical but are in fact Discrete and Continuous.</p>\\n\\n<p>Any inputs on how to go about solving this problem. Also guide me to existing implementations for the same and which toolkit to use to quickly implement the solution.</p>\\n\\n<p>Just a random guess - I think this problem can be targeted by Bayesian Networks. What do you think ?</p>\\n\",\n",
       " '<p>I found that sometimes in necessary to \"linearized\" the relationship between X and Y. if you have an allometric relationship or for instance a power function, a log-log trasnformation help to do that and also ofcourse help to stabilize variances. it all depends in the nature of your data</p>\\n',\n",
       " '<p>This is not a complete answer, but whenever I am exploring my data for the first time I like to use the <code>describe()</code> function from the <a href=\"http://biostat.mc.vanderbilt.edu/wiki/Main/Hmisc\" rel=\"nofollow\">Hmisc</a> library in the free statistical program <a href=\"http://www.r-project.org/\" rel=\"nofollow\">R</a>. The library is available for download from within R or from its <a href=\"http://biostat.mc.vanderbilt.edu/wiki/Main/InstallSplusLibrary\" rel=\"nofollow\">website</a>.</p>\\n\\n<p>Sample code for exploring your data might look like this:</p>\\n\\n<pre><code>## Generate example data\\nData &lt;- as.data.frame(matrix(data = c(10,11,10,27,5,3,4,15,55,10,10,9,1,2,1,9,12,12,12,17,50,40,50,100,1,2,4,3,9,7,8,10), nrow = 4, ncol = 8, byrow = FALSE))\\n\\n## Load Hmisc library\\nlibrary(Hmisc)\\n\\n## Generate descriptive statistics\\ndescribe(Data)\\n</code></pre>\\n\\n<p>The <code>describe()</code> command will provide you with the number of total, unique, and missing observations of each variable along with means and frequencies. You might also consider further exploring your data with the <code>hist()</code>, <code>density()</code>, and <code>summary()</code> functions. Type <code>help(function_name)</code> at the R command line for details.</p>\\n',\n",
       " '<p>Well, I\\'m an engineer by day. Although most of my work revolves around modeling, we generally do pretty basic stuff. An \"Advanced\" model would be a monte carlo simulation validated using R2 tests. </p>\\n\\n<p>Currently, in my field, there is a lot of research using Logistic and bayesian analysis. </p>\\n\\n<p>My question is, which courses would you recommend someone to take from <a href=\"http://ocw.mit.edu/courses/mathematics/\">MIT\\'s open course site</a> or any other sites, for someone who learns best by video/audio first, and reading second?</p>\\n\\n<p>What i\\'d like to learn are the following:</p>\\n\\n<ul>\\n<li>Be able to understand the models and when to employ them</li>\\n<li>able to take in field data (which is generated once and cannot be regenerated) and design and perform experiments</li>\\n<li>Able to understand the results, look at them, and figure out if something is off, \"show stopper\" or \"outliers\", or if everything is fine and dandy</li>\\n<li>Be able to validate and calibrate the model, to actual \"As-built\" results</li>\\n<li>Be able to forecast the results using appropriate sensitivity analysis</li>\\n<li>be able to forecast / \"plug\" missing data</li>\\n<li>be able to write journal papers related to my field</li>\\n</ul>\\n\\n<p>my field in a nutshell is: transportation demand modeling for passenger vehicles, using either the generic four step model, or socio economic activity/tour based models such as PECAS or urbansim</p>\\n',\n",
       " '<p>I\\'ve implemented the Randomized SVD as given in \"Halko, N., Martinsson, P. G., Shkolnisky, Y., &amp; Tygert, M. (2010). An algorithm for the principal component analysis of large data sets. Arxiv preprint arXiv:1007.5510, 0526. Retrieved April 1, 2011, from <a href=\"http://arxiv.org/abs/1007.5510\">http://arxiv.org/abs/1007.5510</a>.\". If you want to get truncated SVD, it really works much much faster than the svd variations in MATLAB. You can get it here:</p>\\n\\n\\n\\n<pre><code>function [U,S,V] = fsvd(A, k, i, usePowerMethod)\\n% FSVD Fast Singular Value Decomposition \\n% \\n%   [U,S,V] = FSVD(A,k,i,usePowerMethod) computes the truncated singular\\n%   value decomposition of the input matrix A upto rank k using i levels of\\n%   Krylov method as given in [1], p. 3.\\n% \\n%   If usePowerMethod is given as true, then only exponent i is used (i.e.\\n%   as power method). See [2] p.9, Randomized PCA algorithm for details.\\n% \\n%   [1] Halko, N., Martinsson, P. G., Shkolnisky, Y., &amp; Tygert, M. (2010).\\n%   An algorithm for the principal component analysis of large data sets.\\n%   Arxiv preprint arXiv:1007.5510, 0526. Retrieved April 1, 2011, from\\n%   http://arxiv.org/abs/1007.5510. \\n%   \\n%   [2] Halko, N., Martinsson, P. G., &amp; Tropp, J. A. (2009). Finding\\n%   structure with randomness: Probabilistic algorithms for constructing\\n%   approximate matrix decompositions. Arxiv preprint arXiv:0909.4061.\\n%   Retrieved April 1, 2011, from http://arxiv.org/abs/0909.4061.\\n% \\n%   See also SVD.\\n% \\n%   Copyright 2011 Ismail Ari, http://ismailari.com.\\n\\n    if nargin &lt; 3\\n        i = 1;\\n    end\\n\\n    % Take (conjugate) transpose if necessary. It makes H smaller thus\\n    % leading the computations to be faster\\n    if size(A,1) &lt; size(A,2)\\n        A = A\\';\\n        isTransposed = true;\\n    else\\n        isTransposed = false;\\n    end\\n\\n    n = size(A,2);\\n    l = k + 2;\\n\\n    % Form a real n×l matrix G whose entries are iid Gaussian r.v.s of zero\\n    % mean and unit variance\\n    G = randn(n,l);\\n\\n\\n    if nargin &gt;= 4 &amp;&amp; usePowerMethod\\n        % Use only the given exponent\\n        H = A*G;\\n        for j = 2:i+1\\n            H = A * (A\\'*H);\\n        end\\n    else\\n        % Compute the m×l matrices H^{(0)}, ..., H^{(i)}\\n        % Note that this is done implicitly in each iteration below.\\n        H = cell(1,i+1);\\n        H{1} = A*G;\\n        for j = 2:i+1\\n            H{j} = A * (A\\'*H{j-1});\\n        end\\n\\n        % Form the m×((i+1)l) matrix H\\n        H = cell2mat(H);\\n    end\\n\\n    % Using the pivoted QR-decomposiion, form a real m×((i+1)l) matrix Q\\n    % whose columns are orthonormal, s.t. there exists a real\\n    % ((i+1)l)×((i+1)l) matrix R for which H = QR.  \\n    % XXX: Buradaki column pivoting ile yapılmayan hali.\\n    [Q,~] = qr(H,0);\\n\\n    % Compute the n×((i+1)l) product matrix T = A^T Q\\n    T = A\\'*Q;\\n\\n    % Form an SVD of T\\n    [Vt, St, W] = svd(T,\\'econ\\');\\n\\n    % Compute the m×((i+1)l) product matrix\\n    Ut = Q*W;\\n\\n    % Retrieve the leftmost m×k block U of Ut, the leftmost n×k block V of\\n    % Vt, and the leftmost uppermost k×k block S of St. The product U S V^T\\n    % then approxiamtes A. \\n\\n    if isTransposed\\n        V = Ut(:,1:k);\\n        U = Vt(:,1:k);     \\n    else\\n        U = Ut(:,1:k);\\n        V = Vt(:,1:k);\\n    end\\n    S = St(1:k,1:k);\\nend\\n</code></pre>\\n\\n<p>To test it, just create an image in the same folder (just as a big matrix,you can create the matrix yourself)</p>\\n\\n<pre><code>% Example code for fast SVD.\\n\\nclc, clear\\n\\n%% TRY ME\\nk = 10; % # dims\\ni = 2;  % # power\\nCOMPUTE_SVD0 = true; % Comment out if you do not want to spend time with builtin SVD.\\n\\n% A is the m×n matrix we want to decompose\\nA = im2double(rgb2gray(imread(\\'test_image.jpg\\')))\\';\\n\\n%% DO NOT MODIFY\\nif COMPUTE_SVD0\\n    tic\\n    % Compute SVD of A directly\\n    [U0, S0, V0] = svd(A,\\'econ\\');\\n    A0 = U0(:,1:k) * S0(1:k,1:k) * V0(:,1:k)\\';\\n    toc\\n    display([\\'SVD Error: \\' num2str(compute_error(A,A0))])\\n    clear U0 S0 V0\\nend\\n\\n% FSVD without power method\\ntic\\n[U1, S1, V1] = fsvd(A, k, i);\\ntoc\\nA1 = U1 * S1 * V1\\';\\ndisplay([\\'FSVD HYBRID Error: \\' num2str(compute_error(A,A1))])\\nclear U1 S1 V1\\n\\n% FSVD with power method\\ntic\\n[U2, S2, V2] = fsvd(A, k, i, true);\\ntoc\\nA2 = U2 * S2 * V2\\';\\ndisplay([\\'FSVD POWER Error: \\' num2str(compute_error(A,A2))])\\nclear U2 S2 V2\\n\\nsubplot(2,2,1), imshow(A\\'), title(\\'A (orig)\\')\\nif COMPUTE_SVD0, subplot(2,2,2), imshow(A0\\'), title(\\'A0 (svd)\\'), end\\nsubplot(2,2,3), imshow(A1\\'), title(\\'A1 (fsvd hybrid)\\')\\nsubplot(2,2,4), imshow(A2\\'), title(\\'A2 (fsvd power)\\')\\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/T1n9m.jpg\" alt=\"Fast SVD\"></p>\\n\\n<p>When I run it on my desktop for an image of size 635*483, I get </p>\\n\\n<pre><code>Elapsed time is 0.110510 seconds.\\nSVD Error: 0.19132\\nElapsed time is 0.017286 seconds.\\nFSVD HYBRID Error: 0.19142\\nElapsed time is 0.006496 seconds.\\nFSVD POWER Error: 0.19206\\n</code></pre>\\n\\n<p>As you can see, for low values of <code>k</code>, it is more than 10 times faster than using Matlab SVD. By the way, you may need the following simple function for the test function:</p>\\n\\n<pre><code>function e = compute_error(A, B)\\n% COMPUTE_ERROR Compute relative error between two arrays\\n\\n    e = norm(A(:)-B(:)) / norm(A(:));\\nend\\n</code></pre>\\n\\n<p>I didn\\'t add the PCA method since it is straightforward to implement using SVD. You may <a href=\"http://math.stackexchange.com/questions/3869/what-is-the-intuitive-relationship-between-svd-and-pca\">check this link</a> to see their relationship.</p>\\n',\n",
       " '<p>This sampling strategy could work but it needs a bit of refinement.</p>\\n\\n<p>You certainly need to use weighting.  Any sampling strategy can be \"representative\" so long as all the population have a known, non-zero probability of selection.  Then you can set weights to the inverse of their probability of selection.  If the probabilities are not equal you need weights.  Definitely in your case individuals have different chances of selection and hence need to have weights calculated for them once the sampling is finished but before you start doing analysis.</p>\\n\\n<p>I think you are using cluster and strata the wrong way around in your description, although it is reasonably clear what you are doing.  Your strata are store sizes, and within those strata you first select two stores, which are clusters of 50 customers each.  If you were specifying the survey design to statistical software it is important to understand the distinction.</p>\\n\\n<p>A challenge with your sampling strategy, as you point out in the comments, is that customers from large stores have a very low relative probability of selection.  This means that you will end up with a relatively good idea of the behaviour of customers in small stores - but there are so few of them is it worthwhile investing that much of your scarce sample in them.  Perhaps you should select more people from the large stores.  This is the sort of question that really needs specialist input to resolve - the best approach depends on your actual research question, the variance in your various variables within the various strata, etc.</p>\\n\\n<p>I don\\'t think your strategy does anything about age and gender balance as you say.  You could introduce these into your sampling strategy someway (eg by setting quotas, if you are worried that interviewer bias is stopping them approaching people of particular age or gender types - but being careful to ensure that selection remains as random as possible and that you have not given interviewers <em>more</em> discretion in who they choose) and as weights after the sampling is over.</p>\\n\\n<p>Can I recommend Thomas Lumley\\'s survey package in R which has <a href=\"http://faculty.washington.edu/tlumley/survey/\" rel=\"nofollow\">a good website</a>.  However, I think you will need to purchase his <a href=\"http://faculty.washington.edu/tlumley/svybook/\" rel=\"nofollow\">book</a> (or a similar one) and read it carefully before you are really in a position to know how best to collect and analyse your data.  He has a good explanation of the issues you are asking about.  Of course, there are other good books on samples too, but his has the advantage of being linked to readily available free and powerful software.</p>\\n\\n<p>With the survey package R is an excellent tool for the sort of analysis you want to do.  SAS and Stata work well with complex surveys.  SPSS cannot work with surveys and weights properly unless you buy an expensive additional complex surveys module.  I wouldn\\'t even contemplate using something like Excel for analysing a survey with weights.</p>\\n',\n",
       " \"<p>Just a quick point:</p>\\n\\n<ul>\\n<li>The R package ecosystem is very big, and it's really up to the individual author, whether they intend to maintain backwards compatibility.</li>\\n<li>I personally haven't had any issues with base R packages changing in ways that led to issues of backwards compatibility. In general, this is one reason why I prefer to use base R packages.</li>\\n</ul>\\n\",\n",
       " '<p>The Burns reference that you are quoting seems to dividing the stochastic part into autocorrelation error, which is a byproduct of any time series analysis (and is systematic), vs. truly random error which is uncontrollable.</p>\\n\\n<p>-Ralph Winters </p>\\n',\n",
       " '<p>Everything you say is reasonable. Your binomial method works as whuber has said. Alternatively, you could add the probability of the first two being wrong, the first being wrong the second right and the third wrong, and the first right and the second and third wrong to get the same result: $P^2 + P(1-P)P + (1-P)P^2 = 3P^2-2P^3$.   </p>\\n\\n<p>I personally would not try to solve a cubic equation that way using complex numbers.  If you have a particular value of $X$, you could find $P$ through numerical methods, or using a look-table, or very approximately using a graph like the following.  </p>\\n\\n<p><img src=\"http://i.stack.imgur.com/m2s3I.png\" alt=\"cubic solution\"></p>\\n',\n",
       " '<p>from a deck of 52 cards, 9 cards are drawn at random without replacement. If X and Y are the number of hearts and diamonds respectively. What is the joint probability of X and Y and also what is the probability of X being greater than Y?</p>\\n',\n",
       " '<p>The concepts of slowly varying, regular varying and second order regular varying functions are used in extreme value statistics to provide regularity conditions on the behavior of the tail of a distribution function to be able to prove theorems. They can be thought of as smoothness conditions for the tail at infinity. </p>\\n\\n<p>The concepts are crucial to extreme value statistics, where you need some assumptions about the tail of the distribution function. One could just assume that the distribution function was from a parametric class, a <a href=\"http://en.wikipedia.org/wiki/Fr%C3%A9chet_distribution\" rel=\"nofollow\">Frechet distribution</a>, for example, but this distribution may not fit the data, and one may only be interested in the tail of the distribution, in which case one would not want to make assumptions about the entire distribution function. </p>\\n\\n<p>Regular variation is used to give a semi-parametric class of distribution functions where we, for instance, know how the extremes behave according to the <a href=\"http://en.wikipedia.org/wiki/Fisher%E2%80%93Tippett%E2%80%93Gnedenko_theorem\" rel=\"nofollow\">Fisher-Tippett-Gnedenko Theorem</a> which classifies distributions into what is called the <em>domains of attraction</em> for one of the three <a href=\"http://en.wikipedia.org/wiki/Generalized_extreme_value_distribution\" rel=\"nofollow\">extreme value distributions</a>. </p>\\n\\n<p>The <em>tail index</em> can then be estimated, the classical estimator being the Hill estimator. More regularity, like second order regular variation, comes into the picture when we want to prove distributional results about the estimators. These conditions are technical are somewhat difficult to comprehend from an intuitive point of view, but they assure that the tail of the distribution function is \"sufficiently nice\". </p>\\n\\n<p>I good book I can recommend is <a href=\"http://books.google.com/books?id=catZCl17d7gC&amp;printsec=frontcover&amp;dq=extreme+value+analysis+an+introduction&amp;hl=en&amp;ei=Zp3yTe1mlL6wA-2FvboL&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=2&amp;ved=0CC8Q6AEwAQ#v=onepage&amp;q=regular%20variation&amp;f=false\" rel=\"nofollow\">Extreme Value Theory. An Introduction</a> by Laurens de Haan and Ana Ferreira. </p>\\n',\n",
       " '<p>Since the data are not continuous and certainly not close to being normally distributed a nonparametric paired test seems to be the answer.  My suggestion would be the Wilcoxon signed rank test.</p>\\n',\n",
       " '<p>Maybe <a href=\"http://www-958.ibm.com/software/data/cognos/manyeyes/\" rel=\"nofollow\">http://www-958.ibm.com/software/data/cognos/manyeyes/</a> is what you want. Beware that the data you upload is public though. Edit: Sorry, I see you asked for open source. My bad.</p>\\n',\n",
       " '<p>Actually, if you\\'re really positive you have the whole population, there\\'s even no need to go into statistics. Then you know exactly how big the difference is, and there is no reason whatsoever to test it any more. A classical mistake is using statistical significance as \"relevant\" significance. If you sampled the population, the difference is what it is.</p>\\n\\n<p>On the other hand, if you reformulate your hypothesis, then the candidates can be seen as a sample of possible candidates, which would allow for statistical testing. In this case, you\\'d test in general whether male and female differ on the test at hand.</p>\\n\\n<p>As ars said, you can use tests of multiple years and add time as a random factor. But if your interest really is in the differences between these candidates on this particular test, you cannot use the generalization and testing is senseless. </p>\\n',\n",
       " \"<p>I want to acknowledge from the beginning that I know relatively little about clustering.  However, I don't see the point of the procedure you describe.  If you think, for example, that first term vs. returning students might be different, why not include a covariate that indexes that?  Likewise if you think another feature of the students is relevant, you can include that as well.  If you are worried that the <em>relationship</em> between your primary predictor of interest and the success rate might differ, you could also include the interaction between that predictor and first term vs. returning, etc.  Logistic regression is well equipped to address these questions via including such terms in the model.  </p>\\n\\n<p>On the other hand, so long as you only cluster on these features, and do so first (without looking at the response), I don't see any problems arising.  I suspect this approach would be inefficient, with each model having lower power because it is fit on only a subset of the data, but I don't think it would bias the parameters or invalidate the tests.  So I suppose you could try this if you really want to.  </p>\\n\\n<p><strong>Update:</strong>  </p>\\n\\n<p>My guess is that it would be best (i.e., most efficient) to fit one model with all the data.  You could include some additional covariates (such as returning vs. not) beyond your primary interest, and a grouping indicator that you discovered via having run a cluster analysis beforehand.  However, if the covariates that went into the cluster analysis are also made available to the logistic regression model, I'm not sure if I can see what would be gained over just including all of the covariates in the LR model <em>without</em> the cluster indicator.  There may well be an advantage to this that I'm not familiar with, since I'm not expert in cluster analysis, but I don't know what it would be.  It seems to me that the CA would not generate additional information that wasn't already there in the covariates, and thus wouldn't add anything to the LR model.  You could try it; maybe I'm wrong.  But my guess is that you would just burn a few extra degrees of freedom.  </p>\\n\\n<p>A different approach would be to enter the cluster indicator into the LR model <em>instead of</em> the covariates on which it's based.  I doubt this would be beneficial.  The CA won't be perfect, any more than any other analysis ever is, and so moving from the original covariates to the derived cluster indicator is likely to entail some amount of information <em>loss</em>.  (Again, I don't know that, but I strongly suspect it's true.)  Again, you could try it both ways and compare as an academic exercise, although just trying a lot of stuff and settling on the outcome that looks best is frowned upon if you want to take your results seriously.  </p>\\n\\n<p>I don't want to just carp on cluster analyses.  There may be many benefits of them in general, and there may be a good use for them here.  However, as I understand your situation, I think just building a LR model with the covariates you think might be relevant is the way to go.  </p>\\n\",\n",
       " '<p>If the cells are not interrelated then the fact that they are in matrices rather than just equal length columns of numbers doesn\\'t matter, right?  </p>\\n\\n<p>Given they are counts, a suitable way of comparing them is probably the sum of squared differences, weighted by the inverse of the average of the two observations for each comparison cell.  </p>\\n\\n<p>$\\\\\\\\sum\\\\\\\\frac{(a_{i,j}-b_{i,j})^2}{(a_{i,j}+b_{i,j})/2}$</p>\\n\\n<p>But what does this tell you about \"how close\", except that smaller means closer?  Turning this statistic into something you can compare to any other number would depend on why you are doing this eg are there a range of candidate Bs trying to look like A.</p>\\n',\n",
       " \"<p>For part 1 of your question, I can't think of any real-world situation in which $F(X)=Y$, with no randomness allowed to Y, has been a useful model.  It's a nice candidate model and worth checking out in any particular situation but I think it would be easy to show that (in any dataset I've come across) that $F(X)$ would be so complex as to be implausible and of no use either for prediction or for explanation.</p>\\n\",\n",
       " '<p>I have several questions regarding the usual gaussianity (<em>broad normality</em>) assumptions in econometrics. Though people often check for normality (with apparently weak tests), I\\'ve seen just one example of \"gaussianity\" testing.</p>\\n\\n<blockquote>\\n  <p>1.Is the \"finite variance\" assumption the same as a gaussianity assumption? Ie., is it the same to assume that the variable follows any distribution in the family of Elliptically Symmetric distributions?\\n  2.If gaussianity is a requirement, how do you test for it?</p>\\n</blockquote>\\n\\n<p>The one example I know of doing some informal testing for gaussianity is NN Taleb\\'s <em>Errors, Robustness, and The Fourth Quadrant</em>. Here\\'s SSRN PDF version <a href=\"http://maint.ssrn.com/?abstract_id=1343042\" rel=\"nofollow\">http://maint.ssrn.com/?abstract_id=1343042</a> and here the technical part of the paper in a friendly html format <a href=\"http://www.fooledbyrandomness.com/EDGE/index.html\" rel=\"nofollow\">http://www.fooledbyrandomness.com/EDGE/index.html</a>.</p>\\n\\n<p>It that paper Taleb uses some measurements in a big number of financial time series trying to show that gaussianity is implausible.</p>\\n\\n<p>He does so by:</p>\\n\\n<ul>\\n<li>Trying to see if the data is consistent with the central limit theorem seeing if kurtosis converges when increasing data aggregation.</li>\\n<li>Trying to see if the data is consistent with either a gaussian decay in the conditional expectations of the variable or a power law (I think, page 8 of the PDF).</li>\\n<li>Trying to see a non gaussian incidence of rare events.</li>\\n</ul>\\n\\n<p>Finally, questions #3 and #4:</p>\\n\\n<blockquote>\\n  <p>3.Taleb performs his tests on data of a much higher frequency than what is commonly found in macroeconomics. What would the appropriate tests be for monthly data?\\n  4.Are there other necessary conditions for the usual econometric models besides finiteness of variance not typically tested for?</p>\\n</blockquote>\\n\\n<p>Please bear in mind that I\\'m a graduate student taking a fairly basic time series course, this is just for intellectual curiosity.</p>\\n\\n<p>Thanks!</p>\\n',\n",
       " '<p>In a linear system with Gaussian noise, the Kalman filter is optimal. In a system that is nonlinear, the Kalman filter can be used for state estimation, but the particle filter may give better results at the price of additional computational effort. In a system that has non-Gaussian noise, the Kalman filter is the optimal <em>linear</em> filter, but again the particle filter may perform better. The <a href=\"http://en.wikipedia.org/wiki/Kalman_filter#Unscented_Kalman_filter\">unscented Kalman filter</a> (UKF) provides a balance between the low computational effort of the Kalman filter and the high performance of the particle filter. </p>\\n\\n<p>The particle filter has some similarities with the UKF in that it transforms a set of points via known nonlinear equations and combines the results to estimate the mean and covariance of the state. However, in the particle filter the points are chosen randomly, whereas in the UKF the points are chosen on the basis of a specific algorithm <strong>*</strong>. Because of this, the number of points used in a particle filter generally needs to be much greater than the number of points in a UKF. Another difference between the two filters is that the estimation error in a UKF does not converge to zero in any sense, but the estimation error in a particle filter does converge to zero as the number of particles (and hence the computational effort) approaches infinity.</p>\\n\\n<p><strong>*</strong>\\xa0The unscented transformation is a method for calculating the statistics of a random variable which undergoes a nonlinear transformation and uses the intuition (which also applies to the particle filter) that it is easier to approximate a probability distribution than it is to approximate an arbitrary nonlinear function or transformation. See also <a href=\"http://stats.stackexchange.com/questions/1709/how-to-draw-a-probable-outcome-from-a-distribution/1717#1717\">this</a> as an example of how the points are chosen in UKF. </p>\\n',\n",
       " '<p>In general, I would say any book that has DOE (design of experiments) in the title would fit the bill (and there are MANY).</p>\\n\\n<p>My rule of thumb for such resource would be to start with the <a href=\"http://en.wikipedia.org/wiki/Design_of_experiments\" rel=\"nofollow\">wiki page</a>, in particular to your question, notice the <a href=\"http://en.wikipedia.org/wiki/Design_of_experiments#Principles_of_experimental_design.2C_following_Ronald_A._Fisher\" rel=\"nofollow\">Principles of experimental design, following Ronald A. Fisher</a></p>\\n\\n<p>But a more serious answer would be domain specific (clinical trial has a huge manual, but for a study on mice, you\\'d probably go with some other field-related book)</p>\\n',\n",
       " '<p>Error bars in general are to convince the plot reader that the differences she/he sees on the plot are statistically significant. In an approximation, you may imagine a small gaussian which $\\\\\\\\pm1\\\\\\\\sigma$ range is shown as this error bar -- \"visual integration\" of a product of two such gaussians is more-less a chance that the two values are really equal.</p>\\n\\n<p>In this particular case, one can see that both the difference between red and violet bar as well as gray and green ones are not too significant.</p>\\n',\n",
       " '<p>According to Microsoft Excel Help:</p>\\n\\n<blockquote>\\n  <p>VAR uses the following formula:<br>\\n  <img src=\"http://i.stack.imgur.com/ywIpP.gif\" alt=\"Formula\"></p>\\n  \\n  <p>where x is the sample mean\\n  AVERAGE(number1,number2,…) and n is\\n  the sample size.</p>\\n</blockquote>\\n\\n<p>Shouldn\\'t it be n, rather than n - 1, in the denominator?</p>\\n',\n",
       " '<p>You are totally right, this is a bad design in terms of selecting the clusters. And targeting the overall sample size with a large number of the SSUs sampled within a PSU is a bad target, too. I would first try to get an idea of the ICC for the variables of interest, and then design the sample to maximize the accuracy given the budget, which will likely lead to more clusters and fewer observations per cluster unless adding more clusters to the sample is prohibitively expensive (travel costs to a given center are prohibitively expensive as compared to the costs of individual interviews). DEFF = 4 really has to be justified by a defensible cost analysis.</p>\\n\\n<p>If you really need to achieve the cluster size of 120, then you would want to combine the small LTCFs together into a single sampling unit. You would probably want to take as dissimilar centers as possible when combining them, so as to minimize the design effect.</p>\\n\\n<p>I would also advise to use an appropriate <a href=\"http://www.hcp.med.harvard.edu/statistics/survey-soft/\" rel=\"nofollow\">complex survey software</a>, like Stata or R, rather than trying to implement the formula that may not be applicable, anyway. You will have unequal weights because (1) you sample PPS (which is not trivial at all in and of itself, see <a href=\"http://books.google.com/books/about/Sampling_with_unequal_probabilities.html?id=SyPvAAAAMAAJ\" rel=\"nofollow\">Brewer and Hanif (1983)</a> and <a href=\"http://rads.stackoverflow.com/amzn/click/041231780X\" rel=\"nofollow\">Thompson (1997)</a>); (2) you may have different probabilities of selection within the clusters for larger units, so the probability of selection of the cluster will get multiplied by 300/120. So the formulae from the book you cited would not apply (and relying on free books may be a bad idea, too; you must have some budget for survey design and post-processing, so you\\'d want to get a solid book on sampling and survey data analysis with this money; if you did not have that budget, then you must be trying to go so cheap that your survey may not meet meet the minimum quality criteria).</p>\\n',\n",
       " '<p>There seem to be several options available for working with Gaussian Mixture Models (GMMs) in Python. At first glance there are at least: </p>\\n\\n<ul>\\n<li>PyMix - <a href=\"http://www.pymix.org/pymix/index.php\">http://www.pymix.org/pymix/index.php</a> Tools for mixture modeling</li>\\n<li>PyEM - <a href=\"http://www.ar.media.kyoto-u.ac.jp/members/david/softwares/em/\">http://www.ar.media.kyoto-u.ac.jp/members/david/softwares/em/</a> which is part of the Scipy toolbox and seems to focus on GMMs <strong>Update: Now known as <a href=\"http://scikit-learn.github.com/scikit-learn.org/dev/modules/mixture.html\">sklearn.mixture</a></strong>\\n.</li>\\n<li>PyPR - <a href=\"http://pypr.sourceforge.net/\">http://pypr.sourceforge.net/</a> pattern recognition and related tools including GMMs</li>\\n</ul>\\n\\n<p>... and perhaps others. They all seem to provide the most basic needs for GMMs, including creating and sampling, parameter estimation, clustering, etc. </p>\\n\\n<p>What\\'s the difference between them, and how should one go about determining which is best suited for a particular need? </p>\\n\\n<p>Ref: <a href=\"http://www.scipy.org/Topical_Software\">http://www.scipy.org/Topical_Software</a></p>\\n',\n",
       " '<blockquote>\\n  <p><strong>Possible Duplicate:</strong><br>\\n  <a href=\"http://stats.stackexchange.com/questions/1194/practical-thoughts-on-explanatory-vs-predictive-modeling\">Practical thoughts on explanatory vs. predictive modeling</a>  </p>\\n</blockquote>\\n\\n\\n\\n<p>This question has been bugging me for some time, and I was going to write a blog post about it. However, I think it is better left for discussion in this forum. </p>\\n\\n<p>Back in April, I attended a talk at the UMD Math Department Statistics group seminar series called \"To Explain or To Predict?\". The talk was given by <a href=\"http://galitshmueli.com/\" rel=\"nofollow\">Prof. Galit Shmueli</a> who teaches at UMD\\'s Smith Business School. Her talk was based on research she did for a paper titled <a href=\"http://www.citi.uconn.edu/cist07/5c.pdf\" rel=\"nofollow\">\"Predictive vs. Explanatory Modeling in IS Research\"</a>, and a follow up working paper titled <a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1351252\" rel=\"nofollow\">\"To Explain or To Predict?\"</a>. </p>\\n\\n<p>Dr. Shmueli\\'s argument is that the terms \"predictive\" and \"explanatory\" in a statistical modeling context have become conflated and that statistical literature lacks a thorough discussion of the differences. In the paper, she contrasts both and talks about their practical implications. I encourage you to read the papers. </p>\\n\\n<p>The question I\\'d like to pose to the practitioner community is: have you ever fallen into the trap of using one when meaning to use the other? I certainly have. How do you know which one to use? How do you define a predictive exercise <em>versus</em> an explanatory/descriptive one? It would be useful if you could talk about the specific application. </p>\\n',\n",
       " \"<p>Not sure, but I think the centroid-based suggestion by Gui11aume assumes that you are going to include each and every case in one of the clusters.  In practice I often find that, when one sets up criteria such as yours to define the clusters, only perhaps 50% to 80% of cases will make a good fit to any of these descriptions. (Why should a priori profiles apply to everyone?) So if it is acceptable to you and/or consistent with your knowledge of the situation to have some cases left out, consider using a series of DO IF and ELSE IF statements to assign <em>some</em> cases to clusters, something like the following.</p>\\n\\n<p>*For those high on all 5 dimensions.</p>\\n\\n<p>DO IF EmployeeDevelopment > [threshold] and Harmony > [threshold] and CustomerOrientation > [threshold] and SocialResponsibility > [threshold] and Innovation > [threshold].</p>\\n\\n<p>Compute Cluster = 1.</p>\\n\\n<p>*For those high only on Customer Orientation.</p>\\n\\n<p>ELSE IF EmployeeDevelopment &lt; [threshold] and Harmony &lt; [threshold] and CustomerOrientation > [threshold] and SocialResponsibility &lt; [threshold] and Innovation &lt; [threshold].</p>\\n\\n<p>Compute Cluster = 2.</p>\\n\\n<p>*Etc. for the other 2 clusters.</p>\\n\\n<p>END IF.</p>\\n\\n<p>EXE. </p>\\n\\n<p>You may need to adjust the structure of this syntax a little.  If it gives you problems, you can simplify using the following.  It's just that on a very large data set it will require more computing time. </p>\\n\\n<p>IF [cluster 1 criteria are met] cluster = 1.</p>\\n\\n<p>IF [cluster 2 criteria are met] cluster = 2.</p>\\n\\n<p>*Etc.</p>\\n\\n<p>EXE.</p>\\n\\n<p>With this framework, whatever command you include last will overwrite the earlier ones, so you'll need to be conscious of the order you choose.</p>\\n\",\n",
       " '<p>I like to think of principal component scores as \"basically meaningless\" until you actually give them some meaning.  Interpretting PC scores in terms of \"reality\" is a tricky business - and there can really be no unique way to do it.  It depends on what you know about the particular variables that are going into the PCA, and how they relate to each other in terms of interpretations.</p>\\n\\n<p>As far as the mathematics goes, I like to interpret PC scores as the co-ordinates of each point, with respect to the principal component axes.  So in the raw variables you have $\\\\\\\\bf{}x_i$ $=(x_{1i},x_{2i},\\\\\\\\dots,x_{pi})$ which is a \"point\" in p-dimensional space.  In these co-ordinates, this means along the $x_{1}$ axis the point is a distance $x_{1i}$ away from the origin.  Now a PCA is basically a different way to describe this \"point\" - with respect to its principal component axis, rather than the \"raw variable\" axis.  So we have $\\\\\\\\bf{}z_i$ $=(z_{1i},z_{2i},\\\\\\\\dots,z_{pi})=\\\\\\\\bf{}A(x_i-\\\\\\\\overline{x})$, where $\\\\\\\\bf{}A$ is the $p\\\\\\\\times p$ matrix of principal component weights (i.e. eigenvectors in each row), and $\\\\\\\\bf{}\\\\\\\\overline{x}$ is the \"centroid\" of the data (or mean vector of the data points).</p>\\n\\n<p>So you can think of the eigenvectors as describing where the \"straight lines\" which describe the PCs are.   Then the principal component scores describe where each data point lies on each straight line, relative to the \"centriod\" of the data.  You can also think of the PC scores in combination with the weights/eigenvectors as a series of rank 1 predictions for each of the original data points, which have the form:</p>\\n\\n<p>$$\\\\\\\\hat{x}_{ji}^{(k)}=\\\\\\\\overline{x}_j+z_{ki}A_{kj}$$</p>\\n\\n<p>Where $\\\\\\\\hat{x}_{ji}^{(k)}$ is the prediction for the $i$th observation, for the $j$th variable using the $k$th PC.</p>\\n',\n",
       " '<p>Assume I have 400 students (that\\'s in a big university) that have to do a computer science project, and that they have to work alone (no group of students). An example of project could be let \"implementing a fast fourier transform algorithm in fortran\"  (I know, that doesn\\'t sound sexy but that makes my question simpler). I am the correcter and I want to sent routines to check if there are groups of student that have proposed implementation that are \"too similar to be truly independently written\". </p>\\n\\n<p>This is unsupervised search for clusters. I think the question is more about which attributes to use rather than the which clustering algorithm to use. The first thing I would do is a letter by letter histogram. Ideally, since cheaters are smarter than that, I would eventually try well chosen random permutations of letters to see if a good matching of letter\\'s histogram (with permutation) exists. Also that those not explore the structure of the code, only the marginal distribution of letters... what solution do you have ? are there existing software or packages dedicated to that problem ? (actually in my old days computer science teachers claimed they had that type of tool, but I now suspect that they had something very simple)</p>\\n\\n<p>I guess lawyer from software developments have that type of issues also (not with 1000 students, but with 2 large codes... which makes things harder) ? </p>\\n',\n",
       " '<p>here is the best book I would recomend for the subject:\\n<a href=\"http://rads.stackoverflow.com/amzn/click/0131011715\" rel=\"nofollow\">http://www.amazon.com/Fuzzy-Sets-Logic-Theory-Applications/dp/0131011715</a></p>\\n\\n<p>Here is an easy to read book:\\n<a href=\"http://rads.stackoverflow.com/amzn/click/0671875353\" rel=\"nofollow\">http://www.amazon.com/Fuzzy-Logic-Revolutionary-Computer-Technology/dp/0671875353</a></p>\\n\\n<p>Besides here are a list of links that might help:\\n<a href=\"http://www.seattlerobotics.org/encoder/mar98/fuz/flindex.html\" rel=\"nofollow\">http://www.seattlerobotics.org/encoder/mar98/fuz/flindex.html</a>\\nhttp://www.fuzzy-logic.com/\\n<a href=\"http://videolectures.net/acai05_berthold_fl/\" rel=\"nofollow\">http://videolectures.net/acai05_berthold_fl/</a></p>\\n',\n",
       " '<blockquote>\\n  <p>correlation by itself is not of much use - so what \"IS\" the use?</p>\\n</blockquote>\\n\\n<p>Let me disagree with this phrase, correlation let to know the level of association between 2 variables. Then, it is useful when trying to explain relation between such variables. \\nOn the other hand, (as Macro wrote) correlation is not a necessary condition for causation, however, is enough to explain the level of association. Furthermore, you can test the independence of the variables, but correlation can give you another useful information, the coefficient of determination. </p>\\n\\n<p>Nevertheless, analyst must know the domain to be able to explain the kind of relation.</p>\\n',\n",
       " '<p>Cosma Shalizi\\'s online notes on his lecture course <a href=\"http://www.cscs.umich.edu/~crshalizi/weblog/cat_36-402.html\">Advanced Data Analysis from an Elementary Point of View</a> are quite good on this subject, looking at things from a perspective where interpolation and regression are two approaches to the same problem. I\\'d particularly draw your attention to the chapters on <a href=\"http://www.cscs.umich.edu/~crshalizi/weblog/862.html\">smoothing methods</a> and <a href=\"http://www.cscs.umich.edu/~crshalizi/weblog/870.html\">splines</a>.</p>\\n',\n",
       " \"<p>I am trying to test whether the maximum likelihood (ML) estimates of a \\nparameter from the the results of running 'optim' in R, are different. </p>\\n\\n<p>In my situation I have two point estimates and I would like to know if \\nthey are significantly different from each other.  From 'optim', I can \\npull out the hessian and extract the variance about a given estimate. I was thinking I could use a wald test to test for a significant difference between estimates, but online documentation is only for testing against a specified value with one variance estimate.  I have two values each with a variance estimate, and Its not clear how to pool the variance.  </p>\\n\\n<p>Are there any resources/ideas on how to perform an appropriate test on these ML parameter estimates? Perhaps a likelihood ratio test? </p>\\n\\n<p>Any help will be much appreciated! </p>\\n\\n<p>Cheers, \\nLouis</p>\\n\\n<p>UPDATE and Response:\\nLRT should be the way to go. But Im not sure how to code it.  Below is the code for the log likelihood:  ['c' is fixed and  c11, c12 c22 are the data]</p>\\n\\n<p>llhood2 &lt;- function(theta, c, c11,c12,c22){</p>\\n\\n<pre><code>s&lt;-theta[1]\\nd&lt;-theta[2]\\n\\nP11 = (1 - (s * (1 - c) * (1 - c + (2 * d * c)))) / (4 - (s * (1 + (2 * d))))\\nP22 = (1 - (s * c * ((2 * d * (1 - c)) + c))) / (4 - (s * (1 + (2 * d))))\\nP12 = (1 - P11 - P22)\\nlogL = c11*log(P11) + c12*log(P12) + c22*log(P22)\\n-logL\\n</code></pre>\\n\\n<p>}</p>\\n\\n<p>And then the optim function, using this equation as the likelihood, produces the parameter estimates for 's' and 'd', and the variance-covariance matrix.  That is for one environment. Then I estimate s and d again in the other environment with different data (c11, c12 c22) using the same equation. </p>\\n\\n<p>Id like to parameterize a variable of difference (well call it 'diff-s') between the two environments for, say, the 's' variable, within this likelihood equation. Not sure how though. then I could compare the model with diff set to zero versus diff=diff. Any advice?</p>\\n\",\n",
       " '<blockquote>\\n  <p><strong>Possible Duplicate:</strong><br>\\n  <a href=\"http://stats.stackexchange.com/questions/298/in-linear-regression-when-is-it-appropriate-to-use-the-log-of-an-independent-va\">In linear regression, when is it appropriate to use the log of an independent variable instead of the actual values?</a>  </p>\\n</blockquote>\\n\\n\\n\\n<p>I am running a series of multiple mediation models.  Each model includes one IV, two mediators, and a DV.  I’m using a macro created for SPSS (provided by Preacher and Hayes) that uses bootstrapping to examine the indirect effects of the mediators.  My DV’s are depression and social anxiety (each run in a separate model) and are both positively skewed.  Normally I would perform a log transformation on these variables, however, bootstrapping is a nonparametric resampling procedure that does not hold the assumption of normality of the sampling distribution.  Therefore, my question is:  Is it necessary to transform positively skewed DV’s even though I am using bootstrapping?     </p>\\n\\n<p>Note: I have run my models with the log transformed means vs. untransformed means for the DV’s and the pattern of results are essentially the same.  However, I would prefer to use the untransformed means as the unstandardized regression coefficients more strongly support my predictions.             </p>\\n',\n",
       " \"<p>This is not a matter of different types of probability.  It is really a matter of what you can use to estimate it.  In the case of the woman they want to predict the probability of an accident say over a 3 year period given personal characteristics.  The insurance company can look at people that have been insured and can compare those that had accidents over a 3 year period with those that don't.  Then with this data they can create a logistic regression model using characteristics such as age and gender.  The model is then applied with the characteristics for the woman in question to estimate the probability that she will have an accident in the next 3 years.  The cost of the premium will then depend on how high this estimated probability is.  The difference between the court and the insurance company is that the insurance company found that gender was useful to include in their model.  The court is saying that inspite of that they see using gender as being discriminatory against women.  So they are telling the insurance company that they cannot use gender in their model.  They would probably also rule against race as a variable to use in the model but would allow age.  Once gender is taken out the new model will give a different estimate of the probability for this woman and probably it would be lower which may mean a lower premium.</p>\\n\",\n",
       " '<p>I was reading <a href=\"http://en.wikipedia.org/wiki/Gaussian_process_regression\" rel=\"nofollow\">this wikipedia article</a> related to kriging . I didn\\'t understand the part when it says that</p>\\n\\n<p>Kriging computes the best linear unbiased estimator, $\\\\\\\\hat Z (x_0)$, of $Z(x_0)$ such that kriging variance of  is minimized with the unbiasedness condition. I didn\\'t get the derivation and also how the variance is minimized. Any suggestions?</p>\\n\\n<p>Specially, I didn\\'t get the part where applies minimized subject to unbiasedness condition.</p>\\n\\n<p>I think it should have been</p>\\n\\n<p>E[Z\\'(x0)-Z(x0)] instead of E[Z\\'(x)-Z(x)] isn\\'t it. \\' is equivalent to hat in the wiki article. Also I didn\\'t get how the kriging error is derived</p>\\n',\n",
       " '<p><strong>Model 1</strong></p>\\n\\n<p><strong>ASReml-R</strong></p>\\n\\n<pre><code>&gt; rcb.asr &lt;- asreml(yield~Variety, random=~idv(Rep), rcov=~idv(units), data=nin89, na.method.X=\"include\")\\n&gt; summary(rcb.asr)\\n$call\\nasreml(fixed = yield ~ Variety, random = ~idv(Rep), rcov = ~idv(units), \\n    data = nin89, na.method.X = \"include\")\\n\\n$loglik\\n[1] -454.4691\\n\\n$nedf\\n[1] 168\\n\\n$sigma\\n[1] 1\\n\\n$varcomp\\n                gamma component std.error  z.ratio constraint\\nRep!Rep.var  9.882911  9.882911  8.792823 1.123975   Positive\\nR!variance   1.000000  1.000000        NA       NA      Fixed\\nR!units.var 49.582368 49.582368  5.458839 9.082951   Positive\\n\\nattr(,\"class\")\\n[1] \"summary.asreml\"\\n&gt; summary(rcb0.asr)$varcomp\\n                gamma component std.error  z.ratio constraint\\nRep!Rep.var 0.1993231  9.882911  8.792829 1.123974   Positive\\nR!variance  1.0000000 49.582368  5.458839 9.082951   Positive\\n&gt; anova(rcb.asr)\\nWald tests for fixed effects\\n\\nResponse: yield\\n\\nTerms added sequentially; adjusted for those above\\n\\n              Df Sum of Sq Wald statistic Pr(Chisq)    \\n(Intercept)    1   242.054        242.054    &lt;2e-16 ***\\nVariety       55    48.152         48.152    0.7317    \\nresidual (MS)        1.000                             \\n---\\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 \\n&gt; coef(rcb.asr)$fixed\\n                    effect\\nVariety_ARAPAHOE    0.0000\\nVariety_BRULE      -3.3625\\nVariety_BUCKSKIN   -3.8750\\nVariety_CENTURA    -7.7875\\nVariety_CENTURK78   0.8625\\nVariety_CHEYENNE   -1.3750\\nVariety_CODY       -8.2250\\nVariety_COLT       -2.4375\\nVariety_GAGE       -4.9250\\nVariety_HOMESTEAD  -1.8000\\nVariety_KS831374   -5.3125\\nVariety_LANCER     -0.8750\\nVariety_LANCOTA    -2.8875\\nVariety_NE83404    -2.0500\\nVariety_NE83406    -5.1625\\nVariety_NE83407    -6.7500\\nVariety_NE83432    -9.7125\\nVariety_NE83498     0.6875\\nVariety_NE83T12    -7.8750\\nVariety_NE84557    -8.9125\\nVariety_NE85556    -3.0500\\nVariety_NE85623    -7.7125\\nVariety_NE86482    -5.1500\\nVariety_NE86501     1.5000\\nVariety_NE86503     3.2125\\nVariety_NE86507    -5.6500\\nVariety_NE86509    -2.5875\\nVariety_NE86527    -7.4250\\nVariety_NE86582    -4.9000\\nVariety_NE86606     0.3250\\nVariety_NE86607    -0.1125\\nVariety_NE86T666   -7.9000\\nVariety_NE87403    -4.3125\\nVariety_NE87408    -3.1375\\nVariety_NE87409    -8.0625\\nVariety_NE87446    -1.7625\\nVariety_NE87451    -4.8250\\nVariety_NE87457    -5.5250\\nVariety_NE87463    -3.5250\\nVariety_NE87499    -9.0250\\nVariety_NE87512    -6.1875\\nVariety_NE87513    -2.6250\\nVariety_NE87522    -4.4375\\nVariety_NE87612    -7.6375\\nVariety_NE87613    -0.0375\\nVariety_NE87615    -3.7500\\nVariety_NE87619     1.8250\\nVariety_NE87627    -6.2125\\nVariety_NORKAN     -5.0250\\nVariety_REDLAND     1.0625\\nVariety_ROUGHRIDER -8.2500\\nVariety_SCOUT66    -1.9125\\nVariety_SIOUXLAND   0.6750\\nVariety_TAM107     -1.0375\\nVariety_TAM200     -8.2000\\nVariety_VONA       -5.8375\\n(Intercept)        29.4375\\n&gt; coef(rcb.asr)$random\\n          effect\\nRep_1  1.8795997\\nRep_2  2.8432658\\nRep_3 -0.8712738\\nRep_4 -3.8515916\\n</code></pre>\\n\\n<p><strong>nlme</strong></p>\\n\\n<p>See the trick</p>\\n\\n<pre><code>&gt; nin89$Int &lt;- 1\\n&gt; rcb.lme &lt;- lme(yield~Variety, random=list(Int=pdIdent(~Rep-1)), data=na.omit(nin89))\\n&gt; print(rcb.lme, corr=FALSE)\\nLinear mixed-effects model fit by REML\\n  Data: na.omit(nin89) \\n  Log-restricted-likelihood: -608.8508\\n  Fixed: yield ~ Variety \\n      (Intercept)      VarietyBRULE   VarietyBUCKSKIN    VarietyCENTURA \\n          29.4375           -3.3625           -3.8750           -7.7875 \\n VarietyCENTURK78   VarietyCHEYENNE       VarietyCODY       VarietyCOLT \\n           0.8625           -1.3750           -8.2250           -2.4375 \\n      VarietyGAGE  VarietyHOMESTEAD   VarietyKS831374     VarietyLANCER \\n          -4.9250           -1.8000           -5.3125           -0.8750 \\n   VarietyLANCOTA    VarietyNE83404    VarietyNE83406    VarietyNE83407 \\n          -2.8875           -2.0500           -5.1625           -6.7500 \\n   VarietyNE83432    VarietyNE83498    VarietyNE83T12    VarietyNE84557 \\n          -9.7125            0.6875           -7.8750           -8.9125 \\n   VarietyNE85556    VarietyNE85623    VarietyNE86482    VarietyNE86501 \\n          -3.0500           -7.7125           -5.1500            1.5000 \\n   VarietyNE86503    VarietyNE86507    VarietyNE86509    VarietyNE86527 \\n           3.2125           -5.6500           -2.5875           -7.4250 \\n   VarietyNE86582    VarietyNE86606    VarietyNE86607   VarietyNE86T666 \\n          -4.9000            0.3250           -0.1125           -7.9000 \\n   VarietyNE87403    VarietyNE87408    VarietyNE87409    VarietyNE87446 \\n          -4.3125           -3.1375           -8.0625           -1.7625 \\n   VarietyNE87451    VarietyNE87457    VarietyNE87463    VarietyNE87499 \\n          -4.8250           -5.5250           -3.5250           -9.0250 \\n   VarietyNE87512    VarietyNE87513    VarietyNE87522    VarietyNE87612 \\n          -6.1875           -2.6250           -4.4375           -7.6375 \\n   VarietyNE87613    VarietyNE87615    VarietyNE87619    VarietyNE87627 \\n          -0.0375           -3.7500            1.8250           -6.2125 \\n    VarietyNORKAN    VarietyREDLAND VarietyROUGHRIDER    VarietySCOUT66 \\n          -5.0250            1.0625           -8.2500           -1.9125 \\n VarietySIOUXLAND     VarietyTAM107     VarietyTAM200       VarietyVONA \\n           0.6750           -1.0375           -8.2000           -5.8375 \\n\\nRandom effects:\\n Formula: ~Rep - 1 | Int\\n Structure: Multiple of an Identity\\n           Rep1    Rep2    Rep3    Rep4 Residual\\nStdDev: 3.14371 3.14371 3.14371 3.14371 7.041475\\n\\nNumber of Observations: 224\\nNumber of Groups: 1 \\n&gt; anova(rcb.lme)\\n            numDF denDF   F-value p-value\\n(Intercept)     1   168 242.05402  &lt;.0001\\nVariety        55   168   0.87549  0.7121\\n&gt; fixef(rcb.lme)\\n      (Intercept)      VarietyBRULE   VarietyBUCKSKIN    VarietyCENTURA \\n          29.4375           -3.3625           -3.8750           -7.7875 \\n VarietyCENTURK78   VarietyCHEYENNE       VarietyCODY       VarietyCOLT \\n           0.8625           -1.3750           -8.2250           -2.4375 \\n      VarietyGAGE  VarietyHOMESTEAD   VarietyKS831374     VarietyLANCER \\n          -4.9250           -1.8000           -5.3125           -0.8750 \\n   VarietyLANCOTA    VarietyNE83404    VarietyNE83406    VarietyNE83407 \\n          -2.8875           -2.0500           -5.1625           -6.7500 \\n   VarietyNE83432    VarietyNE83498    VarietyNE83T12    VarietyNE84557 \\n          -9.7125            0.6875           -7.8750           -8.9125 \\n   VarietyNE85556    VarietyNE85623    VarietyNE86482    VarietyNE86501 \\n          -3.0500           -7.7125           -5.1500            1.5000 \\n   VarietyNE86503    VarietyNE86507    VarietyNE86509    VarietyNE86527 \\n           3.2125           -5.6500           -2.5875           -7.4250 \\n   VarietyNE86582    VarietyNE86606    VarietyNE86607   VarietyNE86T666 \\n          -4.9000            0.3250           -0.1125           -7.9000 \\n   VarietyNE87403    VarietyNE87408    VarietyNE87409    VarietyNE87446 \\n          -4.3125           -3.1375           -8.0625           -1.7625 \\n   VarietyNE87451    VarietyNE87457    VarietyNE87463    VarietyNE87499 \\n          -4.8250           -5.5250           -3.5250           -9.0250 \\n   VarietyNE87512    VarietyNE87513    VarietyNE87522    VarietyNE87612 \\n          -6.1875           -2.6250           -4.4375           -7.6375 \\n   VarietyNE87613    VarietyNE87615    VarietyNE87619    VarietyNE87627 \\n          -0.0375           -3.7500            1.8250           -6.2125 \\n    VarietyNORKAN    VarietyREDLAND VarietyROUGHRIDER    VarietySCOUT66 \\n          -5.0250            1.0625           -8.2500           -1.9125 \\n VarietySIOUXLAND     VarietyTAM107     VarietyTAM200       VarietyVONA \\n           0.6750           -1.0375           -8.2000           -5.8375 \\n&gt; ranef(rcb.lme)\\n    Rep1     Rep2       Rep3      Rep4\\n1 1.8796 2.843266 -0.8712739 -3.851592\\n</code></pre>\\n',\n",
       " '<p>This one is tricky and happened to me in my last project..I would explain it this way..lets say you had variables A and B which came out significant independently and by a business sense you thought that an interaction of A and B seems good. You included the interaction which came out to be significant but B lost its significance. You would explain your model initially by showing two results. The results would show that initially B was significant but when seen in light of A it lost its sheen. So B is a good variable but only when seen in light of various levels of A ( if A is a categorical variable)..Its like saying Obama is a good leader when seen in the light of its SEAL army..so Obama*seal will be a significant variable. But Obama when seen alone might not be as important..( no offense to Obama, just an example)</p>\\n',\n",
       " '<p>Nonlinear regression concerns models that are <em>inherently</em> nonlinear: that is, they cannot be expressed as a linear combination of parameters <em>b</em>.  It is practically the same thing to say that a nonlinear model cannot be put into the form Y = X*b + e after a preliminary mathematical re-expression of X, Y, or both.  For example, Y = log(X)*b + e and Y = exp(X*b + e) are both <em>linear</em> whereas Y = exp(X*b) + e and Y = log(X+b) + e are <em>nonlinear</em>.</p>\\n\\n<p>(As usual, Y is a dependent variable (or vector thereof), X is a vector of independent variables, b is a set of parameters to be estimated, and e is random \"error\" with zero mean.) </p>\\n',\n",
       " '<p>The overall F test tells you if there is a significant difference between the groups but significance does not tell which groups differ.  To do that you need to follow the test up with contrast comparisons looking at the pairwise differences between the means.  You specify the contrasts and the program should compute the $p$-values for the corresponding $t$-tests.  You should do some adjustment to the $p$-value because you did more than one test.  A simple Bonferroni bound may be adequate.</p>\\n',\n",
       " '<p>Think about Y=A+B as random variable Y being explained by two new random variables A and B. why we do this? Maybe Y is complex but A and B are less complex. Anyhow, the portion of variance of Y is explained by those of A and B. var(Y) = var(A) + var (B) + 2cov(A,B). Application this to the linear regression is simple. Think of A being b0+b1X and B is e, then Y=b0+b1X+e. Portion of variance in Y is explained by the regression line, b0+b1X.</p>\\n\\n<p>We use \"proportion of variance\" term because we want to quantify how much regression line is useful to predict (or model) Y.</p>\\n',\n",
       " '<p>If 2 knife types have the exact same mean, but very different variances then you would still have information useful to classification, if you are seeing a cut that lies far from the mean relative to the small variance, but reasonble from the large variance then it seems much more likely to have come from the knife with the larger variance.  So focusing on differences is means when there are other differences is probably not the best approach.</p>\\n\\n<p>You should look into classification analysis, possibly K nearest neighbors methods, or a Bayesian approach (using either the distribution that you believe fits the data, or a smoothed approximation like a logspline estimate).</p>\\n',\n",
       " '<p>Which distributions have closed-form solutions for the maximum likelihood estimates of the parameters from a sample of independent observations? </p>\\n',\n",
       " '<p>Variable selection in your context is not recommended, in the absence of heavy penalization (e.g., using lasso or elastic net).  It will result in invalid standard errors, P-values, regression coefficients, etc.</p>\\n',\n",
       " '<p>So I am trying to further understand Lift and Gain charts as it applies to my employee turnover model (i.e. used CHAID in SPSS Modeler). For my data this means predicting number of people voluntarily leaving the company.  </p>\\n\\n<p>I have reviewed the below references and have the basics down regarding interpretation: what is plotted on the x and y axis and the ideal curve you are looking for. I even practiced constructing my own gains and lifts charts in Excel. </p>\\n\\n<p>But all the examples I have seen thus far are for a direct mail campaign. Now I want to know what this means for my data. Does it merely mean, in the case of the gains chart that if I sample top 10% of my data I can expect 40% of terms vs sampling top 60% of my data get\\'s 80% of terms? (please assume the 40% and 60% are the values). If so then what significance should I take away from that because I really don\\'t get it in the context of my turnover model?</p>\\n\\n<p>References:</p>\\n\\n<p><a href=\"http://stats.stackexchange.com/questions/17119/lift-measure-in-data-mining\">lift-measure-in-data-mining</a></p>\\n\\n<p><a href=\"http://stats.stackexchange.com/questions/31150/what-is-a-lift-chart\">what-is-a-lift-chart</a></p>\\n\\n<p><a href=\"http://www2.cs.uregina.ca/~dbd/cs831/notes/lift_chart/lift_chart.html\">http://www2.cs.uregina.ca/~dbd/cs831/notes/lift_chart/lift_chart.html</a></p>\\n',\n",
       " '<p>If your series oscillates with smooth cycles you may use a spectral approach to forecasting. If you are interested in one step ahead forecasts, use an adaptive model such as an ARIMA(0,1,1) model or exponential smoothing model which models the change from period to period.</p>\\n',\n",
       " \"<p>In the GBM package one is supposed to be able to provide interaction.depth>2, which means higher-order interactions between features. \\nHowever, the resulting trees (as seen by pretty.gbm.tree) never show such interactions (and indeed - each row corresponds to just a single feature).</p>\\n\\n<p>I'm not even sure anymore that the package actually supports depth>2....</p>\\n\\n<p>Does anybody have any idea?</p>\\n\",\n",
       " '<p>I want to graphically show the change in relative risk along with confidence limits over 17 years. </p>\\n\\n<p>Can I use a forest plot without meta-analysis because it will have the added advantage to tabulate Z-values and p-values also? Here I want to show the progressive change in the relative risk and NOT meta-analysis. </p>\\n\\n<p>Or is there a better alternative for the graphic presentation? </p>\\n',\n",
       " '<p>It\\'s worth remembering that information visualisation isn\\'t some island cut off from all other forms of visual communication. If you want to produce work based on evidence based princples, I\\'d argue it\\'s best to look where the evidence is strongest. </p>\\n\\n<p>I\\'ve read specific research on data visualisation techniques, and general research in cognitive science and in general design research, and I find that thinking through how the more powerful, more thorough general research applies to each brief and each element used is often more effective and useful than trying to apply the narrowly applied field-specific research which often suffers from small samples, weak research techniques, narrow investigation and/or deeply ingrained assumptions. </p>\\n\\n<p>There are two excellent books I recommend as an introduction, one with the science as a starting point, one with general principles as a starting point, bringing in evidence:</p>\\n\\n<ul>\\n<li><a href=\"http://books.google.co.uk/books/about/Vision_science.html?id=IEl4QgAACAAJ&amp;redir_esc=y\">Vision Science by Steve Palmer</a>. It\\'s a beast, and as a student I nearly gave myself a back injury on the few occasions I was foolish enough to carry it in a backpack, but it\\'s also possibly the best science textbook I\\'ve ever seen, and a great example of crisp visual and verbal communciation itself. I went through it recently to label out the chapters with content directly relevant to my work in visualisation and information design, expecting to only label a few: I ended up labelling every chapter except one.  </li>\\n<li><a href=\"http://books.google.co.uk/books/about/Universal_principles_of_design.html?id=l0QPECGQySYC&amp;redir_esc=y\">Universal Principles of Design by Rockport Press</a>. A very ambitious and useful book which crunches cognitive science research with case studies and examples from across all branches of design into a series of awesomely clear and straight to the point double-page spreads, each covering one established, evidence based and practical principle, with practical suggestions, worked examples and suggested further reading. Very stimulating, so long as you think of it as a list of tools with suggested uses not a list of rules.</li>\\n</ul>\\n\\n<p>The only downside is, this approach takes more thinking to see how such principles are applicable. If you\\'re looking for a list of arbitrary rules, as many in the data vis community seem to be, I\\'d say there isn\\'t one and never will be except where people make massive unjustified assumptions and generalisations, or make things up. The better quality applied research is useful, but it helps to have a solid framework which it can slot into. </p>\\n\\n<p>Most of Tufte\\'s general principles such as data-ink and chart-junk can be traced back to solid general principles such as signal-noise ratios, figure-ground, attenuation, and others - but on the route to becoming field-specific and prescriptive, they have been combined with hefty assumptions and generalisations about your objectives and audience that turn them into blunt tools. Many of the apparent contradictions and debates in the applied research aren\\'t contradictions at all if you take a step back, take context into account and work through from the underlying core principles and the particular features of each case.</p>\\n',\n",
       " \"<p>Repeatedly rolling a six sided die four times and summing the highest three results gives you a distribution with what mean and standard deviation?</p>\\n\\n<p>I've only taken AP statistics, but I would like to learn how to do this.</p>\\n\",\n",
       " '<p>I have posted a similar question about the same problem, having been suggested to use a polynomial Robust Linear Model, which worked fine for most cases, as can be seen here:</p>\\n\\n<p><a href=\"http://stats.stackexchange.com/questions/43086/non-algebric-curve-fitting-along-weighted-pointcloud-if-possible-using-python\">Non-algebric curve-fitting along weighted pointcloud (if possible using python)</a></p>\\n\\n<p>But since then I have done some research, and I think a Non-Parametric regression model might be a better choice, since the model must fit locally and the result should respect some conditions which are not parametric.</p>\\n\\n<p>The problem statement is this:</p>\\n\\n<p>\"Given a data set consisting of a set of coordinates in the form (positionX, positionY, weight), representing candidate locations of simmetry points of a human back surface, \\'weight\\' being a representation of fitness (bilateral symmetry according to some symmetry function) of each point, find the curve that most likely represent the midline of the back, possibly misaligned due to postural inadequacies, considering that:</p>\\n\\n<ol>\\n<li>The line must run along the full height of the dataset;</li>\\n<li>For each Y (independent variable) there should correspond X (dependent variable) representing the most probable position of the midline;</li>\\n<li>There is coupling between successive X positions, that is, X is not random, there are not discontinuities, for the back midline \"in the real life\" doesn\\'t have discontinuities, sharp corners or points of high curvature. Therefore, the resulting curve should be reasonably \"smooth and well-behaved\".</li>\\n<li>The method should \"bridge\" two regions with good fit separated by a region with more locally scattered points.</li>\\n<li>\"when in doubt\", the midline tends to run along the vertical middle of the bounding box of the dataset.</li>\\n</ol>\\n\\n<p>I have found two articles in Wikipedia that seem to be applyable to this problem (<a href=\"http://en.wikipedia.org/wiki/Kernel_regression\" rel=\"nofollow\">Kernel Regression</a> and <a href=\"http://en.wikipedia.org/wiki/RANSAC\" rel=\"nofollow\">RANSAC</a>), but my current knowledge is not enough (mathematical and statistical notation, programming) in not enough to solve the problem on my own. Here are two representative images found in those articles, that resemble the conditions of my problem:</p>\\n\\n<p>RANSAC algorithm with a linear model:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/KKM1u.png\" alt=\"RANSAC algorithm with a linear model\"></p>\\n\\n<p>Kernel regression:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/Kpz9q.png\" alt=\"Kernel regression\"></p>\\n\\n<p>Also, I want to provide my own data:</p>\\n\\n<p>Sample unordered set of (x,y,weight) coordinates:</p>\\n\\n<pre><code>[[ -0.7898176   -3.35201728   4.36142086]\\n [  2.99221402  -3.35201728   1.11907575]\\n [  6.97475149  -3.35201728   2.4320322 ]\\n [ -4.82443609  -2.35201728   0.6479064 ]\\n [ -1.32418909  -2.35201728   1.88004944]\\n [  0.07067882  -2.35201728   1.10982834]\\n [  3.09169448  -2.35201728   1.8557436 ]\\n [  7.10399403  -2.35201728   2.03906224]\\n [ -3.07207606  -1.35201728   0.35500973]\\n [  2.63202993  -1.35201728   5.32397834]\\n [  5.19884868  -1.35201728   1.63816326]\\n [  7.65721835  -1.35201728   1.13843392]\\n [  2.48172754  -0.35201728   6.65584512]\\n [  6.0905911   -0.35201728   1.15552652]\\n [  8.62497546  -0.35201728   0.30407144]\\n [ -4.7300089    0.64798272   0.31481496]\\n [ -3.03274093   0.64798272   0.95337568]\\n [  2.19653614   0.64798272  10.3675204 ]\\n [  6.20384058   0.64798272   1.42106077]\\n [ -4.08636605   1.64798272   0.28875288]\\n [  2.03344989   1.64798272  13.04648211]\\n [ -4.11717795   2.64798272   0.39713141]\\n [  1.93304283   2.64798272  10.41313242]\\n [ -4.37994815   3.64798272   0.84588643]\\n [  1.66081408   3.64798272  14.96380955]\\n [ -4.19024027   4.64798272   0.73216113]\\n [  1.60252433   4.64798272  14.72419286]\\n [  6.77837359   4.64798272   0.6186005 ]\\n [ -4.14362668   5.64798272   0.93673165]\\n [  1.55372968   5.64798272  12.9421123 ]\\n [ -4.62223541   6.64798272   0.6510101 ]\\n [  1.527865     6.64798272  10.80209351]\\n [  6.86820685   6.64798272   0.82550801]\\n [ -4.68259732   7.64798272   0.45321369]\\n [  1.36167494   7.64798272   6.45338514]\\n [ -5.19205787   8.64798272   0.23935013]\\n [  1.21003466   8.64798272  10.13528877]\\n [  7.6689546    8.64798272   0.32421776]\\n [ -5.36436818   9.64798272   0.79809416]\\n [  1.26248534   9.64798272   7.67036253]\\n [  7.35472418   9.64798272   0.92555691]\\n [ -5.61723652  10.64798272   0.4741007 ]\\n [  1.23101086  10.64798272   7.97064105]\\n [ -7.83024735  11.64798272   0.47557318]\\n [  1.20348982  11.64798272   8.20694816]\\n [  1.14422758  12.64798272   9.26244889]\\n [  9.18164464  12.64798272   0.72428381]\\n [  1.0827069   13.64798272  10.08599118]\\n [  6.80116007  13.64798272   0.4571425 ]\\n [  9.384236    13.64798272   0.42399893]\\n [  1.04053491  14.64798272  10.48370805]\\n [  9.16197972  14.64798272   0.39930227]\\n [ -9.85958581  15.64798272   0.39524976]\\n [  0.9942501   15.64798272   8.39992264]\\n [  8.07642416  15.64798272   0.61480371]\\n [  9.55088151  15.64798272   0.54076473]\\n [ -7.13657331  16.64798272   0.32929172]\\n [  0.92606211  16.64798272   7.83597033]\\n [  8.74291069  16.64798272   0.74246827]\\n [ -7.20022443  17.64798272   0.52555351]\\n [  0.81344517  17.64798272   6.81654834]\\n [  8.52844624  17.64798272   0.70543711]\\n [ -6.97465178  18.64798272   1.04527813]\\n [  0.61959631  18.64798272  10.33529022]\\n [  5.733054    18.64798272   1.2309691 ]\\n [  8.14818453  18.64798272   1.37532423]\\n [ -6.82823664  19.64798272   2.0314052 ]\\n [  0.56391636  19.64798272  13.61447357]\\n [  5.79971126  19.64798272   0.30148347]\\n [  8.01499476  19.64798272   1.72465327]\\n [ -6.78504689  20.64798272   2.88657804]\\n [ -4.79580634  20.64798272   0.36201975]\\n [  0.548376    20.64798272   7.8414544 ]\\n [  7.62258506  20.64798272   1.52817905]\\n [-10.50328534  21.64798272   0.90358671]\\n [ -6.59976138  21.64798272   2.62980169]\\n [ -3.71180255  21.64798272   1.27094175]\\n [  0.5060743   21.64798272  11.06117677]\\n [  4.51983105  21.64798272   1.74626435]\\n [  7.50948795  21.64798272   3.46497629]\\n [ 11.10199877  21.64798272   1.78047269]\\n [-10.15444935  22.64798272   1.47486166]\\n [ -6.26274479  22.64798272   4.73707852]\\n [ -3.45440904  22.64798272   1.72516012]\\n [  0.52759064  22.64798272  12.58470433]\\n [  4.22258017  22.64798272   2.63827535]\\n [  7.03480033  22.64798272   3.506412  ]\\n [ 10.63560314  22.64798272   3.56076386]\\n [ -5.95693623  23.64798272   2.97403863]\\n [ -3.66261423  23.64798272   2.31667236]\\n [  0.52051366  23.64798272  12.5526344 ]\\n [  4.21083787  23.64798272   1.95794387]\\n [  6.82438636  23.64798272   4.77995659]\\n [ 10.18138299  23.64798272   5.21836205]\\n [ -9.94629932  24.64798272   0.4074823 ]\\n [ -5.74101948  24.64798272   2.60992238]\\n [  0.52987226  24.64798272  10.68846987]\\n [  6.29981921  24.64798272   3.56204471]\\n [  9.96431168  24.64798272   2.85079129]\\n [ -9.64229717  25.64798272   0.4503241 ]\\n [ -5.579063    25.64798272   0.64475469]\\n [  0.52053534  25.64798272  10.05046667]\\n [  5.79167815  25.64798272   0.92797027]\\n [ 10.05116919  25.64798272   2.52194933]\\n [ -8.55286247  26.64798272   0.94447148]\\n [  0.45065604  26.64798272  10.97432823]\\n [  5.50068393  26.64798272   2.39645232]\\n [ 10.08992273  26.64798272   2.77716257]\\n [-16.62381217  27.64798272   0.2021621 ]\\n [ -9.62146213  27.64798272   0.62245778]\\n [ -7.66905507  27.64798272   2.84466396]\\n [  0.38656111  27.64798272  10.74369366]\\n [  5.76925402  27.64798272   1.13362978]\\n [  9.83525197  27.64798272   1.18241147]\\n [-15.64874512  28.64798272   0.18279302]\\n [ -7.52932494  28.64798272   2.94012191]\\n [  0.32171219  28.64798272  10.73770466]\\n [  9.4062684   28.64798272   1.41714298]\\n [-12.71287717  29.64798272   0.70268073]\\n [ -7.59473877  29.64798272   2.16183026]\\n [  0.20748772  29.64798272  12.97312987]\\n [  3.92952496  29.64798272   1.54987681]\\n [  9.05148017  29.64798272   2.40563748]\\n [ 14.96021523  29.64798272   0.55258241]\\n [-12.14428813  30.64798272   0.36365363]\\n [ -7.12360666  30.64798272   2.54312163]\\n [  0.40594038  30.64798272  12.64839117]\\n [  4.59465757  30.64798272   1.23496581]\\n [  8.54333134  30.64798272   2.18912857]\\n [-10.6296531   31.64798272   1.4839259 ]\\n [ -7.09532763  31.64798272   2.0113838 ]\\n [  0.37037733  31.64798272  12.2071139 ]\\n [  3.01253349  31.64798272   3.01591777]\\n [  4.64523695  31.64798272   3.50267541]\\n [  8.39369696  31.64798272   2.53195817]\\n [ -7.07947026  32.64798272   1.01324147]\\n [  0.39269437  32.64798272   9.67368625]\\n [  8.58669997  32.64798272   1.00475646]\\n [ 12.02329114  32.64798272   0.50782399]\\n [-10.13060786  33.64798272   0.31475653]\\n [ -7.30360407  33.64798272   0.35065243]\\n [  0.49556923  33.64798272   9.66608818]\\n [ -5.37822311  34.64798272   0.38727401]\\n [  0.4958055   34.64798272   7.5415026 ]\\n [  6.07719006  34.64798272   0.63012453]\\n [ -4.64579055  35.64798272   0.39990249]\\n [  0.46323666  35.64798272   4.60449213]\\n [  4.72819312  35.64798272   0.98050594]\\n [ -4.62418372  36.64798272   0.64160709]\\n [  0.48866236  36.64798272   4.29331656]\\n [  5.06493722  36.64798272   0.59888608]\\n [  0.49730481  37.64798272   1.32828464]\\n [ -1.31849217  38.64798272   0.70780886]\\n [  1.70966455  38.64798272   0.88052135]\\n [  0.06305774  39.64798272   0.47366487]\\n [  2.13639356  39.64798272   0.67971461]\\n [ -0.84726354  40.64798272   0.63787522]\\n [  0.55723562  40.64798272   0.62855097]\\n [  2.22359779  40.64798272   0.33884894]\\n [  0.77309816  41.64798272   0.4605534 ]\\n [  0.56144565  42.64798272   0.43678788]]\\n</code></pre>\\n\\n<p>An image showing one very good fit (leftmost), and two fits that I consider not good, all from different subjects, obtained by fitting a Polynomial Robust Linear Model, acoording to the previously linked question: (the middle fit \"jumps\" out of the main midline, while the rightmost runs a long length parallel to the \"true\" midline instead of right through it)</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/PDRCI.png\" alt=\"enter image description here\"></p>\\n\\n<p>It seems to me that, being parametric and global, polynomial models end up being sensitive to interfering conditions present in the current problem domain.</p>\\n\\n<p>I thank for any help and in special to the fellow who answered the previous question, which took me out of a dead-end and presented me the amazing <code>statsmodels</code> Python module, which I would like to keep using if possible with a non-parametric regression approach.</p>\\n',\n",
       " '<p>I do not know anything about the particular software you have, but I can talk about methodology that can be used.  In regression you can look for outliers by testing for points that exert high influence on the regression parameters.  This is done by computing the influence function which essentially looks at how much the estimate changes when the data point in question is removed.  Normality can be checked graphically by generating qq plots of residuals and looking for departures from a straight line.  Goodness of fit tests on the residuals such as the Shapiro-Wilk test can formally test for normality.</p>\\n\\n<p>One way to look for change in variance would be to estimate the varaince of the residuals in one portion of the x space and test equality of variances by comparing it with the estimate is a different region of the x space.  An F test can be applied.</p>\\n',\n",
       " '<p>I assume you have data on the amount of each donation, not just the two totals for each variation. You don\\'t want to use <a href=\"http://en.wikipedia.org/wiki/Pearson%27s_chi-square_test\" rel=\"nofollow\">Pearson\\'s chi-square test</a>; that\\'s only appropriate for counts or proportions, not for amounts. You want an independent (unpaired) two-sample test for equality of location. The classic such \\'<a href=\"http://en.wikipedia.org/wiki/Parametric_statistics\" rel=\"nofollow\">parametric</a>\\' test is <a href=\"http://en.wikipedia.org/wiki/T-test#Independent_two-sample_t-test\" rel=\"nofollow\">Student\\'s <em>t</em>-test</a>. As it\\'s quite plausible that the variances differ as well as the means, so I\\'d suggest you use <strong><a href=\"http://en.wikipedia.org/wiki/Welch%27s_t-test\" rel=\"nofollow\">Welch\\'s <em>t</em>-test</a></strong>, which is a slight variation that allows for unequal variances. </p>\\n\\n<p>You <em>might</em> also consider \\'<a href=\"http://en.wikipedia.org/wiki/Non-parametric_statistics\" rel=\"nofollow\">non-parametric</a>\\' (\\'distribution-free\\') tests, of which the obvious one is the <a href=\"http://en.wikipedia.org/wiki/Mann-Whitney_U_test\" rel=\"nofollow\">Mann–Whitney U test</a>, but as your outcome of interest is clearly the difference in mean donation and your sample size is reasonably large, you\\'re almost certainly better off sticking with parametric tests such as the above <em>t</em>-tests that use the mean as the location parameter and so directly address the null hypothesis of equality of means of the two samples.</p>\\n\\n<p>By the way, don\\'t forget to include the people who saw either variation of the donation page but didn\\'t donate anything as donations of value zero.</p>\\n',\n",
       " '<p>Just a little bit of fun...</p>\\n\\n<h1>A Bayesian is one who, vaguely expecting a horse, and catching a glimpse of a donkey, strongly believes he has seen a mule.</h1>\\n\\n<p>From this site:</p>\\n\\n<p><a href=\"http://www2.isye.gatech.edu/~brani/isyebayes/jokes.html\">http://www2.isye.gatech.edu/~brani/isyebayes/jokes.html</a></p>\\n\\n<p>and from the same site, a nice essay...</p>\\n\\n<p>\"An Intuitive Explanation of Bayes\\' Theorem\"</p>\\n\\n<p><a href=\"http://yudkowsky.net/rational/bayes\">http://yudkowsky.net/rational/bayes</a></p>\\n',\n",
       " '<p>Look at <a href=\"http://www.cbs.nl/NR/rdonlyres/2E9912EB-534B-4A32-AD22-17A73402C083/0/trim3man.pdf\" rel=\"nofollow\">TRIM manual</a>. It is software for modelling animal population indices over multiple sites, but the described models can be used for your data too (you would just use one site). You can either use the described software or only get inspired by the models and do it yourself with glm in R, but beware of two main problems: <strong>overdispersion and serial correlation</strong> of the counts (which I don\\'t yet know how to solve in glm in R). If you don\\'t take the serial correlation into account, your standard errors will be underestimated.</p>\\n\\n<p>(Anyway, as your data might not be population data, the serial correlation need not to be present...)</p>\\n',\n",
       " '<p>I think its better to put like this. I want to use bootstrap for a test statistic. how can I apply the parametric bootstrap., and the non parametric bootstrap on the other hand. and what is the big difference of the two methods. it will be appriciated if you answered me with some examples.\\nthanks.</p>\\n',\n",
       " \"<p>I have some time series data of measurements taken at random intervals with dimensions >> measurements, with intervals anywhere between 15 seconds and a few minutes. Ideally I would like to have a continuous time estimate to give me the state of the system at any time between the beginning and end of measurements with some sort of confidence interval or statistical guarantee. </p>\\n\\n<p>I've looked into Kalman-type smoothers, but I dont have a model of state transitions, noise, or control-input. Furthermore I know my data are not of a linear system and they are quite noisy. It seems like setting all the matrices to the identity matrix would be okay to get a smoothed data, and would jive well with the random interval-ness of my data but I can't seem to find anywhere where other people have done the same (so it seems like it's probably not a good idea)</p>\\n\\n<p>I'm a bit agnostic and I'll accept approximations, such that my variables are independent of each other and the pretending the system is linear, although techniques like polynomial fitting and linear interpolation are not very satisfactory, as they don't deal with the noise at all. </p>\\n\\n<p>Useful facts about my data:\\nData is acquired at varying-intervals, dimensions >> data points, data is of a non-linear system and data is noisy</p>\\n\\n<p>What I would like:\\nAn estimate of system-state anywhere between the beginning and end of acquisition time</p>\\n\\n<p>Any hints?</p>\\n\",\n",
       " '<p>I am new to Machine Learning, and am trying to learn it on my own. Recently I was reading through <a href=\"http://www.cs.cmu.edu/~epxing/Class/10701/recitation/recitation3.pdf\" rel=\"nofollow\">some lecture notes</a> and had a basic question. </p>\\n\\n<p>Slide 13 says that \"Least Square Estimate is same as Maximum Likelihood Estimate under a Gaussian model\". It seems like it is something simple, but I am unable to see this. Can someone please explain what is going on here? I am interested in seeing the Math. </p>\\n\\n<p>I will later try to see the probabilistic viewpoint of Ridge and Lasso regression also, so if there are any suggestions that will help me, that will be much appreciated also. </p>\\n\\n<p>Thanks in advance.</p>\\n',\n",
       " \"<p>I ran an experiment where participants were randomly allocated to 2 conditions.\\nThey then completed 3 seperate tests with a binary pass/fail outcome for each test.</p>\\n\\n<p>So I have 3 binary (within-subject) test scores for each participant and 2 (between-subject) groups of participants.</p>\\n\\n<p>I need to determine:</p>\\n\\n<ol>\\n<li>Were pass rates were equivalent for the 3 tests?</li>\\n<li>Did the participant's condition influence pass rates overall?</li>\\n<li>Did condition interact with any particular test outcome?</li>\\n</ol>\\n\\n<p>If the DVs were continous this would be a straightforward repeated measures, mixed model ANOVA.</p>\\n\\n<p>I have been researching what I should do. I came across Cochran's Q test which seems a perfect way of determining if pass rates varied between the 3 tests but there does not seem to be any way of accounting for the two experimental conditions. Is there a similar test that allows for an additional IV?</p>\\n\\n<p>I'm using SPSS and have also looked at Generalized Estimating Equations (GEE), which seems like it might be helpful but it appears that there it is only possible to enter a single DV. If I use this procedure how do I enter the 3 repeated test scores?</p>\\n\",\n",
       " '<p>AIC and BIC both penalize models for complexity, but they impose different penalties.</p>\\n\\n<ul>\\n<li><p>AIC = $2k - 2 \\\\\\\\log(L)$  where $k$ is number of parameters and $L$ is likelihood.</p></li>\\n<li><p>BIC = $k*\\\\\\\\log(n) - 2\\\\\\\\log(L)$ where $n$ is number of subjects.</p></li>\\n</ul>\\n\\n<p>So, if $\\\\\\\\log(n) &gt; 2$, BIC penalizes more severely; this is pretty much always the case, since $e^2 = 7.3$ and it\\'s rare to have less than 8 subjects.</p>\\n\\n<p>As to which to use - there\\'s debate about it; see e.g. Burnham and Anderson, <em>2002 Model Selection and Multimodel Inference</em> (a good book); they also have an article about the comparison in <a href=\"http://www.sortie-nd.org/lme/Statistical%20Papers/Burnham_and_Anderson_2004_Multimodel_Inference.pdf\">Sociological Methods and Research</a></p>\\n',\n",
       " '<p>If you are looking for possible transformations to work with correlation, then a tool that has not been mentioned yet that may be useful is <code>ace</code> which can be found in the <code>acepack</code> package (and probably other packages as well).  This does an interative process of trying many different transformations (using smoothers) to find the transformations to maximize the correlation between a set of x variables and a y variable.  Plotting the transformations can then suggest meaningful transformations.</p>\\n',\n",
       " '<p>I have a graph that I did in SPSS </p>\\n\\n<p><img src=\"http://i.stack.imgur.com/FGTir.png\" alt=\"enter image description here\"></p>\\n\\n<p>I would like to replicate this in R. </p>\\n\\n<p>The data file is here: <a href=\"http://dl.dropbox.com/u/22681355/sendergraphR.csv\" rel=\"nofollow\">http://dl.dropbox.com/u/22681355/sendergraphR.csv</a></p>\\n\\n<p>UPDATE:</p>\\n\\n<p>I\\'ve figured out how to to it for the other graph: </p>\\n\\n<pre><code>graph2&lt;-read.csv(file=\"SendergraphR.csv\")\\nddr &lt;- recast(graph2,no.GREEN+no.RED+Senderidentity~variable,\\n    fun.aggregate=mean,id.var=c(\"no.GREEN\",\"no.RED\",\"Senderidentity\"))\\n\\nqplot(x=no.GREEN,y=Mean_Message,data=ddr,colour=Senderidentity, \\n    group=Senderidentity,geom=\"line\")+facet_wrap(~no.RED,ncol=1)\\n</code></pre>\\n\\n<p>Now what I would like to do is to separate this second graph into two columns one looking at the cases where the variable urn1 is blue and the other where its red: here\\'s a graph of how it should look: </p>\\n\\n<p><img src=\"http://i.stack.imgur.com/nX4vd.png\" alt=\"enter image description here\"></p>\\n\\n<p>I was thinking of doing it the following way:</p>\\n\\n<pre><code>ddr &lt;- recast(graph2,no.GREEN+no.RED+Senderidentity+urn1~variable,\\n    fun.aggregate=mean,id.var=c(\"no.GREEN\",\"no.RED\",\"Senderidentity\", \"urn1\"))\\nqplot(x=no.GREEN,y=Mean_Message,data=ddr,colour=Senderidentity, group=Senderidentity,geom=\"line\")+facet_wrap(~no.RED,ncol=1) +\\n    facet_wrap(~urn1, ncol=2)\\n</code></pre>\\n\\n<p>But this doesn\\'t seem to work. What am I doing wrong?</p>\\n',\n",
       " \"<p>I have two years of data which looks basically like this:</p>\\n\\n<p>Date   <strong><em>_</em>__<em></strong>    Violence Y/N? _</em>  Number of patients</p>\\n\\n<p>1/1/2008    <strong><em>_</em>___<em></strong>    0  <strong></em>__<em>_</em>__<em>_</em>____</strong> 11</p>\\n\\n<p>2/1/2008 <strong><em>_</em>__<em>_</em></strong>       0  <strong><em>_</em>__<em>_</em>__<em>_</em>__</strong> 11</p>\\n\\n<p>3/1/2008 <strong><em>_</em>____</strong><em>1  <strong></em>__<em>_</em>__<em>_</em>____</strong> 12</p>\\n\\n<p>4/1/2008 <strong><em>_</em>____</strong><em>0  <strong></em>__<em>_</em>__<em>_</em>____</strong> 12</p>\\n\\n<p>...</p>\\n\\n<p>31/12/2009_<strong><em>_</em>__</strong>      0_<strong><em>_</em>__<em>_</em>__<em>_</em>__</strong>                 14</p>\\n\\n<p>i.e. two years of observations, one per day, of a psychiatric ward, which indicate whether there was a violence incident on that day (1 is yes, 0 no) as well as the number of patients on the ward. The hypothesis that we wish to test is that more patients on the ward is associated with an increased probability of violence on the ward.</p>\\n\\n<p>We realise, of course, that we will have to adjust for the fact that when there are more patients on the ward, violence is more likely because there are just more of them- we are interested in whether each individual’s probability of violence goes up when there are more patients on the ward.</p>\\n\\n<p>I've seen several papers which just use logistic regression, but I think that is wrong because there is an autoregressive structure (although, looking at the autocorrelation function, it doesn’t get above .1 at any lag, although this is above the “significant” blue dashed line that R draws for me).</p>\\n\\n<p>Just to make things more complicated, I can if I wish to break down the results into individual patients, so the data would look just as it does above, except I would have the data for each patient, 1/1/2008, 2/1/2008 etc. and an ID code going down the side so the data would show the whole history of incidents for each patient separately (although not all patients are present for all days, not sure whether that matters).</p>\\n\\n<p>I would like to use lme4 in R to model the autoregressive structure within each patient, but some Googling comes up with the quotation “lme4 is not set up to deal with autoregressive structures”. Even if it were, I’m not sure I grasp how to write the code anyway.</p>\\n\\n<p>Just in case anyone notices, I asked a question like this a while ago, they are different datasets with different problems, although actually solving this problem will help with that one (someone suggested I use mixed methods previously, but this autoregression thing has made me unsure how to do this).</p>\\n\\n<p>So I’m a bit stuck and lost to be honest. Any help gratefully received!</p>\\n\",\n",
       " '<p>I\\'d suggest starting with\\n<a href=\"http://en.wikipedia.org/wiki/Cosine_similarity\">Cosine distance</a>,\\nnot Euclidean, for any data with most vectors nearly orthogonal,\\n$x \\\\\\\\cdot y \\\\\\\\approx$ 0.<br>\\nTo see why, look at\\n$|x - y|^2 = |x|^2 + |y|^2 - 2\\\\\\\\ x \\\\\\\\cdot y$.<br>\\nIf $x \\\\\\\\cdot y \\\\\\\\approx$ 0, this reduces to\\n$|x|^2 + |y|^2$: \\na crummy measure of distance, as Anony-Mousse points out.</p>\\n\\n<p>Cosine distance amounts to using $x / |x|$,\\nor projecting the data onto the surface of the unit sphere, so all $|x|$ = 1.\\nThen \\n$|x - y|^2 = 2 - 2\\\\\\\\ x \\\\\\\\cdot y$<br>\\na quite different and usually better metric than plain Euclidean.\\n$ x \\\\\\\\cdot y$ may be small, but it\\'s not masked by noisy $|x|^2 + |y|^2$.</p>\\n\\n<p>$x \\\\\\\\cdot y$ is mostly near 0 for sparse data.\\nFor example, if $x$ and $y$ each have 100 terms non-zero and 900 zeros,\\nthey\\'ll both be non-zero in only about 10 terms \\n(if the non-zero terms scatter randomly).</p>\\n\\n<p>Normalizing $x$ /= $|x|$ may be slow for sparse data; it\\'s fast in\\n<a href=\"http://scikit-learn.org/stable/developers/utilities.html#efficient-routines-for-sparse-matrices\">scikit-learn</a>.</p>\\n\\n<p>Summary: start with cosine distance, but don\\'t expect wonders on any old data.<br>\\nSuccessful metrics require evaluation, tuning, domain knowledge.</p>\\n',\n",
       " '<p>I have read and seen a lot of Parallel coordinates plots. Can someone answer the following set of questions:</p>\\n\\n<ol>\\n<li>What are parallel coordinates plots (PCP) in simple words, so that a layman can understand?</li>\\n<li>A mathematical explanation with some intuition if possible</li>\\n<li>When are PCP useful and when to use them?</li>\\n<li>When are PCP <em>not</em> useful and when they should be avoided?</li>\\n<li>Possible advantages and disadvantages of PCP</li>\\n</ol>\\n',\n",
       " \"<p>When running a logit/probit model for particular sets of outcomes on a set of participants (whether they did a certain behavior at least two times), how can one best control for differences when the individual observations began? In particular, one person may have started to participate 3 years ago, whereas another person may have joined just last year. I'm using STATA for the analysis and do know when the individuals began observations. Related to this question, what is the most effective way to refine the functional form of a the model?</p>\\n\",\n",
       " '<p>I don\\'t know of a single paper, but I think the current book with the best survey of methods applicable to $p\\\\\\\\gg n$ is still Friedman-Hastie-Tibshirani. It is very partial to shrinkage and lasso (I know from a common acquaintance that Vapnik was upset at the first edition of the book), but covers almost all common shrinkage methods and shows their connection to Boosting. Talking of Boosting, the <a href=\"ftp://ftp.stat.math.ethz.ch/Research-Reports/Other-Manuscripts/buhlmann/STS242.pdf\">survey of Buhlmann &amp; Hothorn</a> also shows the connection to shrinkage.</p>\\n\\n<p>My impression is that, while classification and regression can be analyzed using the same theoretical framework, testing for high-dimensional data is different, since it\\'s not used in conjunction with model selection procedures, but rather focuses on family-wise error rates. Not so sure about the best surveys there. Brad Efron has a ton of papers/surveys/book on <a href=\"http://stat.stanford.edu/~ckirby/brad/papers/\">his page</a>. Read them all and let me know the one I should really read...</p>\\n',\n",
       " \"<p>I am using the Kolmogorov&ndash;Smirnov two-sample test to compare distributions, and I noticed a $p$-value is frequently reported as the test statistic. How is this $p$-value determined? I know it's the probability of obtaining a result at least as large as the one obtained, but how is this $p$-value determined given this is a nonparametric test? That is, we can't assume Gaussian fluctuations in the distribution and compute the $p$-value using a $t$-test.</p>\\n\\n<p>Thanks!</p>\\n\",\n",
       " \"<p>In a text mining application, one simple approach is to use the $tf-idf$ heuristic to create vectors as compact sparse representations of the documents. This is fine for the batch setting, where the whole corpus is known a-priori, as the $idf$ requires the whole corpus</p>\\n\\n<p>$$\\\\\\\\n\\\\\\\\mathrm{idf}(t) = \\\\\\\\log \\\\\\\\frac{|D|}{|\\\\\\\\{d: t \\\\\\\\in d\\\\\\\\}|}\\\\\\\\n$$</p>\\n\\n<p>where $t$ is a term, $d$ is a document, $D$ is the document corpus, and $T$ (not shown) is the dictionary.</p>\\n\\n<p>However typically new documents are received over time. One option is to keep using the existing $idf$ until a certain number of new documents have been received, and the recalculate it. However this seems rather inefficient. Does anyone know of an incremental update scheme that (possibly approximately) converges to the value if all the data were seen in advance? Or alternatively is there another measure which captures the same notion but can be computed in an incremental fashion?</p>\\n\\n<p>There is also a related question of whether the $idf$ remains a good measure over time. Since the idf captures the notion of the corpus word frequency, it is conceivable that older documents in the corpus (say for example, that my corpus includes over 100 years of journal articles), as the frequencies of different words change over time. In this case it might actually be sensible to throw out older documents when new ones come in, in effect using a sliding window $idf$. Conceivably, one could also store all previous $idf$ vectors as new ones are calculated, and then if we wanted to retrieve documents from say 1920-1930, we could use the $idf$ calculated from documents in that date range. Does this approach make sense?</p>\\n\\n<p>Edit: There is a separate but related issue about the dictionary $T$. As time evolves, there will be new dictionary terms that didn't appear before, so $|T|$ will need to grow, and hence the length of the $idf$ vector. It seems like this wouldn't be a problem, as zeros could be appended to old $idf$ vectors.</p>\\n\",\n",
       " \"<p>If you're thinking mainly about an EW version of an MA model, I'm not sure why this would be necessary. In general MA models are more of a pain to estimate since they rely on MLE. Oftentimes, people would just throw in sufficient lags in an AR model to cover the MA structure. If the worry is about fitting a particular PACF structure, then it might be appropriate to estimate fractionally integrated models.</p>\\n\",\n",
       " \"<p>I've got a couple of time series obtained from a small number of replicates,where many variables (all of them continuous) are measured at various time points (longitudinal study). I was looking for a statistical test that could answer whether or not the data supports any type of trend (increasing, decreasing or even oscillations), or alternatively, the actual value stays constant and what I see in my individual or average time series is due to measurement error alone. I've had a look and found the Ljung–Box test - Is is appropriate to answer what I want? If not, what do you suggest?</p>\\n\",\n",
       " '<p>I tried to group a set of elements and to calculate the row means</p>\\n\\n<p>If my list has more than one element, it works fine:</p>\\n\\n<pre><code>tapply(colnames(myMA), c(1,1,1,2,2,2,3,3,4,4), list)    \\nmyMAmean &lt;- sapply(myList, function(x) rowMeans(myMA[,x]))\\n</code></pre>\\n\\n<p>However, in my data some of the rows are unique:</p>\\n\\n<pre><code>myList &lt;- tapply(colnames(myMA), c(1,1,1,2,2,2,3,3,4,5), list)\\nmyMAmean &lt;- sapply(myList, function(x) rowMeans(myMA[,x]))\\n</code></pre>\\n\\n<p>Notice the 4 &amp; 5 it makes them as unique so if I run this it says</p>\\n\\n<pre><code>\"Error in rowMeans(myMA[, x]) : \\n  \\'x\\' must be an array of at least two dimensions\"\\n</code></pre>\\n\\n<p>I don\\'t want to run a loop. Is there any solutions for this?</p>\\n',\n",
       " \"<p>Based on what I <em>think</em> you are asking:</p>\\n\\n<p>It depends on how you coded the variable. Trying an applied example:</p>\\n\\n<p>We have some continuous variable, lets say BMI, and an outcome, say time until first myocardial infarction. You want to find out what the hazard ratio is between a BMI of 25 as the referent group, and say 30, 40, 50 and 55.</p>\\n\\n<p>Your question is what happens if there's no events in group 40.</p>\\n\\n<p>If you used indicator variables to code these, my understanding is the hazard would be 0, not undefined. There is a probability, and it can be calculated - it just happens to be zero.</p>\\n\\n<p>If you used a continuous variable for BMI, it doesn't matter. Regression - including survival-analysis techniques involving regression, with smooth over areas of missing data. You can get an estimate for 40 as it interpolates between other areas where you <em>do</em> have events - say 39 and 41. This is a problem however if for some reason your group doesn't just have 0 events through random chance, but because there's something about them that means they <em>can't</em> have the outcome. For example, the magic heart attack fairy protects you if you have a BMI of exactly 40 (there are clearly more realistic examples of this problem). This is called non-positivity, and it is a threat to the validity of your inference. </p>\\n\",\n",
       " '<p>The \"split file\" function allows you to do this. It will calculate the correlation for every distinct level of the variable \"profession\". So you\\'d run your overall correlation first (without using split file), and then proceed with the split file, rerun your correlation, and there you go.</p>\\n',\n",
       " \"<p>You've heard now a lot about why it is not possible but on the other hand time series modelling depends <em>a lot</em> on how the data looks like and reacts. Probably in your case the solution may be easier - probably it isn't. What about posting a graph of the past year or so and we can tell you whether you need all the fancy stuff...</p>\\n\",\n",
       " '<p>I have three variables:</p>\\n\\n<ul>\\n<li>distance (continuous, variable range negative infinity to positive infinity)</li>\\n<li>isLand (discrete categorical/ Boolean, variable range 1 or 0)</li>\\n<li>occupants (discrete categorical, variable range 0-7)  </li>\\n</ul>\\n\\n<p>I want to answer the following statistical questions:</p>\\n\\n<ul>\\n<li>How to I compare distributions that have both categorical and continuous variable. For example, I like to determine if the data distribution of distance vs occupants varies depending on the value of isLand.</li>\\n<li>Given two of the three variables, can I predict the third using some equation?</li>\\n<li>How can I determine independence with more than two variables?</li>\\n</ul>\\n',\n",
       " '<p>To calculate an upper confidence limit on a Poisson parameter, a $\\\\\\\\chi^2$ distribution with probability $P$ and $${\\\\\\\\rm degrees \\\\\\\\ of \\\\\\\\ freedom} = 2X + 2$$ can be used (Nelson, 1982. Applied Life Data Analysis).  I have a sample of 10 observations. My question concerns the variable $X$: is that the total of the sample or the mean?</p>\\n',\n",
       " '<p>Given a $n$-dimensional multivariate normal distribution $X=(x_i) \\\\\\\\sim \\\\\\\\mathcal{N}(\\\\\\\\mu, \\\\\\\\Sigma)$ with mean $\\\\\\\\mu$ and covariance matrix $\\\\\\\\Sigma$, what is the probability that $\\\\\\\\forall j\\\\\\\\in {1,\\\\\\\\ldots,n}:x_1 \\\\\\\\geq x_j$?</p>\\n',\n",
       " '<p>As Michael notes, when comparing a subgroup to an overall group, researchers typically compare the subgroup to the subset of the overall group that does not include the subgroup.</p>\\n\\n<p>Think about it this way.</p>\\n\\n<p>If $p$ is the proportion that died, and $p-1$ is the proportion who did not die, and </p>\\n\\n<p>$$\\\\\\\\bar{X}_. = p\\\\\\\\bar{X}_d + (p-1)\\\\\\\\bar{X}_a$$</p>\\n\\n<p>where $\\\\\\\\bar{X}_.$ is the overall mean, $\\\\\\\\bar{X}_d$ is the mean of those that died, and $\\\\\\\\bar{X}_a$ is the mean of those that are still alive. Then if </p>\\n\\n<p>$$\\\\\\\\bar{X}_d \\\\\\\\neq \\\\\\\\bar{X}_a$$\\nthen </p>\\n\\n<p>$$\\\\\\\\bar{X}_d \\\\\\\\neq \\\\\\\\bar{X}_.$$</p>\\n\\n<p>Thus, researchers typically test the difference between the subgroup and the subset of the overall group that does not include the subgroup. This has the effect of showing that the subgroup differs from the overall group. It also allows you use conventional methods like an independent groups t-test.</p>\\n',\n",
       " '<p>If you can live with an estimate instead of the true median, the <a href=\"http://web.ipac.caltech.edu/staff/fmasci/home/statistics_refs/Remedian.pdf\" rel=\"nofollow\">Remedian Algorithm (PDF)</a> is one-pass with low storage requirements and well defined accuracy.</p>\\n\\n<blockquote>\\n  <p>The remedian with base b proceeds by computing medians of groups of b observations, and then medians of these medians, until only a single estimate remains. This method merely needs k  arrays of size b (where n =  b^k)...</p>\\n</blockquote>\\n',\n",
       " \"<p>I wasn't able to find one on Google, and was wondering if one exists...</p>\\n\",\n",
       " '<p>I\\'ve asked a few questions here before regarding my thesis. Although I try my best to follow-up on your suggestions, my statistical knowledge is limited but I try my utmost. Adding a predictive model to your thesis is not required (since it isn\\'t taught during the studies) but my thesis coach insists. So I\\'ve just let SPSS dictate the best-fitting ARIMA model for my thesis. </p>\\n\\n<p>Basically, I have taken some internet data (<code>hbVol0LN</code> is number of tweets, <code>hbBullQuality0</code> is the ratio for postive against negative tweets, etc.) for 100 companies over 103 days. Here, the dependent variable is the return of the stock of each of those 100 companies per day. I already performed an OLS (although it has been pointed out that this is not the ideal model for my research, it is accepted by my coach), but now I believe this ARIMA model should hold the predictive value of the data.</p>\\n\\n<p>It is very hard to find annotated ARIMA output online, or a paper which describes the output in a way I can understand. Could you perhaps give me some insights of what this output is telling me? Any help at all is greatly appreciated.</p>\\n\\n<p>If you\\'re having a hard time reading the graph, here\\'s the full-size one: <a href=\"http://i.stack.imgur.com/H42YP.png\" rel=\"nofollow\">http://i.stack.imgur.com/H42YP.png</a></p>\\n\\n<p>Again, I cannot express how frustrating it is for somebody who has hardly had to do any statistics during his studies, having to produce a predictive model. Therefore, really, any help is appreciated.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/H42YP.png\" alt=\"enter image description here\"></p>\\n',\n",
       " '<p>I have a dataset with a large number of Yes/No responses. <strong>Can</strong> I use principal components (PCA) or any other data reduction analyses (such as factor analysis) for this type of data? Please advise <strong>how</strong> I go about doing this using SPSS.</p>\\n',\n",
       " '<p>1) If two-sided p-values are being analyzed, you use the two-sided p-value in the calculation of the $Z_i$.  The two-sided p-value is $\\\\\\\\tilde{p}_i = p_i/2$.</p>\\n\\n<p>2) If left-tailed p-values are used, you use $1-p_i$ instead of $p_i$ in the calculation of the $Z_i$.</p>\\n',\n",
       " '<p>When describing a variable we typically summarise it using two measures: a measure of centre and a measure of spread. Common measures of centre include the mean, median and mode. Common measure of spread include the variance and interquartile range.</p>\\n\\n<p>The variance (represented by the Greek lowercase sigma raised to the power two) is commonly used when the mean is reported. The variance is the average squared deviation of variable. The deviation is calculated by subtracting the mean from each observation. This is squared because the sum would otherwise be zero and squaring removes this problem while maintaining the relative size of the deviations. The problem with using the variation as a measure of spread is that it is in squared units. For example if our variable of interest was height measured in inches then the variance would be reported in squared-inches which makes little sense. The standard deviation (represented by the Greek lowercase sigma) is the square-root of the variance and returns the measure of spread to the original units. This is much more intuitive and is therefore more popular than the variance.</p>\\n\\n<p>When using the standard deviation, one has to be careful of outliers as they will skew the standard deviation (and the mean) as they are not resistant measures of spread. A simple example will illustrate this property. The mean of my terrible cricket batting scores of 13, 14, 16, 23, 26, 28, 33, 39, and 61 is 28.11. If we consider 61 to be an outlier and deleted it, the mean would be 24. </p>\\n',\n",
       " \"<p>This is somewhat of a beginner's question, but how does one interpret an exp(B) result of 6.012 in a multinomial logistic regression model?</p>\\n\\n<p>1) is it 6.012-1.0 = 5.012 = 5012% increase in risk?</p>\\n\\n<p>or</p>\\n\\n<p>2) 6.012/(1+6.012) = 0.857 = 85.7% increase in risk?</p>\\n\\n<p>In case both alternatives are incorrect, can someone please mention the correct way?</p>\\n\\n<p>Ive searched many resources on the internet and I get to these two alternatives, and I am not entirely sure which one is correct.</p>\\n\",\n",
       " '<p>I have a csv data set like this.</p>\\n\\n<pre><code>Levels, Trial, Fail\\n1,100,1\\n2,200,3\\n3,300,4\\n</code></pre>\\n\\n<p>How do I test if the probability of fail or success is the same across all the levels? </p>\\n',\n",
       " '<p>Yes, Collaborative Filtering is the way to go. My first approach would be to create a user-item/group matrix by just ignoring the date and using a binary preference (1, if frequency > 0, else 0, as you said). </p>\\n\\n<p>Now given this information you can use the <a href=\"http://en.wikipedia.org/wiki/Jaccard_index\" rel=\"nofollow\">Jaccard Distance</a> and a metric based cluster algorithm of your choice which can handle discrete data (<a href=\"http://en.wikipedia.org/wiki/K-medoids\" rel=\"nofollow\">k-medoids</a> for a start) to cluster the item/group-vectors v (i.e. $v_i$ refers to whether user i likes the item/group represented by the vector or not).</p>\\n\\n<p><strong>Regarding the frequency</strong></p>\\n\\n<p>My personal experience from working with implicit preferences is, that multiple expressions does not mean anything as long as they don\\'t cost the user anything (so just clicking into the groups indeed costs nothing, but buying from groups would). So converting the frequences to binary preferences is fine. Depending on your type of application, it may be that frequency expresses how much time the user has spent in the particular group. In this case I\\'d try if the usage of the pure frequencies as preferences improves the result. In this case I\\'d also suggest to try the influence of normalization.</p>\\n\\n<p><strong>Regarding \"dislike or unkown ?\"</strong></p>\\n\\n<p>In the drafted approach above you don\\'t have to worry about the difference between \"dislike\" and \"don\\'t know about yet\". It is enough to state that a preference of 0 indicates that the user has not yet expressed a preference yet. It is ok to assume that a user in general visits groups he/she might like more often than groups which are less interesting.</p>\\n\\n<p><strong>Regarding last date</strong></p>\\n\\n<p>This is a hard one. You may use this information to devalue older preferences, but even this is questionable. Does the user really don\\'t like the group anymore or does he just spends his time elsewhere ? My first approach would be to create multiple clusterings for different timeframes (i.e. to model \"what is hot currently\") to see if differences emerge. But as I said, this is hard. You may find some inspiration <a href=\"http://stats.stackexchange.com/questions/23417/dynamic-recommender-systems\">Dynamic recommender systems</a> (or in the paper linked there).</p>\\n',\n",
       " \"<p>The reason that wikipedia says $\\\\\\\\sum_{i=1}^{N}X_{i}\\\\\\\\sim\\\\\\\\Gamma(N,\\\\\\\\theta)$ is actually because we assume a poisson process. That is, the waiting time for the next event is exponentially distributed $X_{i}\\\\\\\\sim EXP(\\\\\\\\theta)$. You can prove with the moment generating function that given $X_{i}\\\\\\\\sim EXP(\\\\\\\\theta)$ for $i=1...N$ IID sample, then $\\\\\\\\sum_{i=1}^{N}X_{i}\\\\\\\\sim\\\\\\\\Gamma(N,\\\\\\\\theta)$.</p>\\n\\n<p>Briefly put, if you assumed the $\\\\\\\\sum_{i=1}^{N}X_{i}\\\\\\\\sim\\\\\\\\Gamma(N,\\\\\\\\theta)$ model for your data, then you can also assume $X_{i}\\\\\\\\sim EXP(\\\\\\\\theta)$. (Meaning that's what they assumed originally...)</p>\\n\\n<p>One last thing, when you write $NX_{i}|\\\\\\\\sum_{i=1}^{N}X_{i}\\\\\\\\sim\\\\\\\\Gamma(N,\\\\\\\\frac{1}{N}\\\\\\\\sum_{i=1}^{N}X_{i})$, that is not correct. Instead, $NX_{i}|\\\\\\\\sum_{i=1}^{N}X_{i}\\\\\\\\sim UNIFORM(0,\\\\\\\\sum_{i=1}^{N}X_{i})$. To more or less convince yourself, think that given the value of $\\\\\\\\sum_{i=1}^{N}X_{i}=M$, then $NX_{i}$ cannot possibly be greater than $M$.</p>\\n\\n<p>I hope this helps!</p>\\n\\n<p>EDIT: I'm wrong about the uniform. See comments.</p>\\n\",\n",
       " '<p>You have time series data and one develops an equation for intra-day usage which may use either an auto-projective ARIMA model or a set of fixed dummies (23 in number) to predict hourly expectations. One has to be concerned with detecting \"unusual data\" so that your model/parameters reflect the main body of data and not being impacted by the exceptions. You might also be concerned with inter-day activity as different days of the week may have different effects. I have found that there are also interaction effects where the hourly distribution depends on the day-of-the-week. Additionally there may be known events/holidays that need to be accounted for. Upon building a suitable model , the residuals provide an estimate as to the expected variability yielding a \"safety stock\" which can be useful in guidance. The suggested statistical model for this is called a Transfer Function which is a hybrid between regression and ARIMA modelling.   </p>\\n',\n",
       " '<p>This is an immediate consequence of the normal approximation to the sampling distribution of the mean (proportion). Note that if $Z$ were a standard normal RV (with mean 0 and sd 1), then we would have:</p>\\n\\n<p>$$\\\\\\\\n\\\\\\\\mbox{P}\\\\\\\\left( -z_{\\\\\\\\alpha/2} &lt; Z &lt; z_{\\\\\\\\alpha/2} \\\\\\\\right) \\\\\\\\approx 1-\\\\\\\\alpha.\\\\\\\\n$$</p>\\n\\n<p>Substitute, then, the centered and scaled sample proportion for Z, i.e. let</p>\\n\\n<p>$$\\\\\\\\nZ = \\\\\\\\frac{\\\\\\\\hat{p} - p}{\\\\\\\\sqrt{p(1-p)/n}}\\\\\\\\n$$</p>\\n\\n<p>and this gives you the confidence interval they presented.</p>\\n',\n",
       " \"<p>How about this:</p>\\n\\n<pre><code># generate some dummy data\\nx1 &lt;- rnorm(50,10,5)\\nx2 &lt;- c(rep(0, 25), rep(1,25))\\ny &lt;- x1*2 + x2*5 + x1*x2*2 + rnorm(50,0,10)\\n\\n# create model with &amp; without covariate\\n\\nm1 &lt;- lm(y ~ x1 + x2 + x1*x2)\\nsummary(m1)\\n\\n#plot\\nplot(y~x1, col = (x2+1))\\nabline(a = coef(m1)[1], b = coef(m1)[2])\\nabline(a = coef(m1)[1]+coef(m1)[3], b = coef(m1)[2] +\\ncoef(m1)[4], col = 'red')\\n</code></pre>\\n\",\n",
       " '<p>Try measuring dependences of $(s, b)$ and $(s,c)$ for fixed $a$.</p>\\n\\n<p>For your case ($g$ and $h$ are monotonic real-valued functions) perhaps the best way is to use <a href=\"http://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient\" rel=\"nofollow\">Spearman\\'s rank correlation coefficient</a>.</p>\\n\\n<p>So you get the coefficients $\\\\\\\\rho_{s,b}(a)$ and $\\\\\\\\rho_{s,c}(a)$ as functions of $a$. If there is such switching you get $$\\\\\\\\max\\\\\\\\{|\\\\\\\\rho_{s,b}(a)|, |\\\\\\\\rho_{s,c}(a)|\\\\\\\\} = 1$$  for all $a$.</p>\\n\\n<p>However, as you know $f,g$ and $h$ beforehand, why not just measure how well does it fit, e.g. using\\n$$\\\\\\\\langle |s_{measured}-s_{expected}(a,b,c)|^2\\\\\\\\rangle$$\\nor Pearson\\'s correlation?</p>\\n',\n",
       " '<p>On <a href=\"http://www.chsbs.cmich.edu/fattah/courses/empirical/multicollinearity.html\" rel=\"nofollow\">this page</a>, I read about multiple ways to deal with multicollinearity. I determined that the last method suggested may be best for my purposes. I copied it below:</p>\\n\\n<blockquote>\\n  <p>Treat the common variance as a separate variable and decontaminate\\n  each covariate by regressing them on the others and using the\\n  residuals. That is, analyze the common variance as a separate\\n  variable.</p>\\n</blockquote>\\n\\n<p>I think that I get this conceptually but do not know how to do it in SPSS or R. If someone could explain how to do it in SPSS (preferably) or R, I\\'d appreciate it!</p>\\n',\n",
       " '<p>Like chl suggested, you need a supervised classifier. I\\'d suggest either linear SVM, or a decision tree such as C4.5. Both have libraries with a lot of documentation. Linear SVM would give you a \"score-like\" classifier, something like \"2 points if test1 is positive\", and \"3 points if test2 is positive\". The disease is present if overall score is above some threshold. A decision tree would give you a series of decisions, such as \"if test1 is positive, do test2, otherwise do test4\".</p>\\n\\n<p>As for feature selection, there are lots of different methods, but not too many standard ones. If you use a tree, then it\\'s likely that not all features will be used along any given path. So if you do the tests sequentially, you will need fewer than 15 for most patients. If you go with SVM, each test will get a score; you can dump features with smallest (in absolute value) scores and try to re-do the SVM with fewer features. Yet another feature selection method is by using mutual information, but that would depend on individual features having good mutual information with the class label.</p>\\n',\n",
       " '<p>I\\'m trying to work through Coursera\\'s probabilistic graphical models class (week 7: Baeysian prediction) and a have several questions. </p>\\n\\n<ol>\\n<li>In the Dirichlet distribution, I\\'m having difficulty trying to understand why there\\'s a &nbsp; -1  &nbsp; in theta\\'s exponent:\\n$$P(\\\\\\\\theta)=Dir(\\\\\\\\alpha_1, ..., \\\\\\\\alpha_k) = \\\\\\\\frac{1}{Z} \\\\\\\\cdot \\\\\\\\prod_{j} \\\\\\\\theta_{j}^{\\\\\\\\alpha_{j}-1}$$</li>\\n<li>How do you get from here:\\n$$P(X)=\\\\\\\\int_{\\\\\\\\theta}P(X|\\\\\\\\theta)P(\\\\\\\\theta)d\\\\\\\\theta$$\\nto here:\\n$$P(X=x^{i}|\\\\\\\\theta) = \\\\\\\\int_{\\\\\\\\theta} \\\\\\\\frac{1}{Z} \\\\\\\\cdot \\\\\\\\theta_{i} \\\\\\\\prod_{j} \\\\\\\\theta_{j}^{\\\\\\\\alpha_{j}-1}$$</li>\\n<li>Also, how do you step through the integration for the following?:\\n$$\\\\\\\\int_{\\\\\\\\theta} \\\\\\\\frac{1}{Z} \\\\\\\\cdot \\\\\\\\theta_{i} \\\\\\\\prod_{j} \\\\\\\\theta_{j}^{\\\\\\\\alpha_{j}-1} = { \\\\\\\\alpha_{i}\\\\\\\\over{\\\\\\\\sum_{j} \\\\\\\\alpha_{j}} }$$</li>\\n</ol>\\n\\n<p><a href=\"http://spark-university.s3.amazonaws.com/stanford-pgm/slides/5.2.4-Learn-BNparam-BayesianPrediction.pdf\" rel=\"nofollow\">These are the lecture notes</a>. My questions refer to the first slide.</p>\\n',\n",
       " '<p>The book <a href=\"http://rads.stackoverflow.com/amzn/click/0387848576\" rel=\"nofollow\">The Elements of Statistical Learning</a> has a lot of information on this.</p>\\n',\n",
       " '<p>I was at a meeting of the ASA (American Statistical Association) a couple years ago where Taleb talked about his \"fourth quadrant\" and it seemed his remarks were well received.  Taleb was much more careful in his language when addressing an auditorium of statisticians than he has been in his popular writing.  </p>\\n\\n<p>Some statisticians are offended by the provocative hyperbole in Taleb\\'s books, but when he states his ideas professionally there\\'s not too much to object to.  It\\'s hard to argue that one can confidently estimate the probability of rare events with little or no data, or that one should make high-stakes decisions on such estimates if they can at all be avoided.</p>\\n\\n<p>(Here\\'s a <a href=\"http://www.johndcook.com/blog/2008/08/07/black-swan-talk/\">blog post</a> I wrote about Taleb\\'s ASA talk shortly after the event.)</p>\\n',\n",
       " '<p>I have two time-series, <code>x</code> and <code>y</code>. I would like to prewhiten <code>x</code> by fitting an ARMA(p,q) (or in my case ARMA(1,1)) process and then use the coefficients to filter <code>y</code>. This seems like a pretty standard thing to want to do.  However, the <code>stats:::filter</code> function does only MA or AR filtering it looks like.  What is the appropriate way to do this? Also, should one use the <code>arima</code> function in R to do this or are there other ways?</p>\\n',\n",
       " \"<p>Repeated measures design is the traditional way to handle this, as drknexus mentions.  When doing that kind of analysis you have to aggregate to one score/condition/subject.  It's sensitive to violations of assumptions of sphericity and other issues.  However, the more modern technique is to use multi-level modelling or linear mixed effects.  Using this technique you do <em>not</em> aggregate the data.  There are several treatments of this available but I don't currently know the best basic tutorial.  Baayen (2008) Chapter 7 is good.  Pinheiro &amp; Bates (2000) is very good but from the sounds of things follow their advice in the intro and read the bits recommended for beginners.</p>\\n\\n<p>If you want to just get an ANOVA style result, assuming all of your data are in long format (one line / data point) and you have columns indicating subject, response (y), and a condition variable (x), you could try looking at something like this in R (make sure the lme4 package is installed).</p>\\n\\n<pre><code>library(lme4)\\ndat &lt;- read.table('myGreatData.txt', header = TRUE)\\nm &lt;- lmer( y ~ x + (1|subject), data = dat)\\nsummary(m)\\nanova(m)\\n</code></pre>\\n\\n<p>You could of course have many more conditions variable columns, perhaps interacting.  Then you might change the lmer command to something like...</p>\\n\\n<pre><code>m &lt;- lmer( y ~ x1 * x2 + (1|subject), data = dat)\\n</code></pre>\\n\\n<p>(BTW, I believe that not aggregating in repeated measures in order to increase power is a formal fallacy.  Anyone remember the name?)</p>\\n\",\n",
       " \"<p>Make sure that you're calculating the marginal effects correctly. For a continuous variable, the marginal effect is $\\\\\\\\frac{\\\\\\\\partial \\\\\\\\mathrm{Prob}(y_i=1|\\\\\\\\mathbf{\\\\\\\\bar{x}},\\\\\\\\mathbf{\\\\\\\\beta})}{\\\\\\\\partial x_j}$. These are typically rather small. However for a dummy variable, thinking about the partial derivative isn't very useful, so instead we define the marginal effect as $\\\\\\\\mathrm{Prob}(y_i=1|x_j=1,\\\\\\\\mathbf{\\\\\\\\bar{x}_{-j}},\\\\\\\\mathbf{\\\\\\\\beta})-\\\\\\\\mathrm{Prob}(y_i=1|x_j=0,\\\\\\\\mathbf{\\\\\\\\bar{x}_{-j}},\\\\\\\\mathbf{\\\\\\\\beta}))$</p>\\n\\n<p>In the context of the Probit regression model, the marginal effect for a continuous variable $x_j$ is\\n$\\\\\\\\phi(\\\\\\\\mathbf{\\\\\\\\bar{x}}&#39;\\\\\\\\mathbf{\\\\\\\\beta})\\\\\\\\beta_j$\\nOn the other hand, the marginal effect for a dummy variable $x_j$ is\\n$\\\\\\\\Phi(\\\\\\\\mathbf{\\\\\\\\bar{x}}_{j1}&#39;\\\\\\\\mathbf{\\\\\\\\beta})-\\\\\\\\Phi(\\\\\\\\mathbf{\\\\\\\\bar{x}}_{j0}&#39;\\\\\\\\mathbf{\\\\\\\\beta})$\\nwhere $\\\\\\\\mathbf{\\\\\\\\bar{x}}_{j1}$ is $\\\\\\\\mathbf{\\\\\\\\bar{x}}$ except the $j$'th element is replaced by 1 and $\\\\\\\\mathbf{\\\\\\\\bar{x}}_{j0}$ is the same except the $j$'th element is replaced by 0.</p>\\n\\n<p>If you accidentally assume that a dummy variable is continuous to calculate it's marginal effect, you'll probably get values that are much smaller than they should be or what you're expecting, so this may be what happened.</p>\\n\",\n",
       " '<p>Given the number of users of an application was 70 in total, it\\'s <a href=\"http://www.measuringusability.com/five-users.php\" rel=\"nofollow\">my understanding that research shows</a>:</p>\\n\\n<blockquote>\\n  <p>Five users is the number of users needed to detect approximately 85%\\n  of the problems in an interface, given that the probability a user\\n  would encounter a problem is about 31%.</p>\\n</blockquote>\\n\\n<p>Thing is that over time, it seems likely that a given user\\'s abilities to detect problems compared to other users would likely become more predictable. Is this true, and if not why? If it is true, what would be the best formula for knowing when to switch from random selection to optimal selection?</p>\\n\\n<p><strong>Note:</strong> Please note that this question is roughly understood by me, and it\\'s very possible that there\\'s a fundamental issue with the premise and/or gaps in the assumptions provided. If so, please comment and I\\'ll attempt to address any concerns, questions, etc. </p>\\n',\n",
       " \"<p>Does anybody know if there're common known disadvantages of a negbin regression? In my opinion it seems to fit every problem pretty good (measured with the estimated dispersionparameter). So why not always use it?</p>\\n\",\n",
       " '<p>Not strictly Bayesian Statistics as such, but I can strongly recommend <a href=\"http://rads.stackoverflow.com/amzn/click/1439824142\" rel=\"nofollow\">\"A First Course on Machine Learning\"</a> by Rogers and Girolami, which is essentially an introduction to Bayesian approaches to machine learning.  Its very well structured and clear and aimed at students without a strong mathematical background.  This means it is a pretty good first introduction to Bayesian ideas.  There is also MATLAB/OCTAVE code which is a nice feature.</p>\\n',\n",
       " '<p>In the book <a href=\"http://rads.stackoverflow.com/amzn/click/0470144483\" rel=\"nofollow\">Statistical Rules of Thumb</a> (explained by Dan Goldstein <a href=\"http://www.decisionsciencenews.com/2010/04/14/get-at-least-12-observations-before-making-a-confidence-interval/\" rel=\"nofollow\">here</a>), Gerald van Belle describes how the width of a Student\\'s t confidence interval decreases with more observations. Here is the useful chart: </p>\\n\\n<p><img src=\"http://i.stack.imgur.com/9lsqh.png\" alt=\"confidence interval asymptote\"></p>\\n\\n<p>His \"rule of thumb\" is to gather at least 12 data points for a given sample. But as you can see from the chart, this is not strictly necessary. I don\\'t fully understand your question, but the answer is that you <strong>can</strong> get a measure of the variation from 4 data points. In fact, you could get one from 3. </p>\\n\\n<p>However, the more data you include in each group, the more accurate your procedure will be. So, if you have the option, you might consider gathering enough data to get 6 observations per group, or splitting the data into only 3 groups instead of 4 (with the odd 6th datum assigned randomly to one of the them).  </p>\\n',\n",
       " '<p>May not really understand what you have done but </p>\\n\\n<blockquote>\\n  <p>for Run I am assuming that the RMSEP values for that run are correlated to some degree</p>\\n</blockquote>\\n\\n<p>Yes, that reflects how challenging the test set was in that run </p>\\n\\n<blockquote>\\n  <p>but are uncorrelated between runs</p>\\n</blockquote>\\n\\n<p>No, given the way you have sampled the test sets some will be more overlapped than others\\n(most definitely not independent replications)</p>\\n\\n<p>You would somehow have to model the dependency based on the overlap or design the assessment so the runs are independent. I would read the stats literature on cross-validation ;-) </p>\\n',\n",
       " '<p>As a seperate approach to the existing answers, shortly after I posted my first long list, <a href=\"http://ivpr.github.com/Weave/\" rel=\"nofollow\">WEAVE</a> emerged: an open source dedicated data visualisation suite. Here\\'s a brief write-up on WEAVE on the <a href=\"http://flowingdata.com/2012/02/07/weave-for-visualization-development/\" rel=\"nofollow\">leading data vis blog Flowing Data</a></p>\\n\\n<p>It\\'s wise to take a different approach to data visualisation depending on where you are in the process. The earlier you are - the more raw and unexplored your data - the more likely you are to benefit from pre-built, flexible, general purpose suites like WEAVE and it\\'s closed source commercial counterparts like Tableau and JMP - you can try things out quickly and painlessly to get to know the data and to figure out what lines of attack to take to get the most out of it. </p>\\n\\n<p>As you discover more about the data, your focus is likely to shift towards communication or \\'guided exploration\\' - more customised exploratory data visualisations designed based on the caveats, nuances and areas of interest you have now discovered in the data. This is where blank slate products like the programmatic vector drawing tools listed above come into their own.</p>\\n',\n",
       " '<p>I am not sure what kind of variable is being audited, so I give 2 alternatives:</p>\\n\\n<ol>\\n<li><p>To be able to compute the required sample size to give an acceptable estimate to a <strong>continuous variable</strong> (= given confidence interval) you have to know a few parameters: mean, standard deviation (and to be precise: population size). If you do not know these, you have to be able to give an accurate estimate to those (based on e.g. researches in the past). \\n$$n=\\\\\\\\left(\\\\\\\\frac{Z_{c}\\\\\\\\sigma}{E}\\\\\\\\right)^2,$$\\nwhere $n$ is sample size, $Z_{c}$ is choosen from standard normal distributon table based on $\\\\\\\\alpha$ and $\\\\\\\\sigma$ is the standard deviation.</p></li>\\n<li><p>I could image that the variable being examined is a <strong>discrete</strong> one, and the confidence interval shows that how many percent of the population is about to choose one category based on the sample (proportion). That way the required sample size could be computed easily with:$$n=p(1-p)\\\\\\\\left(\\\\\\\\frac{Z_{c}}{E}\\\\\\\\right)^2$$ where $n$ is sample size, $p$ is proportion in population, $Z_{c}$ is choosen from standard normal distributon table based on $\\\\\\\\alpha$, and $E$ is the margin of error.</p></li>\\n</ol>\\n\\n<p>Note: you can find a lot of online calculators also (<a href=\"http://www.rad.jhmi.edu/jeng/javarad/samplesize/\" rel=\"nofollow\">e.g.</a>). Worth reading <a href=\"http://www.ltcconline.net/greenl/courses/201/estimation/ciprop.htm\" rel=\"nofollow\">this article</a> also.</p>\\n',\n",
       " '<p>All $X$ are mutually independent and from normal distributions, each with its own mean and variance. If it\\'s easier, $P(X_1 \\\\\\\\geq X_i \\\\\\\\forall i \\\\\\\\in \\\\\\\\{1, ..., n\\\\\\\\})$ is fine although I suspect it\\'s the same. If it matters, $n$ is between 5 and 20.</p>\\n\\n<p>I found three similar questions:</p>\\n\\n<ol>\\n<li>The answer to <a href=\"http://stats.stackexchange.com/questions/29051/what-is-a-method-to-calculate-precisely-py-geq-x-y-leq-z-given-three-inde\">this one</a> is for only three random variables.</li>\\n<li>The answer to <a href=\"http://stats.stackexchange.com/questions/43685/which-is-largest-of-a-bunch-of-normally-distributed-random-variables\">this one</a> is for only mean 0 and variance 1.</li>\\n<li>I\\'m unsure if <a href=\"http://stats.stackexchange.com/questions/4138/what-is-the-probability-that-random-variable-x-1-is-maximum-of-random-vector\">this one</a> applies. If it does, I don\\'t know how to apply it. Its top answer is for three random variables.</li>\\n</ol>\\n\\n<p>(This is not homework.)</p>\\n',\n",
       " '<p>This is really a question about numerical integration because it reduces immediately to the problem of finding the mean distance between two line segments in the plane.</p>\\n\\n<p>Monte-Carlo integration would be wasteful and inefficient compared to other methods that are readily available, including</p>\\n\\n<ol>\\n<li><p>Direct analytic integration.  This is messy but can be done.</p></li>\\n<li><p>Numeric quadrature.  Adaptive methods will be accurate.</p></li>\\n<li><p>Analytic integration of the mean distance between a point and a line segment and numeric quadrature of that mean distance.</p></li>\\n<li><p>Discrete approximations, such as Riemann sums, the Trapezoidal Rule, etc.</p></li>\\n</ol>\\n\\n<p>Because the mean distance function in (3) is differentiable and varies relatively slowly, simple numerical methods such as Simpson\\'s Rule work well.  It\\'s best to do the numeric integration over the shorter of the two line segments.  In experiments I find that using the endpoints and midpoint in Simpson\\'s Rule typically gives better than 1% accuracy and using five equally spaced points is far better than that.  (The toughest cases occur where two sides are equally long, parallel, and close together.)  If you need to, you can do the analysis to estimate the Simpson\\'s Rule error and adaptively decrease the point spacing.</p>\\n\\n<p>For the original problem, the approximations sometimes overestimate and sometimes underestimate the mean distance, so we could expect some cancellation of errors.</p>\\n\\n<p>At any rate, you will definitely achieve 5% accuracy using method (3).  For an $n$ sided polygon it requires $n$ computations (each of constant cost) per vertex and $n-1$ computations per midpoint for a cost of $O(n^2)$.  The individual computations of mean point-segment distances require some basic arithmetic, four square roots, and two logarithms, so they\\'re quick and easy.  (<em>Mathematica</em> computes about 7,500 mean segment-segment distances per core per second.)</p>\\n\\n<p>If you want to avoid all but the simplest arithmetic, you can approximate each segment-segment integral with a double Simpson\\'s Rule pass, one for each segment (method 4).  With three division points (requiring the computation of nine distances) you will achieve 5% accuracy.  With five division points (25 distances) you will be better than 1% accurate (usually better than 0.05%) except for extremely close parallel segments.  In <em>Mathematica,</em> with its interpreted overhead, this technique actually is only half as fast as method (3).</p>\\n\\n<hr>\\n\\n<p><img src=\"http://i.stack.imgur.com/KNPjw.png\" alt=\"Error plot\"></p>\\n\\n<p>This plot shows (negative) log (base 10) relative errors for 1000 segments randomly distributed near the endpoint of a unit segment (which is the larger of the two).  E.g., a value of 2 is a 1% error; a value of 6 is a 0.0001% error.  This is a severe test, because the worst performances occur when the segments are crossing or close to doing so, situations that are impossible or unlikely in convex polygons.  (Note that the smallest possible mean distance is 1/4 (for an infinitesimally short segment near the middle of the unit segment).)</p>\\n\\n<p>Where the approximation is high, the error is plotted on the right, and where the approximation is low, the error is plotted on the left.  Blue dots are the three-point by three-point Simpson\\'s Rule calculation (method 4) and red dots are the three-point Simpson\\'s Rule calculation (method 3).</p>\\n\\n<p>Typically method 3 (red) has much better than 2% accuracy and is better than method 4 (blue), which in a few cases has almost 10% error.  Method 3 has a tendency to overestimate slightly (more of its values are at the right).  Both methods almost always obtain better than 0.1% accuracy (a height of 3 or better on the plot), especially at mean distances greater than the longer side.  This implies that if you want to improve the accuracy, focus first on nearby edges of a convex polygon.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/Fjgez.png\" alt=\"The segments\"></p>\\n\\n<p>In this plot of the 1000 segments, the brighter, thicker, redder ones have the greatest relative error for Method 3.  The black segment is the reference (unit) segment.</p>\\n',\n",
       " '<p>Let me complete Zen\\'s answer. I don\\'t very like the notion of \"representing ignorance\". The important thing is not the Jeffreys prior but the Jeffreys <em>posterior</em>. This posterior aims to reflect as best as possible the information about the parameters brought by the data. The invariance property is naturally required for the two following points. Consider for instance the binomial model with unknown proportion parameter $\\\\\\\\theta$ and odds parameter $\\\\\\\\psi=\\\\\\\\frac{\\\\\\\\theta}{1-\\\\\\\\theta}$.</p>\\n\\n<p>1) The Jeffreys posterior on $\\\\\\\\theta$ reflects as best as possible the information about $\\\\\\\\theta$ brought by the data. There is a one-to-on correspondence between $\\\\\\\\theta$ and $\\\\\\\\psi$. Then, transforming the Jeffreys posterior on $\\\\\\\\theta$ into a posterior on $\\\\\\\\psi$ (via the usual change-of-variables formula) should yield a distribution reflecting as best as possible the information about $\\\\\\\\psi$. Thus this distribution should be the Jeffreys posterior about $\\\\\\\\psi. This is the invariance property.</p>\\n\\n<p>2) An important point when drawing conclusions of a statistical analysis is <em>scientific communication</em>. Imagine you give the Jeffreys posterior on $\\\\\\\\theta$ to a scientific colleague. But he/she is interested in $\\\\\\\\psi$ rather than $\\\\\\\\theta$. Then this is not a problem with the invariance property: he/she just has to apply the change-of-variables formula.</p>\\n',\n",
       " \"<p>With respect to your main question, there is no 'bright line' between small $N$ and large $N$, not at 30, 50 or 100 (where most tables stop before jumping to $\\\\\\\\infty$).  If you are estimating the SD from your data, the t-test is appropriate.  After a certain point, the results you get will be indistinguishable from a z-test, but from a theoretical perspective, the t-test remains appropriate.  </p>\\n\",\n",
       " '<p>Standard ANOVA estimates the common variance from the data.  It uses a \"pooled\" estimate by taking the differences between each data point and the group mean for that data point, squaring those differences, summing, then dividing by the degrees of freedom (which is the number of data points minus the number of groups, for simple one-way ANOVA).</p>\\n\\n<p>It would be possible to derive an equivalent of ANOVA where the common variance is known (probably based on the $\\\\\\\\chi^2$ distribution ranther than the F), but the liklihood of finding a real world situation where the variance is known but the means are not is low enough that most people don\\'t worry about that situation.</p>\\n\\n<p>In truth it is probably never true that the populations are exactly normal or that the variances are exactly equal.  The Central Limit Theorem covers the normality assumption for normal enough data and big enough sample sizes.  The ANOVA tests have been shown to be fairly robust to the assumption that the variances are equal as long as they are at least similar (a common rule of thumb is that ANOVA is ok as long as the ratio of the largest variance and smallest variance is less than 4).</p>\\n',\n",
       " '<p><strong>RColorBrewer</strong> has not been mentioned here, I use it often for plotting if I need color schemes</p>\\n',\n",
       " '<p>Neither form of reasoning is mathematically rigorous--there\\'s no such thing as a normal distribution with infinite variance, nor is there a limiting distribution as the variance grows large--so let\\'s be a little careful.</p>\\n\\n<p>In the Black-Scholes model, the log price of the underlying asset is assumed to be undergoing a random walk.  The problem is equivalent to asking \"what is the chance that the asset\\'s (log) value at the expiration date will exceed its current (log) value?\"  Letting the volatility increase without limit is equivalent to letting the expiration date increase without limit.  Thus, the answer should be the same as asking \"what is the limit, as $t \\\\\\\\to \\\\\\\\infty$, that the value of a random walk at time $t$ is greater than its value at time $0$?\"  By symmetry (exchanging upticks and downticks), (and noting that in the continuous model the chance of being at the money is $0$) those probabilities equal $1/2$ for any $t \\\\\\\\gt 0$, whence their limit indeed exists and equals $1/2$.</p>\\n',\n",
       " '<p>Is there a package in R which implements the weighted maximum likelihood method (Warm, 1996) for estimating the person parameters in Rasch Models?</p>\\n',\n",
       " \"<p>Assuming $n$ is known, an estimate can be obtained via</p>\\n\\n<p>$ \\\\\\\\Phi(x_k)=1-e^{-\\\\\\\\lambda x_k} \\\\\\\\approx (k/n)$\\nwhere $x_k$, $0&lt;k&lt;m$, refers to the $k$'th smallest value in your reduced data set. </p>\\n\\n<p>The logic is: if you had the entire set of $n$ samples, you could construct the empirical CDF, $\\\\\\\\Phi$, from this sample.  Then if you took item $k$ of this sorted array, it would correspond to the CDF value $k/n$.  In many cases, $k=n/2$ is a useful choice.</p>\\n\",\n",
       " '<p>I would appreciate any suggestions of how to analyse the following dataset. It was suggested that I use ANOVA, but I wanted to check with the community.</p>\\n\\n<p>I have a protein where I have calculated the entropy of each reading frame (blue, red and yellow). In the attached figure, the bars are a sum of the values calculated. It would probably be better to represent the values as means rather than total sums. Anyway, eye-balling it, the yellow section has a higher entropy than the blue and red regions, whereas red and blue look similar. However, I need to add some statistical weight to this and I would only compare reading frames within one genotype. I\\'m unsure whether to use ANOVA (repeated as it\\'s 3 groups ?), a pairwise.t.test in R, or even Friedman\\'s test, it\\'s been a while. Perhaps this question is similar to <a href=\"http://stats.stackexchange.com/questions/3038/how-to-test-hypothesis-of-no-group-differences\">How to test hypothesis of no group differences?</a> ?</p>\\n\\n<p>I didn\\'t find questions and answers that I identified with, so apologies if I missed one.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/ICIbD.jpg\" alt=\"enter image description here\"></p>\\n',\n",
       " '<p>The input should be proportions. One data set with used proportions and one with available. Try looking at the example data from Aebischer et al., 1993 supplied by writing:</p>\\n\\n<pre><code>## load data and display it\\ndata(pheasant)\\npheasant\\n</code></pre>\\n\\n<p>It uses the weighted mean λ method, described in Aebischer et al., 1993 paper, to replace zeros in the available matrix automatically. You can convince yourself of this by running the example from the R help on this method, and changing test to <code>test=\"randomization\"</code> :</p>\\n\\n<pre><code>#############################\\n## Pheasant dataset: first\\n## example in Aebischer et al.\\n\\ndata(pheasant)\\n\\n## Second order habitat selection\\n## Selection of home range within the\\n## study area (example of parametric test)\\npheana2 &lt;- compana(pheasant$mcp, pheasant$studyarea,\\n                   test = \"randomization\")\\npheana2\\n</code></pre>\\n\\n<p>Then change some of the availability data (pheasant$studyarea) to 0, and run it again.\\nYou could also take a look at the code by just writing the function without \"(  )\" like this:</p>\\n\\n<pre><code>compana\\n</code></pre>\\n\\n<p>As stated in the examples from R it would be a good idea to first perform an eigenanalysis of selection ratios, preliminary to the use of compositional analysis. To the test if the underlying hypotheses for compositional analysis are correct. E.g the compositional analysis method rely on the following hypotheses:  (i) independence between animals, and (ii) all animals are selecting habitat in the same way. See R help for more information:</p>\\n\\n<pre><code>?compana\\n?eisera\\n</code></pre>\\n\\n<p>I have tried something similar, but ended up pooling the trees in categories, as there where to many 0 in the availability data, and I had 13 different tree species.</p>\\n',\n",
       " '<p>This is not a cartoon, but a joke worth mentioning:</p>\\n\\n<p>A statistic professor travels to a conference by plane. When he passes the security check, they discover a bomb in his carry-on-baggage. Of course, he is hauled off immediately for interrogation. </p>\\n\\n<p>\"I don\\'t understand it!\" the interrogating officer exclaims. \"You\\'re an accomplished professional, a caring family man, a pillar of your parish - and now you want to destroy that all by blowing up an airplane!\" </p>\\n\\n<p>\"Sorry\", the professor interrupts him. \"I had never intended to blow up the plane.\" </p>\\n\\n<p>\"So, for what reason else did you try to bring a bomb on board?!\" </p>\\n\\n<p>\"Let me explain. Statistics shows that the probability of a bomb being on an airplane is 1/1000. That\\'s quite high if you think about it - so high that I wouldn\\'t have any peace of mind on a flight.\" </p>\\n\\n<p>\"And what does this have to do with you bringing a bomb on board of a plane?\" </p>\\n\\n<p>\"You see, since the probability of one bomb being on my plane is 1/1000, the chance that there are two bombs is 1/1000000. This way I am much safer...\"</p>\\n',\n",
       " '<p>I think you could torture your data a bit with bootstrapping. Following cgillspies calculations with the geometric distribution, I played around a bit and came up with the following R-code - any corrections greatly appreciated:</p>\\n\\n<pre><code>fails &lt;- c(100, 22, 36, 44, 89, 24, 74) # Observed data\\nN &lt;- 100000 # Number of replications\\nNcol &lt;- length(fails) # Number of columns in the data-matrix\\nboot.m &lt;- matrix(sample(fails,N*Ncol,replac=T),ncol=Ncol) # The bootstrap data matrix\\n# it draws a vector of Ncol results from the original data, and replicates this N-times\\np.hat &lt;- function(x){p.hat = 1/(sum(x)/length(x))} # Function to calculate the \\n# probability of failure\\np.vec &lt;- apply(boot.m,1,p.hat) # calculates the probabilities for each of the   \\n# replications\\nquant.p &lt;- quantile(p.vec,probs=0.01) # calculates the 1%-quantile of the probs.\\nhist(p.vec) # draws a histogram of the probabilities\\nabline(v=quant.p,col=\"red\") # adds a line where quant.p is\\nno.fail &lt;- 223 # Repetitions without a fail after the repair\\n(prob.fail &lt;- 1 - pgeom(no.fail,prob=quant.p)) # Prob of no fail after 223 reps with \\n# failure prob qant.p\\n</code></pre>\\n\\n<p>The idea was to get a worst-case value for the probability, and then use it to calculate the probability of observing no fail after 223 iterations, given the prior failure probability. The worst case of course being a low failure probability to begin with, which would raise the likelihood of observing no failure after 223 iterations without fixing the problem. \\nThe result was 6.37% - as I understand it, you would have had a 6%-probability of not observing a failure after 223 trials if the problem still exists.</p>\\n\\n<p>Of course, you could generate samples of trials and calculate the probability from  that:</p>\\n\\n<pre><code>boot.fails &lt;- rbinom(N,size=no.fail, prob=quant.p) # repeats draws with succes-rate \\n# quant.prob N times.\\nmean(boot.fails==0) # Ratio of no successes \\n</code></pre>\\n\\n<p>with the result of 6.51%.</p>\\n',\n",
       " '<p>You ask an open-ended question, so responses are formatted (worded) freely. You have to categorize responses into bins of same or similar foods before you can produce summary statistics for the study. To decide that responces A and B are similar enough to constitute the same food for you you need to decide first where similarity between various foods ends and dissimilarity starts. This implies that you need to have pre-existent concept of whole population of foods. Thereby it cannot be infinite population for you anymore.</p>\\n',\n",
       " '<p>There is a list of tutorials on this subject here:</p>\\n\\n<p><a href=\"http://www.r-statistics.com/2010/04/repeated-measures-anova-with-r-tutorials/\" rel=\"nofollow\">http://www.r-statistics.com/2010/04/repeated-measures-anova-with-r-tutorials/</a></p>\\n\\n<p>Good luck.</p>\\n',\n",
       " \"<p>I'm trying to design a statistics package in .NET, and I'd like to apply object-oriented design principles to some of the data structures that I'm developing.</p>\\n\\n<p>As I look through sample data sets for simple classifiers, even though there exist other types of attributes (ordinal being the only other one I've seen, with discrete and continuous being variations on that), ultimately they are all able to be classified by a distinct value in some way.</p>\\n\\n<p>This makes me think that all attribute types are inherently nominal, and that there should be a base type for nominal attributes, as the statement for all other attributes seems to hold true:</p>\\n\\n<blockquote>\\n  <p>&lt;attribute type&gt; <strong>is</strong> a nominal type</p>\\n</blockquote>\\n\\n<p>(Note this doesn't mean it's mutually exclusive) </p>\\n\\n<p>If that's the case, then the class that represents the type of an attribute on a fundamental level would be a nominal attribute type, with the certain methods/properties that are inherent to <em>all</em> attribute types.</p>\\n\\n<p>Off the top of my head, the first method would be to be able to determine whether or not an instance of the variable type is the same as another instance of the same attribute type.</p>\\n\\n<p>Given the above, is every attribute type inherently nominal, even if it might possess other qualities beyond nominal qualities (<em>e.g.</em>, ordinal attributes could have the concept of distance between two instances)?</p>\\n\",\n",
       " '<p>Absolutely it is not just a question of honesty, or anything to do with stats v measurement error.  The standard error of the mean and the standard deviation of the population are two different things.</p>\\n\\n<p>The mean of your sample is a random variable, because it would be different every time you ran the sampling process.  The sampling error of the mean is just the estimated standard deviation of the sample mean.</p>\\n\\n<p>It\\'s not quite clear what you mean by comparing data points to your model.  But if you mean you are interested in whether a particular data point is plausibly from the population you have modelled (eg to ask \"is this number a really big outlier?), you need to compare it to your estimate of the population mean and your estimate of the population standard deviation (not the sample mean\\'s standard deviation, also known as SEM).  So the standard deviation in this case.</p>\\n\\n<p>More generally, it sounds like you are using the standard deviation inappropriately in some other circumstances.  If you are trying to report inferences about the population mean you should use the sample mean\\'s standard deviation / standard error, not the population standard deviation.</p>\\n',\n",
       " \"<p>I've used HMM in a demand / inventory level estimation scenario, where we had goods being purchased from many stores that might or might not be out of inventory of the goods.  The sequence of daily demands for these items thus contained zeroes that were legitimate zero demand days and also zeroes that were because the store was out of stock.  You would think you'd know whether the store was out of stock from the inventory level, but errors in inventory records propagate and it is not at all uncommon to find a store that thinks it has a positive number of items on hand, but actually has none; the hidden state is, more or less, whether the store actually has any inventory, and the signal is the (daily demand, nominal inventory level).  No references for this work, though; we were not supposed to publish the results for competitive reasons.</p>\\n\\n<p>Edit:  I'll add that this is especially important because, with zero demands, the store's nominal on hand inventory doesn't ever decrease and cross an order point, triggering an order for more inventory - therefore, a zero on hand state due to erroneous inventory records doesn't get fixed for a long time, until somebody notices something is wrong or a cycle count occurs, which may be many months after the problem has started.</p>\\n\",\n",
       " '<p>ARIMA is more general.  It allows fitting certain nonstationary time series and even stationary series that cannot be fit by low order autoregressive models.</p>\\n',\n",
       " '<p>This is feature selection overfit and this is pretty known -- see <a href=\"http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=124442&amp;tool=pmcentrez&amp;rendertype=abstract\">Ambroise &amp; McLachlan 2002</a>. \\nThe problem is based on the facts that RF is too smart and number of objects is too small. In the latter case, it is generally pretty easy to randomly create attribute that may have good correlation with the decision. And when the number of attributes is large, you may be certain that some of totally irrelevant ones will be a very good predictors, even enough to form a cluster that will be able to recreate the decision in 100%, especially when the huge flexibility of RF is considered. And so, it becomes obvious that when instructed to find the best possible subset of attributes, the FS procedure finds this cluster.<br>\\nOne solution (CV) is given in A&amp;McL, you can also test our approach to the topic, the <a href=\"http://cran.r-project.org/web/packages/Boruta/index.html\">Boruta algorithm</a>, which basically extends the set with \"shadow attributes\" made to be random by design and compares their RF importance to this obtained for real attributes to judge which of them are indeed random and can be removed; this is replicated many times to be significant. Boruta is rather intended to a bit different task, but as far as my tests showed, the resulting set is free of the FS overfit problem.</p>\\n',\n",
       " \"<p>I applied number of methods of clustering, and I want to evaluate these different methods using Dunn index, in this method I have to calculate the distance among clusters and among points in clusters.</p>\\n\\n<p>My question is: if my algorithm is clustering users depending on their sequences, i.e., each user has sequences, then the algorithm measures the similarity among sequences using seq. alignment technique, then cluster them. Sequence alignment tool considers two users most similar when they have max score. If my clusters are id's of users or sequences of users, how do I compute the distance among clusters and among sequences within a cluster? </p>\\n\",\n",
       " '<p>Here is a whole paper about the problem, with a summary of various approaches.  It\\'s called <a href=\"http://www.vldb.org/conf/1995/P311.PDF\" rel=\"nofollow\">Distinct Value Estimation</a> in the literature.</p>\\n\\n<p>If I had to do this myself, without having read fancy papers, I\\'d do this.  In building language models, one often has to estimate the probability of observing a previously unknown word, given a bunch of text.  A pretty good approach at solving this problem for language models in particular is to use the number of words that occurred exactly once, divided by the total number of tokens.  It\\'s called the <a href=\"http://en.wikipedia.org/wiki/Good%E2%80%93Turing_frequency_estimation\" rel=\"nofollow\">Good Turing Estimate</a>.</p>\\n\\n<p>Let u1 be the number of values that occurred exactly once in a sample of m items.</p>\\n\\n<pre><code>P[new item next] ~= u1 / m.\\n</code></pre>\\n\\n<p>Let u be the number of unique items in your sample of size m.</p>\\n\\n<p>If you mistakenly assume that the \\'new item next\\' rate didn\\'t decrease as you got more data, then using Good Turing, you\\'ll have</p>\\n\\n<pre><code>total uniq set of size s ~= u + u1 / m * (s - m) \\n</code></pre>\\n\\n<p>This has some nasty behavior as u1 becomes really small, but that might not be a problem for you in practice.</p>\\n',\n",
       " '<p>I have two tables (matrix) of same dimensions, one contains correlation coefficients and other with p values. I want to combine them into one table. For example let\\'s say I have correlation coefficient between variable A1 and A2 of 0.75 in table 1 and p value of 0.045 in table 2. Now in my combined table 3, I want to use:</p>\\n\\n<p>condition1 for table 1: if a coefficient value in a cell of table 1 is less than 0.4 then \"+\", 0.4 &lt;= coefficient &lt;0.7 then \"++\" else \"+++\",</p>\\n\\n<p>condition2 for table 2: if a pvalue in a cell of table 2 is less than 0.01 then \"+++\", 0.01 &lt;= pvalue &lt; .05 then \"++\" else \"+\".</p>\\n\\n<p>Thus corresponding cell value for A1 and A2 in table 3 should look like: +++/++ where \"+++\" correspond to table 1 value of 0.75 and ++ correspond to table 2 p value of 0.045 and \"/\" is just a separator.</p>\\n\\n<p>I would like to do this either is SAS or R. Thanks in advance.</p>\\n',\n",
       " '<p>Yes it is very simple to extract the summary estimate from a fixed-effects m-a in STATA.</p>\\n\\n<p>You can put it in a local variable</p>\\n\\n<p>e.g.</p>\\n\\n<p>metan tsample tmean tsd csample cmean csd, fixed nograph // the \"fixed\" option is important here</p>\\n\\n<p>local beta=_ES </p>\\n\\n<p>local sebeta=_seES</p>\\n\\n<p>Be sure to check out the help function - it has all this in it.</p>\\n',\n",
       " '<h2>Feature Space</h2>\\n\\n<p>Feature space refers to the $n$-dimensions where your variables live (not including a target variable, if it is present). The term is used often in ML literature because a task in ML is <em>feature extraction</em>, hence we view all variables as features. For example, consider the data set with:</p>\\n\\n<p><strong>Target</strong></p>\\n\\n<ol>\\n<li>$Y \\\\\\\\equiv$ Thickness of car tires after some testing period</li>\\n</ol>\\n\\n<p><strong>Variables</strong></p>\\n\\n<ol>\\n<li>$X_1 \\\\\\\\equiv$ distance travelled in test</li>\\n<li>$X_2 \\\\\\\\equiv$ time duration of test</li>\\n<li>$X_3 \\\\\\\\equiv$ amount of chemical $C$ in tires</li>\\n</ol>\\n\\n<p>The feature space is $\\\\\\\\mathbf{R}^3$, or  more accurately, the positive quadrant in $\\\\\\\\mathbf{R}^3$ as all the $X$ variables can only be positive quantities. Domain knowledge about tires might suggest that the <em>speed</em> the vehicle was moving at is important, hence we generate another variable, $X_4$ (this is the feature extraction part):</p>\\n\\n<ul>\\n<li>$X_4 =\\\\\\\\frac{X_1}{X_2} \\\\\\\\equiv$ the speed of the vehicle during testing.</li>\\n</ul>\\n\\n<p>This extends our old feature space into a new one, the positive part of $\\\\\\\\mathbf{R}^4$. </p>\\n\\n<h2>Mappings</h2>\\n\\n<p>Furthermore, a <em>mapping</em> in our example is a function, $\\\\\\\\phi$, from $\\\\\\\\mathbf{R}^3$ to $\\\\\\\\mathbf{R}^4$:</p>\\n\\n<p>$$\\\\\\\\phi(x_1,x_2,x_3) = (x_1, x_2, x_3, \\\\\\\\frac{x_1}{x_2} )$$</p>\\n',\n",
       " '<p>There are many issues concerning confidence intervals, but let\\'s focus on the quotations.  The problem lies in possible misinterpretations rather than being a matter of correctness.  When people say a \"parameter has a particular probability of\" something, they are thinking of <em>the parameter</em> as being a random variable.  This is not the point of view of a (classical) confidence interval procedure, for which the random variable is the interval itself and the parameter is determined, not random, yet unknown.  This is why such statements are frequently attacked.</p>\\n\\n<p>Mathematically, if we let $t$ be any procedure that maps data $\\\\\\\\mathbf{x} = (x_i)$ to subsets of the parameter space and if (no matter what the value of the parameter $\\\\\\\\theta$ may be) the assertion $\\\\\\\\theta \\\\\\\\in t(\\\\\\\\mathbf{x})$ defines an event $A(\\\\\\\\mathbf{x})$, then--by definition--it has a probability $\\\\\\\\Pr_{\\\\\\\\theta}\\\\\\\\left( A(\\\\\\\\mathbf{x}) \\\\\\\\right)$ for any possible value of $\\\\\\\\theta$.  When $t$ is a confidence interval procedure with confidence $1-\\\\\\\\alpha$ then this probability is supposed to have an infimum (over all parameter values) of $1-\\\\\\\\alpha$.  (Subject to this criterion, we usually select procedures that optimize some additional property, such as producing short confidence intervals or symmetric ones, but that\\'s a separate matter.)  The Weak Law of Large Numbers then justifies the second quotation.  That, however, is not a definition of confidence intervals: it is merely a property they have.</p>\\n\\n<p>I think this analysis has answered question 1, shows that the premise of question 2 is incorrect, and makes question 3 moot.</p>\\n',\n",
       " \"<p>In SPSS Version 19 there seems to be a new feature called Automatic Linear Modelling.  It creates a 'Model' (which is new to me) and the function seems to combine a number of the functions that is typically required for prediction model development.</p>\\n\\n<p>The functionality seems incomplete with only a subset of prediction selection techniques and most notable it's missing Backwards step wise.</p>\\n\\n<h3>QUESTIONS</h3>\\n\\n<ul>\\n<li>Do people see this as good or evil?  </li>\\n<li>And if 'good' then are there ways to decompose what it is doing? </li>\\n<li>Specifically how do I find the regression equation co-efficients when bagging or boosting? </li>\\n</ul>\\n\\n<p>To me it seems to hides a lot of steps and I'm not exactly sure how it's creating what it presents.  So any pointers to tutorials or the like (as the SPSS documentation isn't great) is appreciated.</p>\\n\",\n",
       " '<p>I always have the feeling that any hyper parameter selection for SVMs is done via cross validation in combination with grid search.</p>\\n',\n",
       " \"<p>You might try the Bayesian approach with Jeffreys' prior. It should yield credibility intervals with a correct frequentist-matching property: the confidence level of the credibility interval is close to its credibility level.</p>\\n\\n<pre><code> # required package\\n library(bayesm)\\n\\n # simulated data\\n mu &lt;- 0\\n sdv &lt;- 1\\n y &lt;- exp(rnorm(1000, mean=mu, sd=sdv))\\n\\n # model matrix\\n X &lt;- model.matrix(log(y)~1)\\n # prior parameters\\n Theta0 &lt;- c(0)\\n A0 &lt;- 0.0001*diag(1)\\n nu0 &lt;- 0 # Jeffreys prior for the normal model; set nu0 to 1 for the lognormal model\\n sigam0sq &lt;- 0\\n # number of simulations\\n n.sims &lt;- 5000\\n\\n # run posterior simulations\\n Data &lt;- list(y=log(y),X=X)\\n Prior &lt;- list(betabar=Theta0, A=A0, nu=nu0, ssq=sigam0sq)\\n Mcmc &lt;- list(R=n.sims)\\n bayesian.reg &lt;- runireg(Data, Prior, Mcmc)\\n mu.sims &lt;- t(bayesian.reg$betadraw) # transpose of bayesian.reg$betadraw\\n sigmasq.sims &lt;- bayesian.reg$sigmasqdraw\\n\\n # posterior simulations of the mean of y: exp(mu+sigma²/2)\\n lmean.sims &lt;- exp(mu.sims+sigmasq.sims/2)\\n\\n # credibility interval about lmean:\\n quantile(lmean.sims, probs = c(0.025, 0.975))\\n</code></pre>\\n\",\n",
       " '<p>Newton-Raphson algorithms can often be employed.  I am not familiar with pSLA, but it is pretty common to use Newton-Raphson algorithms for latent class models.  Newton-Raphson algorithms are a little more troubled by poor initial values than EM, so one strategy is to first use a few iterations (say 20) of the EM and then switch to a Newton-Raphson algorithm.  One algorithm that I have had a lot of success with is:\\nZhu, Ciyou, Richard H. Byrd, Peihuang Lu, and Jorge Nocedal (1997), \"Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization,\" ACM Transactions on Mathematical Software (TOMS) archive, 23 (4), 550-60.</p>\\n',\n",
       " \"<p>Machine learning usually uses sets of past data to create patterns and predictions on future data. For instance, manually categorizing previous letters that people have drawn in the training stage in order to detect what letter someone is drawing. </p>\\n\\n<p>As far as I know, maxmin doesn't have any sort of training based on past data, so wouldn't really fall into machine learning. (This sounds like a homework question though, you might want to check your text book's definition of machine learning to be sure).</p>\\n\",\n",
       " 'Cox proportional hazards regression is a semi-parametric method of survival analysis.',\n",
       " \"<p>I have been using various GARCH-based models to forecast volatility for various North American equities using historical daily data as inputs.</p>\\n\\n<p>Asymmetric GARCH models are often cited as a modification of the basic GARCH model to account for the 'leverage effect' i.e. volatility tends to increase more after a negative return than a similarly sized positive return.</p>\\n\\n<p>What kind of a difference would you expect to see between a standard GARCH and an asymmetric GARCH forecast for a broad-based equity index like the S&amp;P 500 or the NASDAQ-100?</p>\\n\\n<p>There is nothing particularly special about these two indices, but I think it is helpful to give something concrete to focus the discussion, as I am sure the effect would be different depending on the equities used.</p>\\n\",\n",
       " '<p>If \"id\" is your window id, than you may do :</p>\\n\\n<pre><code>rhos = NULL \\nfor w in unique(id)\\n    rhos = c(rhos, cor(data[data$id==w,c(\"score1\", \"score2\")])[1,2])\\n\\nrank = unique(id)[order(data)]\\n</code></pre>\\n\\n<p>wish my assumption fits your data. </p>\\n',\n",
       " '<p>I like @nico\\'s response because it makes clear that statistical and pragmatic thinking shall come hand in hand; this also has the merit to bring out issues like statistical vs. clinical significance. But about your specific question, I would say this is clearly detailed in the two sections that directly follow your quote (p. 10).</p>\\n\\n<p>Rereading Piantadosi\\'s textbook, it appears that the author means that <strong>clinical thinking</strong> applies to the situation where a physician has to interpret the results of RCTs or other studies in order to decide of the best treatment to apply to a <em>new patient</em>. This has to do with the extent to which (population-based) conclusions drawn from previous RCT might generalize to new, unobserved, samples. In a certain sense, such decision or judgment call for some form of clinical experience, which is not necessarily of the resort of a consistent statistical framework. Then, the author said that \"the solution offered by <strong>statistical reasoning</strong> is to control the signal-to-noise ratio by design.\" In other words, this is a way to reduce uncertainty, and \"the chance of drawing incorrect conclusions from either good or bad data.\" In sum, both lines of reasoning are required in order to draw valid conclusions from previous (and \\'localized\\') studies, and choose the right treatment to administer to a new individual, given his history, his current medication, etc. -- treatment efficacy follows from a good balance between statistical facts and clinical experience. </p>\\n\\n<p>I like to think of a statistician as someone who is able to mark off the extent to which we can draw firm inferences from the observed data, whereas the clinician is the one that will have a more profound insight onto the implications or consequences of the results at the individual or population level.</p>\\n',\n",
       " '<p>Here\\'s one attempt using R and ggplot2, I\\'m sure there may be others that are more effective.</p>\\n\\n<pre><code>library(ggplot2)\\n#Load data\\ndat &lt;- data.frame(Number = c(1000, 990, 200, 60, 58, 50, 40, 10),\\n                  Action = c(\"Emails Invited\", \"Emails Received\", \"Visited System\", \"Tried To Create Account\",\\n                             \"Used Valid Email To Create Account\", \"Successfully Created Account\", \\n                             \"Participated by Voting\", \"Posted Content\"),\\n                  stringsAsFactors = FALSE)\\n\\n#Create data needed for geom_segment\\ndat &lt;- transform(dat, lowval = -(Number / 2)\\n                 , highval = Number / 2\\n                 , index = rev(1:nrow(dat)))\\n\\n#plot it\\nggplot(dat, aes(x = lowval, xend = highval, y = index, yend = index)) +\\n  geom_segment(size = 4, colour = \"steelblue\") +\\n  geom_text(aes(x = 0, y = index - 0.25, label = paste(Number, Action, sep = \" \")), size = 3.5) +\\n  theme_bw(18) + #Make default font a bit bigger\\n  opts(title = \"Effects Of Hurdles On User Numbers\") +\\n  opts(axis.text.x = theme_blank(), axis.title.x=theme_blank()) +\\n  opts(axis.text.y = theme_blank(), axis.title.y=theme_blank()) \\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/0qYVI.jpg\" alt=\"enter image description here\"></p>\\n',\n",
       " '<p>It is not true that you are not doing any learning. What you are doing is using the well known classification algorithm called <a href=\"http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\"><em>Nearest Neighbor</em></a> (NN). It is important to realize that you are learning as long as you are using the train data (even if you dont explicitly calculate some parameter) - and in this case you are definitely using it.</p>\\n\\n<p>It is ok that NN is doing well. However, in some cases it may be a sign that there is a problem with your data. This can happen when your data is not <a href=\"http://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables\">IID</a>. For example, in some cases you may have exact or close duplicates in your data. In such a case, many instances in the test set will have a close neighbor in the train set and you will get a high success rate but in fact you are overfitting, because if you get a new point without duplicates your performance will be worse. What you can do in this case is try to remove duplicates in advance, or construct the train/test sets such that duplicates (or tight clusters) have to be in the same set. It is important to look at the data and try to understand what is going on.</p>\\n',\n",
       " '<p>\"Significant\" is the biggest one I run into, because it has both a common English-use meaning <em>and</em> that meaning will crop up in the discussion of research results. I even find myself mixing in \"significant\" to mean important in the same sentence where I\\'ve talked about statistical results.</p>\\n\\n<p>That way lies madness.</p>\\n',\n",
       " \"<p>A LOESS will always give a better fit than regression, unless the fit is perfect. It is a locally linear approximation that is designed to pass close to the data. These methods are basically exploratory. And while it is dangerous to extrapolate a linear model beyond the limits of the fit, you simple could not do it intelligently with a LOWESS. </p>\\n\\n<p>If your model gives you negative costs, that's a pretty good sign that a linear regression is not appropriate on the variables you have.  You say that you tried transformations. Did you take the log of cost against your predictors? </p>\\n\\n<p>In the nature of things, it is unlikely that there is a simple relationship between cost and the variables you mention. Sometimes the purpose of a linear regression is simply to demonstrate that some sort of correlation exists, and perhaps to select a sensible set of predictors. </p>\\n\",\n",
       " \"<p>When specifying a production function for regression, it is well known that one of the features of using a log-log model is that the estimated coefficients are the output elasticities w.r.t. their respective independent variables.</p>\\n\\n<p>My question is does it then follow that if one regresses log(production) on log(price), the coefficient on log(price) will be the demand elasticity?</p>\\n\\n<p>That is, if we specify the production model as follows:</p>\\n\\n<p>$$\\\\\\\\ln q=\\\\\\\\alpha_0 + \\\\\\\\alpha_p \\\\\\\\ln p$$</p>\\n\\n<p>(where q is quantity of output and p is output price)</p>\\n\\n<p>and then differentiate w.r.t. $\\\\\\\\ln p$</p>\\n\\n<p>$$\\\\\\\\frac{d \\\\\\\\ln q}{d \\\\\\\\ln p}=\\\\\\\\alpha_p$$</p>\\n\\n<p>then isn't this the demand elasticity?</p>\\n\\n<p>If it is, then does the omission of other important variables in the production function bias the elasticity (if firms are not homogenous in those variables)?</p>\\n\\n<p>Thanks</p>\\n\",\n",
       " \"<p>I was correct in assuming that there was a trick. Here is the given solution:</p>\\n\\n<p>The trick is that each $p_i$ is independent so you can rewrite the problem as the joint product of m individual integrals.</p>\\n\\n<p>More specifically with $c=\\\\\\\\frac{1}{B(\\\\\\\\alpha, \\\\\\\\beta)}$,</p>\\n\\n<p>$$P(\\\\\\\\alpha,\\\\\\\\beta|\\\\\\\\mathbf X)=\\\\\\\\int_{p_1}\\\\\\\\dots\\\\\\\\int_{p_m}\\\\\\\\pi(\\\\\\\\alpha, \\\\\\\\beta) \\\\\\\\hspace{2mm} c^m\\\\\\\\hspace{2mm} p_i^{x_i+\\\\\\\\alpha-1}(1-p_i)^{n_i-x_i+\\\\\\\\beta-1} dp_{1}\\\\\\\\dots dp_{m}\\\\\\\\\\\\\\\\= \\\\\\\\pi(\\\\\\\\alpha, \\\\\\\\beta) \\\\\\\\hspace{2mm}c^{m} \\\\\\\\prod_{i=1}^{m}\\\\\\\\int_{p_i}p_i^{x_i+\\\\\\\\alpha-1}(1-p_i)^{n_i-x_i+\\\\\\\\beta-1}dp_i$$ </p>\\n\\n<p>Because this is the kernel of a $Beta(\\\\\\\\alpha + x_i, \\\\\\\\beta + n_i-x_i)$  we know each one will integrate to 1 if we add the necessary constant term.</p>\\n\\n<p>$$\\\\\\\\pi(\\\\\\\\alpha, \\\\\\\\beta) \\\\\\\\hspace{2mm}c^{m} \\\\\\\\prod_{i=1}^{m}B(\\\\\\\\alpha + x_i, \\\\\\\\beta + n_i-x_i)* \\\\\\\\\\\\\\\\ \\\\\\\\int_{p_i}B(\\\\\\\\alpha + x_i, \\\\\\\\beta + n_i-x_i)^{-1} p_i^{x_i+\\\\\\\\alpha-1}(1-p_i)^{n_i-x_i+\\\\\\\\beta-1}dp_i\\\\\\\\\\\\\\\\=\\\\\\\\pi(\\\\\\\\alpha, \\\\\\\\beta) \\\\\\\\hspace{2mm}c^{m} \\\\\\\\prod_{i=1}^{m}B(\\\\\\\\alpha + x_i, \\\\\\\\beta + n_i-x_i)$$</p>\\n\\n<p>with $c=\\\\\\\\frac{1}{B(\\\\\\\\alpha, \\\\\\\\beta)}$ the full expression is</p>\\n\\n<p>$$P(\\\\\\\\alpha,\\\\\\\\beta|\\\\\\\\mathbf X)=\\\\\\\\pi(\\\\\\\\alpha, \\\\\\\\beta) \\\\\\\\hspace{2mm}B(\\\\\\\\alpha, \\\\\\\\beta)^{-m} \\\\\\\\prod_{i=1}^{m}B(\\\\\\\\alpha + x_i, \\\\\\\\beta + n_i-x_i)$$</p>\\n\\n<p>In the problem I recieved it wasn't stated that the $p_i$ were independent but the original problem was counting bicycles amongst all traffic on different blocks so I suppose it is a logical assumption.</p>\\n\",\n",
       " '<p>I doubt about your use of repeated measures, because normally people talk about repeated measures (longitudinal studies) when subjects receive measurements at different time points. For example, children are measured heights and weights when they are 2, 4, 6, 8, and 10 years old. </p>\\n\\n<p>Since S_2 (and maybe more subjects) in your data receives <strong>simultaneous</strong> treatments while others are not, I suspect that RM may not be a proper tool to use. I think if you include interactions instead, there would be so many involved, which will further complicate the problem...</p>\\n',\n",
       " '<p>It sounds like any linear classifier will do what you need. Suppose you have $N$ features and the value of feature $i$ is $f_i$. Then a linear classifier will compute a score \\n$$s = \\\\\\\\sum_i w_i f_i + o$$ (where $o$ is the offset). Then, if $s &gt; t$ (where $t$ is some threshold), then the feature belongs to a class (a group), and if $s &lt; t$, then it doesn\\'t. Note that there is a single threshold applied to the entire score (rather than to individual feature values), so indeed a deficiency in one parameter can be compensated for by abundance in another. The weights are intuitively interpretable, in the sense that the higher the weight is, the more important (or more decisive) that feature is.</p>\\n\\n<p>There are a lot of off-the-shelf linear classifiers that can do that, including SVM, LDA (linear discriminant analysis), linear neural networks, and many others. I\\'d start by running linear SVM because it works well in a lot of cases and can tolerate limited training data. There are also a lot of packages in many environments (like Matlab and R), so you can easily try it. The downside of SVM is that it can be computationally heavy, so if you need to learn a lot of classes, it might be less appropriate. </p>\\n\\n<p>If you want to preserve some of the threshold behavior you currently have, you can pass the feature values through a sigmoid with the threshold in the right location. E.g. for a feature $i$ for which you currently use a threshold of $t_i$, first compute \\n$$g_i = \\\\\\\\frac{1}{1 + \\\\\\\\exp(f_i - t_i)},$$\\nand then learn a linear classifier using $g$\\'s rather than $f$\\'s. This way, the compensating behavior will only happen near the threshold, and things that are too far away from the threshold cannot be compensated for (which is sometimes desirable).</p>\\n\\n<p>Another thing that you could try is to use probabilistic classifiers like Naive Bayes or TAN. Naive Bayes is almost like a linear classifier, except it computes\\n$$s = \\\\\\\\sum_i w^i_{f_i}.$$\\nSo there is still a sum of weights. These weights depend on the feature values $f_i$, but not by multiplication like in a usual linear classifier. The score in this case is the log-probability, and the weights are the contributions of the individual features into that log-probability. The disadvantage of using this in your case is that you will need many bins for your feature values, and then learning may become difficult. There are ways around that (for example, using priors), but since you have no experience with this, it might be more difficult.</p>\\n\\n<p>Regarding terminology: what you called \\'test set\\' is usually called a \\'training set\\' in this context, and what you called \\'new data\\' is called the \\'test set\\'.</p>\\n\\n<p>For a book, I\\'d read \"Pattern recognition\" by Duda, Hart, and Stork. The first chapter is a very good introduction for beginners.</p>\\n',\n",
       " '<p>In R, the default setting for random number generation are:</p>\\n\\n<ol>\\n<li>For U(0,1), use the Mersenne-Twister algorithm</li>\\n<li>For Guassian numbers use  the numerical inversion of the standard normal distribution function. </li>\\n</ol>\\n\\n<p>You can easily check this, viz.</p>\\n\\n<pre><code>&gt; RNGkind()\\n[1] \"Mersenne-Twister\" \"Inversion\"\\n</code></pre>\\n\\n<p>It is possible to change the default generator to other PRNGs, such as Super-Duper,Wichmann-Hill, Marsaglia-Multicarry or even a user-supplied PRNG. See the ?RNGkind for further details. I have never needed to change the default PRNG.</p>\\n\\n<p>The <a href=\"http://www.gnu.org/software/gsl/manual/html_node/Random-number-environment-variables.html\">C GSL</a> library also uses the <a href=\"http://www.gnu.org/software/gsl/manual/html_node/Random-number-generator-algorithms.html\">Mersenne-Twister</a> by default.</p>\\n',\n",
       " 'Box Cox transformation refers to a method of [tag: data-transformation] that uses a power law',\n",
       " '<p>Suppose that the two models involved are:</p>\\n\\n<p>$\\\\\\\\newcommand{\\\\\\\\Cov}{\\\\\\\\mathrm{Cov}}$\\n$y=\\\\\\\\alpha_1 + \\\\\\\\beta_1 x_1 + \\\\\\\\epsilon_1$</p>\\n\\n<p>and</p>\\n\\n<p>$\\\\\\\\hat\\\\\\\\epsilon_1=\\\\\\\\alpha_2 + \\\\\\\\beta_2 x_2 + \\\\\\\\epsilon_2$</p>\\n\\n<p>The process goes as follows:</p>\\n\\n<p>1: estimates of $\\\\\\\\hat \\\\\\\\beta_1$ and $\\\\\\\\hat\\\\\\\\alpha_1$ (assuming OLS validity)</p>\\n\\n<p>2: calculate the residuals $\\\\\\\\hat\\\\\\\\epsilon_1=y-\\\\\\\\hat\\\\\\\\alpha_1-\\\\\\\\hat\\\\\\\\beta_1x_1$</p>\\n\\n<p>3: repeat the above with the second model, thus obtaining $\\\\\\\\hat\\\\\\\\epsilon_2=\\\\\\\\hat\\\\\\\\epsilon_1-\\\\\\\\hat\\\\\\\\alpha_2-\\\\\\\\hat\\\\\\\\beta_2x_2$</p>\\n\\n<p>4: check that $\\\\\\\\Cov(\\\\\\\\hat\\\\\\\\epsilon_2,x_1)=0$:</p>\\n\\n<p>$\\\\\\\\Cov(\\\\\\\\hat\\\\\\\\epsilon_2,x_1)=\\\\\\\\Cov(\\\\\\\\hat\\\\\\\\epsilon_1-\\\\\\\\hat\\\\\\\\alpha_2-\\\\\\\\hat\\\\\\\\beta_2x_2,x_1)=\\n\\\\\\\\Cov(y-\\\\\\\\hat\\\\\\\\alpha_1-\\\\\\\\hat\\\\\\\\beta_1x_1\\n-\\\\\\\\hat\\\\\\\\alpha_2-\\\\\\\\hat\\\\\\\\beta_2x_2,x_1)=\\n\\\\\\\\Cov(y-\\\\\\\\hat\\\\\\\\beta_1x_1\\n-\\\\\\\\hat\\\\\\\\beta_2x_2,x_1)$</p>\\n\\n<p>Which is clearly not null, given the presence of $x_1$ itself in the result, by which</p>\\n\\n<p>$\\\\\\\\Cov(\\\\\\\\hat\\\\\\\\beta_1x_1,x_1)=\\\\\\\\hat\\\\\\\\beta_1\\\\\\\\sigma_{x_1}^2$ </p>\\n\\n<p>not to mention the interaction between $y$ and $x_1$ or $x_1$ and $x_2$.</p>\\n',\n",
       " \"<p>I'm trying to analyze for any trends in what I consider to be low observations or counts.</p>\\n\\n<p>Let's say you have a bus service with the following commuters:</p>\\n\\n<pre><code>year   average count\\n2001   12\\n2002   15\\n2003   17\\n2004   13\\n2005   18\\n2006   12\\n2007   9\\n2008   12\\n</code></pre>\\n\\n<p>The above numbers are fictitious but similar to what my problem is. How can I analyze the above data for any trends and perhaps attempt to forecast for 2009 to 2012 values?</p>\\n\",\n",
       " '<p>Does there exist any agreement on what must rejection regions look like topologically? If we identify the region of \"acceptance\" with the corresponding confidence interval (or confidence region in dimensions greater than 1) then, according to <a href=\"http://stats.stackexchange.com/questions/15872/are-confidence-intervals-open-or-closed-intervals\">this question</a>, it seems that rejection regions must be open. On the other hand, the rejection region given by the Neyman-Pearson lemma is usually defined in terms of a non-strict inequality, so at least in the continuous case it will be closed. (I know that this is a somewhat bizantine question, at least from a practical point of view.)</p>\\n',\n",
       " '<p>In my opinion, Matlab is an ugly language. Perhaps it\\'s gotten default arguments and named arguments in its core by now, but many examples you find online do the old \"If there are 6 arguments, this, else if there are 5 arguments this and that...\" and named arguments are just vectors with alternating strings (names) and values. That\\'s so 1970\\'s that I simply can\\'t use it.</p>\\n\\n<p>R may have its issues, and it is also old, but it was built on a foundation (Scheme/Lisp) that was forward-looking and has held up rather well in comparison.</p>\\n\\n<p>That said, Matlab is much faster if you like to code with loops, etc. And it has much better debugging facilities. And more interactive graphics. On the other hand, what passes for documenting your code/libraries is rather laughable compared to R and you pay a pretty penny to use Matlab.</p>\\n\\n<p>All IMO.</p>\\n',\n",
       " \"<p>I would say experience -- basic ideas are:</p>\\n\\n<ul>\\n<li>to fit how classifiers work; giving a geometry problem to a tree, oversized dimension to a kNN and interval data to an SVM are not a good ideas</li>\\n<li>remove as much nonlinearities as possible; expecting that some classifier will do Fourier analysis inside is rather naive (even if, it will waste a lot of complexity there)</li>\\n<li>make features generic to all objects so that some sampling in the chain won't knock them out</li>\\n<li>check previous works -- often transformation used for visualisation or testing similar types of data is already tuned to uncover interesting aspects</li>\\n<li>avoid unstable, optimizing transformations like PCA which may lead to overfitting</li>\\n<li>experiment a lot</li>\\n</ul>\\n\",\n",
       " '<p>$\\\\\\\\newcommand{\\\\\\\\var}{\\\\\\\\mathrm{Var}}$In my textbook I am studying about heteroskedasticity in a linear regression model, and for doing the hypothesis test, it says our hypothesis is: $H_0: \\\\\\\\var(u|x_1,x_2,...,x_k)=\\\\\\\\sigma ^2$, however than it says:</p>\\n\\n<blockquote>\\n  <p>Because we are assuming that $u$ has a zero conditional expectation, $\\\\\\\\var(u|\\\\\\\\mathbf{x})=E(u^2|\\\\\\\\mathbf{x})$ (where $\\\\\\\\mathbf{x}$ is all the regressors $x_1,x_2,x_3,...,x_n$), and so the null hypothessis of homoskedasticity is equivalent to: $$H_0: E(u^2|x_1,x_2,...,x_k)=E(u^2)=\\\\\\\\sigma^2$$</p>\\n</blockquote>\\n\\n<p>Why is this true? I dont understand why we use $u^2$ and then regress the independent variables on it. </p>\\n',\n",
       " '<p>The Gaussian Asuumptions refer to the residuals from the model. There are no assumptions necessary about the original data. As a case in point the distribution of daily beer sales \\n<img src=\"http://i.stack.imgur.com/1rb3x.png\" alt=\"enter image description here\"> .After a reasonable model captured the day-of-the-week, holiday/events effects , level shifts/time trends we get <img src=\"http://i.stack.imgur.com/rbrdY.png\" alt=\"enter image description here\">  </p>\\n',\n",
       " '<p>A parsimonious model is a model that accomplishes a desired level of explanation or prediction with as few predictor variables as possible. </p>\\n\\n<p>For model evaluation there are different methods depending on what you want to know. There are generally two ways of evaluating a model: Based on predictions and based on goodness of fit on the current data. In the first case you want to know if your model adequately predicts new data, in the second you want to know whether your model adequatelly describes the relations in your current data. Those are two different things.</p>\\n\\n<h2>Evaluating based on predictions</h2>\\n\\n<p>The best way to evaluate models used for prediction, is crossvalidation. Very briefly, you cut your dataset in eg. 10 different pieces, use 9 of them to build the model and predict the outcomes for the tenth dataset. A simple mean squared difference between the observed and predicted values give you a measure for the prediction accuracy. As you repeat this ten times, you calculate the mean squared difference over all ten iterations to come to a general value with a standard deviation. This allows you again to compare two models on their prediction accuracy using standard statistical techniques (t-test or ANOVA).</p>\\n\\n<p>A variant on the theme is the PRESS criterion (Prediction Sum of Squares), defined as </p>\\n\\n<p>$\\\\\\\\displaystyle\\\\\\\\sum^{n}_{i=1} \\\\\\\\left(Y_i - \\\\\\\\hat{Y}_{i(-i)}\\\\\\\\right)^2$</p>\\n\\n<p>Where $\\\\\\\\hat{Y}_{i(-i)}$ is the predicted value for the ith observation using a model based on all observations minus the ith value. This criterion is especially useful if you don\\'t have much data. In that case, splitting your data like in the crossvalidation approach might result in subsets of data that are too small for a stable fitting.</p>\\n\\n<h2>Evaluating based on goodness of fit</h2>\\n\\n<p>Let me first state that this really differs depending on the model framework you use. For example, a likelihood-ratio test can work for Generalized Additive Mixed Models when using the classic gaussian for the errors, but is meaningless in the case of the binomial variant.</p>\\n\\n<p>First you have the more intuitive methods of comparing models. You can use the Aikake Information Criterion (AIC) or the Bayesian Information Criterion (BIC) to compare the goodness of fit for two models. But nothing tells you that both models really differ. </p>\\n\\n<p>Another one is the Mallow\\'s Cp criterion. This essentially checks for possible bias in your model, by comparing the model with all possible submodels (or a careful selection of them).  See also <a href=\"http://www.public.iastate.edu/~mervyn/stat401/Other/mallows.pdf\">http://www.public.iastate.edu/~mervyn/stat401/Other/mallows.pdf</a></p>\\n\\n<p>If the models you want to compare are nested models (i.e. all predictors and interactions of the more parsimonious model occur also in the more complete model), you can use a formal comparison in the form of a likelihood ratio test (or a Chi-squared or an F test in the appropriate cases, eg when comparing simple linear models fitted using least squares). This test essentially controls whether the extra predictors or interactions really improve the model. This criterion is often used in forward or backward stepwise methods.</p>\\n\\n<h2>About automatic model selection</h2>\\n\\n<p>You have advocates and you have enemies of this method. I personally am not in favor of automatic model selection, especially not when it\\'s about describing models, and this for a number of reasons:</p>\\n\\n<ul>\\n<li>In every model you should have checked that you deal adequately with confounding. In fact, many datasets have variables that should never be put in a model at the same time. Often people forget to control for that.</li>\\n<li>Automatic model selection is a method to create hypotheses, not to test them. All inference based on models originating from Automatic model selection is invalid. No way to change that.</li>\\n<li>I\\'ve seen many cases where starting at a different starting point, a stepwise selection returned a completely different model. These methods are far from stable.</li>\\n<li>It\\'s also difficult to incorporate a decent rule, as the statistical tests to compare two models require the models to be nested. If you use eg AIC, BIC or PRESS, the cutoff for when a difference is really important is arbitrary chosen.</li>\\n</ul>\\n\\n<p>So basically, I see more in comparing a select set of models chosen beforehand. If you don\\'t care about statistical evaluation of the model and hypothesis testing, you can use crossvalidation to compare the predictive accuracy of your models. </p>\\n\\n<p>But if you\\'re really after variable selection for predictive purposes, you might want to take a look to other methods for variable selection, like Support Vector Machines, Neural Networks, Random Forests and the likes. These are far more often used in eg medicine to find out which of the thousand measured proteins can adequately predict whether you have cancer or not. Just to give a (famous) example : </p>\\n\\n<p><a href=\"http://www.nature.com/nm/journal/v7/n6/abs/nm0601_673.html\">http://www.nature.com/nm/journal/v7/n6/abs/nm0601_673.html</a></p>\\n\\n<p><a href=\"http://www.springerlink.com/content/w68424066825vr3l/\">http://www.springerlink.com/content/w68424066825vr3l/</a></p>\\n\\n<p>All these methods have regression variants for continuous data as well. </p>\\n',\n",
       " 'A discrete, univariate distribution modelling the number of ${\\\\\\\\rm Bernoulli}(p)$ trial successes until a specified number of failures occur.',\n",
       " '<p>While studying bootstrap-based confidence interval, I once read the following statement:</p>\\n\\n<blockquote>\\n  <p>If the bootstrap distribution is skewed to the right, the bootstrap-based confidence interval incorporates a correction to move the endpoints even farther to the right; this may seem counterintuitive, but it is the correct action.</p>\\n</blockquote>\\n\\n<p>I am trying to understand the logic underlying the above statement.</p>\\n',\n",
       " \"<p>Consider the following example:</p>\\n\\n<pre><code>t = 1/24:1/24:365;\\nx = cos((2*pi)/12*t)+randn(size(t));\\n% if you have the signal processing toolbox\\n[Pxx,F] = periodogram(x,rectwin(length(x)),length(x),1);\\nplot(F,10*log10(Pxx)); xlabel('Cycles/hour');\\nylabel('dB/(Cycles/hour');\\n</code></pre>\\n\\n<p>This demonstrates the dominant periodicity in the data set. However, if I have a time series for one year worth of data, similar to that shown above, is it possible to look at how a specific frequency changes through time. For example, the time series above shows the hourly variation of a given variable throughout the year. Therefore if we knew that the dominant period was driven by a diurnal cycle (i.e. once per day) then we would know that the dominant periodicity would be 1/24. So, if we know the dominant periodicity to be 1/24, is it possible to see how the power of this specific periodicity changes in time (i.e. throughout the year)? </p>\\n\",\n",
       " \"<p>I have a large data set which is in .dbf format right now and what I would like to do is be able to manipulate it easily in Excel and do something like subtotal and calculate stdev and ratios.</p>\\n\\n<p>Details of the data set;\\nThis data set contains shopper information. It has 1.2 million rows and 20 columns where the rows are each a unique shopper and the columns hold their shopping data (what they bought).</p>\\n\\n<p>I am using Office 2007 programs, I know Excel the best but was wondering what alternatives I could use to accomplish my goals (subtotal, calculate stdev, and ratio's).</p>\\n\",\n",
       " '<ol>\\n<li>Yes, when using the HDP approach to topic modeling, the number of topics is inferred from the data, whereas with standard LDA the number of topics must be pre-specified.</li>\\n<li>The Pitman-Yor process is a simple generalization of the Dirichlet process which adds another parameter to the DP definition. Both processes are used as priors on cluster models where the number of clusters is unknown. Practically speaking, the Pitman Yor process allows more flexibility in the distribution of weights associated with each cluster (that is, the proportion of data points generated by each cluster). In a draw from the dirichlet process, if you put the cluster weights in decreasing order, they tend to decay exponentially. This is inconsistent with the clustering pattern observed in some domains such as language, hence the prominent use of Pitman Yor process priors in computational linguistics. </li>\\n<li>The DP is used as a prior for clustered data. The HDP is used as a prior on grouped data where clusters are shared across groups. For example, in topic modeling, different documents are \"groups\" of data that share common topics (clusters). A single level DP cannot be used as a prior on topic models. </li>\\n</ol>\\n',\n",
       " '<p>Generally speaking, what you really want from a sample, is to be \"representative\". Random sampling is a good way to go since it allows all subjects the same probability of being sampled; In the hope that all attributes and attribute-relations existing in the population will exist in the sample. Making it \"representative\". \\nIn your case, if you believe all Spanish players had a a-priori equal chance of being drawn in the (sub)sample, then it is \"random\".  </p>\\n\\n<p>Regarding size considerations: A single observation can still be a \"random sample\". Larger samples are needed when you want more precision, and especially when you are looking for rare relations in the population, which might not be present in a small sample. </p>\\n',\n",
       " '<p>The <a href=\"http://en.wikipedia.org/wiki/Davidon%E2%80%93Fletcher%E2%80%93Powell_formula\" rel=\"nofollow\">Davidon-Fletcher-Powell</a> method has worked well for me in problems like this.</p>\\n\\n<p>Below is some super hacky code that I wrote for using this with a line-search method for an Econometrics class. You\\'ll have to implement your own derivatives and log-likelihood wrapper functions to get it to work, but maybe it will be helpful. It should only depend on NumPy and SciPy.</p>\\n\\n<pre><code>def line_search_iteration(data,gamma,pi,v_vector,c,A):\\n\\n    # Calculate the derivative at the current value of beta\\n    diff = -1.0*compute_numerical_derivative(data,gamma,pi,v_vector,c)\\n\\n    # Update normalized direction\\n    d = -A.dot(diff)\\n    d_n = 1.0/(1+np.linalg.norm(d))*d\\n\\n    # Compute a bracket of the root.\\n    # Compute function values to determine a root bracket.\\n    lam_l = 0.0\\n    lam_h = 1.0\\n    Qh = []\\n\\n    tmp0 = np.asarray([[gamma[0,0]],[gamma[1,0]],[gamma[2,0]],[gamma[3,0]],[pi]])\\n    tmpL = tmp0 + lam_l*d_n\\n    tmpH = tmp0 + lam_h*d_n\\n\\n    Qh.append(-compute_log_likelihood(data,tmpL[0:-1,:],tmpL[-1,0],v_vector) )\\n    Qh.append(-compute_log_likelihood(data,tmpH[0:-1,:],tmpH[-1,0],v_vector) )\\n\\n    # Search for a root bracket.\\n    while Qh[-1] &gt;= Qh[-2]:\\n        lam_l = lam_l + (lam_h-lam_l)\\n        lam_h = 2.0*lam_h\\n        tmpH = tmp0 + lam_h*d_n\\n\\n        Qh.append(-compute_log_likelihood(data,tmpH[0:-1,:],tmpH[-1,0],v_vector) )\\n\\n    # Use golden search to compute optimal length\\n    # Do golden section to obtain the optimal value of lam\\n\\n    # 0th iteration\\n    lam_m1 = lam_l + 0.382*(lam_h - lam_l)\\n    lam_m2 = lam_l + 0.618*(lam_h - lam_l)\\n\\n    tmp_m1 = tmp0 + lam_m1*d_n\\n    tmp_m2 = tmp0 + lam_m2*d_n\\n\\n    Q_m1 = -compute_log_likelihood(data,tmp_m1[0:-1,:],tmp_m1[-1,0],v_vector)\\n    Q_m2 = -compute_log_likelihood(data,tmp_m2[0:-1,:],tmp_m1[-1,0],v_vector)\\n\\n    while (lam_h-lam_l) &gt;= 0.0001:\\n\\n        if Q_m2 &gt; Q_m1:\\n            # lam_l doesn\\'t change\\n            lam_h  = np.copy(lam_m2)\\n            lam_m2 = np.copy(lam_m1)\\n            lam_m1 = lam_l + 0.382*(lam_h-lam_l)\\n\\n            tmp_m1 = tmp0 + lam_m1*d_n\\n            tmp_m2 = tmp0 + lam_m2*d_n\\n            Q_m1 = -compute_log_likelihood(data,tmp_m1[0:-1,:],tmp_m1[-1,0],v_vector)\\n            Q_m2 = -compute_log_likelihood(data,tmp_m2[0:-1,:],tmp_m1[-1,0],v_vector)\\n\\n            lam_avg = 0.5*(lam_l + lam_h)\\n\\n        else:\\n            # lam_h doesn\\'t change\\n            lam_l  = np.copy(lam_m1)\\n            lam_m1 = np.copy(lam_m2)\\n            lam_m2 = lam_l + 0.618*(lam_h-lam_l)\\n\\n            tmp_m1 = tmp0 + lam_m1*d_n\\n            tmp_m2 = tmp0 + lam_m2*d_n\\n            Q_m1 = -compute_log_likelihood(data,tmp_m1[0:-1,:],tmp_m1[-1,0],v_vector)\\n            Q_m2 = -compute_log_likelihood(data,tmp_m2[0:-1,:],tmp_m1[-1,0],v_vector)\\n            lam_avg = 0.5*(lam_l + lam_h)\\n\\n    # Now, return the necessary components for making the update.\\n    return diff, d_n, lam_avg\\n####################\\n\\n\"\"\" Function for carrying out a single DFP iteration.\\n\"\"\"\\ndef dfp_iteration(data,gamma,pi,v_vector,c,A):\\n\\n    # Get the necessary step direction and optimal step length for the derivative\\n    diff, d_n, lam = line_search_iteration(data,gamma,pi,v_vector,c,A)\\n\\n    # Update the estimate of beta and the scaling matrix A.\\n    p = lam*d_n\\n    gamma_aug = np.asarray([[gamma[0,0]],[gamma[1,0]],[gamma[2,0]],[gamma[3,0]],[pi]])\\n    gamma_aug = gamma_aug + p\\n\\n    err_prime = np.linalg.norm(p)\\n\\n    gamma_new = gamma_aug[0:-1,:]\\n    pi_new = gamma_aug[-1,0]\\n\\n    q = -1*compute_numerical_derivative(data,gamma_new,pi_new,v_vector,c) - diff\\n\\n    num1 = p.dot(np.transpose(p))\\n    den1 = np.transpose(p).dot(q)\\n    num2 = A.dot(q.dot(np.transpose(q).dot(A)))\\n    den2 = np.transpose(q).dot(A.dot(q))\\n\\n    A = A + num1/den1 - num2/den2\\n    err = np.transpose(diff).dot(diff)\\n\\n    return gamma_new, pi_new, A, err, err_prime\\n</code></pre>\\n\\n<p>There is also <a href=\"http://abel.ee.ucla.edu/cvxopt/\" rel=\"nofollow\">CVXOpt</a> for Python, but I\\'ve never used it. Not sure how adaptable it is for your problem.</p>\\n',\n",
       " \"<p>Technically, the variance is infinite because you are dividing by a variable with a positive density around $0$ while the numerator has a positive density away from $0$, and this forces the tails to be too large for the variance to exist. In fact, the expected value doesn't exist, either. In practice you may be able to ignore this because it is a very rare event for the denominator to be close to $0$.</p>\\n\\n<p>Let $\\\\\\\\Delta X = X-\\\\\\\\mu_X$ and $\\\\\\\\Delta Y = Y-\\\\\\\\mu_Y.$</p>\\n\\n<p>Let $r(X,Y) = \\\\\\\\frac{X}{Y}$</p>\\n\\n<p>$r(X,Y) = r(\\\\\\\\mu_x + \\\\\\\\Delta X, \\\\\\\\mu_y + \\\\\\\\Delta Y)$ </p>\\n\\n<p>$\\\\\\\\approx r(\\\\\\\\mu_X,\\\\\\\\mu_y) + \\\\\\\\frac{\\\\\\\\partial r}{\\\\\\\\partial X}(\\\\\\\\mu_X,\\\\\\\\mu_Y)\\\\\\\\Delta X + \\\\\\\\frac{\\\\\\\\partial r}{\\\\\\\\partial Y}(\\\\\\\\mu_X,\\\\\\\\mu_Y)\\\\\\\\Delta Y + O((\\\\\\\\Delta X)^2+(\\\\\\\\Delta Y)^2)$.</p>\\n\\n<p>If the variances of $X$ and $Y$ are small enough, then we can ignore the higher order terms and compute the variance of the linear approximation.</p>\\n\\n<p>$\\\\\\\\text{Var}\\\\\\\\bigg(r(\\\\\\\\mu_X,\\\\\\\\mu_y) + \\\\\\\\frac{\\\\\\\\partial r}{\\\\\\\\partial X}(\\\\\\\\mu_X,\\\\\\\\mu_Y)\\\\\\\\Delta X + \\\\\\\\frac{\\\\\\\\partial r}{\\\\\\\\partial Y}(\\\\\\\\mu_X,\\\\\\\\mu_Y)\\\\\\\\Delta Y)\\\\\\\\bigg)$</p>\\n\\n<p>$=(\\\\\\\\frac{\\\\\\\\partial r}{\\\\\\\\partial X}(\\\\\\\\mu_X,\\\\\\\\mu_Y))^2 \\\\\\\\text{Var}(X) + (\\\\\\\\frac{\\\\\\\\partial r}{\\\\\\\\partial Y}(\\\\\\\\mu_X,\\\\\\\\mu_Y))^2\\\\\\\\text{Var}(Y)$</p>\\n\\n<p>since this is just a constant plus a linear combination of $X$ and $Y$.</p>\\n\\n<p>$\\\\\\\\frac {\\\\\\\\partial r}{\\\\\\\\partial X}(x,y) = \\\\\\\\frac 1 y$ so at $(\\\\\\\\mu_x,\\\\\\\\mu_y)$ it is $\\\\\\\\frac{1}{\\\\\\\\mu_y}$.</p>\\n\\n<p>$\\\\\\\\frac {\\\\\\\\partial r}{\\\\\\\\partial Y}(x,y) = \\\\\\\\frac {-x} {y^2}$ so at $(\\\\\\\\mu_x,\\\\\\\\mu_y)$ it is $\\\\\\\\frac{-\\\\\\\\mu_x}{\\\\\\\\mu_y^2}$.</p>\\n\\n<p>Therefore, the variance of $\\\\\\\\frac XY$ will appear to be about </p>\\n\\n<p>$$\\\\\\\\frac{\\\\\\\\sigma^2_X}{\\\\\\\\mu_Y^2} + \\\\\\\\frac{\\\\\\\\mu_X^2 \\\\\\\\sigma^2_Y}{\\\\\\\\mu_Y^4}  = \\\\\\\\frac{\\\\\\\\mu_Y^2\\\\\\\\sigma_X^2 + \\\\\\\\mu_X^2 \\\\\\\\sigma^2_Y}{\\\\\\\\mu_Y^4}.$$</p>\\n\",\n",
       " \"<p>I have been trying to use weighted regression on some data where there are three categorical variables with the </p>\\n\\n<pre><code>lm(y ~ A*B*C) \\n</code></pre>\\n\\n<p>command in R. </p>\\n\\n<p>This was to try and deal with the problem that my residual plot does not have constant variance, and that for one of the factors the variance for two of the levels is much smaller than the other level and in the other factors, so I define weights, in a vector ww, for each observation,  so that the corresponding observations do not contribute as much as the other observation, and use </p>\\n\\n<pre><code>lm(y ~ A*B*C, weights=ww)\\n</code></pre>\\n\\n<p>My problem is that I get exactly the same residuals when I use weights than when I don't use weights. Is this supposed to happen? Is it because all the variables A, B, C are categorical? </p>\\n\\n<p>It sort of seems to make some sense because the regression coefficients (the beta's in the linear model) are just the means and the differences from the reference mean in the un-weighted model, and I think that will also be the case using weighted regression, but I am not sure. </p>\\n\\n<p>Can anyone help me with this? I am a bit lost. i.e. can someone tell me  whether I should NOT be using weighted least squares to fix problems with heterogeneous variance of residuals? (I have also tried transforming the response variable, $\\\\\\\\log(y)$, it helped but didn't fix the problem.  </p>\\n\",\n",
       " '<p>I am doing a Cox proportional hazards regression in R using <code>coxph</code>, which includes many variables. The Martingale residuals look great, and the Schoenfeld residuals are great for ALMOST all of the variables. There are three variables whose Schoenfeld residuals are not flat, and the nature of the variables is such that it makes sense that they could vary with time. </p>\\n\\n<p>These are variables I\\'m not really interested in, so making them strata would be fine. However, all of them are continuous variables, not categorical variables. So I perceive strata to not be a viable route*. I have tried building interactions between the variables and time, as described <a href=\"http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-cox-regression.pdf\">here</a>,  but we get the error:</p>\\n\\n<pre><code>  In fitter(X, Y, strats, offset, init, control, weights = weights,  :\\n  Ran out of iterations and did not converge\\n</code></pre>\\n\\n<p>I\\'m working with nearly 1000 data points, and am working with half a dozen variables with many factors each, so it feels like we\\'re pushing the limits of how this data can be sliced and diced. Unfortunately, all the simpler models I\\'ve tried with fewer included variables are clearly worse (ex. Schoenfeld residuals are crumbier for more variables). </p>\\n\\n<p>What are my options? Since I don\\'t care about these particular poorly-behaved variables, I\\'d like to just ignore their output, but I suspect that isn\\'t a valid interpretation!</p>\\n\\n<p>*One is continuous, one is an integer with a range of over 100, and one is an integer with a range of 6. Perhaps binning?</p>\\n',\n",
       " '<p>I would like to know why some languages like R has both NA and NaN. What are the differences or are they equally the same? Is it really needed to have NA?</p>\\n',\n",
       " '<p>If time series are cointegrated they admit VECM representation according to Granger Representation theorem. This is scantily explained in <a href=\"http://en.wikipedia.org/wiki/Johansen_test\" rel=\"nofollow\">this wikipedia page</a>. So if we have I(1) process:</p>\\n\\n<p>$$X_t=\\\\\\\\mu+\\\\\\\\Phi D_t+\\\\\\\\Pi_1 X_{t-1}+...+\\\\\\\\Pi_p X_{t-p}+\\\\\\\\varepsilon_t$$</p>\\n\\n<p>it admits VECM representation</p>\\n\\n<p>$$\\\\\\\\Delta X_t=\\\\\\\\mu+\\\\\\\\Phi D_t+\\\\\\\\Pi X_{t-p}+ \\\\\\\\Gamma_1 \\\\\\\\Delta X_{t-1}+...+\\\\\\\\Gamma_p \\\\\\\\Delta X_{t-p+1}+\\\\\\\\varepsilon_t$$</p>\\n\\n<p>What this means is that if you difference time series and do a linear regression as per your second approach you are not including the cointegration term $\\\\\\\\Pi X_{t-p}$. So your regression suffers from <a href=\"http://en.wikipedia.org/wiki/Omitted-variable_bias\" rel=\"nofollow\">omitted variable problem</a>, which in turn makes the test you are trying to use not viable.</p>\\n\\n<p><strong>Note:</strong> This a classical explanation. If on the other hand you search for \"Granger representation theorem\" quite a few links pop-up with examples when this theorem does not hold. Here <a href=\"http://www.ecosoc.org.au/files/File/TAS/ACE07/presentations%20%28pdf%29/Wilson.pdf\" rel=\"nofollow\">is one for example</a>. I would be wary though as with all claims which goes counter the widely accepted truths. They may be right, but they certainly attract certain kind of people which contribute nothing interesting to discussion, albeit they loudly claim they do. It is a general advice, I may be very wrong on this one, it was a bit of surprise for me to find these links.</p>\\n',\n",
       " '<p>(partially converted from my now-deleted comment above) </p>\\n\\n<p>The expected value and the arithmetic mean are the exact same thing. The median is related to the mean in a non-trivial way but you can say a few things about their relation:</p>\\n\\n<ul>\\n<li><p>when a distribution is symmetric, the mean and the median are the same </p></li>\\n<li><p>when a distribution is negatively skewed, the median is usually greater than the mean </p></li>\\n<li><p>when a distribution is positively skewed, the median is usually less than the mean </p></li>\\n</ul>\\n',\n",
       " '<p>Here are some hints.  I am sure you would have been taught these somewhere.  It is much simpler than the methods you are considering.</p>\\n\\n<p>First, if W=X-2Y, what do you get if you plug the means of X and Y into that equation?</p>\\n\\n<p>Secondly, if you multiply a random variable by a constant, the variance of the new random variable is going to be the original variance multiplied by that constant, squared.  </p>\\n\\n<p>Thirdly, if you then take two independent random variables and add them together, the the variance of the sums is the sum of the variance.</p>\\n',\n",
       " \"<p>I understand that a good way to see if the covariate should be transformed is to do a linear model and plot the residuals. if there is a pattern, a log-transformation may be needed.\\nbut.. I am right now trying to find the best model for a relationship with several possible predictor variables. So I don't know how to test in this case. I was able to do it in SLR, but in MLR I am not sure how to do it. Should I just add1, test it, test it in log-scale and go on? Actually I was going for the backward method; is there a way to do it with that?</p>\\n\\n<ul>\\n<li>I do know it is not that easy, you can't just plot the residuals and decide. this is just to understand how it would be done,so I can figure out the rest on my own (I guess). Has to be interpreted to do this anyway.</li>\\n</ul>\\n\",\n",
       " '<p>Consider a nominal variable A (e.g. smoker/non-smoker) that has been measured in a number of samples. Now i would like to now whether the ratio of A (smokers/non-smokers) differs between samples. Which test would be appropriate?</p>\\n',\n",
       " \"<p>Given the data points $x_1, \\\\\\\\ldots, x_n \\\\\\\\in \\\\\\\\mathbb{R}^d$ and labels $y_1, \\\\\\\\ldots, y_n \\\\\\\\in \\\\\\\\left \\\\\\\\{-1, 1 \\\\\\\\right\\\\\\\\}$, the hard margin SVM primal problem is</p>\\n\\n<p>$$ \\\\\\\\text{minimize}_{w, w_0} \\\\\\\\quad \\\\\\\\frac{1}{2} w^T w $$\\n$$ \\\\\\\\text{s.t.} \\\\\\\\quad  \\\\\\\\forall i: y_i (w^T x_i + w_0) \\\\\\\\ge 1$$</p>\\n\\n<p>which is a quadratic program with $d+1$ variables to be optimized for and $i$ constraints. The dual</p>\\n\\n<p>$$ \\\\\\\\text{maximize}_{\\\\\\\\alpha} \\\\\\\\quad \\\\\\\\sum_{i=1}^{n}{\\\\\\\\alpha_i} - \\\\\\\\frac{1}{2}\\\\\\\\sum_{i=1}^{n}{\\\\\\\\sum_{j=1}^{n}{y_i y_j \\\\\\\\alpha_i \\\\\\\\alpha_j x_i^T x_j}}$$\\n$$ \\\\\\\\text{s.t.} \\\\\\\\quad \\\\\\\\forall i: \\\\\\\\alpha_i \\\\\\\\ge 0 \\\\\\\\land \\\\\\\\sum_{i=1}^{n}{y_i \\\\\\\\alpha_i} = 0$$\\nis a quadratic program with $n + 1$ variables to be optimized for and $n$ inequality and $n$ equality constraints.</p>\\n\\n<p>When implementing a hard margin SVM, <strong>why would I solve the dual problem instead of the primal problem?</strong> The primal problem looks more 'intuitive' to me, and I don't need to concern myself with the duality gap, the Kuhn-Tucker condition etc.</p>\\n\\n<p>It would make sense to me to solve the dual problem if $d \\\\\\\\gg n$, but I suspect there are better reasons. Is this the case?</p>\\n\",\n",
       " '<p>I have three datasets as follows:</p>\\n\\n<ul>\\n<li>Dataset A can have a maximum score of 30. (Individual scores can be from 1 to 30)</li>\\n<li>Dataset B can have a maximum score of 45. (Individual scores can be from 1 to 45)</li>\\n<li>Dataset C can have a maximum score of 25. (Individual scores can be from 1 to 25)</li>\\n</ul>\\n\\n<p>The three datasets contribute equally to a final score, hence it is 33.33% each. i.e. (33.33% for dataset A + 33.33% for dataset B + 33.33% for dataset C) = 100% (final score)</p>\\n\\n<p>I have the following scores in each dataset:</p>\\n\\n<ul>\\n<li>Dataset A: 22</li>\\n<li>Dataset B: 15</li>\\n<li>Dataset C: 10</li>\\n</ul>\\n\\n<p>Is there a quick formula that I can use to work out the final score, based on the above datapoints? </p>\\n\\n<p>(This is not a homework question.)</p>\\n',\n",
       " '<p>I am working similarity searching of an HTTP Request relative to the last N number of days of requests my system has collected using locality sensitive hashing based on Moses Charikar\\'s \"<a href=\"http://www.cs.princeton.edu/courses/archive/spring04/cos598B/bib/CharikarEstim.pdf\" rel=\"nofollow\">Similarity Estimation Techniques from Rounding Algorithms</a>\". To compute the hash I am planning to represent each attribute as a bit vector based on a frequency table - for example, the vendor name passed as an HTTP header - current population\\'s frequency table looks as such:</p>\\n\\n<pre><code>Google Inc.                         54960\\nApple Computer, Inc.                36415\\nSony Computer Entertainment Inc.      142\\nMaxthon Asia Ltd.                      65\\nAdobe Systems Incorporated             28\\nResearch In Motion Limited             24\\nUbuntu                                 21\\nundefined                              11\\nCamino                                 11\\nApple Inc.                             10\\nYandex                                  8\\nFedora                                  5\\nRed Hat                                 3\\nLinux Mint                              2\\nCentOS                                  2\\nUCWEB                                   1\\nSabayon                                 1\\nGentoo                                  1\\n</code></pre>\\n\\n<p>Take a new request whose value is \"Apple Computer, Inc.\" the bit vector for that attribute would be: </p>\\n\\n<pre><code>[0,1,0,0,...] \\n</code></pre>\\n\\n<p>The second bit is \"1\" because \"Apple Computer, Inc.\" is second in the frequency table above. This implies recomputing this vector on some periodic basis as value frequencies change over time. More importantly, it\\'s also possible a value will not be present in this table. </p>\\n\\n<p>For values not present I would flip the last bit of my feature vector. For instance, if the value was \"Some Other Vendor\" the feature vector would be: </p>\\n\\n<pre><code>[0,0,0,0...1]\\n</code></pre>\\n\\n<p>So assuming no one points out some other serious flaw in my thinking, what I\\'m thinking about is if there\\'s a <strong><em>statistically</em></strong> valid way of dropping infrequent values - e.g. \"Gentoo\" and \"Sabayon\" - and just treating both of those as \"Other\" as well. Because this is categorical data it would seem to me there\\'s no meaningful way to calculate a mean and standard deviation: so that I could drop off those values say 3 deviations from the mean. </p>\\n',\n",
       " '<p>This looks to be an acceptable solution to my problem:</p>\\n\\n<pre><code>&gt; fit &lt;- lm(q0 ~ p0 + I(p0^2) + I(p0^3))\\n&gt; as.numeric(fit$coefficients[1]+fit$coefficients[2]*(p) + fit$coefficients[3]*(p^2) + fit$coefficients[4]*(p^3))\\n[1] 0.05947406\\n</code></pre>\\n\\n<p>Thanks to all to considered the problem! (Also, thank you whuber for your suggestion.)</p>\\n',\n",
       " \"<p>I think that very few scientists understand this basic point: It is only possible to interpret results of statistical analyses at face value, if every step was planned in advance. Specifically:</p>\\n\\n<ul>\\n<li>Sample size has to be picked in advance. It is not ok to keep analyzing the data as more subjects are added, stopping when the results looks good. </li>\\n<li>Any methods used to normalize the data or exclude outliers must also be decided in advance. It isn't ok to analyze various subsets of the data until you find results you like.</li>\\n<li>And finally, of course, the statistical methods must be decided in advance. Is it not ok to analyze the data via parametric and nonparametric methods, and pick the results you like. </li>\\n</ul>\\n\\n<p>Exploratory methods can be useful to, well, explore. But then you can't turn around and run regular statistical tests and interpret the results in the usual way.</p>\\n\",\n",
       " '<p>My biology test is to measure the rate of photosynthesis in leaf discs. It\\'s a simple experiment where I will infiltrate the leaf disc space with sodium-bicarbonate and cause a pressure difference to sink the leaf disc. Then I will shine a light on it causing photosynthesis and time it in minutes for it to rise to the top. EDIT** I will have three leaves of which I cut them all in half. So I will have 6 halves of leaves. Each half of a leaf would have been irradiated so in turn I will have 3 halves which are irradiated, and the other three are left as control. I will then use a cork borer to extract three sample circles from EACH half of a leaf. So I, in turn will have 18 samples. This is the results of ONE sample:</p>\\n\\n<pre><code>Time/minutes  Has it floated?\\n0                  No\\n0.5                No\\n\"\"\\n\"\"\\n7.5                Yes\\n</code></pre>\\n\\n<p>I have worked out that it will be two tailed since my hypothesis is noting whether there is a change in photosynthesis if irradiated with UV, but other than that, I\\'m stumped.</p>\\n',\n",
       " \"<p>I am trying to compute inter-rater reliability for ranking multiple objects across multiple scenarios.</p>\\n\\n<p>For example suppose for scenario A we had multiple objects and multiple raters like in the table below.  </p>\\n\\n<pre>\\nObject   V   W   X   Y   Z\\nRater1   2   1   3   4   5\\nRater2   1   3   5   2   4\\nRater3   4   5   2   1   3\\n</pre>\\n\\n<p>If I only had this one table, I would just compute Kendall's W and be on my way.  But what happens if we have multiple scenarios (say B and C in addition to A)?  Suppose in each scenario the objects differ and the raters might differ (think Mechanical Turk data).  How might I compute reliability in ranking across all scenarios?</p>\\n\",\n",
       " '<p>If I understand you correctly, you want to use</p>\\n\\n<pre><code>Variable1 :=  var1_0-20cm + var1_20-50cm + var1_50-100cm\\nVariable2 :=  var2_0-20cm + var2_20-50cm + var2_50-100cm\\n</code></pre>\\n\\n<p>(i.e. \"independent of depth\"; depending on how your data was generated you might want to use the mean or a weighted average instead of the sum. e.g. 0.20 times the first, 0.30 the second and 0.50 the third) instead of the full data space? What exactly is the problem with doing this? A key benefit is that this way you can control what happens quite well.</p>\\n\\n<p>Then in the end you can e.g. use PCA on these non-divided variables. You can however try to use the PCA result to project the original data, too - do the same mapping for the \"divided\" attributes that you got by PCA for the non-divided variables.</p>\\n',\n",
       " \"<p>I won't offer a rule of thumb, although I don't think 80 percent would be achievable in many situations.  </p>\\n\\n<p>I <em>will</em> suggest that a crucial thing to consider is why some people are not responding.  If you are certain non-response is completely random (which is vanishingly unlikely I'm afraid) and unrelated to any variables of interest than almost any response rate is satisfactory.  If not, then you need to consider the whole range of issues of bias, imputation, acceptable tolerance of inaccuracy, etc. before you can venture what is acceptable.</p>\\n\\n<p>I also strongly recommend Lumley's book.</p>\\n\",\n",
       " '<p>Statistical  significance is always measured in relation to a hypothesis. In the case of the correlation coefficient, Matlab reports significance (a p-value) in relation to the hypothesis that the correlation is zero (and it uses the assumption of normality - not uniformity - for the calculations). A low p-value is interpreted as evidence against the hypothesis.</p>\\n\\n<p>In this case, you can talk about significance if you have a hypothesis (eg $Var(X)$>5). Note that the hypothesis is a statement about the underlying population (a random variable) and not about the sample.</p>\\n',\n",
       " \"<p><em>I am not a statistician, so please excuse my lack of statistics knowledge/terminology.</em> </p>\\n\\n<p>I have bunch of network nodes that I want to run cluster analysis on and identify clusters. So as far as I understand, I can follow the following steps to run a hierarchical agglomerative analysis (HAC):</p>\\n\\n<ol>\\n<li>Identify variables</li>\\n<li>Define a distance function</li>\\n<li>Run the algorithm to join closer clusters and create one big cluster</li>\\n<li>Cut the dendrogram tree at the height that makes meaningful clusters based on context</li>\\n</ol>\\n\\n<p>My question is related to the second step even though it is not yet clear for me how I am going to do the last step.</p>\\n\\n<p>The data I want to analyse is bunch of computer network nodes which are down (not responding). I have the following information for each network node:</p>\\n\\n<ul>\\n<li>location (longitude, latitude)</li>\\n<li>time node went down</li>\\n<li>network provider</li>\\n</ul>\\n\\n<p>These are the most relevant information I believe I have to take into consideration for my clustering. Basically I want to cluster the nodes that went down probably because of the same reason in a region. </p>\\n\\n<p>For example if bunch of nodes went down at about the same time and physically they are close to each other and they have the same provider, so probably they fall into the same cluster.</p>\\n\\n<p>Now the question is how do I derive my distance function and include all these variables in it such that it would make sense? In other words <strong>what is the mechanism to derive a distance function based on multiple variables?</strong></p>\\n\\n<p>Also as you notice the variables are of different types. So, in the distance function should I take care of this by considering <strong>Gower's coefficient of similarity</strong>? How?</p>\\n\\n<p>Any examples or suggestion regarding whether I am in the right direction or not can be very helpful too.</p>\\n\",\n",
       " '<p>There are some frameworks under the <a href=\"http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rlai.html\" rel=\"nofollow\">RLAI</a> site.</p>\\n\\n<p>I don\\'t know of any which are already set up with game-play environments, but you might find some good ideas and C source code here,\\n\"<a href=\"https://docs.google.com/viewer?a=v&amp;q=cache%3awvqgxapPdyUJ%3aciteseerx.ist.psu.edu/viewdoc/download?doi%3D10.1.1.4.7712%26rep%3Drep1%26type%3Dpdf+reinforcement+learning+games&amp;hl=en&amp;gl=us&amp;pid=bl&amp;srcid=ADGEEShu99AMomWoXTIVSIE431_3DcqCIY8RahQwRmbrvh8HJn-KeSxJZ1rEyeISlt3LVEU6HsjtNsL_UeL5gnNccNOeHiNjUF26lzQUwK5rSz0p-_iUe6msSGRNSouKBzoAHQSntvGf&amp;sig=AHIEtbTP5EuJRehTSPt5Ob87Qi15pST4Dg\" rel=\"nofollow\">Reinforcement learning In board Games</a>,\" Imran Gohry *</p>\\n\\n<p>This is a table I copied from <a href=\"http://rads.stackoverflow.com/amzn/click/1584504218\" rel=\"nofollow\">AI Application Programming</a>, 2nd edition, M. Tim Jones (p. 226), to give an idea of some historical applications.</p>\\n\\n<p>Table 9.2 Interesting Historical Uses of RL</p>\\n\\n<pre><code> Application              Algorithm Used \\n\\n Backgammon (TD-Gammon)    TD(lambda) with a Neural Network \\n Blackjack                 TD(lambda)\\n Scheduling problems       TD(lambda), Time Delay Neural Networks\\n Robotic Control Systems   SARSA, TD(Lambda), Q Learning \\n Tic-Tac-Toe               Q Learning \\n Elevator Control          Q Learning \\n Chess/Checkers            Q Learning\\n</code></pre>\\n\\n<p>Good Luck. </p>\\n\\n<ul>\\n<li>If you have trouble sourcing, you can select html view or just do a google search for the paper and title.</li>\\n</ul>\\n',\n",
       " '<p>Since you mention Sorensen index, it seems you need a dissimilarity score rather than a dissimilarity test. (A score will give a numerical value indicating how different they are. A test will tell you whether the difference is significant with a given probability.)</p>\\n\\n<p>You can represent species abundance at each location by a histogram. This histogram can be normalized if you just care about relative abundance (e.g. that cats are twice as abundant as dogs), or unnormalized if you care about absolute numbers as well.</p>\\n\\n<p>There are many ways to measure dissimilarity of histograms. Some of the popular ones are:</p>\\n\\n<ul>\\n<li>Chi-squared statistic</li>\\n<li>L2 distance</li>\\n<li>Histogram intersection distance</li>\\n</ul>\\n',\n",
       " '<p>\"Are these enough to have confidence in the negative finding of our study\" - it depends on what you mean by \"have confidence\". Can you walk away and say \"The association is negative, we\\'re done here\". No. You can be confident that, having looked at it several ways, you\\'re not detecting an association in your data.</p>\\n\\n<p>But confidence in the negative finding as a reflection of \"truth\"? Not really. The confidence one should have in your findings aren\\'t a function of the sheer amount of analysis you throw at it. One could, for example, use ever more elaborate regression techniques to look at their data set in entirely new and novel ways, but if their subjects are misclassified, or they\\'ve missed a major confounding variable, then they simply have an impressive volume of incorrect results.</p>\\n\\n<p>There are other considerations as well. Your study may be underpowered. Smaller studies are <em>more</em> underpowered, but it\\'s possible to be both underpowered and lucky. Similarly, you may simply be experiencing some amount of random variation - some number of studies looking at a \"Capital-T Truth, God\\'s Eye View\" positive effect will still find negative or null findings. It\\'s possible you study was one of these.</p>\\n\\n<p>If we assume that your data was collected correctly however, and bias from things like misclassification and confounding are minimal, I think you can be confident in <em>your</em> finding for <em>your</em> study population, and you can be confident that your findings <em>suggest</em> that the association is negative. But a sheer mass of analysis on a single study population does not a body of evidence make.</p>\\n',\n",
       " '<p>For my thesis I am implementing a tracking algorithm that tracks clusters of datapoints.</p>\\n\\n<p>However, I have a hard time finding research papers and/or an overview of commonly used algoritms in this subject.</p>\\n',\n",
       " \"<p>Suppose we have $\\\\\\\\newcommand{\\\\\\\\E}{\\\\\\\\mathrm{Exp}} X \\\\\\\\sim \\\\\\\\E(\\\\\\\\lambda)$, $Y \\\\\\\\sim \\\\\\\\E(\\\\\\\\mu)$, and $W = \\\\\\\\min(X,Y)$. </p>\\n\\n<p>I know that $W \\\\\\\\sim \\\\\\\\E(\\\\\\\\lambda+\\\\\\\\mu)$. I know how to derive it. But, I tried this alternate derivation that gave me a different distribution for $W$, and I still can't figure out what's wrong with it.</p>\\n\\n<p>I started with </p>\\n\\n<p>$\\\\\\\\newcommand{\\\\\\\\rd}{\\\\\\\\,\\\\\\\\mathrm d}\\\\\\\\renewcommand{\\\\\\\\Pr}{\\\\\\\\mathbb P}f_W(t) = f_X(t) \\\\\\\\Pr(X&lt;Y) + f_Y(t) \\\\\\\\Pr(Y&lt;X)$</p>\\n\\n<p>Now I need $\\\\\\\\Pr(X &lt; Y)$. Seems straightforward: </p>\\n\\n<p>$\\\\\\\\begin{align}\\\\\\\\n\\\\\\\\Pr(X &lt; Y) &amp;= \\\\\\\\int_0^\\\\\\\\infty [1-F_X(t)] f_Y(t) \\\\\\\\rd t \\\\\\\\\\\\\\\\\\\\\\\\n&amp;= \\\\\\\\int_0^\\\\\\\\infty e^{-\\\\\\\\lambda t} \\\\\\\\mu e^{-\\\\\\\\mu t} \\\\\\\\rd t \\\\\\\\\\\\\\\\\\\\\\\\n&amp;= \\\\\\\\frac{\\\\\\\\mu}{-(\\\\\\\\lambda + \\\\\\\\mu)} \\\\\\\\left[ e^{-(\\\\\\\\lambda + \\\\\\\\mu)t} \\\\\\\\right]^{\\\\\\\\infty}_0\\\\\\\\\\\\\\\\\\\\\\\\n&amp;= \\\\\\\\frac{\\\\\\\\mu}{\\\\\\\\mu+\\\\\\\\lambda} \\\\\\\\&gt;.\\\\\\\\n\\\\\\\\end{align}$</p>\\n\\n<p>And, if I do the same thing for $\\\\\\\\Pr(Y &lt; X)$, I get \\n$\\\\\\\\Pr(Y &lt; X) = \\\\\\\\frac{\\\\\\\\lambda}{\\\\\\\\mu + \\\\\\\\lambda}$.</p>\\n\\n<p>So $\\\\\\\\Pr(X &lt; Y)$ and $\\\\\\\\Pr(Y &lt; X)$ sum up to 1 as expected. Encouraging.</p>\\n\\n<p>And now I substitute that into my original equation:</p>\\n\\n<p>$ \\\\\\\\begin{align}\\\\\\\\nf_W(t) &amp;= \\\\\\\\frac{\\\\\\\\mu}{\\\\\\\\mu + \\\\\\\\lambda} \\\\\\\\lambda e^{- \\\\\\\\lambda t} + \\\\\\\\frac{\\\\\\\\lambda}{\\\\\\\\mu + \\\\\\\\lambda} \\\\\\\\mu e^{- \\\\\\\\mu t} \\\\\\\\\\\\\\\\\\\\\\\\n&amp;= \\\\\\\\frac{\\\\\\\\lambda\\\\\\\\mu}{\\\\\\\\lambda + \\\\\\\\mu}\\\\\\\\left(e^{-\\\\\\\\lambda t} + e^{-\\\\\\\\mu t}\\\\\\\\right) \\\\\\\\&gt;. \\\\\\\\n\\\\\\\\end{align}$</p>\\n\\n<p>That... That's no exponential.</p>\\n\\n<p>Where did I go wrong?</p>\\n\",\n",
       " '<p>Easy example:  Let $X$ be a random variable that is $-1$ or $+1$ with probability 0.5.  Then let $Y$ be a random variable such that $Y=0$ if $X=-1$, and $Y$ is randomly $-1$ or $+1$ with probability 0.5 if $X=1$.</p>\\n\\n<p>Clearly $X$ and $Y$ are highly dependent (since knowing $Y$ allows me to perfectly know $X$), but their covariance is zero:  They both have zero mean, and </p>\\n\\n<p>$$\\\\\\\\eqalign{\\\\\\\\n\\\\\\\\mathbb{E}[XY] &amp;=&amp;(-1) &amp;\\\\\\\\cdot &amp;0 &amp;\\\\\\\\cdot &amp;P(X=-1) \\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\n&amp;+&amp; 1 &amp;\\\\\\\\cdot &amp;1 &amp;\\\\\\\\cdot &amp;P(X=1,Y=1) \\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\n&amp;+&amp; 1 &amp;\\\\\\\\cdot &amp;(-1)&amp;\\\\\\\\cdot &amp;P(X=1,Y=-1) \\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\n&amp;=&amp;0.\\\\\\\\n}$$</p>\\n\\n<p>Or more generally, take any distribution $P(X)$ and any $P(Y|X)$ such that $P(Y=a|X) = P(Y=-a|X)$ for all $X$ (i.e., a joint distribution that is symmetric around the $x$ axis), and you will always have zero covariance.  But you will have non-independence whenever $P(Y|X) \\\\\\\\neq P(Y)$; i.e., the conditionals are not all equal to the marginal. Or ditto for symmetry around the $y$ axis. </p>\\n',\n",
       " \"<p>I am interested in using a spatial research design.  Imagine a line, like a time zone line.  For example, in the United States, the line that makes between Eastern Standard Time and Central Standard Time runs North to South through the U.S. (and other places), more or less. </p>\\n\\n<p>Suppose the United States implemented a policy where, with respect to county boundaries, all counties west of the 75th longitude were to get a tax deduction.  No counties to the right of it would.  I want to evaluate the impacts of the this tax deduction on crime.  </p>\\n\\n<p>For the sake of argument, can we please assume that the discontinuity is valid.  I am trying to understand this idea for spatial discontinuities, in general?  </p>\\n\\n<p>I can imagine proceeding one of two ways.  One way would a regression discontinuity.  Exploiting the distance of say a county from this line.  The other way would be a matching of counties that straddle this line on opposite sides.  Note that a county might appear more than once if it borders more than one county on the opposite side of the time zone line.  </p>\\n\\n<p>The regression discontinuity includes a larger sample because it may include counties that are not exclusively along the border.  The matching hones in on counties that touch each other, whereas the regression discontinuity, at best, can partition the time zone into smaller segmented lines where segments likely include multiple pairs of counties along the time zone.  </p>\\n\\n<p>What are the trade-offs of the research design?  Which is preferred?  Does one offer something that the other doesn't?  </p>\\n\\n<p>Thank you very much for your help!</p>\\n\",\n",
       " \"<p>As @mbq suggests, a battery of binary classifiers is a good place to start.  Ridge regression classifiers work pretty well on text classification problems (choose the ridge parameter via leave-one-out cross-validation.  If training time is not an issue, also use the bootstrap as a further protection against over-fitting; the committee of boostrap classifiers can be amalgamated into one, so it doesn't have a computational cost at runtime).</p>\\n\\n<p>However, as there are 4K classes and only 10K samples, it will probably be necessary to look at the hierarchy of classes and try to predict whether the page fits into broad categories first.</p>\\n\",\n",
       " '<p>A simple way to turn categorical variables into a set of dummy variables for use in models in SPSS is using the do repeat syntax. This is the simplest to use if your categorical variables are in numeric order.</p>\\n\\n<pre><code>*making vector of dummy variables.\\nvector dummy(3,F1.0).\\n*looping through dummy variables using do repeat, in this example category would be the categorical variable to recode. \\ndo repeat dummy = dummy1 to dummy3 /#i = 1 to 3.\\ncompute dummy = 0.\\nif category = #i dummy = 1.\\nend repeat.\\nexecute. \\n</code></pre>\\n\\n<p>Otherwise you can simply run a set of if statements to make your dummy variables. My current version (16) has no native ability to specify a set of dummy variables automatically in the regression command (like you can in Stata using the <a href=\"http://www.stata.com/support/faqs/data/dummy.html\" rel=\"nofollow\">xi command</a>) but I wouldn\\'t be surprised if this is available in some newer version. Also take note of dmk38\\'s point #2, this coding scheme is assuming nominal categories. If your variable is ordinal more discretion can be used.</p>\\n\\n<p>I also agree with dmk38 and the talk about regression being better because of its ability to specify missing data in a particular manner is a completely separate issue.  </p>\\n',\n",
       " '<p>First of all this is not a problem to pin on frequentist testing.  The problem lies in the null hypothesis that the means are exactly equal.  Therefore if the populations differ in means by any small amount and the sample size is large enough the chance to reject this null hypothesis is very high.  Therefore the p-value for your test turned out to be very small.  The culprit is the choice of null hypothesis.  Pick d>0 and take the null hypothesis to be that the means differ by less than d in absolute value by less than d.  You pick d so that the real difference has to be satisfactorily large to reject.  Your problem goes away.  Bayesian testing does not solve your problem if you insist on a null hypothesis of exact equality of means. </p>\\n',\n",
       " '<p>As mentioned by others, James-Stein is not often used directly, but is really the first paper on shrinkage, which in turn is used pretty much everywhere in single and multiple regression. The link between James-Stein and modern estimation is explained in detail <a href=\"http://www-stat.stanford.edu/~candes/papers/NonlinearEstimation.pdf\" rel=\"nofollow\">in this paper</a> by E.Candes. Going back to your question, I think James-Stein is an intellectual non-curiosity, in the sense that it was intellectual for sure, but had an incredibly disruptive effect on Statistics, and nobody could dismiss it as a curiosity afterwards. <em>Everyone</em> thought that empirical means were an admissible estimator, and Stein proved them wrong with a counterexample. The rest is history.</p>\\n',\n",
       " '<p>I intend to apply Kevin Murphy\\'s <a href=\"http://www.cs.ubc.ca/~murphyk/Software/HMM/hmm.html\" rel=\"nofollow\">Hidden markov model</a> (HMM) toolbox. I have a set of production rules(arbitrary) $A_0 \\\\\\\\to AB [p=1]$, $A\\\\\\\\to aC [p=1]$, $B\\\\\\\\to bbC [p=0.5]$, $B\\\\\\\\to b [p=0.5]$ where $A_0$ is the start symbol, A, B, C are the non terminals, and a and b are the terminals and the probabilities are put in brackets. The observation is {aabc}. </p>\\n\\n<p>I do not know how to use the Viterbi parsing algorithm and cannot understand what all values in the parameters to use in the toolbox. Can anyone explain with a simple example how to use the program so as to select a maximum likelihood parse and how to train?</p>\\n\\n<p>The objective is to classify a language or a text string. Any other example apart from  language would also help. Primary question is how to feed in all these data in the toolbox so as to train the HMM and how to use it for the task since evrything in it uses rand function, how do I customize it?</p>\\n\\n<ol>\\n<li>Number of states Q=4 ? ($A_0$, A, B, C)</li>\\n<li>Number of observation symbols of length T=4; number of sequences nex=1?</li>\\n</ol>\\n',\n",
       " \"<p>I was wondering what's the difference between variance and standard deviation? </p>\\n\\n<p>If you calculate it its is clear that you get the standard deviation out of the variance...</p>\\n\\n<p>But what does that mean in terms of the distribution you are looking at?</p>\\n\\n<p>Why do you really need a standard deviation?</p>\\n\\n<p>I appreciate your answer!!!</p>\\n\",\n",
       " \"<p>In the Breusch-Pagan test, the predictors included in the so-called auxiliary regression were unspecified, but generally assumed to be only those from the original regression. White provided a specific set of predictors that should be included---those from the original regression, those predictors squared, and all of the pairwise interactions. The White test is really just a special case of the Breusch-Pagan test. White's contribution was to show the properties of his proposed case. His version is the one typically used, as far as I've seen.</p>\\n\",\n",
       " '<blockquote>\\n  <p><strong>Possible Duplicate:</strong><br>\\n  <a href=\"http://stats.stackexchange.com/questions/5504/normal-distribution-probability\">Normal distribution probability</a>  </p>\\n</blockquote>\\n\\n\\n\\n<p>Issues getting to the bottom of a HW problem, but I am not looking for the answer, just some guidance.</p>\\n\\n<p>x has a normal distribution with \\n   the specified mean = 15.9\\n   and standard deviation = 3.6</p>\\n\\n<p>Find the indicated probability P(10 ≤ x ≤ 26)?</p>\\n\\n<p>I assume i need to take the values 10 and 26 and calculate Z-Scores, and then go the Z-Score table and take the differences in the P-Values?  Is this the right approach or am I missing something?  I followed that reasoning and the answer was wrong - so either I made a rounding  mistake, or have chosen the wrong path.</p>\\n\\n<p>Thank you for the advice in advance.</p>\\n',\n",
       " '<p>I consulted various books and get confused about what the differences in Assumptions for Regression Models, Ordinary Least Square (OLS), and Multiple Regression Models are?</p>\\n\\n<p>As I read more about it I just get more confused. Is there a rationally understandable way to explain this that would not be confusing?</p>\\n\\n<p>Do these assumptions vary with respect to objective (maybe prediction or descriptive)?</p>\\n\\n<p>What would be the reasonable list of assumptions for </p>\\n\\n<ol>\\n<li>Linear Regression Models</li>\\n<li>Simple Linear Regression Models</li>\\n<li>Multiple Linear Regression Models</li>\\n<li>Ordinary Least Square (OLS) Method?</li>\\n</ol>\\n\\n<p>I have also read the related questions on <a href=\"http://stats.stackexchange.com/questions/16381/what-is-a-complete-list-of-the-usual-assumptions-for-linear-regression\">What are the Complete List....</a> and\\n<a href=\"http://stats.stackexchange.com/questions/17585/assumptions-needed-for-multiple-linear-regression\">Assumption Needed for Multiple linear Regression</a>. But still it is not clear to me.</p>\\n',\n",
       " \"<p>Linear model in matrix form is</p>\\n\\n<p>$\\n\\\\\\\\mathbf{y}=\\\\\\\\mathbf{X}\\\\\\\\beta+\\\\\\\\epsilon\\\\\\\\textrm{ where }\\\\\\\\epsilon\\\\\\\\sim\\\\\\\\mathbb{N}\\\\\\\\left(0,\\\\\\\\sigma^{2}\\\\\\\\mathbf{I}\\\\\\\\right).\\n$</p>\\n\\n<p>If \\n$\\n\\\\\\\\mathbf{K}^{\\\\\\\\prime}\\\\\\\\left(\\\\\\\\mathbf{X}^{\\\\\\\\prime}\\\\\\\\mathbf{X}\\\\\\\\right)^{-} \\\\\\\\mathbf{K}\\n$\\nis nonsingular, then</p>\\n\\n<p>$\\n\\\\\\\\textrm{rank}\\\\\\\\left[\\\\\\\\mathbf{K}^{\\\\\\\\prime}\\\\\\\\left(\\\\\\\\mathbf{X}^{\\\\\\\\prime}\\\\\\\\mathbf{X}\\\\\\\\right)^{-} \\\\\\\\mathbf{K}\\\\\\\\right] = \\\\\\\\textrm{rank}\\\\\\\\left(\\\\\\\\mathbf{K}^{\\\\\\\\prime}\\\\\\\\right).\\n$\\n(From <strong>Linear Models by Searle</strong>)</p>\\n\\n<p>I'm struggling to understand the last expression invloving ranks. Is this a result of any theorem? I'd highly appreciate if you explaing this to me. Thanks</p>\\n\",\n",
       " '<p>This is definitely a question that doesn\\'t have \"an answer\".  It is completely dependent on what you want to do.  That aside, I\\'ll share the packages that I install as a standard with an R update...</p>\\n\\n<pre><code>install.packages(c(\"car\",\"gregmisc\",\"xtable\",\"Design\",\"Hmisc\",\"psych\",\\n                        \"CCA\", \"fda\", \"zoo\", \"fields\",\\n                      \"catspec\",\"sem\",\"multilevel\",\"Deducer\",\"RQDA\"))\\n</code></pre>\\n\\n<p>and leave it to you to investigate those packages and see if they are valuable to you.</p>\\n',\n",
       " '<p>This sounds like what I call an \"added variable\" plot.  The idea behind these is to provide a visual way of whether adding a variable to a model (ht9 in your case) is likely to add anything to the model (soma on wt9 in your case).</p>\\n\\n<p>It was explained to me like this.  When you fit a linear regression, the order of the variables matters.  It\\'s kind of like imagining the variance in the soma variable as an \"island\". The first variables \"claims\" a portion of the variance on the island, and the second variable \"claims\" what it can from what is left over.</p>\\n\\n<p>So basically this plot will show you if \"what is left to explain\" in soma\\'s variation (residuals from soma.wt9) can be explained by \"the capacity of ht9 to explain anything over and above wt9\" (residuals from ht9.wt9).</p>\\n\\n<p>You can also show mathematically what is going on.  Residuals from soma.wt9 are calculate as:\\n$$e_{i}=soma-\\\\\\\\beta_{0}-\\\\\\\\beta_{1}wt9$$\\nresiduals from ht9.wt9 are:\\n$$f_{i}=ht9-\\\\\\\\alpha_{0}-\\\\\\\\alpha_{1}wt9$$\\nRegression of $e_i$ on $f_i$ through the origin (because $\\\\\\\\overline{e}=\\\\\\\\overline{f}=0$, so line will pass through origin) gives\\n$$e_{i}=\\\\\\\\delta f_{i}$$\\nSubstituting the residual equations into this one gives:\\n$$soma-\\\\\\\\beta_{0}-\\\\\\\\beta_{1}wt9=\\\\\\\\delta (ht9-\\\\\\\\alpha_{0}-\\\\\\\\alpha_{1}wt9)$$\\nRe-arranging terms gives:\\n$$soma=(\\\\\\\\beta_{0}-\\\\\\\\delta\\\\\\\\alpha_{0})+(\\\\\\\\beta_{1}-\\\\\\\\delta\\\\\\\\alpha_{1})wt9+\\\\\\\\delta ht9$$</p>\\n\\n<p>Hence, the estimated slope (using OLS regression) will be the same in the model with $soma = \\\\\\\\beta_0+\\\\\\\\beta_{wt9}wt9 + \\\\\\\\beta_{ht9}ht9$ as in the model $resid.soma=\\\\\\\\beta_{ht9} resid.ht9$\\nThis also shows explicitly why having correlated regressor variables ($\\\\\\\\alpha_{1}$ is a rescaled correlation) will make the estimated slopes change, and possibly be the \"opposite sign\" to what is expected.</p>\\n\\n<p>I think this method was actually how Multiple regression was carried out before computers were able to invert large matrices.  It may be quicker to invert lots of $2\\\\\\\\times 2$ matrices than it is to invert one huge matrix.</p>\\n',\n",
       " '<p>I am reading Greene\\'s textbook Econometric Analysis where he says that, if there\\'s multicollinearity, then:</p>\\n\\n<ul>\\n<li>Small changes in data lead to large swings in parameter estimates.</li>\\n<li>Coefficients have high standard errors even though they\\'re jointly significant.</li>\\n<li>Coefficients have the \"wrong\" sign or implausible magnitudes.</li>\\n</ul>\\n\\n<p>I have three question:</p>\\n\\n<ul>\\n<li>What are the consequences for the unbiasedness and consistency of the OLS estimators in the presence of multicollinearity?</li>\\n<li>Is the efficiency of the estimators reduced in the presence of multicollinearity?</li>\\n<li>Do Greene\\'s points hold (yet to a lesser extent) for slightly correlated independent variables? For example, would all three points hold (to a small extent) if the correlation between the regressors is $\\\\\\\\rho = 0.1$, for example?</li>\\n</ul>\\n',\n",
       " '<p>If you want to select amongst <em>pre-specified</em> models, this should work the same with GEE as elsewhere.  For example, if you were comparing a nested model to a full model, you could test that.  If the models weren\\'t nested, you could use an informational criterion (such as the QIC) to help adjudicate between them.  Another approach is to use the <a href=\"http://www.ejwagenmakers.com/2004/PBCM.pdf\" rel=\"nofollow\">Parametric Bootstrap Cross-fitting Method</a>; this is a very solid approach, but computationally very expensive.  </p>\\n',\n",
       " '<p>If you know nothing about director N (ie whether they make similar movies to director G) it would seem to me that your best chance would be to substitute for <code>factor(director)</code> in your linear predictor the \"average\" value of the director coefficients.  This would require estimating who the average director is and using them instead of DN.</p>\\n\\n<p>Alternatively, you could fit the model without the <code>factor(director)</code> as an explanatory variable at all.  If the purpose is to predict a value based on a certain set of explanatory variables, there\\'s not good including in the model something that isn\\'t going to be available to the predictor.</p>\\n\\n<p>A final comment, not really part of the \"answer\" to your question - it seems likely that the variables <code>liked_by_user_2 ... liked_by_user_k</code> will be highly collinear.  Collinearity can play havoc with predictive models; one reason why parsimonious models are generally preferred for predictve purposes.  Whether this is a problem for you (and what should be done about it) depends on how big k is compared to your sample size.</p>\\n',\n",
       " '<p>You might consider testing backward (with a rolling window) for co-integration between <code>x</code> and the long term mean.</p>\\n\\n<p>When <code>x</code> is flopping around the mean, hopefully the windowed Augmented Dickey Fuller test, or whatever co-integration test you choose, will tell you that the two series are co-integrated.   Once you get into the transition period, where the two series stray away from each other, hopefully your test will tell you that the windowed series are not co-integrated.</p>\\n\\n<p>The problem with this scheme is that it is harder to detect co-integration in a smaller window.   And, a window that is too big, if it includes only a small segment of the transition period, will tell you that the windowed series is co-integrated when it shouldn\\'t.    And, as you might guess, there\\'s no way to know ahead of time what the \"right\" window size might be.</p>\\n\\n<p>All I can say is that you\\'ll have to play around with it to see if you get reasonable results.</p>\\n',\n",
       " '<p>1st I would recommend including an intercept term. </p>\\n\\n<p>2nd I would suggest doing an F-test of joint significance for the set of categorical variables. If that is significant, proceed to looking at interpreting individual coefficients. The F-test is an indicator of the significance of the category. </p>\\n\\n<p>3rd if the F-test indicates joint significance, then as Charlie said, the interpretation of individual coefficients is done with respect to the intercept.</p>\\n',\n",
       " '<p>I\\'m not sure #1 has a specific course of action, other than \"add some more predictive features to your model\".</p>\\n\\n<p>There are probably many potential causes to #2, but one can be outliers in the training dataset skewing some of the parameters in a logistic regression model.</p>\\n',\n",
       " '<p>My take, as an Epidemiologist: The question really isn\\'t answerable as given, for several reasons:</p>\\n\\n<ul>\\n<li>Without very subject specific knowledge, there\\'s no way of knowing if there\\'s effect measure modification between some of those estimates. For example, if you\\'re more likely to have lupus as a woman and more likely to have it as a minority, what happens if you are a female minority member? Are the two independent? Do they interact additively? Multiplicatively?</li>\\n<li>There\\'s a key factor missing: Why is he asking? He\\'s probably not sitting at his desk going \"I wonder if she has lupus...\" Next week, we\\'re likely not going to get \"What is the probability my wife has dengue fever?\" There\\'s a <em>reason</em> he thinks this is true, which distorts all those statistics again, as those are population figures, not \"Population where a family member suspects you have lupus\" figures.</li>\\n<li>The closest thing I could peg as a thing where you could produce a specific value is the \"Negative Predictive Value\" of the diagnostic test, but a quick googling suggests there are several brands of that particular type of test available, and without more information, you can\\'t really answer it.</li>\\n</ul>\\n\\n<p>What should this poor guy be told? To consult with his wife and her doctor. Trying to apply population level statistics to an individual is exactly <em>not</em> what epidemiological evidence is meant to do.</p>\\n',\n",
       " \"<p>By using the SciPy built-in function source(), I could see a printout of the source code for the function ttest_ind(). Based on the source code, the SciPy built-in is performing the t-test assuming that the variances of the two samples are equal. It is not using the Welch-Satterthwaite degrees of freedom.</p>\\n\\n<p>I just want to point out that, crucially, this is why you should not just trust library functions. In my case, I actually do need the t-test for populations of unequal variances, and the degrees of freedom adjustment might matter for some of the smaller data sets I will run this on. SciPy assumes equal variances but does not state this assumption.</p>\\n\\n<p>As I mentioned in some comments, the discrepancy between my code and SciPy's is about 0.008 for sample sizes between 30 and 400, and then slowly goes to zero for larger sample sizes. This is an effect of the extra (1/n1 + 1/n2) term in the equal-variances t-statistic denominator. Accuracy-wise, this is pretty important, especially for small sample sizes. It definitely confirms to me that I need to write my own function. (Possibly there are other, better Python libraries, but this at least should be known. Frankly, it's surprising this isn't anywhere up front and center in the SciPy documentation for ttest_ind()).</p>\\n\",\n",
       " '<p>I am having somewhat of a problem setting up contrasts from an anova in R. I have done this in the past, but at the moment I don\\'t seem to be able to get R to use my contrast matrix instead of the treatment contrasts. Any help on this would be greatly appreciated, I guess its possible I am making some basic mistake that I can not see.</p>\\n\\n<p>(I am using R2.15.1)</p>\\n\\n<p>My data consists of growth rate estimates for a 37 of bacterial clones. For these clones we measured growth rate multiple times (4-20) and we were wanting to simply know whether some of the the clones had a different growth rate (mu) from an ancestral clone (Anc). </p>\\n\\n<p>So I set up an anova as:</p>\\n\\n<pre><code>m1&lt;-aov(mu~clone)\\n</code></pre>\\n\\n<p>which gave me a significant effect of clone: </p>\\n\\n<pre><code>             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \\nclone        36  32.52  0.9033   2.986 3.98e-06 ***\\nResiduals   123  37.21  0.3025                     \\n---\\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 \\n</code></pre>\\n\\n<p>I then set up my contrast matrix</p>\\n\\n<pre><code>levels(clone)\\n [1] \"A21\"  \"A22\"  \"A23\"  \"A24\"  \"A25\"  \"A26\"  \"A27\"  \"A28\"  \"A29\"  \"A30\" \\n[11] \"A31\"  \"A32\"  \"A33\"  \"A34\"  \"A35\"  \"A36\"  \"A37\"  \"A38\"  \"A39\"  \"A40\" \\n[21] \"Anc\"  \"T126\" \"T127\" \"T128\" \"T129\" \"T130\" \"T131\" \"T132\" \"T133\" \"T134\"\\n[31] \"T135\" \"T35\"  \"T36\"  \"T37\"  \"T38\"  \"T39\"  \"T40\" \\n</code></pre>\\n\\n<p>As I wanted to compare all other clones to the Ancestral (Anc), I created a matrix with 36 columns in which I assigned 1 to Anc, -1 to the clone I wanted to compare and 0 to all others. Thus the matrix was as: </p>\\n\\n<pre><code>contrasts(clone)&lt;-cbind(\\nc(-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,-1,0,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,-1,0,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,-1,0,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,-1,0,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,-1,0,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,-1,0,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,-1,0,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0)\\n ,c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1))\\n</code></pre>\\n\\n<p>Now, the problem is that when I rerun the anova and do summary.lm, I still get the treatment contrasts being used and obtain:</p>\\n\\n<pre><code>Call:\\naov(formula = mu ~ clone)\\n\\nResiduals:\\n      Min        1Q    Median        3Q       Max \\n-0.059077 -0.010799  0.000524  0.013324  0.057926 \\n\\nCoefficients:\\n            Estimate Std. Error t value Pr(&gt;|t|)    \\n(Intercept) 0.111175   0.011905   9.339 5.35e-16 ***\\ncloneA22    0.057165   0.016836   3.395 0.000923 ***\\ncloneA23    0.034798   0.016836   2.067 0.040843 *  \\ncloneA24    0.009232   0.016836   0.548 0.584436    \\ncloneA25    0.047776   0.016836   2.838 0.005316 ** \\ncloneA26    0.013474   0.016836   0.800 0.425091    \\ncloneA27    0.012300   0.016836   0.731 0.466445    \\ncloneA28    0.047080   0.018185   2.589 0.010787 *  \\ncloneA29    0.036722   0.018185   2.019 0.045626 *  \\ncloneA30    0.035574   0.016836   2.113 0.036625 *  \\ncloneA31    0.060275   0.016836   3.580 0.000493 ***\\ncloneA32    0.045941   0.016836   2.729 0.007290 ** \\ncloneA33    0.017751   0.016836   1.054 0.293792    \\ncloneA34    0.013581   0.016836   0.807 0.421418    \\ncloneA35    0.018089   0.016836   1.074 0.284750    \\ncloneA36    0.035200   0.016836   2.091 0.038607 *  \\ncloneA37    0.048030   0.016836   2.853 0.005086 ** \\ncloneA38    0.046683   0.016836   2.773 0.006423 ** \\ncloneA39    0.040877   0.016836   2.428 0.016632 *  \\ncloneA40    0.051376   0.016836   3.052 0.002790 ** \\ncloneAnc    0.020183   0.013161   1.533 0.127727    \\ncloneT126   0.020191   0.016836   1.199 0.232738    \\ncloneT127   0.051375   0.016836   3.051 0.002790 ** \\ncloneT128   0.043587   0.016836   2.589 0.010789 *  \\ncloneT129   0.038895   0.016836   2.310 0.022543 *  \\ncloneT130   0.017442   0.016836   1.036 0.302238    \\ncloneT131   0.020714   0.016836   1.230 0.220928    \\ncloneT132   0.038433   0.016836   2.283 0.024159 *  \\ncloneT133   0.028807   0.016836   1.711 0.089601 .  \\ncloneT134   0.037626   0.016836   2.235 0.027233 *  \\ncloneT135   0.017300   0.016836   1.028 0.306186    \\ncloneT35    0.050487   0.016836   2.999 0.003282 ** \\ncloneT36    0.040415   0.016836   2.401 0.017871 *  \\ncloneT37    0.054457   0.016836   3.235 0.001565 ** \\ncloneT38    0.041835   0.016836   2.485 0.014304 *  \\ncloneT39    0.038966   0.016836   2.314 0.022301 *  \\ncloneT40    0.025847   0.016836   1.535 0.127292\\n</code></pre>\\n\\n<p>So here clone A21 is still being used for comparison. </p>\\n\\n<p>Thanks in advance for any possible help,</p>\\n\\n<p>Ricardo</p>\\n',\n",
       " '<p>In the field of ecological statistics (e.g. mark-recapture) we often have long time series of data, where any individuals may only be exposed to sampling for only a portion of the total time series. In this context we can consider <em>every</em> individual that was exposed to sampling during the course of the experiment, a measure we call the superpopulation.  This is different from the population at any given time point, which is the total number of individuals exposed to sampling at that time point.  Both of these definitions of a population are finite measures.</p>\\n\\n<p><strong>Aside</strong> -  I used the term \"exposed to sampling\" as population heterogeneity is often the rule rather than the exception in ecology.  Subpopulations may exist that behave differently and may completely avoid detection by our survey techniques.  These individuals thus are not part of our statistical population definition.</p>\\n',\n",
       " '<p>Pursuant to my discussion on the conceptual overlap between effects size and likelihood ratios <a href=\"http://stats.stackexchange.com/questions/4551/what-are-common-statistical-sins/6037#6037\">here</a>, I wonder if the likelihood ratio for each effect against its respective null might serve as a useful metric to achieve the aims sought by those who conventionally employ effect size measures.</p>\\n',\n",
       " '<p>If your response variable is ordinal, you may want to consider and \"ordered logistic regression\".  This is basically where you model the cumulative probabilities {in the simple example, you would model $Pr(Y\\\\\\\\leq 1),Pr(Y\\\\\\\\leq 2),Pr(Y\\\\\\\\leq 3)$}.  This incorporates the ordering of the response into the model, <em>without</em> the need for an arbitrary assumption which transforms the ordered response into a numerical one (although having said that, this can be a useful first step in exploratory analysis, or in selecting which $X$ and $Z$ variables are not necessary)</p>\\n\\n<p>There is a way that you can get the glm() function in R to give you the MLE\\'s for this model (other wise you would need to write your own algorithm to get the MLEs).  You define a new set of variables, say $W$, where these are defined as</p>\\n\\n<p>$$W_{1jk} = \\\\\\\\frac{Y_{1jk}}{\\\\\\\\sum_{i=1}^{i=I} Y_{ijk}}$$\\n$$W_{2jk} = \\\\\\\\frac{Y_{2jk}}{\\\\\\\\sum_{i=2}^{i=I} Y_{ijk}}$$\\n$$...$$\\n$$W_{I-1,jk} = \\\\\\\\frac{Y_{I-1,jk}}{\\\\\\\\sum_{i=I-1}^{i=R} Y_{ijk}}$$</p>\\n\\n<p>Where $i=1,..,I$ indexes the $Y$ categories, $j=1,..,J$ indexes the $X$ categories, and $k=1,..,K$ indexes the $Z$ categories.  Then fit a glm() of W on X and Z using the complimentary log-log link function.  Denoting $\\\\\\\\theta_{ijk}=Pr(Y_{ijk}\\\\\\\\leq i)$ as the cumulative probability, the MLE\\'s of the theta\\'s (assuming a multi-nomial distribution for $Y_{ijk}$ values) is then</p>\\n\\n<p>$$\\\\\\\\hat{\\\\\\\\theta}_{ijk}=\\\\\\\\hat{W}_{ijk}+\\\\\\\\hat{\\\\\\\\theta}_{(i-1)jk}(1-\\\\\\\\hat{W}_{ijk}) \\\\\\\\ \\\\\\\\ \\\\\\\\ i=1,\\\\\\\\dots ,I-1$$</p>\\n\\n<p>Where $\\\\\\\\hat{\\\\\\\\theta}_{0jk}=0$ and $\\\\\\\\hat{\\\\\\\\theta}_{Ijk}=1$ and $\\\\\\\\hat{W}_{ijk}$ are the fitted values from the glm.</p>\\n\\n<p>You can then use the deviance table (use the anova() function on the glm object) to assess the significance of the regressor variables.</p>\\n\\n<p><em>EDIT: one thing I forgot to mention in my original answer was that in the glm() function, you need to specify weights when fitting the model to</em> $W$, <em>which are equal to the denominators in the respective fractions defining each</em> $W$.</p>\\n\\n<p>You could also try a Bayesian approach, but you would most likely need to use sampling techniques to get your posterior, and using the multinomial likelihood (but parameterised with respect to $\\\\\\\\theta_{ijk}$, so the likelihood function will have <em>differences</em> of the form $\\\\\\\\theta_{ijk}-\\\\\\\\theta_{i-1,jk}$), the MLE\\'s are a good \"first crack\" at genuinely fitting the model, and give an approximate Bayesian solution (as you may have noticed, I prefer Bayesian inference)</p>\\n\\n<p>This method is in my lecture notes, so I\\'m not really sure how to reference it (there are no references given in the notes) apart from what I\\'ve just said.</p>\\n\\n<p>Just another note, I won\\'t harp on it, but I p-values are not all they are cracked up to be.  A good post discussing this can be found <a href=\"http://stats.stackexchange.com/questions/6308/article-about-misuse-of-statistical-method-in-nytimes\">here</a>.  I like Harlod Jeffrey\\'s quote above p-values (from his book <em>probability theory</em>) \"A null hypothesis may be rejected because it did not predict something that was not observed\" (this is because p-values ask for the probability of events <em>more extreme</em> than what was observed).</p>\\n',\n",
       " '<p>I addressed your confusion about PCA in your other question.  The correct version of your statement is that PCA attempts to characterize most of the variation in the in a lower dimensional space. As I explain in the answer to the other post PCA is not used for discrimination.  It has been used to do clustering after reducing dimension to a subset of the principal components.</p>\\n\\n<p>If by imbalance of the data you mean one cluster center has many more points arund it than another then that may affect the ability of any clustering method at identifying the smaller cluster but the magnitude of separation of the clusters is probably more important.</p>\\n',\n",
       " '<p>Sounds like you are looking for something like relative risk or first differences. If so, calculate the predicted probability of observing a 1 with everything but your variable of interest set at its mean or median and your treatment at a low (or high) value, giving something like this</p>\\n\\n<pre><code>RR_rs = Pr(Y1 =r,Y2 =s|x1) / Pr(Y1 =r,Y2 =s|x)\\n</code></pre>\\n\\n<p>Or first differences</p>\\n\\n<pre><code>FD_rs =Pr(Y1 =r,Y2 =s|x1)−Pr(Y1 =r,Y2 =s|x).\\n</code></pre>\\n\\n<p>I suppose looking at some type of information criteria might partially get you what you want as well. </p>\\n',\n",
       " '<p>Survival analysis is a good idea, but:</p>\\n\\n<ul>\\n<li>Do not discard anybody - even those with only one data point. The fact that they did or did not have HPV at that age is informative.</li>\\n<li>Your data is interval censored: left-censored for those who are positive at first measurement, right-censored for those who stay negative, and censored between the two measurement times for those who change status.</li>\\n</ul>\\n\\n<p>I would set up age 0 (or some other fixed value before the earliest measurement) as the starting point for time, and then estimate the hazard of the infection non-parametrically - Stata probably has a routine for that, but if not R certainly does. The result would be the age-dependent instantenous probability of infection - pretty much what you are looking for.</p>\\n\\n<p>If you want to estimate how covariates affect this hazard, then interval-censored regression is needed - standard Cox regression does not handle this situation.</p>\\n',\n",
       " '<p>I think one of the most reliable methods for comparing models is to cross-validate out-of-sample error (e.g. MAE).  You will need to un-transform the exogenous variable for each model to directly compare apples to apples.</p>\\n',\n",
       " \"<p>I can sign under what Dirk and EpiGrad said; yet there is one more thing that makes R an unique lang in its niche -- data-oriented type system. </p>\\n\\n<p>R's was especially designed for handling data, that's why it is vector-centered and has stuff like data.frames, factors, NAs and attributes.<br>\\nJulia's types are on the other hand numerical-performance-oriented, thus we have scalars, well defined storage modes, unions and structs.</p>\\n\\n<p>This may look benign, but everyone that has ever try to do stats with MATLAB knows that it really hurts.</p>\\n\\n<p>So, at least for me, Julia can't offer anything which I cannot fix with a few-line C chunk and kills a lot of really useful expressiveness. </p>\\n\",\n",
       " \"<p>You're looking for PROC FREQ. That will build a frequency table from your data, and from there you can calculate a number of frequency table-based statistics, including most Chi-squared statistics, Fisher's Exact tests, etc.</p>\\n\",\n",
       " '<p>A good textbook on multivariate data analysis, mixing introductory material and more advanced theory, is <a href=\"http://astro.temple.edu/~alan/MMST/\" rel=\"nofollow\"><em>Modern Multivariate Statistical Techniques</em></a>, by Alan J. Izenman (Springer, 2008). A <a href=\"http://www.jstatsoft.org/v29/b11/paper\" rel=\"nofollow\">review</a> by John Maindonald was published in the JSS.</p>\\n\\n<p>It features a complete chapter dedicated to MDS (chapter 13), with a lot of illustration using the open-source <a href=\"http://cran.r-project.org/\" rel=\"nofollow\">R</a> statistical software. More on R packages can be found on CRAN <a href=\"http://cran.r-project.org/web/views/Multivariate.html\" rel=\"nofollow\">Multivariate</a> Task View, among others.</p>\\n\\n<p>As an alternative, I would suggest the <em>Handbook of Applied Multivariate Statistics and Mathematical Modeling</em>, by Howard E. A. Tinsley and Steven D. Brown (Academic Press, 2000). Again, a complete chapter is devoted to MDS. Less mathematical background is required.</p>\\n\\n<p>As for online reference, I can also recommend Forrest W. Young\\'s course on <a href=\"http://forrest.psych.unc.edu/teaching/p230/p230.html\" rel=\"nofollow\">Multidimensional Scaling</a>.</p>\\n',\n",
       " '<p>Parrondo\\'s Paradox:</p>\\n\\n<p>From <a href=\"http://en.wikipedia.org/wiki/Parrondo%27s_paradox\" rel=\"nofollow\">wikipdedia</a>: \"Parrondo\\'s paradox, a paradox in game theory, has been described as: A combination of losing strategies becomes a winning strategy. It is named after its creator, Juan Parrondo, who discovered the paradox in 1996. A more explanatory description is:</p>\\n\\n<blockquote>\\n  <p>There exist pairs of games, each with a higher probability of losing\\n  than winning, for which it is possible to construct a winning strategy\\n  by playing the games alternately.</p>\\n</blockquote>\\n\\n<p>Parrondo devised the paradox in connection with his analysis of the Brownian ratchet, a thought experiment about a machine that can purportedly extract energy from random heat motions popularized by physicist Richard Feynman. However, the paradox disappears when rigorously analyzed.\"</p>\\n\\n<p>As alluring as the paradox might sound to the financial crowd, it does have requirements that are not readily available in financial time series.  Even though a few of the component strategies can be losing, the offsetting strategies require unequal and stable probabilities of much greater or less than 50% in order for the ratcheting effect to kick in.\\nIt would be difficult to find financial strategies, whereby one has $P_B(W)=3/4+\\\\\\\\epsilon$ and the other, $P_A(W)=1/10 + \\\\\\\\epsilon$, over long periods.</p>\\n\\n<p>There\\'s also a more recent related paradox called the \"<a href=\"http://www.google.com/url?sa=t&amp;rct=j&amp;q=allison%20mixture&amp;source=web&amp;cd=3&amp;ved=0CD0QFjAC&amp;url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.115.9270&amp;rep=rep1&amp;type=pdf&amp;ei=k-_YULSUA6mGiQKZxoH4DA&amp;usg=AFQjCNHkT5KO8YHdkUPOe6vqR95NDpSW6Q&amp;bvm=bv.1355534169,d.cGE&amp;cad=rja\" rel=\"nofollow\">allison mixture</a>,\" that shows we can take two IID and non-correlated series, and randomly scramble them such that certain mixtures can create a resulting series with non-zero autocorrelation.</p>\\n',\n",
       " \"<p>I'd suggest that in that case you'd want to look at the standardised residuals for normality rather than the raw residuals. We'd be checking for the normality of the error term, not the raw residuals.</p>\\n\\n<p>Since you could rewrite the two way ANOVA as a a multi-variable linear regression. In that case the expected variance in our residuals is...</p>\\n\\n<p>$\\\\\\\\newcommand{\\\\\\\\Var}{\\\\\\\\mathrm{Var}}\\\\\\\\boldsymbol e = (\\\\\\\\boldsymbol I-\\\\\\\\boldsymbol H)y$ </p>\\n\\n<p>Where $\\\\\\\\boldsymbol H = \\\\\\\\boldsymbol X (\\\\\\\\boldsymbol X^T \\\\\\\\boldsymbol X) \\\\\\\\boldsymbol X^T$</p>\\n\\n<p>And $(\\\\\\\\boldsymbol I-\\\\\\\\boldsymbol H)(\\\\\\\\boldsymbol I-\\\\\\\\boldsymbol H)^T=(\\\\\\\\boldsymbol I-\\\\\\\\boldsymbol H)$</p>\\n\\n<p>So that (probably with some abuse of notation):</p>\\n\\n<p>$\\\\\\\\Var(\\\\\\\\boldsymbol e) = \\\\\\\\Var((\\\\\\\\boldsymbol I-\\\\\\\\boldsymbol H)\\\\\\\\boldsymbol y) = (\\\\\\\\boldsymbol I-\\\\\\\\boldsymbol H)\\\\\\\\Var(\\\\\\\\boldsymbol y) = (\\\\\\\\boldsymbol I-\\\\\\\\boldsymbol H)\\\\\\\\sigma^2$</p>\\n\\n<p>That means we can't expect the $i$ th residual to be standard normally distributed unless we divide through by the square root of the $i$,$i$ th entry of the last expression. That's what the standardized residuals are intended to provide.</p>\\n\",\n",
       " '<p>Sample splitting might perhaps reduce the problem with the distribution of the statistic, but it doesn\\'t remove it.</p>\\n\\n<p>Your idea avoids the issue that the estimates will be \\'too close\\' relative to the population values because they\\'re based on the same sample.</p>\\n\\n<p>You aren\\'t avoiding the problem that they\\'re still estimates. The distribution of the test statistic is not the tabulated one.</p>\\n\\n<p>In this case it increases the rejection rate under the null, instead of dramatically reducing it.</p>\\n\\n<p>A better choice is to use a test where the parameters aren\\'t assumed known, such as a Shapiro Wilk.</p>\\n\\n<p>If you\\'re wedded to a Kolmogorov-Smirnov type of test, you can take the approach of Lilliefors\\' test.</p>\\n\\n<p>That is, to use the KS statistic but have the distribution of the test statistic reflect the effect of parameters estimation - simulate the distribution of the test statistic under parameter estimation. (It\\'s no longer distribution-free, so you need new tables for each distribution.)</p>\\n\\n<p><a href=\"http://en.wikipedia.org/wiki/Lilliefors_test\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Lilliefors_test</a></p>\\n\\n<p>Liliefors used simulation for the normal and the exponential case, but you can easily do it for any specific distribution; in something like R it\\'s a matter of moments to simulate 10,000 or 100,000 samples and get a distribution of the test statistic under the null.</p>\\n\\n<p>[An alternative might be to consider the Anderson-Darling, which does have the same issue, but which - judging from the book by D\\'Agostino and Stephens (<em>Goodness-of-fit-techniques</em>) seems to be less sensitive to it. You could adapt the Lilliefors idea, but they suggest a relatively simple adjustment that seems to work fairly well.]</p>\\n\\n<p>But there are other approaches still; there are families of smooth tests of goodness of fit, for example (e.g. see the book by Rayner and Best) that in a number of specific cases can deal with parameter estimation.</p>\\n\\n<p>* the effect can still be pretty large - perhaps bigger than would normally be regarded as acceptable; Momo is right to express concern about it. If a higher type I error rate (and a flatter power curve) is a problem, then this may not be an improvement!</p>\\n',\n",
       " \"<p>It seems to me that if Y doesn't actually already exist you probably want to just make it from scratch like this.</p>\\n\\n<pre><code>Y &lt;- X[sample(1:nrow(X), size = 10000, replace = TRUE, prob = X$PROB),]\\nY &lt;- Y[,-3]\\n</code></pre>\\n\\n<p>BTW, you can test to see that you got what you want with...</p>\\n\\n<pre><code>table(factor(Y$A):factor(Y$B))/nrow(Y)\\n</code></pre>\\n\\n<p>If Y really does already exist then yes, you have to go through the unique values of A, but not all values of A.</p>\\n\\n<pre><code>b &lt;- lapply(unique(X$A), function(x){\\n    n &lt;- nrow(Y[Y$A==x,])\\n    df &lt;- X[X$A == x,]\\n    Y$B[Y$A==x] &lt;&lt;- with( df, sample(B, n, replace = TRUE, prob = PROB) )\\n    })\\n</code></pre>\\n\\n<p>In that case <code>b</code> is garbage.  A <code>for</code> loop version of this wouldn't even make the garbage <code>b</code></p>\\n\\n<pre><code>for(x in unique(X$A)){\\n    n &lt;- nrow(Y[Y$A == x,])\\n    df &lt;- X[X$A == x,]\\n    Y$B[Y$A == x] &lt;- with( df, sample(B, n, replace = TRUE, prob = PROB) )\\n    }\\n</code></pre>\\n\\n<p>And thus... this might be a case where using a <code>loop</code> is the cleanest way to go.  :)</p>\\n\\n<p>I did some quick time testing of these two methods and the plyr version.  The fastest is the <code>for</code> loop, followed by <code>lapply</code>, followed by (far in the back), <code>ddply</code> (50% with given dataset).  The cost gets bigger as Y grows in either size or complexity such that the <code>for</code> loop can get 2 to 5x the performance of <code>ddply</code> and it uses much less memory.</p>\\n\",\n",
       " '<p>In a fixed-effects model, you are assuming that the true correlation estimated in each study is the same. In the Random effects model you accept that there is variation in the true correlation being estimate in each study.</p>\\n\\n<p>Thus, the fixed-effects model assumes that observed variation in estimated correlations is due only to effect of random sampling.</p>\\n\\n<p>It deciding between the two, you would often use a combination of theoretical knowledge and observed data. Theory will often suggest that the true correlation should vary somewhat between studies. You can also examine various test statistics on the observed correlations to assess whether the variation appears more than you would expect based on random sampling (e.g., see <a href=\"http://www.statsdirect.com/help/meta_analysis/heterogeneity_in_meta_analysis.htm\">this discussion</a> about Cochran\\'s Q and related indices).</p>\\n',\n",
       " \"<p>I have a time series where I need to detect gross anomalies due to coding errors, not small shifts in the structure of the series.  I am interested in the most recent data points, not historical data so I don't need to filter everything.  What is the best way to use moving means and averages in combination with spread estimators to find these points?</p>\\n\",\n",
       " \"<p>I am looking at setting up an experiment concerning a hobby of mine, basically measuring a variety of parameters 'before' and 'after' and see which one, if any, gives the most reliable prediction of a final parameter i.e. do they have a linear relationship, etc.  The object being to save some time and effort later <em>not</em> sorting by the parameters that have little actual bearing on the final results, and to see if some of the more tedious sorting methods are actually as useful as thought.</p>\\n\\n<p>The experiment is an extension of one I did for my final paper in my stats class a couple years ago:  investigating the relationship between case weight &amp; volume vs. muzzle velocity for centerfire rifle cases as used in modern target competition (in which I'm fairly active).  For that paper I had to spend a fair amount of 'time' demonstrating various methods that didn't really pertain to what I wanted to know, but had to cover anyway to get an 'A' ;)  </p>\\n\\n<p>For this go-around... I'm looking at taking one hundred pieces of brass cases from one box, one lot.  These are the 'non-consumable' parts, as they can and do get re-used and reloaded multiple times.  Everything else - bullet, powder, primer - get shot down the barrel and cannot be re-used/re-measured.  Specifically, the cases can and do become 'fire-formed' to the interior dimensions of the chamber under extreme pressure (50-70k psi).  In my original experiment, I found a significant, but not especially strong, correlation between the initial case weight and the muzzle velocity... of the first firing.  Case <em>volume</em>, on the other hand, which should be very strongly related to MV... was not, theoretically because the 'virgin' cases don't necessarily 'fit' the internal dimensions of the chamber and a certain amount of the energy generated is expended not just in heat, but in squashing the brass against the case wall during firing.</p>\\n\\n<p>So again, I'm looking at taking 100pcs of brass, weighing them straight out of the box, taking various measurements (case wall thickness inside near the case web, case neck thickness near the mouth, weight, volume, etc.) in their 'virgin' untouched state, then performing several routine case prep steps (trim to length, chamfer/debur the mouth, debur the flash hole, uniform primer pocket) and repeat the measurements.  Then load and fire the rounds as uniformly as possible - bullets sorted for maximum consistency, powder weighed on a milligram-capable analytic lab scale, etc. while controlling the rate of fire so as to regulate temperature rise along the barrel and measuring the muzzle velocity via a chronograph approx. 15 feet down range.  Then I plan to completely clean the cases inside and out to remove any buildup of carbon or powder residue, and repeat the measurements and then load and fire again and then clean and measure one last time.</p>\\n\\n<p>Part of what I want to see is how the distributions of some of the measurements change as the cases are prepped, then again as they are fired and formed to the chamber.  After that... I want to see how much difference is there <em>really</em> between 'virgin' cases and fire-formed cases in terms of muzzle velocity, and finally... of all the tedious measurement steps mentioned above, which ones actually give a reasonable indication of consistent MV so I can successfully cull suspect pieces of brass during preliminary sorting rather than suffer lost points when using them on target in competition.</p>\\n\\n<p>The single biggest problem in my mind, at least on the surface, is the chronograph.  Getting anything approaching credible numbers for accuracy or % error from the vendors is somewhere between difficult and impossible.  Given the nature of the device, it is extremely difficult to test on a consumer level - every round through it may (or may not) be just slightly different, so determining how much of the variation displayed on its screen is really the ammunition and how much is the variability of the instrument itself... has me scratching my head, to say the least.</p>\\n\\n<p>As to why I was kind of vague before about the details of the experiment... well, sometimes people get kinda weird as soon as they realize something involves GUNS and won't touch it with a 10 ft pole regardless of how reasonable it may be.</p>\\n\\n<p>Thanks,</p>\\n\\n<p>Monte</p>\\n\",\n",
       " '<p>Following from <a href=\"http://stats.stackexchange.com/questions/16312/what-is-the-difference-between-confidence-intervals-and-hypothesis-testing\">this question on the difference between confidence intervals and hypothesis testing</a>, I would love to have a simple example to better understand the the relationship between confidence intervals and p-values.</p>\\n\\n<p>For example, if I have $r=.8768 (n=300)$, then it is significant at $0.05$. </p>\\n\\n<p>The confidence interval is (lower) $0.847729   &lt; \\t(r) 0.8768    &lt; \\t0.900619$ (higher).</p>\\n\\n<p>This is against 0 (i.e. no relationship).</p>\\n\\n<ul>\\n<li><strong>How does it rate with the confidence level testing?</strong> </li>\\n<li><em>What do I look for?</em> </li>\\n</ul>\\n',\n",
       " '<p>Often, a number is useful only in reference to other numbers.  For example, customers rated their satisfaction as 9.1 out of 10 sounds good, but not when you hear that elsewhere in the industry reported satisfaction of 9.5/10 is the norm.</p>\\n\\n<p>The same applies to the width of confidence intervals.  Unless you have a reference of some sort, as @Macro says there is no way of providing an answer to your question.  It looks like you have some statistical output that compares your value to zero and says you are unlikely to have gotten the result you have if the real value is zero (this is the p value).  This may or may not be of interest.  </p>\\n\\n<p>If your main research question is \"can we find evidence that this parameter is not zero\", then your confidence interval is plenty narrow enough. If your research question is \"can we decide between X who claims this number is 0.3 and Y who claims it is 0.4\" then it is too wide.</p>\\n\\n<p><strong>Edit / addition after comments and the update of the question:</strong></p>\\n\\n<p>To explain the last sentence further.  If all you want is to establish that there is <em>some</em> relationship between income and savings, you have already done it, because your confidence interval is a long way from zero.  So there is no need to look for a smaller confidence interval from more data, etc.  If, however, you are interested in the nature of that relationship, eg because there is a theoretical dispute about whether the correlation coefficient is really 0.3 or 0.4, then you do not have enough data to answer the question.  Both alternatives are in your confidence interval - you will need to find more data.</p>\\n\\n<p>In my comments on the original question however, I suggested that the correlation coefficient is not a good way of showing the relationship between two variables like this.  Consider the two examples shown below.  The first has a very high correlation coefficient and a negligible confidence interval, yet the relationship between savings and income is what I would call weak - an increase in income does not lead to much of an increase in savings.  The second plot is the other way around - there is a lot more (realistic) noise in the data, but an increase in income leads to quite a significant increase in savings.  For most purposes, you want to say the second plot shows a stronger relationship than the first.  To do this you need to fit a regression model of some sort.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/3bwV6.jpg\" alt=\"enter image description here\"></p>\\n\\n<p>The R code that produced these plots is below.  </p>\\n\\n<pre><code>income &lt;- exp(rnorm(100,6,.5))\\nsummary(income)\\nsavings1 &lt;- 80 + income*.05\\nsavings2 &lt;- 8 + income*.5 + exp(rnorm(100,4.5,.5)) + rnorm(100,0,150)\\n\\nwin.graph(10,5)\\npar(mfrow=c(1,2), adj=0)\\nplot(income, savings1, ylim=c(0,1500), bty=\"l\", adj=.5)\\ntext(200,1500, \"r=1.00\")\\ntext(200,1300, \"Confidence interval for r=[1,1]\")\\ntext(200,1100, \"Slope confidence interval=[0.05, 0.05]\")\\n\\nlibrary(boot)\\ntest &lt;- data.frame(income, savings2)\\ntest.b &lt;- boot(test, function(x,w){cor(test[w,1], test[w,2])}, R=1000)\\nci &lt;- round(as.vector(round(quantile(test.b$t, prob=c(0.025, 0.975)),2)),2)\\nci2 &lt;- round(as.vector(confint(lm(savings2~income))[2,]),2)\\n\\nplot(income, savings2, ylim=c(0,1500), bty=\"l\", adj=.5)\\nabline(lm(savings2~income))\\ntext(200,1500, paste0(\"r=\", round(cor(income, savings2),2)))\\ntext(200,1300, paste0(\"Confidence interval for r=[\", ci[1], \", \", ci[2],\"]\"))\\ntext(200,1100, paste0(\"Slope confidence interval=[\",ci2[1], \", \", ci2[2],\"]\"))\\n</code></pre>\\n',\n",
       " '<p>I have many pairs like:</p>\\n\\n<pre><code>A  B\\n10 20\\n15.3 19.5\\n23 13\\n45 32\\n</code></pre>\\n\\n<p>etc etc, \\nI create those pairs calculating the standard deviation of many vectors (R vectors) and then group them by 2(A - B).</p>\\n\\n<p>Now, I have to filter those pairs by their percentage difference.\\nI set a percentage like <code>30%</code> then I filter all the pairs to get those who have a difference <strong>BELOW</strong> or <strong>EQUAL</strong> 30%</p>\\n\\n<p>My question is, is this formula correct to get the difference (in % of two numbers)</p>\\n\\n<pre><code>((A / B) -1) * 100\\n</code></pre>\\n\\n<p>My doubt is that I need their difference to understand if i can take them or not but if I do: ((B / A) -1) *100 obviously I get a different result, example:</p>\\n\\n<pre><code>A = 10\\nB = 20\\nA-B % = -50 %  ((10 / 20) -1) *100\\nB-A % = 100 %  ((20 / 10) -1) *100\\n</code></pre>\\n\\n<p>So, How can I better study the real difference of two numbers(standard deviation difference)?</p>\\n\\n<p><strong>EDIT (@Henry)</strong></p>\\n\\n<pre><code>&gt; a &lt;- log(sd(rnorm(250)))\\n&gt; b &lt;- log(sd(rnorm(250)))\\n&gt; a/b\\n[1] 0.6049963\\n</code></pre>\\n\\n<p><strong>0.60%</strong> of change ?</p>\\n\\n<p>Thank you!</p>\\n',\n",
       " '<p>In the last line of your derivation, you substituted the expectation of the Poisson distribution, rather than the expectation of the truncated Poisson distribution. Fix that, and the correct result should follow.</p>\\n',\n",
       " '<p>I have a dataset with a categorical treatment (1 if treated, 0 if control) and many columns (34) of response variables.  Each column represents a species and its response (some measured abundance) to the presence or absence of the treatment.  Each row is a separate site; exactly half of them were treated and the other half were controls.  The experiment was replicated over two years.</p>\\n\\n<p>I am interested in finding out which of these species respond to the treatment, so a LDA was suggested, but I am confused because my treatment is categorical and all of the examples I can find of LDA use continuous predictor variables.</p>\\n\\n<p>So the data look like:</p>\\n\\n<p>Treatment(0/1)    Site(ID#)      Year(1/2)    Species 1     Species 2    ...    Species 34</p>\\n\\n<p>Can I use a LDA for this question?  If not, what sort of ordination is capable of accounting for categorical predictor variables?</p>\\n',\n",
       " '<p>Just to amplify - I am the most recent requestor, I believe.</p>\\n\\n<p>In specific comment on Mike\\'s points:</p>\\n\\n<ol>\\n<li><p>It\\'s clearly true that the I/II/III difference only applies with correlated predictors (of which unbalanced designs are the most common example, certainly in factorial ANOVA) - but this seems to me to be an argument that dismisses the analysis of the unbalanced situation (and hence any Type I/II/III debate). It may be imperfect, but that\\'s the way things happen (and in many contexts the costs of further data collection outweigh the statistical problem, caveats notwithstanding).</p></li>\\n<li><p>This is completely fair and represents the meat of most of the \"II versus III, favouring II\" arguments I\\'ve come across. The best summary I\\'ve encountered is Langsrud (2003) \"ANOVA for unbalanced data: Use Type II instead of Type III sums of squares\", Statistics and Computing 13: 163-167 (I have a PDF if the original is hard to find). He argues (taking the two-factor case as the basic example) that if there\\'s an interaction, there\\'s an interaction, so consideration of main effects is usually meaningless (an obviously fair point) - and if there\\'s no interaction, the Type II analysis of main effects is more powerful than the Type III (undoubtedly), so you should always go with Type II. I\\'ve seen other arguments (e.g. Venables, Fox) that emphasize the meaning (or lack of) of considering hypotheses about main effects in the presence of interactions, and/or/equivalently suggesting that the Type III assumptions about the null hypothesis are often not sensible (e.g. Langsrud).</p></li>\\n<li><p>And I agree with this: if you have an interaction but have some question about the main effect as well, then you\\'re probably into do-it-yourself territory.</p></li>\\n</ol>\\n\\n<p>Clearly there are those who just want Type III because SPSS does it, or some other reference to statistical Higher Authority. I am not wholly against this view, if it comes down to a choice of a lot of people sticking with SPSS (which I have some things against, namely time, money, and licence expiry conditions) and Type III SS, or a lot of people shifting to R and Type III SS. However, this argument is clearly a lame one statistically.</p>\\n\\n<p>However, the argument that I found rather more substantial in favour of Type III is that made independently by Myers &amp; Well (2003, \"Research Design and Statistical Analysis\", pp. 323, 626-629) and Maxwell &amp; Delaney (2004, \"Designing Experiments and Analyzing Data: A Model Comparison Perspective\", pp. 324-328, 332-335). That is as follows:</p>\\n\\n<ul>\\n<li>if there\\'s an interaction, all methods give the same result for the interaction sum of squares</li>\\n<li>Type II assumes that there\\'s no interaction for its test of main effects; type III doesn\\'t</li>\\n<li>Some (e.g. Langsrud) argue that if the interaction is not significant, then you\\'re justified in assuming that there isn\\'t one, and looking at the (more powerful) Type II main effects</li>\\n<li>But if the test of the interaction is underpowered, yet there is an interaction, the interaction may come out \"non-significant\" yet still lead to a violation of the assumptions of the Type II main effects test, biasing those tests to be too liberal.</li>\\n<li>Myers &amp; Well cite Appelbaum/Cramer as the primary proponents of the Type II approach, and go on [p323]: \"... More conservative criteria for nonsignificance of the interaction could be used, such as requiring that the interaction not be significant at the .25 level, but there is insufficient understanding of the consequences of even this approach. As a general rule, Type II sums of sqaures should not be calculated unless there is strong a priori reason to assume no interaction effects, and a clearly nonsignificant interaction sum of squares.\" They cite [p629] Overall, Lee &amp; Hornick 1981 as a demonstration that interactions that do not approach significance can bias tests of main effects. Maxwell &amp; Delaney [p334] advocate the Type II approach if the population interaction is zero, for power, and the Type III approach if it isn\\'t [for the interpretability of means derived from this approach]. They too advocate using Type III in the real-life situation (when you\\'re making inferences about the presence of the interaction from the data) because of the problem of making a type 2 [underpowered] error in the interaction test and thus accidentally violating the assumptions of the Type II SS approach; they then make similar further points to Myers &amp; Well, and note the long debate on this issue!</li>\\n</ul>\\n\\n<p>So my interpretation (and I\\'m no expert!) is that there\\'s plenty of Higher Statistical Authority on both sides of the argument; that the usual arguments put forward aren\\'t about the usual situation that would give rise to problems (that situation being the common one of interpreting main effects with a non-significant interaction); and that there are fair reasons to be concerned about the Type II approach in that situation (and it comes down to a power versus potential over-liberalism thing).</p>\\n\\n<p>For me, that\\'s enough to wish for the Type III option in ezANOVA, as well as Type II, because (for my money) it\\'s a superb interface to R\\'s ANOVA systems. R is some way from being easy to use for novices, in my view, and the \"ez\" package, with ezANOVA and the rather lovely effect plotting functions, goes a long way towards making R accessible to a more general research audience. Some of my thoughts-in-progress (and a nasty hack for ezANOVA) are at <a href=\"http://www.psychol.cam.ac.uk/statistics/R/anova.html\">http://www.psychol.cam.ac.uk/statistics/R/anova.html</a> .</p>\\n\\n<p>Would be interested to hear everyone\\'s thoughts!</p>\\n',\n",
       " '<p>I use <a href=\"http://cran.r-project.org/web/packages/plyr/index.html\"><strong>plyr</strong></a> and <a href=\"http://cran.r-project.org/web/packages/ggplot2/index.html\"><strong>ggplot2</strong></a> the most on a daily basis.</p>\\n\\n<p>I also rely heavily on time series packages; most especially, the <a href=\"http://cran.r-project.org/web/packages/zoo/index.html\"><strong>zoo</strong></a> package.</p>\\n',\n",
       " '<p>Leave time to edit. Making a good graph takes time and it often takes (at least for me) multiple tries. </p>\\n',\n",
       " '<p>Zen used method 1. Here is method 2: Map $x$ to a spherically symmetric Gaussian distribution centered at $x$ in the Hilbert space $L^2$. The standard deviation and a constant factor have to be tweaked for this to work exactly. For example, in one dimension, </p>\\n\\n<p>$$ \\\\\\\\int_{-\\\\\\\\infty}^\\\\\\\\infty \\\\\\\\frac{\\\\\\\\exp[-(x-z)^2/(2\\\\\\\\sigma^2)]}{\\\\\\\\sqrt{2 \\\\\\\\pi} \\\\\\\\sigma} \\\\\\\\frac{\\\\\\\\exp[-(y-z)^2/(2 \\\\\\\\sigma^2)}{\\\\\\\\sqrt{2 \\\\\\\\pi} \\\\\\\\sigma} dz = \\\\\\\\frac{\\\\\\\\exp [-(x-y)^2/(4 \\\\\\\\sigma^2)]}{2 \\\\\\\\sqrt \\\\\\\\pi \\\\\\\\sigma}. $$</p>\\n\\n<p>So, use a standard deviation of $\\\\\\\\sigma/\\\\\\\\sqrt 2$ and scale the  Gaussian distribution to get $k(x,y) = \\\\\\\\langle \\\\\\\\Phi(x), \\\\\\\\Phi(y)\\\\\\\\rangle$. This last rescaling occurs because the $L^2$ norm of a normal distribution is not $1$ in general.</p>\\n',\n",
       " '<p>You want the <code>arima</code> function in base R, or the <code>Arima</code> function in the forecast package.  <code>auto.arima</code> may be useful as well.</p>\\n\\n<p>type <code>?arima</code> into R to read the help page, which gives a good overview for fitting such models.  You\\'ll want to split your data into 2 parts: a vector of class <code>ts</code> for \"value,\" and a matrix of your external regressors, which you pass to the <code>xreg</code> argument of the various arima functions.</p>\\n\\n<p>You can then predict these models using the <code>predict</code> function. You will need to make forecasts of your xregs and pass them to the prediction function in the <code>newxreg</code> argument.  You can also use the <code>forecast</code> function in the forecast package to produce confidence intervals and plots.  <code>?predict.Arima</code> in base R and <code>?forecast.Arima</code> in forecast.</p>\\n\\n<p>Before you do any of this; however, you need to think about your forecasting problem.  Say you want to forecast 1 step ahead (for ValueN).  Will you have the flags for ValueN at time M?  If not, your ARIMAX model will be useless.</p>\\n',\n",
       " '<p>I believe that this is an experiment where it is safe to assume a monotone relationship: for a longer exposition time the infection probability can not be smaller. So you can run monotone/isotonic regression.\\nYou can even incorporate into your model that the infection probability at time=0 is 0.</p>\\n',\n",
       " '<p>The MASS package has a function called mvrnorm() that can generate a group or random numbers to a specified level of correlation. An example of the setup can be found in the beginning of the example here: <a href=\"http://menugget.blogspot.de/2011/11/propagation-of-error.html\" rel=\"nofollow\">http://menugget.blogspot.de/2011/11/propagation-of-error.html</a></p>\\n',\n",
       " \"<p>Consider an integer random walk starting at 0 with the following conditions: </p>\\n\\n<ul>\\n<li><p>The first step is plus or minus 1, with equal probability. </p></li>\\n<li><p>Every future step is: 60% likely to be in the same direction as \\nthe previous step, 40% likely to be in the opposite direction </p></li>\\n</ul>\\n\\n<p>What sort of distribution does this yield? </p>\\n\\n<p>I know that a non-momentum random walk yields a normal \\ndistribution. Does the momentum just change the variance, or change \\nthe nature of the distribution entirely? </p>\\n\\n<p>I'm looking for a generic answer, so by 60% and 40% above, I really \\nmean <strong>p</strong> and <strong>1-p</strong> </p>\\n\",\n",
       " '<p>A completely different way would be to directly calculate the area of your polygon:</p>\\n\\n<pre><code>library(geometry)\\npolyarea(x=Data$Q, y=Data$DOC)\\n</code></pre>\\n\\n<p>This yields 0.606.</p>\\n',\n",
       " '<p>As Ben Bolker says, the answer to this question can be found in the code for <code>summary.lm()</code>.</p>\\n\\n<p>Here\\'s the header:</p>\\n\\n<pre><code>function (object, correlation = FALSE, symbolic.cor = FALSE, \\n    ...) \\n{\\n</code></pre>\\n\\n<p>So, let <code>x &lt;- 1:1000; y &lt;- rep(1,1000); z &lt;- lm(y ~ x)</code> and then take a look at this slightly modified extract:</p>\\n\\n<pre><code>    p &lt;- z$rank\\n    rdf &lt;- z$df.residual\\n    Qr &lt;- stats:::qr.lm(z)\\n    n &lt;- NROW(Qr$qr)\\n    r &lt;- z$residuals\\n    f &lt;- z$fitted.values\\n    w &lt;- z$weights\\n    if (is.null(w)) {\\n        mss &lt;- sum((f - mean(f))^2)\\n        rss &lt;- sum(r^2)\\n    }\\n    ans &lt;- z[c(\"call\", \"terms\")]\\n    if (p != attr(z$terms, \"intercept\")) {\\n        df.int &lt;- 1L\\n        ans$r.squared &lt;- mss/(mss + rss)\\n        ans$adj.r.squared &lt;- 1 - (1 - ans$r.squared) * ((n - \\n            df.int)/rdf)\\n    }\\n</code></pre>\\n\\n<p>Notice that ans\\\\\\\\$r.squared is $0.4998923$...</p>\\n\\n<p>To answer a question with a question: what do we draw from this?  :)</p>\\n\\n<p>I believe the answer lies in how R handles floating point numbers.  I think that <code>mss</code> and <code>rss</code> are the sums of very small (squared) rounding errors, hence the reason $R^2$ is about 0.5.  As for the progression, I suspect this has to do with the number of values that it takes for the +/- approximations to cancel out to 0 (for both <code>mss</code> and <code>rss</code>, as <code>0/0</code> is likely the source of these <code>NaN</code> values).  I don\\'t know why the values differ from a <code>2^(1:k)</code> progression, though.</p>\\n\\n<hr>\\n\\n<p>Update 1: <a href=\"https://stat.ethz.ch/pipermail/r-help/2000-February/010105.html\" rel=\"nofollow\">Here is a nice thread from R-help</a> addressing some of the reasons that underflow warnings are not addressed in R.</p>\\n\\n<p>In addition, <a href=\"http://stackoverflow.com/questions/5802592/dealing-with-very-small-numbers-in-r\">this SO Q&amp;A</a> has a number of interesting posts and useful links regarding underflow, higher precision arithmetic, etc.</p>\\n',\n",
       " '<p>As part of an advisory team in my educational institution for undergraduate nurses, I (among others) have been assigned to design a questionnaire that will assess students\\' and academic staff\\'s perceptions about the ability of the final year\\'s exams as an adequate tool for evaluating students\\': knowledge-understanding /competence-skills/ and judgment-approach (in other words, to judge whether they are ready to safely work in the real world).\\nThe final questionnaire comprises 21 questions regarding the above mentioned 3 basic concepts. I proposed the answers to be given on a 5-point balanced Likert-type scale (strongly disagree to strongly agree with a neutral midpoint). However, the rest of the team is in favour of using an unbalanced 4-point scale (disagree/agree a little/agree/strongly agree).</p>\\n\\n<p><strong>Question 1:</strong> Which scale would be more appropriate?</p>\\n\\n<p>Also, I believe that this proposed tool needs to be validated and checked for reliability first before being administered to the target population, however, most of my colleagues disagree, since, the questions arise from the corresponding 21 statements of our national regulatory board, according to which:</p>\\n\\n<ol>\\n<li>\"Nurses must demonstrate knowledge of planning of health care measures\",</li>\\n<li>\"Nurses must demonstrate the ability to draw care plans\",</li>\\n<li>\"Nurses must demonstrate the ability to apply their knowledge so as to deal with different situations\" and so on.</li>\\n</ol>\\n\\n<p>The majority of the team is in favour of converting these statutory statements into a questionnaire as follows:</p>\\n\\n<blockquote>\\n  <p>I believe that the test: Q1 \"demonstrated knowledge of planning of\\n  health care measures\", Q2 \"demonstrated the ability to draw care\\n  plans\"</p>\\n</blockquote>\\n\\n<p>and so on up to question 21, without checking for validity and reliability first.</p>\\n\\n<p><strong>Question 2:</strong> Shouldn\\'t this tool be tested first for its reliability and validity (convergent, discriminant, criteria validity, factor analysis, etc)?</p>\\n',\n",
       " '<p>I have inherited some data analysis code that, not being an econometrician, I am struggling to understand. One model runs an instrumental variables regression with the following Stata command</p>\\n\\n<pre><code>ivreg my_dv var1 var2 var3 (L.my_dv = D2.my_dv D3.my_dv D4.my_dv)\\n</code></pre>\\n\\n<p>This dataset is a panel with multiple sequential observations for this set of variables.</p>\\n\\n<p>Why is this code using the lagged values of the DV as instruments? As I understand it (from digging into an old textbook), IV estimation is used when there is a problem because of a regressor being correlated with the error term. However, nothing is mentioned of choosing lags of the DV as instruments. </p>\\n\\n<p>A comment on this line of the code mentions \"causality\". Any help in figuring out what was the goal here would be most welcome.</p>\\n',\n",
       " '<p>(At first I thought it could be a problem resulting from the fact that <code>max</code> is not vectorized, but that\\'s not true.  It <em>does</em> make it a pain to work with changePoint, wherefore the following modification:</p>\\n\\n<pre><code>changePoint &lt;- function(x, b0, slope1, slope2, delta){ \\n   fit &lt;- b0 + (x*slope1) + (sapply(x-delta, function (t) max(0, t)) * slope2)\\n}\\n</code></pre>\\n\\n<p>)</p>\\n\\n<p><a href=\"http://tolstoy.newcastle.edu.au/R/help/06/06/28419.html\">This R-help mailing list post</a> describes one way in which this error may result: the rhs of the formula is overparameterized, such that changing two parameters in tandem gives the same fit to the data.  I can\\'t see how that is true of your model, but maybe it is.</p>\\n\\n<p>In any case, you can write your own objective function and minimize it.  The following function gives the squared error for data points (x,y) and a certain value of the parameters (the weird argument structure of the function is to account for how <code>optim</code> works):</p>\\n\\n<pre><code>sqerror &lt;- function (par, x, y)\\n  sum((y - changePoint(x, par[1], par[2], par[3], par[4]))^2)\\n</code></pre>\\n\\n<p>Then we say:</p>\\n\\n<pre><code>optim(par = c(50, 0, 2, 48), fn = sqerror, x = x, y = data)\\n</code></pre>\\n\\n<p>And see:</p>\\n\\n<pre><code>$par\\n[1] 54.53436800 -0.09283594  2.07356459 48.00000006\\n</code></pre>\\n\\n<p>Note that for my fake data (<code>x &lt;- 40:60; data &lt;- changePoint(x, 50, 0, 2, 48) + rnorm(21, 0, 0.5)</code>) there are lots of local maxima depending on the initial parameter values you give.  I suppose if you wanted to take this seriously you\\'d call the optimizer many times with random initial parameters and examine the distribution of results.</p>\\n',\n",
       " '<p>The concept is not at all difficult to make mathematical precise given a bit of general knowledge of $n$-dimensional Euclidean geometry, subspaces and orthogonal projections.</p>\\n\\n<p>If $P$ is an <em>orthogonal projection</em> from $\\\\\\\\mathbb{R}^n$ to a $p$-dimensional subspace $L$ and $x$ is an arbitrary $n$-vector then $Px$ is in $L$, $x - Px$ and $Px$ are orthogonal and $x - Px \\\\\\\\in L^{\\\\\\\\perp}$ is in the orthogonal complement of $L$. The dimension of this orthogonal complement, $L^{\\\\\\\\perp}$, is $n-p$. If $x$ is free to vary in an $n$-dimensional space then $x - Px$ is free to vary in an $n-p$ dimensional space. For this reason we say that $x - Px$ has <strong>$n-p$ degrees of freedom</strong>.</p>\\n\\n<p>These considerations are important to statistics because if $X$ is an $n$-dimensional random vector and $L$ is a model of its mean, that is, the mean vector $E(X)$ is in $L$, then we call $X-PX$ the vector of <em>residuals</em>, and we use the residuals to estimate the variance. The vector of residuals has $n-p$ degrees of freedom, that is, it is constrained to a subspace of dimension $n-p$. </p>\\n\\n<p>If the coordinates of $X$ are independent and normally distributed with the same variance $\\\\\\\\sigma^2$ then</p>\\n\\n<ul>\\n<li>The vectors $PX$ and $X - PX$ are independent.</li>\\n<li>If $E(X) \\\\\\\\in L$ the distribution of the squared norm of the vector of residuals $||X - PX||^2$ is a $\\\\\\\\chi^2$-distribution with scale parameter $\\\\\\\\sigma^2$ and another parameter that happens to be the degrees of freedom $n-p$. </li>\\n</ul>\\n\\n<p>The sketch of proof of these facts is given below. The two results are central for the further development of the statistical theory based on the normal distribution. Note also that this is why the $\\\\\\\\chi^2$-distribution has the parametrization it has. It is also a $\\\\\\\\Gamma$-distribution with scale parameter $2\\\\\\\\sigma^2$ and shape parameter $(n-p)/2$, but in the context above it is natural to parametrize in terms of the degrees of freedom.</p>\\n\\n<p>I must admit that I don\\'t find any of the paragraphs cited from the Wikipedia article particularly enlightening, but they are not really wrong or contradictory either. They say in an imprecise, and in a general loose sense, that when we compute the estimate of the variance parameter, but do so based on residuals, we base the computation on a vector that is only free to vary in a space of dimension $n-p$. </p>\\n\\n<p>Beyond the theory of linear normal models the use of the concept of degrees of freedom can be confusing. It is, for instance, used in the parametrization of the $\\\\\\\\chi^2$-distribution whether or not there is a reference to anything that could have any degrees of freedom. When we consider statistical analysis of categorical data there can be some confusion about whether the \"independent pieces\" should be counted before or after a tabulation. Furthermore, for constraints, even for normal models, that are not subspace constraints, it is not obvious how to extend the concept of degrees of freedom. Various suggestions exist typically under the name of <em>effective</em> degrees of freedom.</p>\\n\\n<p>Before any other usages and meanings of degrees of freedom is considered I will strongly recommend to become confident with it in the context of linear normal models. A reference dealing with this model class is <a href=\"http://rads.stackoverflow.com/amzn/click/1584882476\">A First Course in Linear Model Theory</a>, and there are additional references in the preface of the book to other classical books on linear models.  </p>\\n\\n<p><strong>Proof of the results above:</strong> Let $\\\\\\\\xi = E(X)$, note that the variance matrix is $\\\\\\\\sigma^2 I$ and choose an orthonormal basis $z_1, \\\\\\\\ldots, z_p$ of $L$ and an orthonormal basis $z_{p+1}, \\\\\\\\ldots, z_n$ of $L^{\\\\\\\\perp}$. Then $z_1, \\\\\\\\ldots, z_n$ is an orthonormal basis of $\\\\\\\\mathbb{R}^n$. Let $\\\\\\\\tilde{X}$ denote the $n$-vector of the coefficients of $X$ in this basis, that is \\n$$\\\\\\\\tilde{X}_i = z_i^T X.$$\\nThis can also be written as $\\\\\\\\tilde{X} = Z^T X$ where $Z$ is the orthogonal matrix with the $z_i$\\'s in the columns. Then we have to use that $\\\\\\\\tilde{X}$ has a normal distribution with mean $Z^T \\\\\\\\xi$ and, because $Z$ is orthogonal, variance matrix $\\\\\\\\sigma^2 I$. This follows from general linear transformation results of the normal distribution. The basis was chosen so that the coefficients of $PX$ are $\\\\\\\\tilde{X}_i$ for $i= 1, \\\\\\\\ldots, p$, and the coefficients of $X - PX$ are $\\\\\\\\tilde{X}_i$ for $i= p+1, \\\\\\\\ldots, n$. Since the coefficients are uncorrelated and jointly normal, they are independent, and this implies that \\n$$PX = \\\\\\\\sum_{i=1}^p \\\\\\\\tilde{X}_i z_i$$ \\nand \\n$$X - PX = \\\\\\\\sum_{i=p+1}^n \\\\\\\\tilde{X}_i z_i$$\\nare independent. Moreover, \\n$$||X - PX||^2 = \\\\\\\\sum_{i=p+1}^n \\\\\\\\tilde{X}_i^2.$$ \\nIf $\\\\\\\\xi \\\\\\\\in L$ then $E(\\\\\\\\tilde{X}_i) = z_i^T \\\\\\\\xi = 0$ for $i = p +1, \\\\\\\\ldots, n$ because then $z_i \\\\\\\\in L^{\\\\\\\\perp}$ and hence $z_i \\\\\\\\perp \\\\\\\\xi$. In this case $||X - PX||^2$ is the sum of $n-p$ independent $N(0, \\\\\\\\sigma^2)$-distributed random variables, whose distribution, by definition, is a $\\\\\\\\chi^2$-distribution with scale parameter $\\\\\\\\sigma^2$ and $n-p$ degrees of freedom. </p>\\n',\n",
       " \"<p>I'm currently writing a .NET library to handle various statistical/classification tasks and I am currently writing the structures to represent nominal and ordinal data.</p>\\n\\n<p>In doing so, I've been debating whether or not the structures that handle ordinal data should derive from the structures that handle nominal data.</p>\\n\\n<p>My thought is that yes, all ordinal data <em>is</em> nominal data; even though it means huge (or infinite) sets, each of them is an attribute that while being able to compared to another attribute for rank (for lack of a better term, which implies ordinal) can still be compared to for equality and is still a label (nominal).</p>\\n\",\n",
       " '<p>For two normally distributed samples, is there a way to test for $H_0: \\\\\\\\mu_1=\\\\\\\\mu_2$ and also $\\\\\\\\sigma_1^2=\\\\\\\\sigma_2^2$. I have computed the likelihood ratio, but cannot recognize the underlying distribution.</p>\\n',\n",
       " \"<p>Whenever boosting is brought up, Adaboost is the first algorithm to be listed.  What are the most popular boosting algorithms that <em>aren't</em> Adaboost?</p>\\n\",\n",
       " \"<p>I'm aware that this one is far from <em>yes or no</em> question, but I'd like to know which techniques do you prefer in categorical data analysis - i.e. cross tabulation with two categorical variables.</p>\\n\\n<p>I've come up with: </p>\\n\\n<ul>\\n<li>&chi;<sup>2</sup> test - well, this is quite self-explanatory\\n<ul>\\n<li>Fisher's exact test - when n &lt; 40,</li>\\n<li>Yates' continuity correction - when n > 40,</li>\\n</ul></li>\\n<li>Cramer's V - measure of association for tables which have more than <em>2 x 2</em> cells,</li>\\n<li>&Phi; coefficient - measure of association for <em>2 x 2</em> tables,</li>\\n<li>contingency coefficient (C) - measure of association for <em>n x n</em> tables,</li>\\n<li>odds ratio - independence of two categorical variables,</li>\\n<li>McNemar marginal homogeniety test,</li>\\n</ul>\\n\\n<p>And my question here is: Which statistical techniques for cross-tabulated data (two categorical variables) do you consider relevant (and why)?</p>\\n\",\n",
       " '<p>To model the global effects of co-variates such as exposure to advertising on joining/not-joining membership plans <a href=\"https://en.wikipedia.org/wiki/Logistic_regression\" rel=\"nofollow\">logistic regression</a> is a useful approach. Modeling duration between exposures and joining/not-joining will require other more complicated forms of regression.</p>\\n\\n<p>After accounting for global effects, a tool from spatial statistics-- <a href=\"https://en.wikipedia.org/wiki/Moran%27s_I\" rel=\"nofollow\">Moran\\'s $I$</a>-- could be used for testing whether network effects are significant. The calculation of $I$ (which can be done using existing software such as the R package <a href=\"http://cran.r-project.org/web/packages/spdep/index.html\" rel=\"nofollow\">spdep</a>) relies on introducing a weight matrix $W$ with entries $w_{ij}$, representing the distance between two individuals. Obviously, in spatial statistics, distance retains its geographical interpretation; however, there is no reason $W$ could not be used with the host of distance measures used in social network analysis. For example, weights could be based on the <a href=\"https://en.wikipedia.org/wiki/Jaccard_index\" rel=\"nofollow\">Jaccard distance</a> e.g. $w_{ij} = 1 - \\\\\\\\frac{|N_i \\\\\\\\cap N_j|}{|N_i \\\\\\\\cup N_j|}$, where $N_i$ is the set of neighbors of individual $i$ and $|\\\\\\\\cdot|$ represents the cardinality of a given set.</p>\\n\\n<p>To test for statistical significance of the observed $I$ from data that is not normally distributed (as would be expected from residuals from a logistic regression), one could use the bootstrap, sampling individuals and their neighbors at random with replacement, recalculating the weight matrix and then recalculating $I$. After repeating this procedure $B$ times the bootstrap $p$-value is the number of bootstrap $I$\\'s more extreme than the observed $I$ divided by $B$.</p>\\n\\n<p>Thus, roughly speaking, the procedure would involve first performing logistic regression, calculating a weight matrix based on a social dissimilarity measure between individuals, calculating $I$ with $W$ and the residuals from the regression, and then bootstrapping the data to test whether $I$ is significant.</p>\\n',\n",
       " \"<p>I've been using the <em>K-Fold cross validation</em> a few times now to evaluate performance of some learning algorithms, but I've always been puzzled as to how I should choose the value of K.</p>\\n\\n<p>I've often seen and used a value of <code>K = 10</code>, but this seems totally arbitrary to me, and I now just use 10 by habit instead of thinking it over. To me it seems that you're getting a better granularity as you improve the value of K, so ideally you should make your K very large, but there is also a risk to be biased.</p>\\n\\n<p>I'd like to know on what the value of K should depend, and how I should be thinking about this when I evaluate my algorithm. Does it change something if I use the <em>stratified</em> version of the cross validation or not?</p>\\n\",\n",
       " '<p>After knowing how LSA works, I went on continue reading on pLSA but couldn\\'t really make sense of the mathematical formula. This is what I get from <a href=\"http://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis\" rel=\"nofollow\">wikipedia</a> (other academic papers/tutorial show similar form)</p>\\n\\n<p>\\\\\\\\begin{align*} \\nP(w,d) = \\\\\\\\sum_{c} P(c) P(d|c) P(w|c)\\\\\\\\\\\\\\\\\\n&amp;= P(d) \\\\\\\\sum_{c} P(c|d) P(w|c)\\\\\\\\\\\\\\\\\\n\\\\\\\\end{align*}</p>\\n\\n<p>I gave up trying to derive it, and found <a href=\"http://www.hongliangjie.com/2010/01/04/notes-on-probabilistic-latent-semantic-analysis-plsa/\" rel=\"nofollow\">this</a> instead</p>\\n\\n<p>\\\\\\\\begin{align*}\\nP(c|d) = \\\\\\\\frac{P(d|c)P(c)}{P(d)}\\\\\\\\\\\\\\\\\\nP(c|d)P(d) = P(d|c)P(c)\\\\\\\\\\\\\\\\\\nP(w|c)P(c|d)P(d) = P(w|c)P(d|c)P(c)\\\\\\\\\\\\\\\\\\nP(d) \\\\\\\\sum_{c} P(w|c)P(c|d) = \\\\\\\\sum_{c} P(w|c)P(d|c)P(c)\\n\\\\\\\\end{align*}</p>\\n\\n<p>How does the summation appear at the last line? I am currently reading through some tutorial on <a href=\"http://en.wikipedia.org/wiki/Bayesian_inference\" rel=\"nofollow\">Bayesian Inferencing</a> (learnt basic probability rules and Bayesian theorem before but can\\'t really see them being useful enough here).</p>\\n\\n<p>p/s: not sure how to write multiline equation in MathJax way, will fix when I figure out the right way...</p>\\n',\n",
       " '<p>my friend is a chemist and his problem is to predict the level of ozone concentration in a single site. We have the data for the last 12 years.</p>\\n\\n<p>We want to predict the concentration for the coming years (as much as possible).</p>\\n\\n<p>I know this data set is small, so my question is is this possible? And how much of data is required if not?</p>\\n\\n<p>What is the tool/method to use?</p>\\n\\n<p>Here is a plot of my data:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/KP3IQ.png\" alt=\"alt text\"></p>\\n\\n<p>any suggestions/ help?</p>\\n',\n",
       " \"<p>I have a time series of data (about 300-750 elements, depending on the sample) and a model that has some random residues. I used the Kolmogorov–Smirnov test to make sure that the normality hypothesis can't be rejected, so I assume that the residuals are normally distributed. But now I guess I should test if they are independent of each other - so that there is no autoregression? Which test should I use (preferably one that is easily implementable in java)? </p>\\n\",\n",
       " '<p>If I understood your notation correctly, the answer is as follows.</p>\\n\\n<p>You can find probabilities $P(x_t=k|y_{1:T})$ from this equation:\\n$\\\\\\\\sum_{k=1}^K P(x_t=i|y_{1:T}) = 1$, here $k \\\\\\\\in [1, K]$ - the index of the latent state.</p>\\n\\n<p>Just use the equation that you stated and you will see that:\\n$\\\\\\\\sum_{k=1}^K \\\\\\\\frac{\\\\\\\\alpha_t(k) \\\\\\\\beta_t(k)}{P(Y)} = 1$.</p>\\n\\n<p>From this you get what you need:\\n$P(Y) = \\\\\\\\sum_{k=1}^K \\\\\\\\alpha_t(k) \\\\\\\\beta_t(k)$.</p>\\n\\n<p>Hope it will help.</p>\\n',\n",
       " '<p>How, if at all, can one compare the \"fit\" of a simple linear vs. non-linear regression model to observed data? </p>\\n\\n<p>I apologize if I didn\\'t search long/hard enough for the answer, but I cannot find anything concrete. </p>\\n\\n<p>-Patrick </p>\\n',\n",
       " '<p>I am trying to mine product-usage sequences for multiple users of online gaming site. I have found the R package <a href=\"http://cran.r-project.org/web/packages/arulesSequences/index.html\" rel=\"nofollow\">arulesSequences</a> but am not sure how to fit it to my problem. The data format would be tuples similar to those used in <a href=\"http://cran.r-project.org/web/packages/arulesSequences/index.html\" rel=\"nofollow\">arulesSequences</a>, but instead of just mentioning products A and B in transactions, I would like to mention the quantities in which those products (games of different types) were bought (played, in my case). </p>\\n\\n<p>Example:</p>\\n\\n<p>sequence1 (for uid=1)</p>\\n\\n<pre><code>Date   UID Game1     Game2      Game3     Game4\\nJan1    1   125times 0times     0times     0times\\nJan2    1   0times   1time      0times     0times\\n</code></pre>\\n\\n<p>Each user would have such a sequence. However, I see that arulesSequences function cspades would only allow to operate with bollean types, e.g. whether each game was actually played on that date:</p>\\n\\n<p>Example:</p>\\n\\n<p>sequence1 (for uid=1)</p>\\n\\n<pre><code>Date   UID \\nJan1    1   Game1 Game2\\nJan2    1   Game1\\nJan3    1   Game2 Game3\\n</code></pre>\\n\\n<p>My goal is to determine rules like \"if a user playes Game3 on that date, it causes them to play much more of Game2 one week later\". </p>\\n',\n",
       " \"<p>For Kolmogorov-Smirnov, consider the null hypothesis.  It says that a sample is drawn from a particular distribution.  So if you construct the empirical distribution function for $n$ samples $f(x) = \\\\\\\\frac{1}{n} \\\\\\\\sum_i \\\\\\\\chi_{(-\\\\\\\\infty, X_i]}(x)$, in the limit of infinite data, it will converge to the underlying distribution.</p>\\n\\n<p>For finite information, it will be off.  If one of the measurements is $q$, then at $x=q$ the empirical distribution function takes a step up.  We can look at it as a random walk which is constrained to begin and end on the true distribution function.  Once you know that, you go ransack the literature for the huge amount of information known about random walks to find out what the largest expected deviation of such a walk is.</p>\\n\\n<p>You can do the same trick with any $p$-norm of the difference between the empirical and underlying distribution functions.  For $p=2$, it's called the Cramer-von Mises test.  I don't know the set of all such tests for arbitrary real, positive $p$ form a complete class of any kind, but it might be an interesting thing to look at.</p>\\n\",\n",
       " '<p>Assuming that you had data that were appropriate to use parametric tests on, an ANOVA with an interaction term will tell you if the size of the difference in one condition (1,2) depends on the level of the other variable (x,y).  If the observed interaction has a low probability of occurring under an assumption that the null hypothesis is true you can conclude that is the case.</p>\\n\\n<p>Or, you can just directly test the effects with a t-test of x1-y1 compared to x2-y2.</p>\\n',\n",
       " '<p>The simple correlation approach isn\\'t the right way to analyze results from method comparison studies. There are (at least) two highly recommended books on this topic that I referenced at the end (1,2). Briefly stated, when comparing measurement methods we usually expect that (a) our conclusions should not depend on the particular sample used for the comparison, and (b) measurement error associated to the particular measurement instrument should be accounted for. This precludes any method based on correlations, and we shall turn our attention to variance components or mixed-effects models that allow to reflect the systematic effect of item (here, item stands for individual or sample on which data are collected), which results from (a).</p>\\n\\n<p>In your case, you have single measurements collected using two different methods (I assume that none of them might be considered as a gold standard) and the very basic thing to do is to plot the differences ($X_1-X_2$) versus the means ($(X_1+X_2)/2$); this is called a Bland-Altman plot. It will allow you to check if (1) the variations between the two set of measurements are constant and (2) the variance of the difference is constant across the range of observed values. Basically, this is just a 45° rotation of a simple scatterplot of $X_1$ vs. $X_2$, and its interpretation is close to a plot of fitted <em>vs.</em> residuals values used in linear regression. Then,</p>\\n\\n<ul>\\n<li>if the difference is constant (<em>constant bias</em>), you can compute the limit of agreement (see (3))</li>\\n<li>if the difference is not constant across the range of measurement, you can fit a linear regression model between the two methods (choose the one you want as predictor)</li>\\n<li>if the variance of the differences is not constant, try to find a suitable transformation that makes the relationship linear with constant variance</li>\\n</ul>\\n\\n<p>Other details may be found in (2), chapter 4.</p>\\n\\n<p><strong>References</strong></p>\\n\\n<ol>\\n<li>Dunn, G (2004). <em>Design and Analysis of Reliability Studies</em>. Arnold. See the review in the <a href=\"http://ije.oxfordjournals.org/content/34/2/499.1.full\"><em>International Journal of Epidemiology</em></a>.</li>\\n<li>Carstensen, B (2010). <em>Comparing clinical measurement methods</em>. Wiley. See the <a href=\"http://staff.pubhealth.ku.dk/~bxc/MethComp/\">companion website</a>, including R code.</li>\\n<li>The original article from Bland and Altman, <a href=\"http://www-users.york.ac.uk/~mb55/meas/ba.htm\">Statistical methods for assessing agreement between two methods of clinical measurement</a>.</li>\\n<li>Carstensen, B (2004). <a href=\"http://biostatistics.oxfordjournals.org/content/5/3/399.full.pdf\">Comparing and predicting between several methods of measurement</a>. <em>Biostatistics</em>, <em>5(3)</em>, 399–413.</li>\\n</ol>\\n',\n",
       " \"<p>I'm processing some data that requires binning before it goes through a regression algorithm.  The script is in Python and uses the Numpy <code>histogram</code> function, but the code should be self explanatory.  For reference, <code>histogram</code> outputs either an array containing the integer number of points in each bin, or you can weight by the value of the points in the bin (e.g. a sum).  The y errors are standard devs.</p>\\n\\n<p>So the data is binned like:</p>\\n\\n<pre><code>#Number of items in each bin\\n(binned, dump) = np.histogram(x,bins)\\n#Binned data, weighted by value of each point\\n(xbinned, dump) = np.histogram(x,bins, weight=x)\\n#etc for y and y error ...\\n</code></pre>\\n\\n<p>The errors on the y values (this assumes no x errors) are weighted by the variance so the errors are added in quadrature:</p>\\n\\n<pre><code>(varbinned, dump) = np.histogram(x,bins,weight=error**2)\\n</code></pre>\\n\\n<p>To finish off the function returns an averaged array of points, e.g.</p>\\n\\n<pre><code>xbinned = xbinned/binned\\n</code></pre>\\n\\n<p>What I'm not sure about is how to handle the errors.  The result should be square rooted (to convert back from the summed variances), but how is the number of bins factored into the answer?  Is it as straightforward as doing:</p>\\n\\n<pre><code>errorbinned = np.sqrt(varbinned/binned)\\n</code></pre>\\n\\n<p>Thanks!</p>\\n\",\n",
       " '<p><a href=\"http://rads.stackoverflow.com/amzn/click/1584885459\" rel=\"nofollow\">Statistical Computing with R - Maria L. Rizzo</a> covers a lot of the topics in Probability and Statistics for Computer Scientists - basic probability and statistics, random variables, Bayesian statistics, Markov chains, visualization of multivariate data, Monte Carlo methods, Permutation tests, probability density estimation, and numerical methods.</p>\\n\\n<p>The equations and formulas used are presented both as mathematical formulas as well as in R code. I would say that a basic knowledge of probability, statistics, calculus, and maybe discrete mathematics would be advisable for anyone who wants to read this book. A programming background would also be helpful, but there are some references for the R language, operators, and syntax.</p>\\n',\n",
       " '<p>The usual partial correlation between X and Y given the set of variables Z is the Pearson correlation between residuals resulting from the liner regression of X on Z and Y on Z. It can be computed using the recursive formula (see <a href=\"http://en.wikipedia.org/wiki/Partial_correlation#Using_recursive_formula\" rel=\"nofollow\">wikipedia</a>) from the correlation matrix. </p>\\n\\n<p>My question is what is the interpretation of the partial correlation if the correlation matrix contains other correlation coefficients, like Kendall\\'s tau or Spearman\\'s rho, and not Pearson correlation coefficients. </p>\\n',\n",
       " '<p>If your data really is <a href=\"http://en.wikipedia.org/wiki/Exponential_random_variable\" rel=\"nofollow\">exponentially distributed</a>, find the <a href=\"http://en.wikipedia.org/wiki/Exponential_random_variable#Maximum_likelihood\" rel=\"nofollow\">maximum likelihood estimate of the rate</a> $\\\\\\\\lambda$, then transform the samples $X_n$ to a sequence $Y_n$ uniformly distributed in $[0,1]$ using the equation $Y_n=1-\\\\\\\\mathrm e^{-\\\\\\\\lambda X_n}$</p>\\n',\n",
       " '<p>The <a href=\"http://en.wikipedia.org/wiki/Anova#Assumptions_of_ANOVA\">Wikipedia page on ANOVA lists three assumptions</a>, namely:</p>\\n\\n<ul>\\n<li>Independence of cases – this is an assumption of the model that simplifies the statistical analysis.</li>\\n<li>Normality – the distributions of the residuals are normal.</li>\\n<li>Equality (or \"homogeneity\") of variances, called homoscedasticity...</li>\\n</ul>\\n\\n<p>Point of interest here is the second assumption. Several sources list the assumption differently. Some say normality of the raw data, some claim of residuals.</p>\\n\\n<p>Several questions pop up:</p>\\n\\n<ul>\\n<li>are normality and normal distribution of residuals the same person (based on Wikipedia entry, I would claim normality is a property, and does not pertain residuals directly (but can be a property of residuals (deeply nested text within brackets, freaky)))?</li>\\n<li>if not, which assumption should hold? One? Both?</li>\\n<li>if the assumption of normally distributed residuals is the right one, are we making a grave mistake by checking only the histogram of raw values for normality?</li>\\n</ul>\\n',\n",
       " '<blockquote>\\n  <p>\"To me, a layman, I would say that this shows that the gradients should be zero given the very small mean value and the tight standard distribution.\"</p>\\n</blockquote>\\n\\n<p>One can infer from the numbers you provided (using, e.g., Chebyshev\\'s inequality) that the observed gradients, not only their mean and SD, were small.   To the extent that the gradients derived from the data sets are independent observations (from similar distributions) of one underlying \"true\" gradient this is evidence that the parameter itself is small.</p>\\n\\n<p>That the SD is ten times larger than the mean is consistent with the idea that the observed gradients are random errors around a true value of zero.  It is also consistent with the mean being small (but nonzero) for systematic reasons, maybe similar to whatever reasons led to the suspicion that the mean is exactly zero, and observational noise larger than the true size of the mean.  </p>\\n\\n<p>To say more, additional information is needed about the number, size and nature of the data sets used to determine the gradients.</p>\\n',\n",
       " '<p>I have various clinical data on participants in a study. I\\'m looking at a continuous variable (\"A\") and a (binary) categorical variable (group) (\"O\"). I used a Wilcoxon test in R (the data are not normally distributed) to see if \"A\" is significantly different between the two groups. I got a borderline p-value of 0.054.  </p>\\n\\n<p>If I run the Wilcoxon again but include only the males (30 of 72), the p-value is ~0.3; for females only it\\'s ~0.25.  </p>\\n\\n<p>How is it possible that there is no difference in \"A\" between the groups for males and females separately, but when combined there is a difference?</p>\\n',\n",
       " '<p>In a previous post I’ve wondered how to <a href=\"http://stats.stackexchange.com/questions/22494/is-using-a-questionnaire-score-euroqols-eq-5d-with-a-bimodal-distribution-as\">deal with EQ-5D scores</a>. Recently I stumbled upon logistic quantile regression suggested by <a href=\"http://www.ncbi.nlm.nih.gov.proxy.kib.ki.se/pubmed/19941281\">Bottai and McKeown</a> that introduces an elegant way to deal with bounded outcomes.\\nThe formula is simple:</p>\\n\\n<p>$logit(y)=log(\\\\\\\\frac{y-y_{min}}{y_{max}-y})$</p>\\n\\n<p>To avoid log(0) and division by 0 you extend the range by a small value, $\\\\\\\\epsilon$. This gives an environment that respects the boundaries of the score. </p>\\n\\n<p>The problem is that any $\\\\\\\\beta$ will be in the logit scale and that makes doesn’t make any sense unless transformed back into the regular scale but that means that the $\\\\\\\\beta$ will be non-linear. For graphing purposes this doesn’t matter but not with more $\\\\\\\\beta$:s this will be very inconvenient. </p>\\n\\n<p>My question:</p>\\n\\n<p><strong>How do you suggest to report a logit $\\\\\\\\beta$ without reporting the full span?</strong></p>\\n\\n<hr>\\n\\n<h2>Implementation example</h2>\\n\\n<p>For testing the implementation I’ve written a simulation based on this basic function:</p>\\n\\n<p>$outcome=\\\\\\\\beta_0+\\\\\\\\beta_1* xtest^3+\\\\\\\\beta_2*sex$</p>\\n\\n<p>Where $\\\\\\\\beta_0 = 0$, $\\\\\\\\beta_1 = 0.5$ and $\\\\\\\\beta_2 = 1$. Since there is a ceiling in scores I’ve set any outcome value above 4 and any below -1 to the max value.</p>\\n\\n<h3>Simulate the data</h3>\\n\\n<pre><code>set.seed(10)\\nintercept &lt;- 0\\nbeta1 &lt;- 0.5\\nbeta2 &lt;- 1\\nn = 1000\\nxtest &lt;- rnorm(n,1,1)\\ngender &lt;- factor(rbinom(n, 1, .4), labels=c(\"Male\", \"Female\"))\\nrandom_noise  &lt;- runif(n, -1,1)\\n\\n# Add a ceiling and a floor to simulate a bound score\\nfake_ceiling &lt;- 4\\nfake_floor &lt;- -1\\n\\n# Just to give the graphs the same look\\nmy_ylim &lt;- c(fake_floor - abs(fake_floor)*.25, \\n             fake_ceiling + abs(fake_ceiling)*.25)\\nmy_xlim &lt;- c(-1.5, 3.5)\\n\\n# Simulate the predictor\\nlinpred &lt;- intercept + beta1*xtest^3 + beta2*(gender == \"Female\") + random_noise\\n# Remove some extremes\\nlinpred[linpred &gt; fake_ceiling + abs(diff(range(linpred)))/2 |\\n    linpred &lt; fake_floor - abs(diff(range(linpred)))/2 ] &lt;- NA\\n#limit the interval and give a ceiling and a floor effect similar to scores\\nlinpred[linpred &gt; fake_ceiling] &lt;- fake_ceiling\\nlinpred[linpred &lt; fake_floor] &lt;- fake_floor\\n</code></pre>\\n\\n<p>To plot the above:</p>\\n\\n<pre><code>library(ggplot2)\\n# Just to give all the graphs the same look\\nmy_ylim &lt;- c(fake_floor - abs(fake_floor)*.25, \\n             fake_ceiling + abs(fake_ceiling)*.25)\\nmy_xlim &lt;- c(-1.5, 3.5)\\nqplot(y=linpred, x=xtest, col=gender, ylab=\"Outcome\")\\n</code></pre>\\n\\n<p>Gives this image:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/luZGu.png\" alt=\"Scatterplot from simulation\"></p>\\n\\n<h3>The regressions</h3>\\n\\n<p>In this section I create the regular linear regression, quantile regression (using the median) and logistic quantile regression. All estimates are based on bootstrapped values using the bootcov() function.</p>\\n\\n<pre><code>library(rms)\\n\\n# Regular linear regression\\nfit_lm &lt;- Glm(linpred~rcs(xtest, 5)+gender, x=T, y=T)\\nboot_fit_lm &lt;- bootcov(fit_lm, B=500)\\np &lt;- Predict(boot_fit_lm, xtest=seq(-2.5, 3.5, by=.001), gender=c(\"Male\", \"Female\"))\\nlm_plot &lt;- plot.Predict(p, \\n             se=T, \\n             col.fill=c(\"#9999FF\", \"#BBBBFF\"), \\n             xlim=my_xlim, ylim=my_ylim)\\n\\n# Quantile regression regular\\nfit_rq &lt;- Rq(formula(fit_lm), x=T, y=T)\\nboot_rq &lt;- bootcov(fit_rq, B=500)\\n# A little disturbing warning:\\n# In rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique\\n\\np &lt;- Predict(boot_rq, xtest=seq(-2.5, 3.5, by=.001), gender=c(\"Male\", \"Female\"))\\nrq_plot &lt;- plot.Predict(p, \\n             se=T, \\n             col.fill=c(\"#9999FF\", \"#BBBBFF\"), \\n             xlim=my_xlim, ylim=my_ylim)\\n\\n# The logit transformations\\nlogit_fn &lt;- function(y, y_min, y_max, epsilon)\\n    log((y-(y_min-epsilon))/(y_max+epsilon-y))\\n\\n\\nantilogit_fn &lt;- function(antiy, y_min, y_max, epsilon)\\n    (exp(antiy)*(y_max+epsilon)+y_min-epsilon)/\\n        (1+exp(antiy))\\n\\n\\nepsilon &lt;- .0001\\ny_min &lt;- min(linpred, na.rm=T)\\ny_max &lt;- max(linpred, na.rm=T)\\nlogit_linpred &lt;- logit_fn(linpred, \\n                          y_min=y_min,\\n                          y_max=y_max,\\n                          epsilon=epsilon)\\n\\nfit_rq_logit &lt;- update(fit_rq, logit_linpred ~ .)\\nboot_rq_logit &lt;- bootcov(fit_rq_logit, B=500)\\n\\n\\np &lt;- Predict(boot_rq_logit, xtest=seq(-2.5, 3.5, by=.001), gender=c(\"Male\", \"Female\"))\\n\\n# Change back to org. scale\\ntransformed_p &lt;- p\\ntransformed_p$yhat &lt;- antilogit_fn(p$yhat,\\n                                    y_min=y_min,\\n                                    y_max=y_max,\\n                                    epsilon=epsilon)\\ntransformed_p$lower &lt;- antilogit_fn(p$lower, \\n                                     y_min=y_min,\\n                                     y_max=y_max,\\n                                     epsilon=epsilon)\\ntransformed_p$upper &lt;- antilogit_fn(p$upper, \\n                                     y_min=y_min,\\n                                     y_max=y_max,\\n                                     epsilon=epsilon)\\n\\nlogit_rq_plot &lt;- plot.Predict(transformed_p, \\n             se=T, \\n             col.fill=c(\"#9999FF\", \"#BBBBFF\"), \\n             xlim=my_xlim, ylim=my_ylim)\\n</code></pre>\\n\\n<h3>The plots</h3>\\n\\n<p>To compare with the base function I’ve added this code:</p>\\n\\n<pre><code>library(lattice)\\n# Calculate the true lines\\nx &lt;- seq(min(xtest), max(xtest), by=.1)\\ny &lt;- beta1*x^3+intercept\\ny_female &lt;- y + beta2\\ny[y &gt; fake_ceiling] &lt;- fake_ceiling\\ny[y &lt; fake_floor] &lt;- fake_floor\\ny_female[y_female &gt; fake_ceiling] &lt;- fake_ceiling\\ny_female[y_female &lt; fake_floor] &lt;- fake_floor\\n\\ntr_df &lt;- data.frame(x=x, y=y, y_female=y_female)\\ntrue_line_plot &lt;- xyplot(y  + y_female ~ x, \\n                         data=tr_df,\\n                         type=\"l\", \\n                         xlim=my_xlim, \\n                         ylim=my_ylim, \\n                         ylab=\"Outcome\", \\n                         auto.key = list(\\n                           text = c(\"Male\",\" Female\"),\\n                           columns=2))\\n\\n\\n# Just for making pretty graphs with the comparison plot\\ncompareplot &lt;- function(regr_plot, regr_title, true_plot){\\n  print(regr_plot, position=c(0,0.5,1,1), more=T)\\n  trellis.focus(\"toplevel\")\\n  panel.text(0.3, .8, regr_title, cex = 1.2, font = 2)\\n  trellis.unfocus()\\n  print(true_plot, position=c(0,0,1,.5), more=F)\\n  trellis.focus(\"toplevel\")\\n  panel.text(0.3, .65, \"True line\", cex = 1.2, font = 2)\\n  trellis.unfocus()\\n}\\n\\ncompareplot(lm_plot, \"Linear regression\", true_line_plot)\\ncompareplot(rq_plot, \"Quantile regression\", true_line_plot)\\ncompareplot(logit_rq_plot, \"Logit - Quantile regression\", true_line_plot)\\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/74Uid.png\" alt=\"Linear regression for bounded outcome\"></p>\\n\\n<p><img src=\"http://i.stack.imgur.com/xHRtF.png\" alt=\"Quantile regression for bounded outcome\"></p>\\n\\n<p><img src=\"http://i.stack.imgur.com/XfLy8.png\" alt=\"Logistic quantile regression for bounded outcome\"></p>\\n\\n<h3>The contrast output</h3>\\n\\n<p>Now I\\'ve tried to get the contrast and it\\'s almost \"right\" but it varies along the span as expected:</p>\\n\\n<pre><code>&gt; contrast(boot_rq_logit, list(gender=levels(gender), \\n+                              xtest=c(-1:1)), \\n+          FUN=function(x)antilogit_fn(x, epsilon))\\n   gender xtest Contrast   S.E.       Lower      Upper       Z      Pr(&gt;|z|)\\n   Male   -1    -2.5001505 0.33677523 -3.1602179 -1.84008320  -7.42 0.0000  \\n   Female -1    -1.3020162 0.29623080 -1.8826179 -0.72141450  -4.40 0.0000  \\n   Male    0    -1.3384751 0.09748767 -1.5295474 -1.14740279 -13.73 0.0000  \\n*  Female  0    -0.1403408 0.09887240 -0.3341271  0.05344555  -1.42 0.1558  \\n   Male    1    -1.3308691 0.10810012 -1.5427414 -1.11899674 -12.31 0.0000  \\n*  Female  1    -0.1327348 0.07605115 -0.2817923  0.01632277  -1.75 0.0809  \\n\\nRedundant contrasts are denoted by *\\n\\nConfidence intervals are 0.95 individual intervals\\n</code></pre>\\n',\n",
       " '<p>Cov(∑X$_i$, ∑Y$_i$)=E(∑X$_i$ ∑Y$_i$)-E(∑X$_i$)E(∑Y$_i$)  where the sum is taken up to the random integer N.  To calculate this you do need to take the expectation with respect to the geometric distribution for N of the conditional expectation given N=n. Now conditioned on N=n ∑X$_i$ is binomial n, 1/4 and ∑Y$_i$ is binomial n, 3/4 So E(∑X$_i$|N=n)=E(∑Y$_i$|N=n)=3n/16. Now you need to compute E(∑X$_i$ ∑Y$_i$|N=n).</p>\\n\\n<p>Suppose f(n) denotes the conditional covariance given N=n.  Then to get the unconditional covariance you would compute the following sum:</p>\\n\\n<p>∑f(n)/2$^n$  where the sum runs from n=1 to ∞.</p>\\n',\n",
       " '<p>You certainly should <em>not</em> assume that all companies that have income of 200,000 or above have income of 200,000 exactly. That makes no sense. But what <em>should</em> you assume? I am not sure what answer would be expected in a first course in statistics.</p>\\n\\n<p>For the first question (find the lowest quartile) you can probably assume that the data are uniformly distributed in each category - that makes more sense than normal distribution. So, you would figure the total number of categories, then figure which category had the 20th percentile, then interpolate.</p>\\n\\n<p>For the second question, though, it\\'s much trickier. The desired answer <em>might</em> be \"I don\\'t have enough information\" or something like that. Or possibly you ought to say \"at least XXX\". But, if you want to get fancy, distributions like this often follow some sort of power law. But that may be beyond what is being asked. </p>\\n',\n",
       " '<p>I would appreciate some help getting some EM stuff straight. So, say I generate data in R as follows:</p>\\n\\n<pre><code>N       &lt;- 100\\nepsilon &lt;- rnorm(N)\\nX       &lt;-  10*runif(N)\\nbeta.0  &lt;- 10\\nbeta.1  &lt;-  3\\nsigma   &lt;- 2\\nY       &lt;-  beta.0 + beta.1 * X + sigma * epsilon\\nepsilon2 &lt;- rnorm(N)\\nX2 &lt;- 10*runif(N)\\nY2 &lt;-  3 - X2 + 0.25 * epsilon2\\nY.mix &lt;- c(Y, Y2)\\nX.mix &lt;- c(X, X2)\\n</code></pre>\\n\\n<p>Now, in expectation maximization, in the first step, I have some prior probability, say 0.5, of the data being from either one or the other distribution. So, using EM I know I can estimate the mean and variance of the two mixtures. From looking at a density plot, it seems like the means are at about -2 and 30 for the data I simulated. But, at what stage in EM do I back out the betas? I want to recover the slope, intercept, and sd deviation parameters for the 2 regression-type equations.</p>\\n\\n<p>Thanks for an clarification.</p>\\n',\n",
       " '<p>This one is brand new, and Allen Wilcox is an epidemiologist, not a statistician, but whatever, I\\'m running with it.</p>\\n\\n<p>\"Data do not speak for themselves - they need context, and they need skeptical evaluation\"</p>\\n',\n",
       " '<p>If I can add an example of when $R^2$ is dangerous.  Many years ago I was working on some biometric data and being young and foolish I was delighted when I found some statistically significant $R^2$ values for my fancy regressions which I had constructed using stepwise functions.  It was only afterwards looking back after my presentation to a large international audience did I realize that given the massive variance of the data - combined with the possible poor representation of the sample with respect to the population, an $R^2$ of 0.02 was utterly meaningless even if it was \"statistically significant\"...</p>\\n\\n<p>Those working with statistics need to understand the data!</p>\\n',\n",
       " '<p>My question builds on a <a href=\"http://stats.stackexchange.com/questions/1142/simple-algorithm-for-online-outlier-detection-of-a-generic-time-series/1153#1153\">previous post</a> on outlier detection in generic time series, and specifically on the answer provided by the always great Rob H.</p>\\n\\n<p>I work for a small-sized manufacturing company that currently handles the issue, i.e. detecting outliers in sales data time series, employing a (dubious) automated off-the-shelf software procedure. </p>\\n\\n<p>I think this kind of approach is questionable at best and, more often than not, I\\'m not happy with the results I get. I would therefore like to \"double check\" the output from our software using some alternative method.</p>\\n\\n<p>Rob\\'s idea seemed reasonable, straightforward and easy to implement, so I decided to give it a try. Question is: what if my time series are not \"generic\"?</p>\\n\\n<p>Stl decomposition highlights a strong seasonality and a varying trend in my data:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/fGuz7.jpg\" alt=\"enter image description here\"></p>\\n\\n<p>(BTW I used <code>stl(x,s.window=\"periodic\")</code> like Rob suggested, but IMHO <code>stl(x,s.window=\"periodic\",robust=TRUE)</code> would be a better choice since outlier detection is the issue at hand here. Also I\\'m not really sure about the <code>s.window=\"periodic\"</code> part, I tried experimenting with different values a bit, but I don\\'t know how to interpret results. Maybe someone can point me in the right direction?).</p>\\n\\n<p>Back to my question, mine being sales data, the seasonal pattern is (or I think it is) strongly affected by calendar effects. Also I have reason to believe the big level shift in 2009 is due to the financial crisis and it has nothing to do with trend.</p>\\n\\n<p>What do I do here? Should I let the model handle this, or should I pre-process data? Do I perform working-day adjustment and re-allign (is there such a thing?) before-2009 and after-2009 data, or do I let STL decomposition do the work?</p>\\n\\n<p>I could write another 1000 lines, but I think this should be enough to get the message through. I apologize for the WOT and for my bad english. Also I hope I did not break too many forum rules...</p>\\n\\n<p>I hope someone out there can help!</p>\\n',\n",
       " '<p>Should the n for sample size be capitalized?  Is there a difference between n and N?</p>\\n',\n",
       " '<p>It is true that if $P(A)=P(A\\\\\\\\mid B)$ then the two events are intependent.</p>\\n\\n<p>Now we know that: $P(A\\\\\\\\mid B)=\\\\\\\\frac{P(A\\\\\\\\cap B)}{P(B)}$. Note the intersection in the numerator. If the two events didn\\'t have \"anything in common\" then we would have $P(A\\\\\\\\cap B)=0$ since the set $A\\\\\\\\cap B$ would be empty. However independence works in the space of probabilities. Two events are called independent if $P(A\\\\\\\\cap B)=P(A)P(B)$, ie the probability of having <em>both events occuring is equal to the probability of the first one occuring times the probability of the second one.</em> The fact that event $A$ occured doesn\\'t tell us anything about event $B$.</p>\\n\\n<p>If on the other hand $P(A\\\\\\\\cap B)=0$ this simply says that the two events cannot happen at the same time: they are disjoint.</p>\\n\\n<p>Example 1.\\nWhat is the probability of throwing a die and getting both 1 and 3 at the same time? Since the two events are disjoint, we have that this probability is zero.</p>\\n\\n<p>Example 2.\\nWhat is the probability of throwing a die two times and getting 1 in the first time and 2 in the second? The two events are not disjoint; the fact that one happened doesn\\'t exclude the other. However they are independent: the fact that one happened tells us nothing about the other. The probability of getting 1 in the first throw is 1/6, the probability of getting 2 in the second throw is 1/6. Then (abusing notation a bit): $P(2\\\\\\\\mid 1)= \\\\\\\\frac{P(1 \\\\\\\\cap 2)}{P(1)} = \\\\\\\\frac{ \\\\\\\\frac{1}{36}}{\\\\\\\\frac{1}{6}}=\\\\\\\\frac{1}{6}=P(2)$.</p>\\n',\n",
       " '<p>I often see in various tutorials and webpages that explains fuzzy c-means clustering and soft k-means clustering. But i was not able to find any material which differentiates them.</p>\\n\\n<p>Is it that both fuzzy c-means and soft k-means clustering are same or different?</p>\\n',\n",
       " '<p>Could someone provide overhead of the following model for training (With respect to input size or if there are any relevant parameters). Overhead I mean somewhat like asymptotic time complexity form. </p>\\n\\n<ol>\\n<li>Multiple linear regression models (Least Sqaures)</li>\\n<li>Decision Tree (C 4.5 OR ID3)</li>\\n<li>K nearest neighbor </li>\\n</ol>\\n\\n<p>Thank you.</p>\\n',\n",
       " '<p>You can compare the <a href=\"http://en.wikipedia.org/wiki/Five-number_summary\" rel=\"nofollow\">five number summaries</a> of each sample.  Also a boxplot can be informative.</p>\\n',\n",
       " '<p>I assume you are refering to linear regression.  Thus we have\\n$$y=x^T\\\\\\\\beta+e$$\\nNow the <em>homoscedasticity</em> assumption means that the variance does not depend on $x$.  so we have\\n$$var[e|x]=var[e]$$\\nThis means each observation is equally important for estimating the mean square error.</p>\\n',\n",
       " '<p>I haven\\'t seen the <a href=\"http://scikit-learn.org/\" rel=\"nofollow\">scikit-learn</a> explicitly mentioned in the answers above. It\\'s a Python package for machine learning in Python. It\\'s fairly young but growing extremely rapidly (disclaimer: I am a scikit-learn developer). It\\'s goals are to provide standard machine learning algorithmic tools in a unified interface with a focus on speed, and usability. As far as I know, you cannot find anything similar in Matlab. It\\'s strong points are:</p>\\n\\n<ul>\\n<li><p>A <a href=\"http://scikit-learn.org/user_guide.html\" rel=\"nofollow\">detailed documentation</a>, with <a href=\"http://scikit-learn.org/stable/auto_examples/index.html\" rel=\"nofollow\">many examples</a></p></li>\\n<li><p>High quality standard <a href=\"http://scikit-learn.org/stable/supervised_learning.html\" rel=\"nofollow\">supervised learning</a> (regression/classification) tools. Specifically: </p>\\n\\n<ul>\\n<li><p>very versatile <a href=\"http://scikit-learn.org/stable/modules/svm.html\" rel=\"nofollow\">SVM</a> (based on libsvm, but with integration of external patches, and a lot of work on the Python binding)</p></li>\\n<li><p><a href=\"http://scikit-learn.org/stable/modules/linear_model.html\" rel=\"nofollow\">penalized linear models</a> (<a href=\"http://scikit-learn.org/stable/modules/linear_model.html#lasso\" rel=\"nofollow\">Lasso</a>, <a href=\"http://scikit-learn.org/stable/auto_examples/logistic_l1_l2_coef.html\" rel=\"nofollow\">sparse logistic regression</a>...) with efficient implementations.</p></li>\\n</ul></li>\\n<li><p>The ability to perform <a href=\"http://scikit-learn.org/stable/modules/grid_search.html\" rel=\"nofollow\">model selection</a> by cross-validation using multiple CPUs</p></li>\\n<li><p><a href=\"http://scikit-learn.org/stable/unsupervised_learning.html\" rel=\"nofollow\">Unsupervised learning</a> to explore the data or do a first dimensionality reduction, that can easily be chained to supervised learning.</p></li>\\n<li><p>Open source, BSD licensed. If you are not in a purely academic environment (I am in what would be a national lab in the state) this matters a lot as Matlab costs are then very high, and you might be thinking of deriving products from your work.</p></li>\\n</ul>\\n\\n<p>Matlab is a great tool, but in my own work, scipy+scikit-learn is starting to give me an edge on Matlab because Python does a better job with memory due to its view mechanism (and I have big data), and because the scikit-learn enables me to very easily compare different approaches.</p>\\n',\n",
       " '<p>I have written a computer program that can detect coins in a static image (.jpeg, .png, etc.) using some standard techniques for computer vision (Gaussian Blur, thresholding, Hough-Transform etc.). Using the ratios of the coins picked up from a given image, I can establish with good certainty which coins are which. However, I wish to add to my confidence levels and also determine if a coin that I deduce to be of type-A (from radius ratios) is also of the correct colo[u]r. The problem is that for British coins et al. (copper, silver, gold), the respective colors (esp. of copper to gold) are very similar.</p>\\n\\n<p>I have a routine that extracts the mean color of a given coin in terms of the RedGreenBlue (RGB) \\'color-space\\' and routines to convert this color into HueSaturationBrightness (HSB or HSV) \\'color-space\\'. </p>\\n\\n<p>RGB is not very nice to work with in attempting to differentiating between the three coin colors (see attached [basic] image for an example). I have the following ranges and typical values for the colours of the different coin types:</p>\\n\\n<p><em>Note: the typical value here is one selected using a \\'pixel-wise\\' mean of a real image.</em></p>\\n\\n<pre><code>**Copper RGB/HSB:** typicalRGB = (153, 117, 89)/(26, 0.42, 0.60).\\n\\n**Silver RGB/HSB:** typicalRGB = (174, 176, 180)/(220, 0.03, 0.71).\\n\\n**Gold RGB/HSB:** typicalRGB = (220, 205, 160)/(45, 0.27, 0.86) \\n</code></pre>\\n\\n<p>I first tried to use the \\'Euclidian distance\\' between a given mean coin color (using RGB) and the typical values for each coin type given above treating the RGB values as a vector; for copper we would have: </p>\\n\\n<p>$$D_{copper} = \\\\\\\\sqrt((R_{type} - R_{copper})^{2} + (G_{type} - G_{copper})^{2} + (B_{type} - B_{copper})^{2})$$</p>\\n\\n<p>where the smallest value of the difference ($D$) would tell us which type the given coin is most likely to be. This method has shown itself to be very inaccurate.</p>\\n\\n<p>I have also tried just comparing the hue of the coins with the typical values of the types provided above. Although theoretically this provides a much better \\'color-space\\' to deal with varying brightness and saturation levels of the images, it too was not accurate enough.</p>\\n\\n<p><strong>Question:</strong> What is the best method to determine a coins type based on color (from a static image)?</p>\\n\\n<p>Thanks very much for your time.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/GpZHm.jpg\" alt=\"Typical Coin Colors\"></p>\\n\\n<p><strong>Edit 1</strong></p>\\n\\n<p>Note: I have tried all of the ideas discussed below and have achieved next to nothing. Variance in lighting conditions (even within the same image) make this problem very tough and should be taken into consideration. </p>\\n\\n<p><strong>Edit 2 (Summery of Outcome)</strong></p>\\n\\n<p>Thank you for your answers. Further research of my own (including your answers and comments) has highlighted just how tough this problem is to deal with in the generic case of arbitrary lighting, arbitrary camera (mobile device), fluctuation in coin colour (even for same species/type) etc. I first looked at skin colour recognition (a very active field of research) as a starting point and there are still numerous problems even with the recognition of skin colour for Caucasians alone (see <a href=\"http://graphicon.ru/oldgr/en/publications/text/gc2003vsa.pdf\">this paper</a> for a review of the current techniques), and the fact that this problem contains three distinct colour objects all of which can have continuous and varying chromacities make this topic of computer vision a very hard one to classify and deal with accordingly (in fact you could do a good Ph.D. on it!). </p>\\n\\n<p>I looked into the Gamut Constraint Method from the very helpful <a href=\"http://stats.stackexchange.com/a/23678/7258\">post</a> by D.W. below. This was at first sight very promising as a pre-processing step to transform the image and the separate coin objects to colours that are independent of lighting conditions. However, even this technique does not work perfectly (and involves a library of images/histograms for mappings – which I don’t want to get into) and neither does the much more complex Neural Network Architecture methodologies. In fact <a href=\"http://www.cs.ubc.ca/~lowe/525/papers/funt98.pdf\">this paper</a> states in the abstract that:</p>\\n\\n<pre><code>\"current machine colour constancy algorithms are not good enough for colour-based \\n object recognition.\".\\n</code></pre>\\n\\n<p>That is not to say that there aren’t much more up-to-date papers on this subject out there, but I can\\'t find them and it does not seem to be a very active research area at this time.</p>\\n\\n<p>The <a href=\"http://stats.stackexchange.com/a/23701/7258\">answer</a> by AVB was also helpful and I have looked into L*A*B* briefly. </p>\\n\\n<pre><code>\"The nonlinear relations for L*, a*, and b* are intended to mimic the nonlinear\\nresponse of the eye. Furthermore, uniform changes of components in the L*a*b* colour\\nspace aim to correspond to uniform changes in perceived colour, so the relative \\nperceptual differences between any two colours in L*a*b* can be approximated by \\ntreating each colour as a point in a three dimensional space.\"\\n</code></pre>\\n\\n<p>From what I have read, the transformation to this colour space for my device dependent images will be tricky - but I will look into this in detail (with a view to some sort of implementation) when I have a bit more time. </p>\\n\\n<p>I am not holding my breath for a concrete solution to this problem and after the attempt with L*A*B* I shall be neglecting coin colour and looking to sure-up my current geometric detection algorithms (accurate Elliptic Hough Transform etc.).</p>\\n\\n<p>Thanks you all. And as a end note to this question, here is the same image with a new geometric detection algorithm, which has no colour recognition:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/G4Pnp.jpg\" alt=\"enter image description here\"></p>\\n',\n",
       " '<p>Assume a linear regression with metric predictors: y ~ x1 + x2 + x3</p>\\n\\n<p>Assume all x are significant predictors. </p>\\n\\n<p>Now I want to find out if predictors differ from each other, that is, if one predictor is a stronger predictor of y than the others, and on top of it, if it is <em>significantly</em> stronger. </p>\\n\\n<p>Results: </p>\\n\\n<blockquote>\\n  <p>X | Unstd. B | Std. Beta | t value | p value | CI lower | CI upper </p>\\n  \\n  <p>1--- .140----- .170 ------ 9.806 --- .000 --- .112 --- .168 ------</p>\\n  \\n  <p>2--- .022----- .035 ------ 2.252 --- .024 --- .003 --- .041 ------</p>\\n  \\n  <p>3--- .256----- .152 ------ 9.898 --- .000 --- .210 --- .302 ------</p>\\n</blockquote>\\n\\n<p>Row 1 = x1, row 2 = x2, row 3 = x3. From the p values x2 is the weakest predictor, and also has the lowest std beta weight. But is it significantly weaker than the other predictors? Which of the other two is stronger?</p>\\n\\n<p>EDIT: </p>\\n\\n<blockquote>\\n  <p>So that we can understand this question, please tell us what it means for predictors to be \"strong\" or \"weak.\"</p>\\n</blockquote>\\n\\n<p>I am predicting impairment/disability by different symptoms of a disorder, and want to find out whether one symptom is associated with more impairment than the other symptoms, that is, whether it explains more variance of impairment than the others, although all symptoms are significant predictors. It\\'s about the degree of prediction, not about significance. </p>\\n',\n",
       " '<p>From a purely practical point of view, I am not a fan of methods which require lots of computation (I am thinking of Gibbs sampler and MCMC, often used in the Bayesian framework, but this also applies to <em>e.g.</em> bootstrap techniques in frequentist analysis). The reason being that any kind of debugging (testing the implementation, looking at robustness with respect to assumptions, <em>etc</em>) itself requires a bunch of Monte Carlo simulations, and you are quickly in a computational morass. I prefer the underlying analysis techniques to be fast and deterministic, even if they are only approximate. </p>\\n\\n<p>This is a purely practical objection, of course: given infinite computing resources, this objection would disappear. And it only applies to a subset of Bayesian methods. Also this is more of a preference given my workflow.</p>\\n',\n",
       " '<p>I am attempting to build a Walsh-Fourier spectral density and it appears that it is first required to compute the logical covariance which in turn involves a dyadic add.</p>\\n\\n<p>I am not at all familiar with dyadics or their operations and introductory references are hard to come by. \\nIn <a href=\"http://www.jstor.org/stable/2290119\" rel=\"nofollow\">Stoffer (1988)</a> the logical covariance of a categorical series $X(0), X(1),\\\\\\\\dots,X(N-1)$ is described as being:\\n\\\\\\\\begin{align}\\n\\\\\\\\tau(j)=N^{-1} \\\\\\\\sum_{j=0}^{N-1} \\\\\\\\gamma(j\\\\\\\\oplus k-k)\\n\\\\\\\\end{align}\\nwhere $j\\\\\\\\oplus k$ is the dyadic addition. $\\\\\\\\gamma$, is our usual autocovariance, $\\\\\\\\gamma(h)=cov\\\\\\\\{X(n), X(n+h)\\\\\\\\}$.</p>\\n\\n<p>The Walsh-Fourier spectral density is then:</p>\\n\\n<p>\\\\\\\\begin{align}\\nf(\\\\\\\\lambda)=\\\\\\\\sum_{j=0}^{\\\\\\\\infty}\\\\\\\\tau(j)W(j, \\\\\\\\lambda)\\n\\\\\\\\end{align}</p>\\n\\n<p>where $W(j, \\\\\\\\lambda)$ is the $j$th sequency (zero-crossings) with $0\\\\\\\\leq\\\\\\\\lambda &lt; 1$.</p>\\n\\n<p>I\\'m sure a HMM would be great for categorical times series but at the moment I am restricted to spectral analysis so I must continue with this approach. It\\'s not exactly homework. It\\'s a final project that has moved a little beyond the coursework. The professor is a little hard to get a hold of and thus the question is posed here.</p>\\n\\n<p>Is it just addition? I haven\\'t even been able to confirm that.</p>\\n',\n",
       " '<p>I admit I\\'m relatively new to propensity scores and causal analysis.</p>\\n\\n<p>One thing that\\'s not obvious to me as a newcomer is how the \"balancing\" using propensity scores is mathematically different from what happens when we add covariates in a regression? What\\'s different about the operation, and why is it (or is it) better than adding subpopulation covariates in a regression?</p>\\n\\n<p>I\\'ve seen some studies that do an empirical comparison of the methods, but I haven\\'t seen a good discussion relating the mathematical properties of the two methods and why PSM lends itself to causal interpretations while including regression covariates does not. There also seems to be a lot of confusion and controversy in this field, which makes things even more difficult to pick up.</p>\\n\\n<p>Any thoughts on this or any pointers to good resources/papers to better understand the distinction? (I\\'m slowly making my way through Judea Pearl\\'s causality book, so no need to point me to that)</p>\\n',\n",
       " \"<p>I'm doing some reading on topic modeling (with Latent Dirichlet Allocation) which makes use of Gibbs sampling. As a newbie in statistics -- well, I know things like binomials, multinomials, priors etc -- I find it difficult to grasp how Gibbs sampling works. Can someone please explain it in simple English and/or using simple examples? (If you are not familiar with topic modeling, any examples will do.) Thank you all in advance.</p>\\n\",\n",
       " \"<p>I recently learned about using bootstrapping techniques to calculate standard errors and confidence intervals for estimators. What I learned was that if the data is IID, you can treat the sample data as the population, and do sampling with replacement and this will allow you to get multiple simulations of a test statistic.</p>\\n\\n<p>In this case of time series, you clearly can't do so because autocorrelation is likely to exist. I have a time series and would like to calculate the mean of the data before and after a fixed date. Is there a correct way to do so using a modified version of bootstrapping? Any other suggestions is also welcomed. </p>\\n\",\n",
       " \"<p>Not an actuarial, but in case none wander by and answer your question...</p>\\n\\n<p>What software does your school use? What software does your school offer classes in? Does your school help you to get STATA at a student price? When you read actuarial journals and textbooks, what tools do they use?</p>\\n\\n<p>As jem77bfp said, R is free and you can continue to use it and explore it all you want. Professionals in most fields use multiple tools, and you can't tell what future employers might require (SAS, SPSS, etc), so it might be a good idea to branch out a bit now.</p>\\n\\n<p>It wouldn't be unusual for a professional economist/statistician to use R for general-purpose analysis and graphing, JAGS/BUGS/STAN for MCMC analysis, ArcGIS for spatial data, Gephi or some other program for analyzing graphs, etc.</p>\\n\\n<p>But the bottom line is that R is free and has a LOT of packages to do particular types of analysis. A couple of years ago I was in a graduate machine learning class where the teacher did everything in Matlab and recommended Matlab toolkits. (The school also sold student discount Matlab in the school store.) But I used R and did very well, while other students used Stata, Java, etc. I wasn't going to get any help from the teacher or TA's -- or even other students -- if I had a problem doing something in R, but that wasn't a concern for me.</p>\\n\",\n",
       " '<p>One option is to leave the rating as a 2.5 and run the analysis for an ordinal scale. The key characteristic of an ordinal scale is that the values are assumed to have a certain order without necessarily representing an additive increment (which they would in an interval scale) or a ratio increment (which they would in a ratio scale).  Your treatment of the scale values as ordinal data merely assumes that 2 &lt; 2.5 &lt; 3 without assuming that the distance of 0.5 between points represents an interval or ratio increment.</p>\\n',\n",
       " '<p>I would like to obtain 95% confidence intervals for centroids based on Gower similarity between some mulivariate samples (community data from sediment cores). I have so far used the <code>vegan{}</code> package in R to obtain modified Gower similarity between cores (based on Anderson 2006; now included in R as part of <code>vegdist()</code>). Does anyone know how I can calculate 95% confidence intervals for the centroids of, for example, sampling sites, based on modified Gower similarity?</p>\\n\\n<p>Additionally, if possible, I would like to plot these 95% CIs on a PCO that shows the centroids, so it\\'s evident if they\\'re overlapping. </p>\\n\\n<p>To get modified Gower similarity, I used:</p>\\n\\n<pre><code>dat.mgower &lt;- vegdist(decostand(dat, \"log\"), \"altGower\")\\n</code></pre>\\n\\n<p>But as far as I know, you don\\'t get centroids from <code>vegdist()</code>. I need to get centroids, then 95% CIs, then plot them... in R. Help!</p>\\n\\n<p>Anderson, M. J., K. E. Ellingsen, and B. H. McArdle. 2006. Multivariate dispersion as a measure of beta diversity. Ecology Letters 9:683–693.</p>\\n',\n",
       " '<p>One approach to choosing the cutoff value $\\\\\\\\epsilon$ for ABC rejection sampling is the following (similar to Aniko\\'s answer).  Simulate several test data sets from known parameter values which are vaguely similar to your observed data (e.g. by performing ABC with a relatively large $\\\\\\\\epsilon$).  From the ABC output for a test data set, some criterion of performance compared to the true parameters can be calculated, such as mean squared error.  Calculate this for all test data sets at many $\\\\\\\\epsilon$ values, and choose $\\\\\\\\epsilon$ to optimise the mean criterion (as this is a Monte Carlo estimate of its expectation).  This requires many repetitions of the ABC algorithm, but can be done efficiently by using the same $N$ data simulations in every ABC algorithm (although this introduces some dependency between simulations).</p>\\n\\n<p>In general, there is not a lot of published work on the choice of $\\\\\\\\epsilon$.  I think the approach above has been used somewhere and I will edit if I remember the references.  An alternative is in \"Choosing the Summary Statistics and the Acceptance Rate in Approximate Bayesian Computation\" by Michael Blum.  Other methods that I\\'m aware of apply only to SMC or MCMC methods. </p>\\n',\n",
       " \"<p>My guess would be that, in econometrics at least, cross-sectional studies are often trying to get at causal relationships; the causal effect of an additional year of education on earnings, for example. </p>\\n\\n<p>In time series, we are usually only after a prediction for future values of our outcome. Given some set of factors, what do we expect the GDP growth rate to be next quarter? We don't care if the measure of consumer confidence that we include directly <em>causes</em> growth, but are instead content to use its predictive power (as opposed to true explanatory power) to help with our forecast. </p>\\n\\n<p>So perhaps positive news stories cause consumer confidence, which leads to economic growth. Leaving out a measure of the positivity of news stories would lead to omitted variables bias in that the coefficient on confidence isn't really a measure of the effect of confidence itself. But we are still able to get useful forecasts despite the omitted variable.</p>\\n\",\n",
       " '<h3>1. How do I input them in SPSS?</h3>\\n\\n<p>You can open an Excel file in SPSS.\\nUse the standard file open option, and select <code>file type = *xls</code>.\\nTry to ensure that the first row has the variable names.</p>\\n\\n<h3>2. How do I work out the frequency of replies for each recipient?</h3>\\n\\n<ul>\\n<li>Do you mean the frequency of responses for each question?</li>\\n<li>Check out the menu <code>Descriptive Statistics - Frequencies</code></li>\\n</ul>\\n\\n<h3>3. How do I work out frequency of replies i.e agrees/disagrees etc for each group?</h3>\\n\\n<ul>\\n<li>Check out <code>Descriptive Statistics - Crosstabs</code></li>\\n</ul>\\n\\n<h3>4. How can I rank each individual question (12 of them)? Remember, there are 3 individual statements to each question.</h3>\\n\\n<ul>\\n<li>Rank them in terms of what?</li>\\n<li>If you intend to rank each question in terms of their mean (e.g., on a one to five scale). One way would be to run <code>Descriptive Statistics - Descriptives</code> and get the mean for each item. Then copy and paste the table of item means into Excel and sort by the <code>Mean</code> column.</li>\\n</ul>\\n\\n<h3>5. How do I compare UK architects to US architects to show congruence or not?</h3>\\n\\n<ul>\\n<li>Check out <code>Descriptive Statistics - Explore</code>; you could also look at some of the compare mean options.</li>\\n</ul>\\n\\n<h3>6. How would show correlation between the two groups UK and US?</h3>\\n\\n<ul>\\n<li>These are different participants so I don\\'t know what you mean by asking for correlations.</li>\\n</ul>\\n\\n<h3>7. Will SPSS develop graphs etc for me showing frequency or correlation?</h3>\\n\\n<ul>\\n<li>Yes, it will.</li>\\n<li>Just have a play around with the Graphs menu (e.g., <code>Legacy - Scatter</code> or <code>Legacy - Bar</code>)</li>\\n</ul>\\n\\n<h3>General Suggestions</h3>\\n\\n<p>It sounds like you need a basic book explaining how to use SPSS.\\nA good one is the SPSS Survival Manual.\\nI also wrote a <a href=\"http://web.psych.unimelb.edu.au/jkanglim/Anglim2006-SPSSIntroductionWorkshop.pdf\">120 page PDF Introduction to SPSS</a> several years back which explains all the things mentioned above with examples.</p>\\n',\n",
       " '<p>Although ANOVA stands for ANalysis Of VAriance, it is about comparing means of data from different groups. It is part of the general linear model which also includes linear regression and ANCOVA. In matrix algebra form, all three are:</p>\\n\\n<p>$Y = XB + e$</p>\\n\\n<p>Where $Y$ is a vector of values for the dependent variable (these must be numeric), $X$ is a matrix of values for the independent variables and $e$ is error.</p>\\n\\n<p>The chief difference among ANOVA, ANCOVA and linear regression is that they arose in different fields. Also, ANOVA is usually restricted to cases where the independent variables are categorical, ANCOVA where some are numeric but most categorical. Regression (through dummy variables) can handle any type of independent variable. </p>\\n',\n",
       " '<p>I am attempting to show the correlation between two features of a dataset is weak. I am under the impression I should be using a t test for this to measure statistical power but would like to know how many samples I would need to show that the correlation is less than x%. Am I to understand that I would need many samples in order to show a weak correlation between two features or is there a different method which I could use in order to show that a strong correlation does not exist? If there is another method, how might I determine how many samples would be needed to be statistically certain that say, only a 30% or less correlation exists?</p>\\n\\n<p>I am most familiar with R and MATLAB.</p>\\n',\n",
       " '<p>The coefficient that tells you the effect of a one unit increase in time/age on the outcome is the coefficient on the age variable, where age is entered as a level one predictor. But you don\\'t get just one such coefficient. You get one per subject, plus a coefficient representing the mean of all those coefficients. </p>\\n\\n<p>Let\\'s say you fit a linear growth random-coefficients model, no predictors at level two. The level one model includes age as a predictor:</p>\\n\\n<p>$$\\\\\\\\nY_{ti} = \\\\\\\\pi_{0i} + \\\\\\\\pi_{1i} \\\\\\\\cdot Age_{ti} +e_{ti}\\\\\\\\n$$\\nwhere\\n$$\\\\\\\\ne_{ti} \\\\\\\\sim N(0,\\\\\\\\sigma^2)\\\\\\\\n$$</p>\\n\\n<p>Individuals are indexed by $i$, time by $t$, so $Y_{ti}$ gives the outcome at time $t$ when individual $i$ has age $Age_{ti}$. Then level two models the $\\\\\\\\pi_{pi}$ (level one intercepts and slopes) as random:</p>\\n\\n<p>$$\\\\\\\\n\\\\\\\\pi_{0i} = \\\\\\\\beta_{00} + r_{0i}\\\\\\\\n$$\\n$$\\\\\\\\n\\\\\\\\pi_{1i} = \\\\\\\\beta_{10} + r_{1i}\\\\\\\\n$$\\nAgain with errors distributed normally. $\\\\\\\\pi_{1i}$ is the coefficient of interest. It tells you the expected change during one unit of time. One such coefficient is estimated per subject; $\\\\\\\\beta_{10}$ gives the mean of those slopes. </p>\\n\\n<p>If you use R\\'s nlme package to fit a linear growth model like this, you could use <a href=\"http://stat.ethz.ch/R-manual/R-devel/library/nlme/html/ranef.lme.html\" rel=\"nofollow\">ranef()</a> to get the values of $\\\\\\\\pi_{1i}$:</p>\\n\\n<pre><code>&gt; fm1 &lt;- lme(distance ~ age, Orthodont, random = ~ age | Subject)\\n&gt; ranef(fm1)\\n\\n    (Intercept)          age\\nM16  -0.1877570 -0.068853740\\nM05  -1.1766673  0.025600299\\nM02  -0.7275013  0.014507808\\nM11   0.8904899 -0.118825903\\n...\\n</code></pre>\\n\\n<p>You get an estimated slope on age for each individual subject. You can also get the fixed effects (values of $\\\\\\\\beta_{00}$ and $\\\\\\\\beta_{10}$) from the fit object:</p>\\n\\n<pre><code>&gt; fixef(fm1)\\n(Intercept)         age \\n 16.7611111   0.6601852 \\n</code></pre>\\n\\n<p>The fixed effect here under age is the mean of the $\\\\\\\\pi_{1i}$ coefficients.</p>\\n\\n<p><strong>Reference</strong></p>\\n\\n<p>Raudenbush, S. W., &amp; Bryk, A. S. (2002). Hierarchical linear models: Applications and data analysis methods. Thousand Oaks: Sage Publications.</p>\\n',\n",
       " \"<p>I want to predict the outcome of a particular treatment (remitted or not) using demographic, plasma biomarker, genetic, and clinical data. IS a neural network model the best way of doing this? What advantages does this have over traditional logistic regression model building? How limited am I with only 120 cases and up to 40 covariates, depending on collinearity? How do I pare these down? I would normally tend towards factor analysis but will a neural net combine collinear variables sensibly? Any ideas on combining multimodal data like that would be helpful, or a starting point for reading - already have Ripley's MASS.</p>\\n\",\n",
       " '<p>I have read countless pages on google and cannot find a satisfactory answer. I have also read <a href=\"http://castatistics.wikispaces.com/file/view/normal+der..pdf\">http://castatistics.wikispaces.com/file/view/normal+der..pdf</a>, but I doubt that was the original motivation for the Gaussian function. I am currently an undergraduate and my textbook just tells me the function f(x) = a e -(x - b)^2/c is used as the probability density function for a normal curve. But my textbook gives me no clues as to where this function actually came from. What was the original motivation for the development of such a function? Can someone please offer a proof that I can actually undertsand with clearly labeled steps? I have a undertsanding of basic calculus and I am a beginner when it comes to statistics. Please no complicated proofs.</p>\\n',\n",
       " '<p>We have a database with different countries and each country is divided in a number of industries. </p>\\n\\n<p>We want to generate a new measure for industry profitability which will be the median per industry per country. </p>\\n\\n<p>What would the command be in Stata? We already have the measure for profitability.</p>\\n',\n",
       " '<p>Suppose an estimated simple linear regression equation is given as $$E[y| \\\\\\\\textbf{x}] = \\\\\\\\hat{\\\\\\\\beta}_{0}+ \\\\\\\\hat{\\\\\\\\beta}_{1}\\\\\\\\textbf{x}$$</p>\\n\\n<p>Then the interpretation of the slope is as follows: For a unit increase in $\\\\\\\\textbf{x}$, $E[y| \\\\\\\\textbf{x}]$ increases by $\\\\\\\\hat{\\\\\\\\beta}_{1}$. Can this same equation be used for predicting $y$ from some given $\\\\\\\\textbf{x}$?</p>\\n',\n",
       " '<p>The reason that there is \"no follow up\" is that very few people understand the work of Rodriguez on this going back many years.  It\\'s important stuff and we will see more of it in the future I am sure.</p>\\n\\n<p>However, some would argue that the Fisher metric is only a 2nd order approximation to the true metric (e.g. <a href=\"http://www.mendeley.com/research/bayesian-inference-featuring-entropic-priors/\" rel=\"nofollow\">Neumann\\'s paper on establishing entropic priors</a> ) which is actually defined by the Kullback-Liebler distance (or generalisations thereof) and which leads to Zellner\\'s formulation of MDI priors.</p>\\n',\n",
       " '<p>The t-test with two groups assumes that each group is normally distributed with the same variance (although the means may differ under the alternative hypothesis). That is equivalent to a regression with a dummy variable as the regression allows the mean of each group to differ but not the variance. Hence the residuals (equal to the data with the group means subtracted) have the same distribution --- that is, they are normally distributed with zero mean.</p>\\n\\n<p>A t-test with unequal variances is not equivalent to a one-way ANOVA.</p>\\n',\n",
       " '<p>I have a set of observed values (shown with black dots in the figure) that I would like to compare to some simulated data (100 simulated datasets shown as box plots with quartiles, extremes (excluding outliers) as whiskers, and outliers as white dots).</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/UARxG.png\" alt=\"Box plots\"></p>\\n\\n<p>The observed values are outside of the 95% confidence intervals of the simulated dataset so it seems pretty obvious that there is a significant difference between the observed and the simulated; but in some cases, the difference might not be so obvious. Which statistical test (preferably in R) can I do on this data to get a P-value for the significant difference between the observed and the simulated sets?</p>\\n',\n",
       " '<p>I was reading about HMM in C.M. Bishop\\'s book <a href=\"http://research.microsoft.com/en-us/um/people/cmbishop/prml/\" rel=\"nofollow\">Pattern Recognition and Machine Learning</a>. I was going through the forward and backward algorithm using $\\\\\\\\alpha$ &amp; $\\\\\\\\beta$</p>\\n\\n<p>For forward messaging passing</p>\\n\\n<p>$\\\\\\\\alpha(z_n) = p(x_n|z_n)*\\\\\\\\sum_{z_n-1}\\\\\\\\alpha(z_{n-1})*P(z_n|z_{n-1})$</p>\\n\\n<p>Since the probabilities are &lt;=1. As we move from $z_1$ to $z_N$ the $\\\\\\\\alpha(z_n)$ tends to get smaller and smaller and we might have floating point issues. For that the book mentioned something called scaling factors where u normalize the $\\\\\\\\alpha$</p>\\n\\n<p>So we have </p>\\n\\n<p>$\\\\\\\\hat{\\\\\\\\alpha(z_n)} = p(z_n|x_1...x_n) = \\\\\\\\alpha(z_n)/p(x_1...x_n)$</p>\\n\\n<p>which we expect to be well behaved numerically because it is a probability distribution over K variables for any value of n. I didn\\'t get how it is well behaved and how it solves the floating point issue and what does it mean by probability distribution over K variables. I just didn\\'t get this part</p>\\n',\n",
       " '<p>I met Laura Trinchera who contributed a nice R package for PLS-path modeling, <a href=\"http://cran.r-project.org/web/packages/plspm/\" rel=\"nofollow\">plspm</a>. It includes several graphical output for various kind of 2- and k-block data structures.</p>\\n\\n<p>I just discovered the <a href=\"http://www.bethanykok.com/plotSEMM.html\" rel=\"nofollow\">plotSEMM</a> R package. It\\'s more related to your second point, though, and is restricted to graphing bivariate relationships. </p>\\n\\n<p>As for recent references on diagnostic plot for SEMs, here are two papers that may be interesting (for the second one, I just browsed the abstract recently but cannot find an ungated version):</p>\\n\\n<ol>\\n<li>Sanchez BN, Houseman EA, and Ryan LM. <a href=\"http://deepblue.lib.umich.edu/bitstream/2027.42/65983/1/j.1541-0420.2008.01022.x.pdf\" rel=\"nofollow\">Residual-Based Diagnostics for Structural Equation Models</a>. <em>Biometrics</em> (2009) 65, 104–115</li>\\n<li>Yuan KH and Hayashi K. <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/20853955\" rel=\"nofollow\">Fitting data to model: Structural equation modeling diagnosis using two scatter plots</a>, <em>Psychological Methods</em> (2010)</li>\\n<li>Porzio GC and Vitale MP. <a href=\"http://isi2011.congressplanner.eu/pdfs/p_950622.pdf\" rel=\"nofollow\">Discovering interaction in Structural Equation Models through a diagnostic plot</a>. <em>ISI 58th World Congress</em> (2011).</li>\\n</ol>\\n',\n",
       " '<p>I am deriving a Gibbs sampler with a model similar to the model in <a href=\"http://www.cs.berkeley.edu/~jpaisley/Papers/Paisley_BP-FA_ICML.pdf\" rel=\"nofollow\">this paper</a> (a graphical model is shown in page 4). To put it simple, my question only concerns $w_i$ (a $K$-dimensional vector drawn from a normal distribution) and its precision $\\\\\\\\gamma_w$, thus I will just make these 2 variables unknown and treat the rest as known (hopefully without losing the mathematical rigors):</p>\\n\\n<p>$w_i \\\\\\\\sim \\\\\\\\mathcal{N}(0, \\\\\\\\sigma_w^2I_K)$</p>\\n\\n<p>$\\\\\\\\gamma_w = \\\\\\\\frac{1}{\\\\\\\\sigma_w^2} \\\\\\\\sim \\\\\\\\Gamma(\\\\\\\\alpha_0, \\\\\\\\beta_0)$, here $\\\\\\\\alpha_0, \\\\\\\\beta_0$ are hyperparameters.</p>\\n\\n<p>And the conditional probability is:</p>\\n\\n<p>$p(w_{ik}|-) \\\\\\\\sim \\\\\\\\mathcal{N}(\\\\\\\\mu_{w_{ik}}, \\\\\\\\sigma_{w_{ik}}^2)$, where $\\\\\\\\sigma_{w_{ik}}^2 = (\\\\\\\\gamma_w + T)^{-1}$. $T$ is actually a pretty complicated term but it\\'s nonnegative. </p>\\n\\n<p>$p(\\\\\\\\gamma_w|-) = \\\\\\\\Gamma(\\\\\\\\alpha_0 + \\\\\\\\frac{1}{2}KN, \\\\\\\\beta_0 + \\\\\\\\frac{1}{2}\\\\\\\\sum_{i=1}^N w_i^Tw_i)$</p>\\n\\n<p>I code it up and generate some simulated data according to the graphical model. I set all the variables other than $w_i$ and $\\\\\\\\gamma_w$ to their true value and keep unchanged. The hyperparameters are set to 0.001 which basically forces the prior of $\\\\\\\\gamma_w$ to be 0 (I am actually not certain if this is a good choice). The sampler runs with $\\\\\\\\gamma_w$ goes to infinity. The log-likelihood of samples goes to much larger than the log-likelihood of the true data. </p>\\n\\n<p>From the conditional distribution, this non-convergence is actually kind of \"observable\", as each $w_{ik}$ is drawn with a larger precision than the sample from $p(\\\\\\\\gamma_w|-)$ (because of the nonnegative term). Then when sampling $\\\\\\\\gamma_w$, we are using a smaller $\\\\\\\\sum_{i=1}^N w_i^Tw_i$. Thus the samples of $\\\\\\\\gamma_w$ get larger. </p>\\n\\n<p>This seems wrong, but I cannot figure out where the problem is. Any comments will be highly appreciated. Thanks!</p>\\n\\n<p>Updated: I think this may be generally true for a hierarchical normal model with unknown variance, then I found <a href=\"http://probability.ca/jeff/ftpdir/james.pdf\" rel=\"nofollow\">this paper</a> which seems to partially agree with my observation. But this paper is a little beyond my crappy math background... </p>\\n',\n",
       " \"<p>This is generally hard to tell without knowing what the problem exactly is, but I would advise you to try some machine learning methods. For start you may try random forest, which is almost trivial to apply and quite probably will achieve better accuracy than just using one, best-correlated variable.<br>\\nAlso, it will produce importance measure which will tell you which variables contribute most to the prediction accuracy -- possibly taking into account even quite complex multivariate interactions obviously invisible to Pearson's correlation.</p>\\n\",\n",
       " \"<p>To echo everyone else: MORE DETAILS ABOUT YOUR DATA. Please give a qualitative description of what your independent and dependent variable(s) is/are.</p>\\n\\n<p><b>EDIT: Yes this is confusing; hopefully it's cleared up now.</b></p>\\n\\n<p>In general, you probably want to avoid using sample statistics to estimate population parameters if you have the population data. This is because sample statistics are <b>estimates</b> of population parameters, thus the methods used to compute sample statistics always have less power than those same methods in their population parameter version(s). Of course, most of the time you have to use sample statistics because you don't have complete population data.</p>\\n\\n<p>In your case either way you slice it inferring anything about a population from a case study is dubious because case studies are, by definition, case by case. You could make an inference about the case on which you collected data, but how useful is that? Maybe in your case it is.</p>\\n\\n<p>Either way, forget about whether or not you can/should use a sample method when you have the population data. You don't have population data if it's a case study. Also, sample vs. population has to do with making inferences. You do not need to worry about sample vs. population methods if all you want is a correlation coefficient, because it is a purely descriptive statistic.</p>\\n\\n<p>Your fourth bullet point is completely unintelligible. Please clear that up if you would like people to help you with it.</p>\\n\\n<p>@mpiktas A Spearman rank correlation is NOT the proper correlation coefficient to use here. To use that test all data must be ranked and discrete (unless >= 2 values compete for a rank), i.e., they must be ordinal data. Maybe the HVOC table could be analyzed via Spearman's $\\\\\\\\rho$, however more information must be provided by the poster to make that conclusion.</p>\\n\\n<p>@whuber Yes all data are discrete when represented on a computer, however in this case it seems like what BB01 was referring to was the scale of measurement, not the electronic representation of numbers.</p>\\n\",\n",
       " '<p>I would suggest that transformations aren\\'t important to get a normal distribution for your errors. Normality isn\\'t a necessary assumption. If you have \"enough\" data, the central limit theorem kicks in and your standard estimates become asymptotically normal. Alternatively, you can use bootstrapping as a non-parametric means to estimate the standard errors. (Homoskedasticity, a common variance for the observations across units, is required for your standard errors to be right; robust options permit heteroskedasticity).</p>\\n\\n<p>Instead, transformations help to ensure that a linear model is appropriate. To give a sense of this, let\\'s consider how we can interpret the coefficients in transformed models:</p>\\n\\n<ul>\\n<li>outcome is units, predictors is units: A one unit change in the predictor leads to a beta unit change in the outcome.</li>\\n<li>outcome in units, predictor in log units: A one percent change in the predictor leads to a beta/100 unit change in the outcome.</li>\\n<li>outcome in log units, predictor in units: A one unit change in the predictor leads to a beta x 100% change in the outcome.</li>\\n<li>outcome in log units, predictor in log units: A one percent change in the predictor leads to a beta percent change in the outcome.</li>\\n</ul>\\n\\n<p>If transformations are necessary to have your model make sense (i.e., for linearity to hold), then the estimate from this model should be used for inference. An estimate from a model that you don\\'t believe isn\\'t very helpful. The interpretations above can be quite useful in understanding the estimates from a transformed model and can often be more relevant to the question at hand. For example, economists like the log-log formulation because the interpretation of beta is an elasticity, an important measure in economics.</p>\\n\\n<p>I\\'d add that the back transformation doesn\\'t work because the expectation of a function is not the function of the expectation; the log of the expected value of beta is not the expected value of the log of beta. Hence, your estimator is not unbiased. This throws off standard errors, too.</p>\\n',\n",
       " \"<p>There's no optimal algorithm, but universal algorithmic induction comes close to optimality, in the sense that the difference in compressed file size between it and any other compression algorithm is bounded by a value that depends on the algorithm but not the data.</p>\\n\\n<p>It's not computable, so that's a disadvantage.</p>\\n\\n<p>The method is as follows: choose a programming language (really, a universal turing machine), and look at every possible program in order of length. Skip the programs that don't halt (this is why it's not computable). Choose the first program whose output is equal to your uncompressed file. Use it as a self-expanding archive.</p>\\n\",\n",
       " '<p>I am wondering what is the best way to extract knowledge about periodic time series data?</p>\\n\\n<p>In my case, I am trying to analyse <a href=\"https://www2.ameren.com/RetailEnergy/rtpDownload.aspx\" rel=\"nofollow\">historical hourly electricity prices</a> to get information about its volatility, when the peaks occur, what is its distribution among the hours of the day or something similar to be able to make good decisions on when to throttle devices to save money and stay energy efficient.</p>\\n\\n<p>My original idea was to simply aggregate the time series by calculating mean values for each hour over a period of several months and then sort them to pick the top M% hours and say that they are the most expensive on the average and should be avoided.</p>\\n\\n<p>This is a very crude approach, so I\\'m wondering whether I\\'m making some obvious mistake, disregarding some information? Is there some better approach? It seems to me that the information on how high the current prices are in comparison to the average should also be used to sometimes exclude, say, 3 hours from a day and on other times only 1 hour.</p>\\n',\n",
       " '<p>You can use the <code>Amelia</code> package to impute the data (full disclosure: I am one of the authors of <code>Amelia</code>). The <a href=\"http://cran.r-project.org/web/packages/Amelia/vignettes/amelia.pdf\" rel=\"nofollow\">package vignette</a> has an extended example of how to use it to impute missing data. </p>\\n\\n<p>It seems as though you have units which are district-gender-ageGroup observed at the monthly level. First you create a factor variable for each type of unit (that is, one level for each district-gender-ageGroup). Let\\'s call this <code>group</code>. Then, you would need a variable for time, which is probably the number of months since January 2003. Thus, this variable would be 13 in January of 2004. Call this variable <code>time</code>. Amelia will allow you to impute based on the time trends with the following commands:</p>\\n\\n<pre><code>library(Amelia)\\na.out &lt;- amelia(my.data, ts = \"time\", cs = \"group\", splinetime = 2, intercs = TRUE)\\n</code></pre>\\n\\n<p>The <code>ts</code> and <code>cs</code> arguments simply denote the time and unit variables. The <code>splinetime</code> argument sets how flexible should time be used to impute the missing data. Here, a 2 means that the imputation will use a quadratic function of time, but higher values will be more flexible. The <code>intercs</code> argument here tells Amelia to use a separate time trend for each district-gender-ageGroup. This adds many parameters to the model, so if you run into trouble, you can set this to <code>FALSE</code> to try to debug. </p>\\n\\n<p>In any event, this will get you imputations using the time information in your data. Since the missing data is bounded at zero, you can use the <code>bounds</code> argument to force imputations into those logical bounds. </p>\\n\\n<p><strong>EDIT: How to create group/time variables</strong></p>\\n\\n<p>The time variable might be the easiest to create, because you just need to count from 2002 (assuming that is the lowest year in your data):</p>\\n\\n<pre><code>my.data$time &lt;- my.data$Month + 12 * (my.data$Year - 2002)\\n</code></pre>\\n\\n<p>The group variable is slightly harder but a quick way to do it is using the paste command:</p>\\n\\n<pre><code>my.data$group &lt;- with(my.data, \\n                      as.factor(paste(District, Gender, AgeGroup, sep = \".\")))\\n</code></pre>\\n\\n<p>With these variables created, you want to remove the original variables from the imputation. To do that you can use the <code>idvars</code> argument:</p>\\n\\n<pre><code>a.out &lt;- amelia(my.data, ts = \"time\", cs = \"group\", splinetime = 2, intercs = TRUE,\\n                idvars = c(\"District\", \"Gender\", \"Month\", \"Year\", \"AgeGroup\"))\\n</code></pre>\\n',\n",
       " \"<p>If the best linear approximation (using least squares) of my data points is the line $y=mx+b$, how can I calculate the approximation error? If I compute standard deviation of differences between observations and predictions $e_i=real(x_i)-(mx_i+b)$, can I later say that a real (but not observed) value $y_r=real(x_0)$ belongs to the interval $[y_p-\\\\\\\\sigma, y_p+\\\\\\\\sigma]$ ($y_p=mx_0+b$) with probability ~68%, assuming normal distribution?</p>\\n\\n<p>To clarify:</p>\\n\\n<p>I made observations regarding a function $f(x)$ by evaluating it a some points $x_i$. I fit these observations to a line $l(x)=mx+b$. For $x_0$ that I did not observe, I'd like to know how big can $f(x_0)-l(x_0)$  be. Using the method above, is it correct to say that $f(x_0) \\\\\\\\in [l(x_0)-\\\\\\\\sigma, l(x_0)+\\\\\\\\sigma]$ with prob. ~68%?</p>\\n\",\n",
       " '<p>Some packages are very useful in R.</p>\\n\\n<p>I will just recommand <strong>kernlab</strong> for Kernel-based Machine Learning Lab and <strong>e1071</strong> for SVM and <strong>ggplot2</strong> for graphics</p>\\n',\n",
       " \"<p>The reason why you are conducting this test is to determine which policy is more valuable, and if value is measured in profitability, then it makes no sense to do statistical testing on any other variable.  A properly conducted test on profitability gives you all the information needed for your companies' decision: once you have the results of this test, results about other variables (e.g. response rate) provide no decision-relevant information.</p>\\n\",\n",
       " \"<p>I have been trying to understand the representation of the joint probability density of Markov random fields in the form of factors of the potential functions. I am finding it difficult to grasp the idea of potential functions and how we are supposed to choose a potential function. Any suggestions on how I should get started. I have gone through the wiki page and some other resources, but still I couldn't grasp the essence of it. </p>\\n\",\n",
       " '<p>Look up the delta method.  Essentially you have a function $g(\\\\\\\\boldsymbol{\\\\\\\\beta}) = w_1\\\\\\\\beta_1 + w_2\\\\\\\\beta_2$.  The variance of $g$ is asymptotically:\\n\\\\\\\\begin{equation}\\nVar(g(\\\\\\\\boldsymbol{\\\\\\\\beta})) \\\\\\\\approx [\\\\\\\\nabla g(\\\\\\\\boldsymbol{\\\\\\\\beta})]^T Var(\\\\\\\\boldsymbol{\\\\\\\\beta})[\\\\\\\\nabla g(\\\\\\\\boldsymbol{\\\\\\\\beta})]\\n\\\\\\\\end{equation}\\nWhere $Var(\\\\\\\\boldsymbol{\\\\\\\\beta})$ is your covariance matrix for $\\\\\\\\boldsymbol{\\\\\\\\beta}$ (given by the inverse of the Fisher information, see: <a href=\"http://en.wikipedia.org/wiki/Fisher_information\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Fisher_information</a> if you\\'re unsure of this).</p>\\n',\n",
       " '<p>In a multiple linear regression with  highly correlated regressors, what is the best strategy to use? Is it a legitimate approach to add the product of all the correlated regressors?</p>\\n',\n",
       " '<p>This is a cross-over design.  Typically to compare drugs in this case you would look at differences paired on subjects.  It is not a split plot design.</p>\\n',\n",
       " '<p>Is there a reasonable way to quantify the amount of local correlations in an image?  For example, I want to justify the correlations between a neighbourhood of pixels is much higher than the correlations between pixels in entirely different regions of the image.</p>\\n\\n<p>Would showing the xcorr2(A,A) as the 2-d autocorrelation of the image be a valid way to show this?  ie: if if there are large values mostly located at the center of the matrix.</p>\\n',\n",
       " '<p>I am trying to interpreting the output of nls(). I have read this <a href=\"http://stats.stackexchange.com/questions/5135/interpretation-of-rs-lm-output\">post</a> but I still don\\'t understand how to choose the best fit. From my fits I have two outputs:</p>\\n\\n<pre><code>&gt; summary(m)\\n\\n  Formula: y ~ I(a * x^b)\\n\\n  Parameters:\\n  Estimate Std. Error t value Pr(&gt;|t|)    \\n  a 479.92903   62.96371   7.622 0.000618 ***\\n  b   0.27553    0.04534   6.077 0.001744 ** \\n  ---\\n  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 \\n\\n  Residual standard error: 120.1 on 5 degrees of freedom\\n\\n  Number of iterations to convergence: 10 \\n  Achieved convergence tolerance: 6.315e-06 \\n</code></pre>\\n\\n<p>and</p>\\n\\n<pre><code>&gt; summary(m1)\\n\\n  Formula: y ~ I(a * log(x))\\n\\n  Parameters:\\n  Estimate Std. Error t value Pr(&gt;|t|)    \\n  a   384.49      50.29   7.645 0.000261 ***\\n  ---\\n  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 \\n\\n  Residual standard error: 297.4 on 6 degrees of freedom\\n\\n  Number of iterations to convergence: 1 \\n  Achieved convergence tolerance: 1.280e-11\\n</code></pre>\\n\\n<p>The first one has two parameters and smaller residual error. The second only one parameter but worst residual error. Which is the best fit?</p>\\n',\n",
       " '<p>The original suggestion for displaying an interaction via box-plot does not quite make sense in this instance, since both of your variables that define the interaction are continuous. You could dichotomize either <code>G</code> or <code>P</code>, but you do not have much data to work with. Because of this, I would suggest coplots (a description of what they are can be found in;</p>\\n\\n<ul>\\n<li>Cleveland, William. 1994. <a href=\"http://dx.doi.org/10.1214/lnms/1215463783\">Coplots, nonparametric regression, and conditionally parametric fits</a>. IMS Lecture Notes Monograph Series 24: 21-36. PDF available in link from Project Euclid.</li>\\n</ul>\\n\\n<p>Below is a coplot of the <code>election2012</code> data generated by the code <code>coplot(VP ~ P | G, data = election2012)</code>. So this is assessing the effect of <code>P</code> on <code>VP</code> conditional on varying values of <code>G</code>. </p>\\n\\n<p><img src=\"http://i.stack.imgur.com/Ye7LY.png\" alt=\"coplot interaction\"></p>\\n\\n<p>Although your description makes it sound like this is a fishing expedition, we may entertain the possibility that an interaction between these two variables exist. The coplot seems to show that for lower values of <code>G</code> the effect of <code>P</code> is positive, and for higher values of <code>G</code> the effect of <code>P</code> is negative. After assessing marginal histograms and bivariate scatterplots of <code>VP, P, G</code> and the interaction between <code>P</code> and <code>G</code>, it seemed to me that 1932 was likely a high leverage value for the interaction effect. </p>\\n\\n<p>Below are four scatterplots, showing the marginal relationships between <code>VP</code> and the mean centered <code>V</code>, <code>G</code> and the interaction of <code>V</code> and <code>G</code> (what I named <code>int_gpcent</code>). I have highlighted 1932 as a red dot. The last plot on the lower right is the residuals of the linear model <code>lm(VP ~ g_cent + p_cent, data = election2012)</code> against <code>int_gpcent</code>. </p>\\n\\n<p><img src=\"http://i.stack.imgur.com/owQMV.png\" alt=\"high leverage regression\"></p>\\n\\n<p>Below I provide code that shows when removing 1932 from the linear model <code>lm(VP ~ g_cent + p_cent + int_gpcent, data = election2012)</code> the interaction of <code>G</code> and <code>P</code> fail to reach statistical significance. Of course this is all just exploratory (one would also want to assess if any temporal correlation occurs in the series, but hopefully this is a good start. Save ggplot for when you have a better idea of what you exactly want to plot!</p>\\n\\n<pre><code>#data and directory stuff\\nmydir &lt;- \"C:\\\\\\\\\\\\\\\\Documents and Settings\\\\\\\\\\\\\\\\andrew.wheeler\\\\\\\\\\\\\\\\Desktop\\\\\\\\\\\\\\\\R_interaction\"\\nsetwd(mydir)\\nelection2012 &lt;- read.table(\"election2012.txt\", header=T, quote=\"\\\\\\\\\"\")\\n\\n#making interaction variable\\nelection2012$g_cent &lt;- election2012$G - mean(election2012$G)\\n    election2012$p_cent &lt;- election2012$P - mean(election2012$P)\\nelection2012$int_gpcent &lt;- election2012$g_cent * election2012$p_cent\\n\\nsummary(election2012)\\nView(election2012)\\npar(mfrow= c(2,2))\\nhist(election2012$VP)\\n    hist(election2012$G)\\nhist(election2012$P)\\n    hist(election2012$int_gpcent)\\n\\n#scatterplot &amp; correlation matrix\\ncor(election2012[c(\"VP\",\"g_cent\",\"p_cent\",\"int_gpcent\")])\\npairs(election2012[c(\"VP\",\"g_cent\",\"p_cent\",\"int_gpcent\")])\\n\\n#lets just check out a coplot for interactions\\n#coplot(VP ~ G | P, data = election2012)\\ncoplot(VP ~ P | G, data = election2012)\\n#example of coplot - http://stackoverflow.com/questions/5857726/how-to-delete-the-given-in-a-coplot-using-r\\n\\n#onto models\\n\\nmodel1 &lt;- lm(VP ~ g_cent + p_cent, data = election2012)\\nsummary(model1)\\nelection2012$resid_m1 &lt;- residuals(model1)\\n\\nelection2012$color &lt;- \"black\"\\n    election2012$color[14] &lt;- \"red\"\\n\\nattach(election2012)\\npar(mfrow = c(2,2))\\nplot(x = g_cent,y = VP, col = color, pch = 16)\\nplot(x = p_cent,y = VP, col = color, pch = 16)\\nplot(x = int_gpcent,y = VP, col = color, pch = 16)\\nplot(x = int_gpcent,y = resid_m1, col = color, pch = 16)\\n\\n#what does the same model look like with 1932 removed\\n\\nmodel1_int &lt;- lm(VP ~ g_cent + p_cent + int_gpcent, data = election2012)\\nsummary(model1_int)\\nmodel2_int &lt;- lm(VP ~ g_cent + p_cent + int_gpcent, data = election2012[-14,])\\nsummary(model2)\\n</code></pre>\\n',\n",
       " '<p>I have carried out simple linear regression and I am now checking the models meets the assumption of homogeneity of variance:</p>\\n\\n<p>• am I correct in concluding that the Levenes tests which gave a p&lt;0.05 indicates a violation of homogeneity of variance?</p>\\n\\n<p>• some models contain 2 or 3 explanatory variables, but in some models, only 1 explanatory variable gave p&lt;0.05 in the Levenes test. Therefore, should only those explanatory variables which gave p&lt;0.05 be corrected?  </p>\\n\\n<p>• is transformation the best place to start when correcting for violation of homogeneity?</p>\\n',\n",
       " \"<p>You need to understand that your sample size limits the complexity of the model that it's reasonable to fit. With three independent variables, three interactions between them, an intercept, &amp; a variance, you have eight parameters. If you try to fit a model like this to seven data points, you are not able to estimate the error variance. With a few more, your parameter estimates will still be very uncertain, &amp; wrong assumptions will not easily be caught.</p>\\n\\n<p>No-one can tell you how to build your model; it depends on your expertise in the subject matter. A rough idea of a useful approach would be to include 'Year' as a independent variable, most likely in a general linear model, perhaps as a linear trend term.  You would need to consider very carefully which interactions of the four (now) independent variables to include, taking into account the small sample size, and any correlation between them (e.g. do 'Growth' &amp; 'Z' tell you very different things?).</p>\\n\\n<p>It's good to try out different models, plot graphs to see what's going on, transform variables in different ways, &amp; so on; but remember that the more you do this the more your analysis becomes exploratory rather than confirmatory.</p>\\n\",\n",
       " '<p>The F ratio is a statistic.\\nWhen the null hypothesis of no group differences is true, then the expected value of the numerator and denominator of the F ratio will be equal. As a consequence, the expected value of the F ratio when the null hypothesis is true is also close to one (actually it\\'s not exactly one, because of the properties of expected values of ratios).</p>\\n\\n<p>When the null hypothesis is false and there are group differences between the means, the expected value of the numerator will be larger than the denominator.\\nAs such the expected value of the F ratio will be larger than under the null hypothesis, and will also more likely be larger than one.</p>\\n\\n<p>However, the point is that both the numerator and denominator are random variables, and so is the F ratio. The F ratio is drawn from a distribution. If we assume the null hypothesis is true we get one distribution, and if we assume that it is false with various assumptions about effect size, sample size, and so forth we get another distribution. We then do a study and get an F value.\\nWhen the null hypothesis is false, it is still possible to get an F ratio less than one.\\nThe larger the population effect size is (in combination with sample size), the more the F distribution will move to the right, and the less likely we will be to get a value less than one.</p>\\n\\n<p>The following graphic extracted from the <a href=\"http://www.psycho.uni-duesseldorf.de/abteilungen/aap/gpower3/\">G-Power3</a> demonstrates the idea given various assumptions.\\nThe red distribution is the distribution of F when H0 is true.\\nThe blue distribution is the distribution of F when H0 is false given various assumptions.\\nNote that the blue distribution does include values less than one, yet they are very unlikely.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/1ospI.png\" alt=\"enter image description here\"></p>\\n',\n",
       " \"<p>I have a very basic query. I ran a logistic model in SAS Entreprise Miner and got the output. </p>\\n\\n<p>Let's say we have a categorical variable with 3 categories. It creates a dummy variable for 2 categories and so gives us 2 coefficients for each. When we run the same logistic equation with Base SAS using <code>proc logistic</code> we get only one coefficient for that categorical variable. </p>\\n\\n<p>Are these two output saying the same thing? How does Base SAS take into account the number of categories while giving just one coefficient for the categorical variable? </p>\\n\\n<p>Further, is there any way of getting the same kind of output with Base SAS as I got in Entreprise Miner (basically, one with different coefficients for different category)?</p>\\n\\n<p>Any additional pointers would be greatly appreciated.</p>\\n\",\n",
       " \"<p>I'm a little bit confused....</p>\\n\\n<p>Lets say I assume $x$ to be endogenous and I observe that</p>\\n\\n<p>$x = a v_1 + b v_2 + c v_3$</p>\\n\\n<p>so that $x$ can be expressed as the perfect linear combination of $v_1, v_2, v_3$. By this I mean that if I regress $x$ onto $v_1,v_2,v_3$ the $R^2$ will be 1 i.e. the model is deterministic.</p>\\n\\n<p>In this particular case is it now wrong to assume that $v_1, v_2, v_3$ are all exogenous? I mean how can a perfect fit of exogenous variables be endogenous. One of the $v$'s has to be endogenous to begin with? In terms of Regression-Analysis I would guess this is not true since we do this kind of thing all the time. The only difference is that we won't get a perfect fit hence my confusion.</p>\\n\\n<p>If this would not be a problem I could always use one of the $v$'s as an instrument for $x$.</p>\\n\",\n",
       " \"<p>The mean would be totally acceptable. Some people like to think that it doesn't make sense because the distance between two values (e.g. 2 - 3 vs. 4 - 5) are not necessarily homogeneous. Those aren't the kinds of issues that keep me awake at night. Another option is to consider proportions of top-box responses. In survey design, you can look at the binomial outcome of getting a 5 or 4 or above as Y/N indicators of achieving a desired level of satisfaction. You would easily be able to cluster within sets of questions by taking the average number of top-box responses. A 100% here would indicate that the trainer achieved top-box responses on all questions whereas 66% indicates that 4 out of 6 achieved satisfactory levels.</p>\\n\",\n",
       " '<p>The n-1 is a result of the <a href=\"http://en.wikipedia.org/wiki/Bessel%27s_correction\" rel=\"nofollow\">Bessel\\'s correction</a>. The link has the proof but my own intuitive understanding of it is: </p>\\n\\n<p>The $\\\\\\\\sigma$ is a result of a difference between two values and if you have only one observation your mean is that observation, in other words - you can\\'t estimate a $\\\\\\\\sigma$ from 1 person. This means that you need the combination of two individuals to get 1 $\\\\\\\\sigma$ measure - hence you only have n-1 possibilities (degrees of freedom).</p>\\n\\n<p>Regarding a cutoff for the t-test I haven\\'t encountered any rules of thumb. I have a hard time imagining an experiment where it matters when there are more than 40 studied subjects. I work with uncertainties daily and no patient of mine cares if the risk for re-operation is 3-5% or 2-6%. I guess a good rule is when you think that the chance of any possible study bias in your study is bigger than the added uncertainty of the t-test --> you have bigger issues to worry about than the type of test.</p>\\n\\n<p><strong>Minor update</strong></p>\\n\\n<p>I\\'ve looked at your math but I\\'m not sure what your aim is. If your teaching stats: keep it simple... Statistics is all about living with uncertainties - when I\\'ve taught the subject - <em>most</em> experience the mathematical exactness and this uncertainty very unsettling :-)</p>\\n\\n<p>Using the true $\\\\\\\\sigma^2$ no sense to me - why would you study something you already know? Even if your study group is part of a large known sample - you can\\'t be sure that the $\\\\\\\\sigma^2$ is the same even if they are very similar.</p>\\n',\n",
       " '<p>I had eleven people play five versions of a video game in single sessions, for a total of 55 sessions (11x5). I used the <a href=\"http://www.psych.rochester.edu/SDT/measures/IMI_scales.php\" rel=\"nofollow\">Task Evaluation Questionnaire</a> from the <a href=\"http://www.psych.rochester.edu/SDT/measures/IMI_description.php\" rel=\"nofollow\">Intrinsic Motivation Inventory</a> to capture their experiences.</p>\\n\\n<p>Do I need to test the reliability and validity of this survey for the data that I collected? Someone said that I may have to and I don\\'t know how it\\'s done, or whether these 55 different sessions are enough. </p>\\n\\n<p>On the website the authors say:</p>\\n\\n<blockquote>\\n  <p>The IMI items have often been modified slightly to fit specific activities. Thus, for example, an item such as \"I tried very hard to do well at this activity\" can be changed to \"I tried very hard to do well on these puzzles\" or \"...in learning this material\" without effecting its reliability or validity.</p>\\n</blockquote>\\n\\n<p>This makes me think that I don\\'t have to, but I don\\'t know much about survey research. What are your thoughts? </p>\\n\\n<p>Here\\'s a follow-up question:\\nEven if you think that I don\\'t need to, let\\'s say I really want to compute reliability and validity of this survey for my experiment. How would I do that? Would the data that I already have (55 sessions) be enough? I\\'m asking because collecting new data is very inconvenient at the moment. </p>\\n\\n<p>Thank you so much.</p>\\n',\n",
       " '<p>I am finishing Geoffrey Hinton\\'s <a href=\"https://www.coursera.org/course/neuralnets\" rel=\"nofollow\">Neural Networks for Machine Learning</a> on Coursera, and he explains this in lecture 6b: \"A bag of tricks for mini-batch gradient descent.\"  You can <a href=\"https://class.coursera.org/neuralnets-2012-001/lecture/download.mp4?lecture_id=61\" rel=\"nofollow\">preview the video</a> without signing up or signing in.</p>\\n',\n",
       " '<p>I have to analyse data from an experiment in which participants performed two tasks. Each task had a different content and also a different \"space condition\". Content, space condition and order of presentation of content and condition were counter-balanced between participants. When participant finished the tasks, they answered a questionnaire about the difficulty of each one, containing a set of 18 questions (for each task). \\nI ran a factor analysis for the 18 questions referred to task with space condition 1, showing that there was just a single component, and another factor analysis with the questions related to space condition 2, that also resulted in only a single component.\\nMy question is:\\n Is it correct for me to run a repeated measures ANOVA comparing the two resulting factor scales (DV) as within subject IV (\"space condition\"), and with \"combination of content and space condition\" as between subjects IV?\\nThank you for your help.   </p>\\n',\n",
       " \"<p>A bit more info; suppose that</p>\\n\\n<ol>\\n<li>you know before hand how many variables to select and that you set the complexity penalty in the LARS procedure such as to have exactly that many variables with non 0 coefficients,</li>\\n<li>computation costs are not an issue (the total number of variable is small, say 50),</li>\\n<li>that all the variables (y,x) are continuous.</li>\\n</ol>\\n\\n<p>In what setting would the LARS model (i.e. the OLS fit of those variables having non zero coefficients in the LARS fit) be most different from a model with the same number of coefficients but found through exhaustive search (a la regsubsets())?</p>\\n\\n<p>Edit: I'm using 50 variables and 250 observations with the real coefficients drawn from a standard gaussian except for 10 of the variables having 'real' coefficients of 0 (and all the features being strongly correlated with one another). These settings are obviously not good as the differences between the two set of selected variables are minute. This is really a question about what type of data configuration should one simulate to get the most differences.</p>\\n\",\n",
       " '<p>I need to compare sample distributions with theoretical ones, which is typically done with a chi-squared test. The problem is that I have distributions where one or more cells have low values, and consequently the chi-squared test reports very small p-values. \\nFor example, a typical expected and observed frequencies are  [152   2   9]  and [140   5  18], with a p-value of 0.0007. Based on domain knowledge, these two distributions are not significantly different.</p>\\n\\n<p>What test could be used instead of chi-squared, which would take out the bias that occurs with the small-valued cells?</p>\\n\\n<p><strong>Edit: adding some background information for this problem.</strong></p>\\n\\n<p>I have a number of processes which produce as output certain technical parameters, recorded as time series. I have around 4000 of such process, each producing around 150 such time series (the number of time series a process has follows a power law). I would like to find which of these processes are anomalous, i.e. producing output which is significantly different from others.\\nTo do this I cluster the time series using k-means, and then based on the clusters, produce the \"expected\" distribution (average over all time series) and the distribution of clusters for each process. </p>\\n\\n<p>For example, after clustering I might have 4 clusters with following sizes.</p>\\n\\n<pre><code>Cluster number | Cluster size\\n-----------------------------\\n1              | 100\\n2              | 200\\n3              | 300\\n4              | 400\\n</code></pre>\\n\\n<p>The distribution of the clusters among the processes might be the following</p>\\n\\n<pre><code>          | Cluster 1 | Cluster 2 | Cluster 3 | Cluster 4\\n----------------------------------------------------------\\nProcess 1 | 11        | 19        | 35        | 42\\nProcess 2 | 3         | 10        | 14        | 19\\nProcess 3 | 30        | 8         | 12        | 12              &lt;----anomaly\\n....\\n</code></pre>\\n\\n<p>In this case, process 1 and process 2 are sufficiently close to the expected, while process 3 has a different distribution from the average. I would like to find a good test to measure this discrepancy. (Any other suggestion for the anomaly detection is also welcome)</p>\\n',\n",
       " '<p><strong>Practical situation</strong>: I’ve got 120 days of data collected during rainy season. On an average it rained on 52.5 of those days.</p>\\n\\n<ol>\\n<li>What is the probability of it raining at least once in 30 days?</li>\\n<li>If it rains at least once in 30 days, then what is the probability that it continues to rain for 4 days in a stretch in those 30 days?</li>\\n</ol>\\n\\n<p>My Answer for (1): I considered a Poisson process with $\\\\\\\\lambda = 52.5/120 = 0.4375$,\\ncalculated the non-occurrence of an event (in this case \"no rain\") in 30 days and subtracted that value from 1 to get .9999 as the probability of it raining in 30 days.</p>\\n\\n<p>Please advise if I\\'m on the right track and also how do we go about part (2) of the question. </p>\\n',\n",
       " '<p>What I know that people do (and I do it myself in some sort of way in GWAS studies) is that you combine all your permuted p values into a null distribution and then just see how many p values are above a threshold in your real experiment and your permuted null. </p>\\n\\n<p>So your FDR would then be something like: number of null p values &lt; x / number of real p values &lt; x</p>\\n',\n",
       " '<p>I am a novice Stata user (forced here from R to conform to coauthor\\'s quirky option choices). I need access to the Pearson residuals from a negative binomial regression. I currently have the regression equations specified in this manner:</p>\\n\\n<pre><code>xi: nbreg cntpd09 logpop08 pcbnkthft07 pccrunion07 urbanpop pov00 pov002 edu4yr ///\\n black04 hispanic04 respop i.pdpolicy i.maxloan rollover i.region if isser4 != 1,\\n cluster(state)\\nest store pd\\n</code></pre>\\n\\n<p>As I understand it, to get the Pearson residuals I need to undertake the same regression using the glm function. Something like this:</p>\\n\\n<pre><code>xi: glm cntpd09 logpop08 pcbnkthft07 pccrunion07 urbanpop pov00 pov002 edu4yr ///\\n black04 hispanic04 respop i.pdpolicy i.maxloan rollover i.region if isser4 !=1, ///\\n family(nb) vce(cluster state)\\n</code></pre>\\n\\n<p>Some details to note: 1)I use an if statement to select a subset of the total records, 2)I am using robust standard errors clustered by \"state\"</p>\\n\\n<p>Sorry for not including a replicable example at this stage, but I am new enough to Stata that I am not sure what each part of these statements is doing exactly and wanted the formatting to be exact. </p>\\n\\n<p>Is there something basic in the coding that would explain differences in coefficients and standard errors between these methods as I have them written out? Should I be worried about these differences? How can I make the glm version conform to the nbreg version?</p>\\n\\n<p>A secondary question since I have your attention already, how do I save, aggregate, and export the Pearson residuals assuming I get the glm function to work properly and am able to run it on each of the 6 regression models in our paper (Each model has the same observations, just different dependent and independent variables)? Ideally I would have a table with six columns of residuals in any format readable by R (including dta or other Stata defaults).</p>\\n\\n<p>Thanks in advance,</p>\\n\\n<p>Chris</p>\\n',\n",
       " '<p>Thank you very much for your help. Let me write the three solutions working for me.</p>\\n\\n<p>The first one is the answer form Joshua Ulrich: </p>\\n\\n<p>Read the data, create an xts object and use indexmon</p>\\n\\n<pre><code>data=read.csv(\"peira.dat\",sep=\";\",header=T,na.strings=\"-99.9\")\\ndia=as.Date(data[,1],\"%y/%m/%d\")\\nxdata=xts(data[,c(\"PRECIP\")],dia)\\nmdat=xdata[.indexmon(xdata) %in% c(5,6,7)]\\n</code></pre>\\n\\n<p>Here is a plot of mdat <img src=\"http://i.stack.imgur.com/QGyfL.png\" alt=\"enter image description here\"></p>\\n\\n<p>The second solution came from the spanish R mailing list and uses lubridate package:</p>\\n\\n<pre><code>library(lubridate)\\n\\ndata=read.csv(\"peira.dat\",sep=\";\",header=T,na.strings=\"-99.9\")\\n\\nf.dat&lt;-parse_date(data$FECHA, c(\"%y\", \"%m\", \"%d\"), seps=\"/\")\\ndata$m.dat&lt;-month(f.dat, label=F, abbr=F)\\ndat.gd&lt;-data[data$m.dat&gt;5 &amp; data$m.dat&lt;9,]\\ndia=as.Date(dat.gd[,1],\"%y/%m/%d\")\\ndataz=zoo(dat.gd[,c(\"PRECIP\")],dia)\\n</code></pre>\\n\\n<p>The plot of dataz:\\n<img src=\"http://i.stack.imgur.com/yN6Ew.png\" alt=\"enter image description here\"></p>\\n\\n<p>And the last one also from the spanish mailing list it uses POSIXct to take into account both date and time</p>\\n\\n<pre><code>tt=as.POSIXct(paste(data$FECHA,data$H_SOLAR), format=\"%y/%m/%d %H:%M:%S\") \\n\\ndatZoo &lt;- zoo(data[,-c(1,2)], tt)\\n\\nmonth &lt;- function (x) as.numeric(format(x, \"%m\"))\\nveranoIdx &lt;- which(month(tt) %in% 6:8)\\nveranoZoo &lt;- datZoo[veranoIdx]\\nveranoZoo\\n</code></pre>\\n',\n",
       " \"<p>For some time, I wanted to stop copy-pasting my R results into word, but climbing the LaTex mountain seemed to much to be worth it.  Recently, I came to discover LyX, as a laymen's solution to people like me who do not wish to code their text, but do wish to combine R analysis with text.</p>\\n\\n<p>However, I found there is very little, <strong>updated</strong>, documentation about LyX+R - which leads me to writing my question:</p>\\n\\n<ol>\\n<li>What tools do you combine in your R+LyX workflow? (do you combine text editors on top of LyX? which? and why?)</li>\\n<li>What is your common folder structure for an analysis project?</li>\\n<li>What is the order of steps you take for constructing your analysis? What code do you keep in file.r? and what in other types of files? (images, .RData backups, .TeX, and so on)</li>\\n<li>Do you use different work strategies for different types of project (due to size of project or size of dataset)</li>\\n<li>What R packages do you combine in your work? and how?</li>\\n<li>Recommended links?</li>\\n</ol>\\n\\n<p>Thank you for any input!</p>\\n\",\n",
       " \"<p>Imagine that you have three classes of points that look like a target with a bullseye: one group in the center, then another circling around that, then another circling around those. You can't take a pair of scissors and cut a straight line to separate any of these groups.</p>\\n\\n<p>Now imagine that this graph is on a sheet of rubber, and you stick a pin in the middle to hold it down and then pull the corners up, making a funnel shape. You've deformed the points by moving them from 2 dimensions to 3 dimensions. Now, you can make a straight cut with your scissors near the tip of the funnel, totally isolating the first group. Then a second straight snip farther up the funnel separates the second and third groups.</p>\\n\\n<p>What were curves in a lower dimension can become straight lines in a higher dimension. (In this example, the $z$ is related to the distance from the center.) This isn't the way that SVM actually does its job, but is the way I can see clearly that projecting data to a higher dimension can do nice things.</p>\\n\",\n",
       " '<p>First of all it is my opinion and probably the opinion of many statisticians that autimatically \"cleaning\" (i.e. removing extreme observations) is a bad idea.  Data should not be removed unless there is a reason to think there is an error.  What basis do you have to think that each group of data should follow a normal distribution?  It may not and the data may be perfectly valid.  By doing this you are forcing the data to \"look\" normal.  I find it particularly suspect if you are removing a lot of observations.</p>\\n\\n<p>I have done a lot of research on outlier detection in my time.  For two years while work at the Oak Ridge National Laboratory I concentrated on outlier detection in my job to validation DOE data sets.  I published articles in the early 1980s in JASA and the American Statistician on outlier detection.</p>\\n',\n",
       " '<p>The rho formula divides the covariance by SD of X and SD of Y. Does it result in a valid estimate of the true correlation (i.e., in the population)?</p>\\n',\n",
       " '<p>You write</p>\\n\\n<blockquote>\\n  <p>An example would be if I have one population of samples that all lie\\n  between 0 and 1 and another population of samples that all lie between\\n  10 and 11.</p>\\n</blockquote>\\n\\n<p>I am not sure what you mean by \"population of samples\" but if one population varies between 0 and 1 and the other between 10 and 11, you don\\'t need <em>any</em> hypothesis test. But if you wanted one, a t-test might be OK - the assumptions of a t-test do NOT include that the means are close. They do include equal variances, but there are corrections for that (e.g. Satterthwaite). However, t-tests are not robust to a combination of very different variances and very different sample sizes.</p>\\n',\n",
       " '<p>The AIC and BIC are both methods of assessing model fit penalized for the number of estimated parameters.  As I understand it, BIC penalizes models more for free parameters than does AIC.  Beyond a preference based on the stringency of the criteria, are there any other reasons to prefer AIC over BIC or vice versa?</p>\\n',\n",
       " '<p>If you use R you can maybe </p>\\n\\n<p>1/ merge the 2 time series<br>\\n2/ carry forward the values except if the delay is too long (kind of enhanced na.locf)</p>\\n',\n",
       " '<blockquote>\\n  <p>A Poisson variable $Y_i$ is believed to depend on a covariate $x_i$ and it is propsed that log-linear model with a systematic component $a + bx_i$ is appropriate. In an experiment the following values were observed:</p>\\n  \\n  <p><em>values not important to question</em></p>\\n  \\n  <p>Write down the log-likelihood function for the proposed log-linear model for these data.</p>\\n</blockquote>\\n\\n<p>So I did this, the normal linear model would just be that $\\\\\\\\lambda = a + bx_i$ and this would go into the PDF for a Poisson distribution and would give you:</p>\\n\\n<p>$$f(y: a + bx_i) = \\\\\\\\exp \\\\\\\\{y \\\\\\\\ln(a + bx_i) - (a + bx_i) - \\\\\\\\ln(y!)\\\\\\\\}$$</p>\\n\\n<p>So the log-linear model would just be the log of this function giving us</p>\\n\\n<p>$$y \\\\\\\\ln(a + bx_i) - (a + bx_i) - \\\\\\\\ln(y!)$$</p>\\n\\n<p>And so to find the log-likelihood function would be</p>\\n\\n<p>$$\\\\\\\\sum_{i =1}^n y_i \\\\\\\\ln(a + bx_i) - (a + bx_i) - \\\\\\\\ln(y_i!)$$</p>\\n\\n<p>But the answers say it should be</p>\\n\\n<p>$$\\\\\\\\sum_{i =1}^n y_i (a + bx_i) - \\\\\\\\exp(a + bx_i) - \\\\\\\\ln(y_i!)$$</p>\\n\\n<p>How do they get this?</p>\\n',\n",
       " '<p>Whatever method you use - be it ARIMA forecasting, or a plain, old exponential smoothing - be sure to place confidence limits around your forecasts. This will show management how accuracy diminishes as you move into the future.</p>\\n',\n",
       " '<p>First of all: NEAT is an algorithm that is usually used for reinforcement learning. I guess it won\\'t work very well in your case. You should rather find a good network architecture by cross-validation and train your network with stochastic gradient descent, conjugate gradient, ...</p>\\n\\n<p>Since it is just a fitness function it usually does not matter if it has an upper bound. You almost certainly will not find a perfect solution in every classification problem. So you should think about another stop criterion than \"find the maximal fitness\".</p>\\n\\n<p>However, you could take -E. Its upper bound is 0.</p>\\n',\n",
       " '<p>I am trying to do a multi-scale analysis and I\\'ve stumbled upon, either by personal research or recommendations (as in this <a href=\"http://stats.stackexchange.com/questions/34803/hidden-markov-model-segmentation-of-different-proportions-of-binary-data\">Q</a>), with the concept of Space-scale analysis.</p>\\n\\n<p>Do you know how can I plot things like this in R?:</p>\\n\\n<p>(source: <a href=\"http://www.ploscompbiol.org/article/fetchSingleRepresentation.action?uri=info%3adoi/10.1371/journal.pcbi.1002443.s015\" rel=\"nofollow\">Supplementary Fig. 15</a> from <a href=\"http://www.ploscompbiol.org/article/info%3adoi/10.1371/journal.pcbi.1002443\" rel=\"nofollow\">this article</a>)\\n<img src=\"http://i.stack.imgur.com/AEH8s.png\" alt=\"scale-space representation\"></p>\\n',\n",
       " \"<p>If you have the basics (identifying outliers, missing values, weighting, coding) depending on the topic there's a lot more in the plain academic literature to be found. For example in survey research (which is a topic where many things can go wrong, and prone to many sources of bias) there are a lot of good articles to be found. </p>\\n\\n<p>When preparing for regular crossectional regression, things <strong>may</strong> be less complex. \\nProblem there may for example that you remove too many 'outliers' and thus artificially fitting your model well. </p>\\n\\n<p>I thus also recommend you besides learning good techniques, also keep common sense in mind. Make sure you apply the techniques rightfully and not blindly. As for the software discussion in the other answers. I think SPSS is not bad for data preparation (I also heard good things about SAS) depending on your dataset size. The drop down menus are very intuitive.</p>\\n\\n<p>But as a direct answer to your question, academic literature may or may not be a very good source for your data preparation depending on the topic and analysis.</p>\\n\",\n",
       " '<p>$f(x;\\\\\\\\theta)$ is the density of the random variable $X$ at the point $x$, with $\\\\\\\\theta$ being the parameter of the distribution. $f(x,\\\\\\\\theta)$ is the joint density of $X$ and $\\\\\\\\Theta$ at the point $(x,\\\\\\\\theta)$ and only makes sense if $\\\\\\\\Theta$ is a random variable. $f(x|\\\\\\\\theta)$ is the conditional distribution of $X$ given $\\\\\\\\Theta$, and again, only makes sense if $\\\\\\\\Theta$ is a random variable. This will become much clearer when you get further into the book and look at Bayesian analysis.</p>\\n',\n",
       " '<p><strong>animation</strong>: A Gallery of Animations in Statistics and Utilities to Create Animations</p>\\n\\n<p>An R package.  Enables the teacher to create many animation that can be made into webapps.</p>\\n\\n<p>Great for the teacher to create a children webapp.</p>\\n\\n<p><a href=\"http://cran.r-project.org/web/packages/animation/index.html\" rel=\"nofollow\">http://cran.r-project.org/web/packages/animation/index.html</a></p>\\n\\n<p>Examples:\\n<a href=\"http://animation.yihui.name/\" rel=\"nofollow\">http://animation.yihui.name/</a></p>\\n',\n",
       " '<p>A lot of Social Science / Psychology students with minimal mathematical background like Andy Field\\'s book: <a href=\"http://rads.stackoverflow.com/amzn/click/0761944524\" rel=\"nofollow\">Discovering Statistics Using SPSS</a>. He also has a website that shares a <a href=\"http://www.statisticshell.com/html/woodofsuicides.html\" rel=\"nofollow\">lot of material</a>.</p>\\n',\n",
       " '<p>The individual plots are on the scale of the linear predictor, i.e. a scale that is <code>-Inf</code> to <code>+Inf</code>. The inverse of the link function is used to map from this scale to the <code>0, ..., 1</code> scale of the response. Further note that each smooth is subject to centring constraints and so is centred about 0.</p>\\n',\n",
       " '<p>I would like to understand how can I check if some points around a line have constant distance between the point and the line.</p>\\n\\n<p>So image a normal chart(cartesian plane) with a line that is formed with points where its coordinates are: x > 0 and y > 0 so always numbers bigger then zero.</p>\\n\\n<p>Then, when I have this line i need to check if the distances of a list of points(x,y) are constant. With \"constant\" I mean that the distance between the points and the line is similar. Example:</p>\\n\\n<p><strong>LINE:</strong> (x - y coordinates)</p>\\n\\n<pre><code>1 - 1\\n2 - 2\\n3 - 3\\n4 - 4\\n5 - 5\\n6 - 6\\n</code></pre>\\n\\n<p><strong>POINTS:</strong> (x - y coordinates)</p>\\n\\n<pre><code>1 - 1.1\\n2 - 1.9\\n3 - 2.9\\n4 - 4.2\\n5 - 5.1\\n6 - 5.8\\n</code></pre>\\n\\n<p>How can I check if the difference between the point on the line and the point (of the list of points)  are constant?</p>\\n\\n<p>Thanks</p>\\n',\n",
       " '<p>I am a newbie to statistics and found <a href=\"http://wiki.answers.com/Q/What_does_%27theta%27_mean_in_statistics\" rel=\"nofollow\">this</a>.</p>\\n\\n<blockquote>\\n  <p>In statistics, θ, the lowercase Greek letter \\'theta\\', is the usual\\n  name for a (vector of) parameter(s) of some general probability\\n  distribution. A common problem is to find the value(s) of theta.\\n  Notice that there isn\\'t any meaning in naming a parameter this way. We\\n  might as well call it anything else. In fact, a lot of distributions\\n  have parameters which are usually given other names. For example, it\\n  is common use to name the the mean and deviation of the normal\\n  distribution μ (read: \\'mu\\') and deviation σ (\\'sigma\\'), respectively.</p>\\n</blockquote>\\n\\n<p>But I still don\\'t know what that means in plain English?</p>\\n',\n",
       " '<p>If you have a large number of variables, there is uncertainty regarding the choice of predictors. I think this is root of OP\\'s problem. Bayesian model averaging (BMA) helps take this uncertainty into account. </p>\\n\\n<p>Check out Adrian Raftery\\'s page on BMA. Through BMA, you can select a subset of potentially helpful predictors. R packages exist for most of these methods.</p>\\n\\n<p><a href=\"http://www.stat.washington.edu/raftery/Research/bma.html\" rel=\"nofollow\">http://www.stat.washington.edu/raftery/Research/bma.html</a></p>\\n\\n<p>I found Raftery\\'s paper on BMA modeling economic growth to be particularly interesting</p>\\n\\n<p><a href=\"http://www.stat.washington.edu/raftery/Research/PDF/Eicher2010.pdf\" rel=\"nofollow\">http://www.stat.washington.edu/raftery/Research/PDF/Eicher2010.pdf</a></p>\\n',\n",
       " '<p>Well, following your update, it seems you are dealing with a factorial experiment (<em>factorial</em> means that every factors are crossed, or, in other words, each unit is subjected to every possible combination of your factors), with five replicates. Let assume that these are not the same statistical units whose temperature is repeatedly measured across each of the 12 combinations (for the sake of clarity).</p>\\n\\n<p>An <a href=\"http://en.wikipedia.org/wiki/Analysis_of_variance\" rel=\"nofollow\">ANalysis Of VAriance</a> (ANOVA) seems to be the most appropriate method to deal with this design. Basically, it will allow you to estimate the contribution of each source of variance (decay, particles, and velocity) wrt. the total variance in the observed temperature. What is not explained by these factors is called the residual variance (what you call the \\'random effect\\'). A full additive model (i.e., without modeling interaction between your factors) will read something like</p>\\n\\n<p>$$\\\\\\\\ny_{ijkl}=\\\\\\\\mu + \\\\\\\\alpha_i + \\\\\\\\beta_j + \\\\\\\\gamma_k + \\\\\\\\varepsilon_{ijkl},\\\\\\\\n$$\\r</p>\\n\\n<p>where $y_{ijkl}$ is the temperature for unit $l$ when considering levels $i=1\\\\\\\\dots a$, $j=1\\\\\\\\dots b$, and $k=1\\\\\\\\dots c$, of factors $\\\\\\\\alpha$ (decay), $\\\\\\\\beta$ (particles), and $\\\\\\\\gamma$ (velocity); the $\\\\\\\\varepsilon_{ijk}$ are the residuals assumed to follow a gaussian distribution of unknown variance, $\\\\\\\\sigma^2$. They can be viewed as random fluctuations around $\\\\\\\\mu$, the overall mean, and reflect the between-unit variations that are not accounted for by the other factors. The $\\\\\\\\alpha_i$, $\\\\\\\\beta_j$, and $\\\\\\\\gamma_k$ can be viewed as factor-specific deviations from the overall mean $\\\\\\\\mu$.</p>\\n\\n<p>The so-called <em>main effect</em> of decay, particles, and velocity will be estimated by forming a ratio between the variance that they account for (known as <em>mean squares</em>) and the residual variance (what is left after considering all variance explained by those factors), which is known to follow a Fisher-Snedecor (F) distribution, with $d-1$ and $N-abc$ degrees of freedom, where $d=a$, $b$, or $c$ stands for the number of levels of $\\\\\\\\alpha$ (decay), $\\\\\\\\beta$ (particles), and $\\\\\\\\gamma$ (velocity). A significant effect (following an <a href=\"http://en.wikipedia.org/wiki/Statistical_hypothesis_testing\" rel=\"nofollow\">hypothesis test</a> of a null effect, i.e. $H_0:\\\\\\\\, \\\\\\\\mu_i=\\\\\\\\mu_j\\\\\\\\,\\\\\\\\, \\\\\\\\forall i\\\\\\\\neq j$ <em>vs.</em> $H_1:$ at least two of the $\\\\\\\\mu_i$\\'s differ) would indicate that the factor under consideration has a significant effect on the outcome. This is readily obtained by any statistical software. For instance, in R you would use something like</p>\\n\\n<pre><code>summary(aov(temperature ~ decay + particles + velocity, data=df))\\n</code></pre>\\n\\n<p>provided temperature and factor levels are organized in four columns, in a data.frame named <code>df</code>, as suggested below:</p>\\n\\n<pre><code>t1 0.1 10 30\\nt2 0.1 10 30\\nt3 0.1 10 30\\nt4 0.1 10 30\\nt5 0.1 10 30\\nt6 0.2 10 30\\nt7 0.2 10 30\\n...\\nt60 0.3 100 70\\n</code></pre>\\n\\n<p>The effect of any of the three factors can also be summarized under an equation like the one you referred to sy simply calling (again under R):</p>\\n\\n<pre><code>summary.lm(aov(temperature ~ decay + particles + velocity))\\n</code></pre>\\n\\n<p>This follows from the fact that an ANOVA is nothing more than a <a href=\"http://en.wikipedia.org/wiki/Linear_model\" rel=\"nofollow\">Linear Model</a> that you may have heard about (think of a regression model where the explanatory variables are all categorical).</p>\\n\\n<p>Should you want to account for possible interactions between all three factors, you need to add three second-order and one three-order interaction terms. If any of these effects prove to be significant, this would mean that the effect of the corresponding factors cannot be considered in isolation one from the other (e.g., the effect of decay on temperature is not the same depending on the number of particles).</p>\\n\\n<p>As for references, I would suggest starting with on-line tutorial or textbook like <a href=\"http://www.psych.nyu.edu/cohen/three_way_ANOVA.pdf\" rel=\"nofollow\">Three-Way ANOVA</a>, by Barry Cohen, or <a href=\"http://cran.r-project.org/doc/contrib/Faraway-PRA.pdf\" rel=\"nofollow\">Practical Regression and Anova using R</a>, by John MainDonald (but see also other textbooks available on <a href=\"http://cran.r-project.org/other-docs.html\" rel=\"nofollow\">CRAN documentation</a>). The definitive reference is Montgomery, <a href=\"http://bcs.wiley.com/he-bcs/Books?action=index&amp;itemId=047148735X&amp;bcsId=2172\" rel=\"nofollow\">Design and Analysis of Experiments</a> (Wiley, 2005).</p>\\n',\n",
       " '<p>I do not know the answer, so I can only offer some thoughts hoping that someone else can throw light on the issue.</p>\\n\\n<p>It seems to me that there is no problem in computing the likelihood. To compute the value of the AIC criterion (which may or may not make sense in this context), what would be required is the number of fitted parameters.</p>\\n\\n<p>Here is were things become slippery. To take the simplest example, the (univariate) local level model requires fitting two parameters (the variances of the state and of the noise), but those are really best described as metaparameters. If the variance of the state is zero, you are fitting a single mean (1 parameter). If the variance of the state goes to infinity, you are effectively fitting one parameter per observation.</p>\\n\\n<p>One way out is to define the \"number of equivalent parameters\" as in,</p>\\n\\n<p>Hodges, J. S. and Sargent, D. J. (2001) Counting Degrees of Freedom in \\n        Hierarchical and Other Richly-Parameterised Models, <em>Biometrika,</em> <strong>88</strong>(2),\\n        p. 367-379.</p>\\n\\n<p>and this is what I have done in my own work (on purely heuristic grounds and with some trepidation!). See for instance,</p>\\n\\n<p><a href=\"http://www.et.bs.ehu.es/~etptupaf/nuevo/ficheros/papiros/riw.pdf\" rel=\"nofollow\">Pérez-Castroviejo, P. and Tusell, F. (2007) Using redundant and incomplete time series for the estimation of cost of living indices, <em>Review of Income and Wealth</em>, vol. <strong>53</strong>, p. 673-691.</a> </p>\\n\\n<p>There are alternative ways of calculating \"equivalent parameters\"; if you want to follow this route I can give you some references.</p>\\n',\n",
       " '<blockquote>\\n  <p>The greatest value of a picture is when it forces us to notice what we\\n  never expected to see.</p>\\n</blockquote>\\n\\n<p>-- John Tukey</p>\\n',\n",
       " \"<p>As is known to all, SVM can use kernel method to project data points in higher spaces so that points can be separated by a linear space. \\nBut we can also use logistic regression to choose this boundary in the kernel space, so what's the advantages of SVM?\\nSince SVM uses a sparse model in which only those support vectors make contributions when predicting, does this make SVM faster in prediction?</p>\\n\",\n",
       " '<p>It seems to me like you need to formulate a question you want your data to answer. Let me suggest a few (perhaps you can edit your post to reflect what questions make sense for your data):</p>\\n\\n<ul>\\n<li>As the DSMC value increases, does the theoretical result also increase?</li>\\n<li>If I know the value of the theoretical result, how accurately can I estimate the value of DSMC?</li>\\n</ul>\\n\\n<p>If the points (1,x1) and (1,y1) refer to the same measurement or the same run of the experiment, or one is an estimate of the other. One natural way to see how related they are related is to plot {(x1,y1),(x2,y2) ...}.</p>\\n\\n<p>You can read about Pearson correlation and Kendall tau and Spearman rho here:</p>\\n\\n<p><a href=\"http://en.wikipedia.org/wiki/Correlation_and_dependence\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Correlation_and_dependence</a></p>\\n',\n",
       " '<p>The AdaBoost algorithm states that it is to train a classifier based on the training data according to a weight vector.</p>\\n\\n<p>Assume the size of training data is N, the weight vector is of dimension N as well.  I have three questions regarding this sampling procedure,</p>\\n\\n<p>1)  Will the size of sampled data be the same as the original data set?\\n2)  What does the weight vector look like? If it is a distribution, then the sum of them has to be 1. Is that possible to have a weight vector with entries of integer number?\\n3)  Generally, which algorithm can be used to sample a data set based on a given weight vector or a distribution?</p>\\n',\n",
       " '<p>If you flip a fair coin 10 times and it comes up heads 3 times, the <strong>proportion</strong> of heads is .30 but the <strong>probability</strong> of a head on any one flip is .50.</p>\\n',\n",
       " '<p>I will outline an approach that requires no \"training\" at all; it is up to you to determine its utility in this case.</p>\\n\\n<p>A simple (and nonparametric) hypothetical model is that all datasets are independent, that none has a trend, and that their variations from one time period to the next are mutually independent.  This implies the probability with which two pre-specified datasets simultaneously have local minima would equal the product of the probabilities with which each has local minima, with obvious (but more complex) generalizations to three or more pre-specified datasets (which I illustrate below).  In particular, you can estimate the <em>probabilities</em> of local minima by means of their observed <em>frequencies</em> in each dataset.  From these you can compute the probabilities of simultaneous local minima among 2, 3, or, generally, $k$ or more of the datasets.  When the probability for $k$ or more is so low that it is unlikely to occur during the time span you have observed, you can take the simultaneous occurrence of $k$ or more local minima to be \"significant\" relative to this null hypothesis of independence.</p>\\n\\n<p>For example, suppose you have five datasets, each observed 100 times, with local minima appearing 8, 9, 10, 11, and 12 times in them.  All five would simultaneously exhibit a local minimum (8/100) * (9/100) * (10/100) * (11/100) * (12/100) = 0.00095% of the time, so even within 100 observations the expected number of simultaneous minima (of 100 * 0.00095% = 0.00095) is so ridiculously low that five simultaneous minima surely would be significant evidence of an \"interesting\" point.</p>\\n\\n<p>Local minima among the first four datasets (unaccompanied by a local minimum in the fifth) would have an expected frequency of 100 * (8/100) * (9/100) * (10/100) * (11/100) * ((100-12)/100) = .00697.  Similarly we could compute the expected frequency of local minima among the other combinations of four of the datasets.  The total frequency of exactly four simultaneous minima is 0.04375. Added to the frequency of five simultaneous minima this gives 0.0447 as the expected number of times you would observe four <em>or</em> five simultaneous local minima in 100 observations: still pretty rare and therefore significant if it turns up.  A similar computation for the ten combinations of three simultaneous local minima shows that you would expect <em>at least</em> three local minima 0.8452 times out of 100.  So, observing one or two such events would not be unusual and you might not consider them significant.  Obviously the expected number of two-way minima would be substantial (you should expect to see around 40 of them out of 100) and you would be unlikely to consider any of those significant.</p>\\n\\n<p>The example illustrates how you could go about computing thresholds for significance in terms of the number of simultaneous local minima for any number of datasets that are observed for any number of time periods.</p>\\n\\n<p>You can give a more precise accounting of the situation by means of the Poisson distribution.  Take the occurrence of four or more simultaneous minima in the example.  Under the null hypothesis (of independent datasets), this is rare enough that the actual count should have a Poisson distribution with expectation 0.8452.  This implies there is a 94.59% chance of observing two or fewer such events.  Thus, if you see three or more three- or four- or five-way minima you could take this to be significant evidence of lack of independence (with about 95% confidence).  However, in this case you could not point to a specific time that is significant; you could only say that there are more threefold minima than there should be.  Any one of them would be a reasonable candidate for an \"interesting\" time, but further investigation should ensue before you stipulate that any <em>particular</em> one of these times really demonstrates a departure from independence.</p>\\n\\n<p>This model might or might not be appropriate for your data.  You can check that by examining the data.  If your data have trends or exhibit serial correlation you would need a more complex version of this model.  Nevertheless, the same kind of analysis can help you decide what constitutes an \"interesting\" or \"significant\" syzygy of local minima.</p>\\n',\n",
       " '<p>\"How much\" matters a great deal!  The adjustment is unlikely to be zero, after all; this would only happen if <em>z</em> were totally uncorrelated with <em>x</em> or <em>y</em>.</p>\\n\\n<p>By common convention one would test the statistical significance of the relationship between <em>z</em> and <em>y</em> as a way of deciding whether it is necessary to use <em>z</em> to adjust <em>x</em>\\'s coefficient.  That said, significance will depend on other things such as alpha and sample size.  You may find yourself having to make a largely subjective judgment as to whether to adjust, based on some combination of <em>p</em>-value and the magnitude of the proposed adjustment.  Which may be ok, as long as you document the factors that went into your decision.</p>\\n\\n<p>In practice, small adjustments are likely to matter more in domains such as pharmaceuticals, where variables are measured more objectively, and less in areas such as opinion research.</p>\\n',\n",
       " '<p>Do not use filenames with spaces. They are pain in any OS. The pain is somewhat lessened on Unix type OSes. Rename the file to something <code>stabletest1.txt</code> and then try to feed it to the program. Note that Windows sometimes do not show the file extension. So the filename might be <code>stable test 1.txt</code> not <code>stable test 1</code>. This is even more plausible since you use Notepad, which tends to add the extension <code>.txt</code> to any file you edit, whether you want it or not. So check the file extension, purge the spaces from its name and then try to use it.</p>\\n\\n<p>Carriage return is simply <code>enter</code> at the end of the file. When editing file press enter at the end of the last line and do not write anything when the cursor is at the beginning of the new line. I would not trust Notepad however to not \"clean up\", i.e. delete unnecessary last line. Try using some other text editor.  And take suggestion of @David, use R, or use other interfaces to this software. </p>\\n',\n",
       " '<p>I am confused by two, yet inconsistent for me, facts: Since the PLS regression is expressed by  matrices of scores and loadings as\\n$$X=TP^T+E\\\\\\\\\\\\\\\\Y=UQ^T+F$$\\nhow it can be translated into linear equation like $Y=a+b_1X_1+b_2X_2+...+b_nX_n$?</p>\\n\\n<p>(I have found this in several papers).</p>\\n',\n",
       " '<p>Try using <code>na.StructTS</code> in the zoo package. It has methods for zoo and ts series. e.g. using the built in <code>USAccDeaths</code> insert some NAs and then interpolate them:</p>\\n\\n<pre><code>library(zoo)\\nwindow(USAccDeaths, 1975, c(1975, 12)) &lt;- NA\\nna.StructTS(USAccDeaths)\\n</code></pre>\\n\\n<p>See ?na.StructTS for more.</p>\\n',\n",
       " '<p>As Srikant suggests, you need to look at <a href=\"http://en.wikipedia.org/wiki/Order_statistic\" rel=\"nofollow\">order statistics</a>.</p>\\n\\n<p>To add to Srikant\\'s answer, you can simulate this process easily in R:</p>\\n\\n<pre><code>n = 10\\nN = 1000;sims = numeric(N)\\nfor(i in 1:N)\\n  sims[i] = min(runif(n))\\n\\nhist(sims, freq=FALSE)\\nx = seq(0,1,0.01)\\nlines(x, dbeta(x, 1, n), col=2)\\n</code></pre>\\n\\n<p>To get</p>\\n\\n<p><img src=\"http://img441.imageshack.us/img441/6826/tmpe.jpg\" alt=\"alt text\"></p>\\n\\n<hr>\\n\\n<p><strong>Slight digression</strong> </p>\\n\\n<p>This question is related to one of my favourite statistics problems, the <a href=\"http://en.wikipedia.org/wiki/German_tank_problem\" rel=\"nofollow\">German tank problem</a>. This problem is about the maximum of uniform distributions, and can be summarised as:</p>\\n\\n<blockquote>\\n  <p>Suppose one is an Allied intelligence\\n  analyst during World War II, and one\\n  has some serial numbers of captured\\n  German tanks. Further, assume that the\\n  tanks are numbered sequentially from 1\\n  to N. How does one estimate the total\\n  number of tanks?</p>\\n  \\n  <p>Taken from wikipedia</p>\\n</blockquote>\\n\\n<p>Check out the wikipedia page for more details.</p>\\n',\n",
       " '<p>The chi-square test is a general term for tests, many of which were first developed by Karl Pearson in the beginning of the 20th century.  There are many tests that have an asymptotic chi-square distribution.  So the answer to your question may depend a little on which chi-square tests you are referring to.  I can answer for the contingency table and goodness of fit tests.  The asymptotics does not work well in the case of \"sparse cells\" in the contingency table.  This has been extensively studied by William Cochran and rules of thumb have been used.  One of the common ones is to require that the expected number of cases in any one cell should be 5 or more under the null hypothesis. Others require that only a very small percentage of the cells should have expected count size under 5.  Some people may also apply this to the observed cell frequencies.  SAS checks using a rule like the ones I\\'ve described and provides a warning that the approximation may not be accurate when the check fails.  For more details look at Agresti\\'s book \"Categorical Data Analysis\" or this wikipedia article:\\n<a href=\"http://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test</a></p>\\n',\n",
       " '<p>Traditionally, the individual coefficient significance test can be solved by applying t-test. However, in my case, as follows:</p>\\n\\n<ol>\\n<li>the predictor values have been standardized</li>\\n<li>the standardized coefficient have been known</li>\\n<li>we would not know the values of predictors, thus we can not use the t-test</li>\\n</ol>\\n\\n<p>I argue that whether i can directly use the standardized coefficient to derive which one is significant, such as using confidence interval.</p>\\n\\n<p>Thanks!</p>\\n',\n",
       " '<h2>Load data</h2>\\n\\n<p>Assuming <code>fd.txt</code> contains the following</p>\\n\\n<pre><code>N: toto\\nY: 2000\\nS: tata\\n\\nN: titi\\nY: 2004\\nS: tutu\\n\\nN: toto\\nY: 2000\\nS: tata2\\n\\nN: toto\\nY: 2000\\nS: tata3\\n\\nN: tete\\nY: 2002\\nS: tyty\\n\\nN: tete\\nY: 2002\\nS: tyty2\\n</code></pre>\\n\\n<p>here is one solution in R:</p>\\n\\n<pre><code>tmp &lt;- scan(\"fd.txt\", what=\"character\")\\nres &lt;- data.frame(matrix(tmp[seq(2, length(tmp), by=2)], nc=3, byrow=TRUE))\\n</code></pre>\\n\\n<p>The first command read everything a single vector of character, skipping blank lines; then we remove every odd elements (\"N:\", \"S:\", \"Y:\"); finally, we arrange them in a data.frame (this a convenient way to make each column a factor).</p>\\n\\n<p>The output is</p>\\n\\n<pre><code>    X1   X2   X3\\n1 toto 2000 tata\\n2 titi 2004 tutu\\n3 toto 2000 tata2\\n4 toto 2000 tata3\\n5 tete 2002  tyty\\n6 tete 2002 tyty2\\n</code></pre>\\n\\n<p>Please note that if you have some GNU utilities on your machine, you can use <code>awk</code></p>\\n\\n<pre><code>sed \\'s/[NYS]: //\\' fd.txt | awk \\'ORS=(FNR%4)?FS:RS\\' &gt; res.txt\\n</code></pre>\\n\\n<p>The first command uses <code>sed</code> to filter the descriptor (replace by blank); then <code>awk</code> will produce its output (Output Record Separator) as follows:  arrange each record using default Field Separator (space), and put a new Record Separator (new line) every 4 fields. Of note, we could filter the data using <code>awk</code> directly, but I like separating tasks a little.</p>\\n\\n<p>The result is written in <code>res.txt</code> and can be imported into R using <code>read.table()</code>:</p>\\n\\n<pre><code>toto 2000 tata \\ntiti 2004 tutu \\ntoto 2000 tata2 \\ntoto 2000 tata3 \\ntete 2002 tyty \\ntete 2002 tyty2 \\n</code></pre>\\n\\n<h2>Process and transform data</h2>\\n\\n<p>I didn\\'t find a very elegant solution in R, but the following works:</p>\\n\\n<pre><code>library(plyr)\\ntmp &lt;- ddply(res, .(X1,X2), mutate, S=list(X3))[,-3]\\nresf &lt;- tmp[!duplicated(tmp[,1:2]),]\\n</code></pre>\\n\\n<p>Then, <code>resf</code> has three columns, where column <code>S</code> list the levels of the <code>X3</code> factor (siblings\\' name). So, instead of putting siblings in different columns, I concatenated them in a list. In other words, </p>\\n\\n<pre><code>as.character(resf$S[[1]])\\n</code></pre>\\n\\n<p>gives you the name of <code>tete</code>\\'s siblings, which are <code>tyty</code> and <code>tyty2</code>.</p>\\n\\n<p>I\\'m pretty sure there\\'s a better way to do this with <code>plyr</code>, but I didn\\'t manage to get a nice solution for the moment.</p>\\n\\n<hr>\\n\\n<p>With repeated \"S:\" motif, here is one possible quick and dirty solution. Say <code>fd.txt</code> now reads</p>\\n\\n<pre><code>N: toto\\nY: 2000\\nS: tata\\nS: tata2\\nS: tata3\\n\\nN: titi\\nY: 2004\\nS: tutu\\n\\nN: tete\\nY: 2002\\nS: tyty\\nS: tyty2\\n</code></pre>\\n\\n<p>then, </p>\\n\\n<pre><code>tmp &lt;- read.table(\"fd.txt\")\\ntmp$V1 &lt;- gsub(\":\",\"\",tmp$V1)\\nstart &lt;- c(which(tmp$V1==\"N\"), nrow(tmp)+1)\\nmax.col &lt;- max(diff(start))\\nres &lt;- matrix(nr=length(start)-1, nc=max.col)\\nfor (i in 1:(length(start)-1))\\n  res[i,1:diff(start[i:(i+1)])] &lt;- t(tmp[start[i]:(start[i+1]-1),])[2,]\\nres &lt;- data.frame(res)\\ncolnames(res) &lt;- c(\"name\",\"year\",paste(\"S\",1:(max.col-2),sep=\"\"))\\n</code></pre>\\n\\n<p>will produce</p>\\n\\n<pre><code>  name year   S1    S2    S3\\n1 toto 2000 tata tata2 tata3\\n2 titi 2004 tutu  &lt;NA&gt;  &lt;NA&gt;\\n3 tete 2002 tyty tyty2  &lt;NA&gt;  \\n</code></pre>\\n',\n",
       " \"<p>I have a sequence of integers that represent total sales of my product for each day. From time to time, we have large press or marketing events that increase sales on the day of the event and for a few days after that but then eventually taper down to the long-run average. Here's some made-up numbers showing what I mean:</p>\\n\\n<pre><code>34, 40, 35, 36, 150, 110, 140, 107, 80, 68, 75, 50, 45, 35, 38, 41, 42,...\\n                ^^^                                   ^^^\\n          event occurs here                        reversion to mean\\n</code></pre>\\n\\n<p>I have limited experience with statistics, so I'm looking for some guidance on statistical methods that I could use to determine:</p>\\n\\n<ul>\\n<li>how much of the sales on the event day and the following days should be attributed to the event</li>\\n<li>whether the press or marketing event contributes to a permanently higher sales average, even after its initial effect has worn off.</li>\\n</ul>\\n\",\n",
       " '<p>A power-law is a function that increases proportionally to a power of its argument (ax^b). </p>\\n\\n<p>Often seen in fitted relationships or in some forms of densities (power-law distributions). </p>\\n\\n<p>Power-law relationships are common in physics, though non-power-law relationships may also look somewhat like power-laws.</p>\\n\\n<p>Power laws are also associated with economics, the Pareto principle and the 80-20 rule.</p>\\n\\n<p>Reference: <a href=\"http://en.wikipedia.org/wiki/Power_law\" rel=\"nofollow\">Wikipedia - Power law</a></p>\\n',\n",
       " '<p><a href=\"http://cran.r-project.org/web/views/Bayesian.html\" rel=\"nofollow\">Bayesian CRAN task view</a></p>\\n',\n",
       " '<p>The <a href=\"http://en.wikipedia.org/wiki/Anscombe_transform\" rel=\"nofollow\">Anscombe transform</a> is $a(x) = 2\\\\\\\\sqrt{x+3/8}$.</p>\\n\\n<p>Can anyone show me how to prove that an Anscombe-transformed version $Y = a(X)$ of a Poisson distributed random variable $X$ is approximately normal distributed (when $\\\\\\\\lambda>4$)? </p>\\n',\n",
       " '<p>Technical answers to these questions can be found in the (sub)discipline of <a href=\"http://en.wikipedia.org/wiki/Influence_function_(statistics)#Empirical_influence_function\" rel=\"nofollow\">robust statistics</a>. It operates with the concept of influence function:\\n$$\\\\\\\\n{\\\\\\\\rm IF}(x;T,F) = \\\\\\\\lim_{\\\\\\\\epsilon\\\\\\\\to +0} \\\\\\\\frac{T[ (1-\\\\\\\\epsilon) F + \\\\\\\\epsilon \\\\\\\\delta(x)] - T[F]}{{\\\\\\\\rm d}\\\\\\\\epsilon}\\\\\\\\n$$\\nwhere $F(\\\\\\\\cdot)$ is the distribution function of interest (typically, the sample cdf $F_n(x) = \\\\\\\\sum_i 1\\\\\\\\{ x \\\\\\\\le x_i \\\\\\\\}$, and $T(F)$ is a statistic that can be expressed as a functional of the distribution function. Sample mean is such a functional: ${\\\\\\\\rm E}[X] = \\\\\\\\int x \\\\\\\\, {\\\\\\\\rm d}F(x)$, and $\\\\\\\\bar X = \\\\\\\\int x \\\\\\\\, {\\\\\\\\rm d}F_n(x)$. Sample median is also a functional, although a more complicated one: the one that solves $\\\\\\\\int {\\\\\\\\rm sign}(x-m) \\\\\\\\, {\\\\\\\\rm d}F = 0$ with respect to $m$. Sample variance, $s^2 = \\\\\\\\frac1{n-1} \\\\\\\\sum_i (X_i - \\\\\\\\bar X)^2$, does not belong to this class, as it has an ugly $n-1$ factor that cannot be related to $F_n(\\\\\\\\cdot)$ function. Simply speaking, the influence function shows by how much your statistics changes when we add an observation with value $x$ to the data set.</p>\\n\\n<p>If your statistic of interest belongs to the class of statistics that can be defined as functionals of the cdf, then it won\\'t be affected by your second transformation (copies of the original sample). The answer to your first question is given by the IF: if you replace $x_1 \\\\\\\\mapsto 10 x_1$, then the new statistic will change approximately by $T \\\\\\\\mapsto T + \\\\\\\\frac1n [{\\\\\\\\rm IF}( 10 x_1, T, F_n)-{\\\\\\\\rm IF}( x_1, T, F_n)]$. The IF of the sample mean is ${\\\\\\\\rm IF}(x;\\\\\\\\bar X, F) = \\\\\\\\frac 1n x$, and of course it also easy to see algebraically that the change in the sample mean will be $\\\\\\\\bar X \\\\\\\\mapsto \\\\\\\\bar X + 9x_1/n$.</p>\\n',\n",
       " '<p>I\\'d stick with logistic or probit regression, enter both factors as covariates, but enter the ordinal factor as if it was continuous. To test for interaction, do a <a href=\"http://en.wikipedia.org/wiki/Likelihood-ratio_test\" rel=\"nofollow\">likelihood-ratio test</a> comparing models with and without an interaction between the two factors. This test will have a single degree of freedom and therefore retain good power. </p>\\n\\n<p>After using this to decide whether or not you want to include an interaction between the two factors, you can then move on to decide how best to code the 5-level factor in your final model. It could make sense to keep treating it as if it were continuous, or you might wish to code it as four dummy (indicator) variables, or you choose to collapse it into fewer levels, or use some other type of contrast. The choice probably depends on the scientific meaning of the model and its intended use, as well as the fit of the various models.</p>\\n',\n",
       " '<blockquote>\\n  <p>Here is a <a href=\"http://en.wikipedia.org/wiki/Tinkertoy\" rel=\"nofollow\">TinkerToy</a> set. Show me\\n  how Euclidean distance works in three\\n  dimensions. Now show me how multiple regression works.</p>\\n</blockquote>\\n\\n<p>Can they explain how statistics works in the physical world?</p>\\n',\n",
       " '<h3>1. Coding scheme</h3>\\n\\n<p>In terms of assessing statistical significance using a t-test, it is the relative distances between the scale points that matters. Thus, (0, 0.25, 0.5, 0.75, 1) is equivalent to (1, 2, 3, 4, 5).\\nFrom my experience an equal distance coding scheme, such as those mentioned previously are the most common, and seem reasonable for Likert items.\\nIf you explore optimal scaling, you might be able to  derive an alternative coding scheme.</p>\\n\\n<h3>2. Statistical test</h3>\\n\\n<p>The question of how to assess group differences on a Likert item has already been answered <a href=\"http://stats.stackexchange.com/questions/203/group-differences-on-a-five-point-likert-item\">here</a>.</p>\\n\\n<p>The first issue is whether you can link observations across the two time points. It sounds like you had a different sample.\\nThis leads to a few options:</p>\\n\\n<ul>\\n<li><strong>Independent groups t-test</strong>: this is a simple option; it also does test for differences in group means; purists will argue that the p-value may be not entirely accurate; however, depending on your purposes, it may be adequate.</li>\\n<li><strong>Bootstrapped test of differences in group means</strong>: If you still want to test differences between group means but are uncomfortable with the discrete nature of dependent variable, then you could use a bootstrap to generate confidence intervals from which you could draw inferences about changes in group means.</li>\\n<li><strong>Mann-Whitney U test</strong> (among other non-parametric tests): Such a test does not assume normality, but it is also testing a different hypothesis.</li>\\n</ul>\\n',\n",
       " '<p>We are looking at how a change in the environment (confinement level) alters which of two behaviors our model organism chooses. We perform several trials at each confinement level. Each of these trials elicits one of the two possible behaviors. From this we get a fraction of trials which exhibited each behavior under each of the environmental conditions. I want to calculate the confidence interval for each of the fraction estimates under each environmental condition. </p>\\n\\n<p>So an example would be at a confinement level of 10 (on some arbitrary scale) behavior one is used in 23 of 31 trials, and behavior two is used in the other 8. </p>\\n\\n<p>My guess is that the proper thing to do it to use r\\'s 1−sample proportions test with alternate k values for each pair of fractions (as in: <a href=\"http://www.r-tutor.com/elementary-statistics/interval-estimation/interval-estimate-population-proportion\" rel=\"nofollow\">http://www.r-tutor.com/elementary-statistics/interval-estimation/interval-estimate-population-proportion</a>). So for our example, prop.test(23, 31) and prop.test(8, 31) giving confidence intervals of (0.55,0.87) and (0.12, 0.44). Or is there some reason not to use a 1-sample proportion test on each member of the fraction pair? If the answer is just, \"nope, that\\'s fine\", well... great!</p>\\n',\n",
       " \"<p>Regarding the last part of your question, the technique you are looking for is called analysis of variance or ANOVA but it is not going to help you here.</p>\\n\\n<p>You can look at ANOVA as an “adaptation” of Student's t-test for multiple groups. ANOVA is a complex topic and covers many different models but if you run a regular one-way between-subject ANOVA in a two-group setting in which you could also use a t-test, you will in fact notice that the F statistic from the ANOVA is equal to the square of the t-statistic and the respective p values are equal.</p>\\n\\n<p>The reason why it is not applicable to your example is that your observations cannot be assumed to be independent (e.g. some but not all pupils at a school will share the same teacher) and this independence is one of the major assumptions of the F-Test used in ANOVA. There is also most likely some dependency between schools and teachers (e.g. if each teacher is teaching in one school and one school only, teachers are said to be nested within schools). Educational researchers use something called “multilevel models” to deal with these issues but this is really a lot to handle if this is all new to you.</p>\\n\\n<p>At least two other issues also need to be considered before jumping into the analysis:</p>\\n\\n<ul>\\n<li>10000 is a lot of observations. It's perfectly possible, likely even, that you will find some “significant” differences that really don't mean all that much. You have to ask yourself what counts as a meaningful difference in performance and look at the notions of “power” and “effect size” to better understand this issue.</li>\\n<li>If you want a meaningful significance test, your analysis should probably include all the schools in your sample. Looking at the means and then only formally comparing a handful of schools that look different from each other is a very questionable approach.</li>\\n</ul>\\n\\n<p>The problem you have in mind is really more complex than it looks, it could be very useful for you to look for a statistician or at least an experienced researcher in your field that might be able to help you with your analysis.</p>\\n\",\n",
       " '<p>One transforms the <em>dependent</em> variable to achieve approximate <em>symmetry</em> and <em>homoscedasticity</em> of the <em>residuals</em>.  Transformations of the <em>independent</em> variables have a different purpose: after all, in this regression all the independent values are taken as fixed, not random, so \"normality\" is inapplicable.  The main objective in these transformations is to achieve <em>linear</em> relationships with the dependent variable (or, really, with its logit).  (This objective over-rides auxiliary ones such as reducing excess <a href=\"http://www.jerrydallal.com/LHSP/diagnose.htm\">leverage</a> or achieving a simple interpretation of the coefficients.)  These relationships are a property of the data and the phenomena that produced them, so you need the flexibility to choose appropriate re-expressions of each of the variables separately from the others.  Specifically, not only is it not a problem to use a log, a root, and a reciprocal, it\\'s rather common.  The principle is that there is (usually) nothing special about how the data are originally expressed, so you should let the data suggest re-expressions that lead to effective, accurate, useful, and (if possible) theoretically justified models.</p>\\n\\n<p>The histograms--which reflect the univariate distributions--often hint at an initial transformation, but are not dispositive.  Accompany them with scatterplot matrices so you can examine the relationships among all the variables.</p>\\n\\n<hr>\\n\\n<p>Transformations like $\\\\\\\\log(x + c)$ where $c$ is a positive constant \"start value\" can work--and can be indicated even when no value of $x$ is zero--but sometimes they destroy linear relationships.  When this occurs, a good solution is to create <em>two</em> variables.  One of them equals $\\\\\\\\log(x)$ when $x$ is nonzero and otherwise is anything; it\\'s convenient to let it default to zero.  The other, let\\'s call it $z_x$, is an indicator of whether $x$ is zero: it equals 1 when $x = 0$ and is 0 otherwise.  These terms contribute a sum</p>\\n\\n<p>$$\\\\\\\\beta \\\\\\\\log(x) + \\\\\\\\beta_0 z_x$$\\r</p>\\n\\n<p>to the estimate.  When $x \\\\\\\\gt 0$, $z_x = 0$ so the second term drops out leaving just $\\\\\\\\beta \\\\\\\\log(x)$.  When $x = 0$, \"$\\\\\\\\log(x)$\" has been set to zero while $z_x = 1$, leaving just the value $\\\\\\\\beta_0$.  Thus, $\\\\\\\\beta_0$ estimates the effect when $x = 0$ and otherwise $\\\\\\\\beta$ is the coefficient of $\\\\\\\\log(x)$.</p>\\n',\n",
       " \"<p>To find association between peer's support (independent variable) and work satisfaction (dependent variable) I wish to apply chi-square test. Peer's support is categories in four groups according to the extent of support: 1=very less extent, 2=to some extent, 3=to great extent and 4=to very great extent. Work satisfaction is categories into two: 0=not satisfied and 1=satisfied.</p>\\n\\n<p>The SPSS output says than 37.5 percent cell frequencies are less than 5. My sample size is 101 and I don't want to reduce categories in independent variable into lesser number. In this situation is there any other test that can be applied to test this association?</p>\\n\",\n",
       " \"<p>I have a data set with gene expression levels measured for three different treatments, and for each treatment we have three biological replicates. Treatments are: Control, 24h at $4$°C, and 24h at $-3$°C. This analysis was carried out on two genotypes.</p>\\n\\n<pre><code>                       CTRL1 CTRL2 CTRL3 4°C_1 4°C_2 4°C_3 -3°C_1 -3°C_2 -3°C_3\\nGene1  Genotype 1       20    22    25     85    92    120   450    380    372\\n       Genotype 2       50    72    78     130   122   184   250    490    700\\n</code></pre>\\n\\n<p>I'd like to know how to compare <code>gene1</code> expression level in <code>genotype1</code> vs.  <code>genotype2</code>. The aim is to find a similarity between expression trend in <code>genotype1</code> and <code>genotype2</code>.</p>\\n\",\n",
       " \"<p>The short answer is probably not, since:</p>\\n\\n<ul>\\n<li>the Poisson distribution is discrete, your data is continuous;</li>\\n<li>the Poisson distributions has support on 0,1,2, ..., whereas (I think) your data has a range from 0 to 100.</li>\\n</ul>\\n\\n<p>Without seeing your data and knowing your problem, it's tricky to give you a suggestion. A good starting position would be to look at the statistical analysis section of publications that analyse data similar to your data.</p>\\n\",\n",
       " '<p>In order to avoid the spurious correlation problem, you should regress two <em>stationary</em> time series against one another. This can (potentially) provide a causal story. It is non-stationary series that lead to spurious correlation. See the reasoning given by my answer to <a href=\"http://stats.stackexchange.com/questions/7975/what-to-make-of-explanatories-in-time-series/8037#8037\">this question</a> (As a footnote, you may not need stationary series if they are integrated series, but I\\'d point you to any of the applied time series books to learn more about that.)</p>\\n',\n",
       " '<p>This suggests to me that you may have one or a few observations that are extreme multivariate outliers for your dependent variables <code>malesM</code>, and that the MANOVA test is just picking up that the mean of any group containing such an outlier is far from the mean of those that don\\'t. </p>\\n\\n<p>To check for this, you could look at the <a href=\"http://en.wikipedia.org/wiki/Mahalanobis_distance\" rel=\"nofollow\">Mahalanobis distances</a> of all the observations from the sample mean using the sample covariance matrix.</p>\\n',\n",
       " '<p>Training c linear discriminant functions is a example of a \"1-vs-all\" or \"1-against-the-rest\" approach to building a multiclass classifier given a binary classifier learning algorithm.  Training C(c,2) 2-class classifiers is an example of the \"1-vs-1\" approach.  As c gets larger, the \"1-vs-1\" approach builds a lot more classifiers (but each from a smaller training set).</p>\\n\\n<p>This paper compares these two approaches, among others, using SVMs as the binary classifier learner: </p>\\n\\n<p><a href=\"http://mnd.ly/qZ9e9t\" rel=\"nofollow\">C.-W. Hsu and C.-J. Lin. A comparison of methods for multi-class support vector machines , IEEE Transactions on Neural Networks, 13(2002), 415-425</a>.</p>\\n\\n<p>Hsu and Lin found 1-vs-1 worked best with SVMs, but this would not necessarily hold with all binary classifier learners, or all data sets.  </p>\\n\\n<p>Personally, I prefer polytomous logistic regression. </p>\\n',\n",
       " \"<p>This can be explained as follows. Mathematically, given two variables X and Y, their Correlation is defined as the </p>\\n\\n<pre><code>covariance(X,Y)/(Standard Deviation(X)*Standard Deviation(Y)). \\n</code></pre>\\n\\n<p>In other words, the correlation is proportional to the the covariance of the two variables. The divisor in the equation acts has a scaling effect on the covariance so that the resulting correlation will lie between -1 and +1.</p>\\n\\n<p>So, all other things being equal, reducing the covariance will reduce the correlation. The effect of having similar school achievement is to reduce the covariance between IQ and school achievement. For example, given a wide range of IQ's, if school achievement is similar then school achievement doesn't co-vary with IQ, i.e. there is a relatively random relationship between achievement and IQ, i.e. the correlation is close to zero indicating (relatively speaking) no relationship. </p>\\n\\n<p>On the other hand, given a wide range of IQ's, if school achievement is also spread over a wide range then correlation can still take any value between -1 (a negative relationship and +1 (a positive relationahip) including 0 (indicating  no relationship)</p>\\n\\n<p>Getting back to your question, it is the reduction in covariance that is important here rather than the reduction in variance. </p>\\n\",\n",
       " '<p>As kwak has pointed out, my question was answered on another forum:</p>\\n\\n<p><a href=\"http://www.or-exchange.com/questions/695/analytical-solutions-to-limits-of-correlation-stress-testing\" rel=\"nofollow\">http://www.or-exchange.com/questions/695/analytical-solutions-to-limits-of-correlation-stress-testing</a></p>\\n\\n<p>I did several quick calculations and the suggested analytical solution is consistent with the numerical ones. I still need to check the proof.</p>\\n',\n",
       " '<p>Which is more correct? </p>\\n\\n<ul>\\n<li>\"Accept the null hypothesis\"; or</li>\\n<li>\"Do not reject the null hypothesis\"</li>\\n</ul>\\n\\n<p>Is there truly a separate meaning that I\\'m missing here?</p>\\n',\n",
       " '<p>A p value is a measure of how embarrassing the data are to the null hypothesis</p>\\n\\n<p>Nicholas Maxwell, Data Matters: Conceptual Statistics for a Random World Emeryville CA: Key College Publishing, 2004.</p>\\n',\n",
       " '<p>You cannot share parameters between Q and R, as you have specified in the model. \\nSee <a href=\"http://journal.r-project.org/archive/2012-1/RJournal_2012-1_Holmes~et~al.pdf\" rel=\"nofollow\">http://journal.r-project.org/archive/2012-1/RJournal_2012-1_Holmes~et~al.pdf</a> pg 13 \"Elements with the same character name are constrained to be equal (no sharing across parameter matrices, only within).\"</p>\\n\\n<p>I don\\'t know if this helps much, since you already discovered it didn\\'t work for you, but at least you know there is no official support for this type of parameter sharing.  I don\\'t know the solution, or if there is a solution, but you might try asking the authors of the package. I have found them to be very gracious with their time and expertise. </p>\\n',\n",
       " '<h3>The question:</h3>\\n\\n<ol>\\n<li><p>How can normality be validated without using visual cues such as QQ plots? (the validation will be a part of larger software)</p></li>\\n<li><p>Can a \"goodness of fit\" score be calculated?</p></li>\\n</ol>\\n\\n<p>Although enumerated separately, these parts are (appropriately) one question: you compute an appropriate goodness of fit and use that as a test statistic in a hypothesis test.</p>\\n\\n<h3>Some answers</h3>\\n\\n<p>There are plenty of such tests; the best among them are the Kolmogorov-Smirnov, Shapiro-Wilks, and Anderson-Darling tests.  Their properties have been extensively studied.  An excellent resource is the work of M. A. Stephens, especially the 1974 article, <a href=\"http://www.jstor.org/pss/2286009\" rel=\"nofollow\">EDF Statistics for Goodness of Fit and Some Comparisons</a>.  Rather than supply a long list of references, I will leave it to you to Google this title: the trail quickly leads to useful information.</p>\\n\\n<p>One thing I like about Stephens\\' work, in addition to the comparisons of the properties of various GoF tests, is that it provides clear descriptions of how to compute the statistics and how to compute, or at least approximate, their null distributions.  This gives you the option to implement your favorite test yourself.  The EDF statistics (empirical distribution function) are easy to compute: they tend to be linear combinations of the order statistics, so all you have to do is sort the data and go.  The complications concern (a) computing the coefficients--this used to be a barrier in applying the S-W test, but good approximations now exist--and (b) computing the null distributions.  Most of those can be computed or have been adequately tabulated.</p>\\n\\n<p>What is characteristic about any GoF tests for distributions is that (a) they need a certain amount of data to become powerful (for detecting true deviations) and (b) very quickly thereafter, as you acquire more data, they become <em>so</em> powerful that deviations that are <em>practically</em> inconsequential become <em>statistically significant.</em>  (This is very well known and is easily confirmed with simulation or mathematical analysis.)  In this is the origin of the reluctance to answer the original question without obtaining substantial clarification.  If you have a few hundred values or more, you will find that any of these tests demonstrate your data are not \"normal.\"  But does this matter for your intended analysis?  We simply cannot say.</p>\\n',\n",
       " '<p>Quick R site is basic, but quite nice for start <a href=\"http://www.statmethods.net/index.html\">http://www.statmethods.net/index.html</a> . </p>\\n',\n",
       " '<p>You asked if your requirements look reasonable.  My opinion: No, your requirements are not reasonable.  In particular, you proposed:</p>\\n\\n<blockquote>\\n  <p>Ratings above and below the median should be equivalent.</p>\\n  \\n  <ul>\\n  <li>{1,1,3,4,4} = {2,2,3,4,4} = {2,2,3,5,5}</li>\\n  </ul>\\n</blockquote>\\n\\n<p>This is not reasonable.  {1,1,3,4,4} is a significantly worse score than {2,2,3,5,5}.</p>\\n\\n<p>I have studied app market ratings, and found that there is a significant difference between what a 4 vs a 5 (in terms of what this tends to mean to the reviewer).  For instance, a common meaning of 5 is \"I love it\"; a common meaning of 4 is \"I love it, though it would be great if the developer would add feature X\".  There is also a significant difference between a 1 and a 2.  For instance, a common meaning of 1 is \"it does not work at all for me\"; a common meaning of 2 is \"it works under some situations, or some beneficial qualities, but overall is very poor\".</p>\\n\\n<p>Your requirement insists that we must throw away this information.  That is not a reasonable thing to insist upon.</p>\\n\\n<hr>\\n\\n<p>Added 7/9: Here is another way to look at it.</p>\\n\\n<p>By your criteria, {1,1,4,4} &lt; {2,2,5,5}, since the median is lower.  Now suppose a new user walks by and rates each of these two apps a 3.  I would expect this to maintain any existing relationship, so we should have {1,1,3,4,4} &lt; {2,2,3,5,5} -- but your proposal violates this expectation.  So the mere act of rating two products, with the same rating, can actually change their relative ranking.  That is surprising, to say the least!</p>\\n\\n<p>To formalize it a bit, I am proposing the following criteria as one that seems reasonable:</p>\\n\\n<ul>\\n<li>If we have $S &lt; T$, then we also have $S \\\\\\\\cup U &lt; T  \\\\\\\\cup U$ for any set $U$.  If we have $S = T$, then we also have $S \\\\\\\\cup U = T  \\\\\\\\cup U$ for any set $U$.</li>\\n</ul>\\n\\n<p>However, this is not consistent with the set of criteria you listed, so we can\\'t have them all: we have to throw something out.  I would argue that your third criteria should be thrown out.</p>\\n',\n",
       " '<p>The function <code>cv.glmnet</code> from the R package <a href=\"http://cran.r-project.org/web/packages/glmnet/\">glmnet</a> does automatic cross-validation on a grid of $\\\\\\\\lambda$ values used for $\\\\\\\\ell_1$-penalized regression problems. In particular, for the lasso. The glmnet package also supports the more general <em>elastic net</em> penalty, which is a combination of $\\\\\\\\ell_1$ and $\\\\\\\\ell_2$ penalization. As of version 1.7.3. of the package taking the $\\\\\\\\alpha$ parameter equal to 0 gives ridge regression (at least, this functionality was not documented until recently).</p>\\n\\n<p>Cross-validation is an estimate of the expected generalization error for each $\\\\\\\\lambda$ and $\\\\\\\\lambda$ can sensibly be chosen as the minimizer of this estimate. The <code>cv.glmnet</code> function returns two values of $\\\\\\\\lambda$. The minimizer, <code>lambda.min</code>, and the always larger <code>lambda.1se</code>, which is a heuristic choice of $\\\\\\\\lambda$ producing a less complex model, for which the performance in terms of estimated expected generalization error is within one standard error of the minimum. Different choices of loss functions for measuring the generalization error are possible in the glmnet package. The argument <code>type.measure</code> specifies the loss function.</p>\\n\\n<p>Alternatively, the R package <a href=\"http://cran.r-project.org/web/packages/mgcv/index.html\">mgcv</a> contains extensive possibilities for estimation with quadratic penalization including automatic selection of the penalty parameters. Methods implemented include generalized cross-validation and REML, as mentioned in a comment. More details can be found in the package authors book: <em>Wood, S.N. (2006) Generalized Additive Models: an introduction with R, CRC.</em> </p>\\n',\n",
       " \"<p>Since nobody's mentioned it, you could also think of your data as the result of a two step process, e.g. some personal facts determine whether a subject smokes, and then some possibly different facts determine how the intensity of smoking affects the question outcome.</p>\\n\\n<p>This approach puts in the domain of explicit selection models, of which Heckman and Tobit regression are familiar examples for continuous dependent variables.  The statistical issues arise due to the possibility of correlated errors in the two steps.  There exist relatively straightforward extensions to probit models to cover your categorical dependent variable: try googling 'double probit' for details.  I think this would be the model class corresponding to @zbicyclist's answer.</p>\\n\",\n",
       " '<p>I have a discrete random variable $X$ and two random vectors $\\\\\\\\vec{Y}$ and $\\\\\\\\vec{Z}$.</p>\\n\\n<p>For a given $x$ (i.e. an instance of $X$), I am interested in estimating:</p>\\n\\n<p>$$E[\\\\\\\\vec{Y} \\\\\\\\vec{Z}^{\\\\\\\\top} | X = x]$$</p>\\n\\n<p>based on samples $(x_1,y_1,z_1),\\\\\\\\ldots,(x_n,y_n,z_n)$.</p>\\n\\n<p>I thought of using a kernel estimation method, and estimating the above as:</p>\\n\\n<p>$$E[\\\\\\\\vec{Y} \\\\\\\\vec{Z}^{\\\\\\\\top} | X = x] \\\\\\\\approx \\\\\\\\sum_{i=1}^n K(x_i, x) y_i z_i^{\\\\\\\\top}.$$</p>\\n\\n<p>My question is: is there any mention of that in the literature?</p>\\n\\n<p>I am looking for conditions under which this estimation is \"consistent\" (i.e. it converges to the true $E[\\\\\\\\vec{Y} \\\\\\\\vec{Z}^{\\\\\\\\top} | X = x],$ and maybe some anchor that I can look for something similar being done.</p>\\n\\n<p>The closest thing I could find is kernel density estimation, but it is not quite the same.</p>\\n',\n",
       " '<p>Suppose I am given $n$ samples of sizes $N_1, \\\\\\\\dots, N_n$ from a Dirichlet&ndash;multinomial distribution: Fixed and given is a $k$-vector $\\\\\\\\mathbf{\\\\\\\\alpha}$ of positive real numbers. For each $i, \\\\\\\\, 1 \\\\\\\\le i \\\\\\\\le n$, a random probability vector $\\\\\\\\mathbf{p}_i$ is drawn from a Dirichlet distribution $\\\\\\\\mathrm{Dir}(\\\\\\\\mathbf{\\\\\\\\alpha})$, and then a sample of size $N_i$ is drawn from a multinomial distribution on $\\\\\\\\{1, \\\\\\\\dots, k\\\\\\\\}$ with probabilities given by $\\\\\\\\mathbf{p}_i$. The observed frequencies are recorded in an $n \\\\\\\\times k$ table. Assume that there are no problems with low cell counts. </p>\\n\\n<p>What can one say about the value $X$ of the $\\\\\\\\chi^2$-statistic that can be computed for this random table? We can always write $\\\\\\\\mathbf{\\\\\\\\alpha} = M \\\\\\\\mathbf{p_o}$  for some probability vector $\\\\\\\\mathbf{p_o}$, and if $M$ is large, then all rows in this random table come from approximately the same multinomial distribution, so approximately $X \\\\\\\\sim \\\\\\\\chi^2_{r}$ with $r = (n-1)(k-1)$. How large must $M$ be for this to happen? And what happens for small $M$?</p>\\n',\n",
       " '<p>In general inference, why orthogonal parameters are useful, and why is it worth trying to find a new parametrization that makes the parameters orthogonal ? </p>\\n\\n<p>I have seen some textbook examples, not so many, and would be interested in more concrete examples and/or motivation.</p>\\n',\n",
       " '<p>I am using a control chart to try to work on some infection data, and will raise an alert if the infection is considered \"out of control\".</p>\\n\\n<p>Problems arrive when I come to a set of data where most of the time points have zero infection, with only a few occasions of one to two infections, but these already exceed the control limit of the chart, and raise an alert.</p>\\n\\n<p>How should I work on the control chart if the data set is having very few positive infection counts?</p>\\n\\n<p>Thanks!</p>\\n',\n",
       " \"<p>A simple solution is to incorporate the hometown advantage (that is if your data holds this info). This makes it possible to give a definite meaning to your outcome. So if you have that data, it'll likely be a better model and solves your problem: go there!</p>\\n\\n<p>Right now, your outcome's definition depend on the order, but your data doesn't.</p>\\n\\n<p>A possible solution (though I haven't checked this completely) would be to duplicate every record in your data, but change the order and the outcome (so for every observation, both representations are in your dataset), and then do a weighted logistic regression, giving every observation a weight of 1/2 (I think this correctly adjusts your variances, but I'd have to check)</p>\\n\\n<p>Another option is to change your outcome so it is not dependent on the order anymore (i.e.: the alphabetically former team wins or not), or to always code your two teams in a steady order (i.e. make that the alphabetically former team is always in column 1).</p>\\n\\n<p>These things are bound to be a bit harder to interpret, though...</p>\\n\",\n",
       " \"<p>Rotation of extracted factors or principal components is not prohibited for binary data: it is not the <em>data</em> what is rotated, it is the loading matrix. By the way, your data <em>for which</em> loadings are computed isn't seen as binary anymore since you computed tetrachoric (polychoric) correlations for it.</p>\\n\\n<p>Unlike factor analysis application, typical application of PCA involve interpretation of the components only rarely, because the components are usually seen as derivative simplifying variables which only summarize the multivariate cloud, they do not pretend to be latent <em>essences</em> behind the observed variables, like factors do: factors govern the covariations and always call for interpretation. Rotation aids interpretation. That's probably why <code>PRINCOMP</code> doesn't offer rotation and <code>FACTOR</code> does, as you say.</p>\\n\",\n",
       " '<p><a href=\"http://books.google.com.br/books?id=ERSSDBDcYOIC&amp;pg=PA197&amp;lpg=PA197&amp;dq=generate%20random%20numbers%20from%20the%20multivariate%20normal%20distribution%20in%20spherical%20coordinates&amp;source=bl&amp;ots=hWWENS2Vst&amp;sig=FZW65dk9z86ksEd9_RcB8_zIF1A&amp;hl=pt-PT&amp;sa=X&amp;ei=n-EfUKffFciB7AGo3oDgBA&amp;redir_esc=y#v=onepage&amp;q=generate%20random%20numbers%20from%20the%20multivariate%20normal%20distribution%20in%20spherical%20coordinates&amp;f=false\" rel=\"nofollow\">This book</a> has <a href=\"http://books.google.com.br/books?id=ERSSDBDcYOIC&amp;pg=PA344&amp;lpg=PA344&amp;dq=Deak%201990%20random%20number&amp;source=bl&amp;ots=hWWENS2Wsp&amp;sig=kVIaUfmUFh_le1VjX5eoTy4iYfY&amp;hl=pt-PT&amp;sa=X&amp;ei=_-EfUKbNA6LW6wHSyYDgDQ&amp;redir_esc=y#v=onepage&amp;q=Deak%201990%20random%20number&amp;f=false\" rel=\"nofollow\">this reference</a> to <a href=\"http://www.alibris.com/booksearch?qwork=10918703\" rel=\"nofollow\">this book</a> here. Not exactly cheap, but I think it\\'s the one you are looking for. ;)</p>\\n',\n",
       " '<p>I think you misremember the end of the process. In R, it would go like this:</p>\\n\\n<pre><code># generating random x1 x2 x3 in (0,1) (10 values each)\\n&gt; x1 &lt;- runif(10)\\n&gt; x2 &lt;- runif(10)\\n&gt; x3 &lt;- runif(10)\\n\\n# generating y\\n&gt; y &lt;- x1 + 2*x2 + 3*x3 + rnorm(10)\\n\\n# classical regression\\n&gt; lm(y ~ x1 + x2 + x3)\\n\\nCall:\\nlm(formula = y ~ x1 + x2 + x3)\\n\\nCoefficients:\\n(Intercept)           x1           x2           x3  \\n 0.2270       2.0088       0.2746       3.1529  \\n\\n\\n# \"orthogonalized\" regression\\n&gt; lm(x1 ~ x2 + x3)$residuals -&gt; z1\\n&gt; lm(x2 ~ x1 + x3)$residuals -&gt; z2\\n&gt; lm(x3 ~ x1 + x2)$residuals -&gt; z3\\n\\n&gt; lm(y ~ z1) \\n\\nCall:\\nlm(formula = y ~ z1)\\n\\nCoefficients:\\n(Intercept)           z1  \\n      3.056        2.009  \\n\\n&gt; lm(y ~ z2)\\n\\nCall:\\nlm(formula = y ~ z2)\\n\\nCoefficients:\\n(Intercept)           z2  \\n     3.0560       0.2746  \\n\\n&gt; lm(y ~ z3)\\n\\nCall:\\nlm(formula = y ~ z3)\\n\\nCoefficients:\\n(Intercept)           z3  \\n      3.056        3.153  \\n</code></pre>\\n\\n<p>See? You get the same estimates $\\\\\\\\hat \\\\\\\\beta_i$ for $i = 1,2,3$. Note that the intercepts are differents; the residual $z_i$ are centered so the intercept of eg the regression <code>y ~ z1</code> is just the mean of $y$ (and similarly for $z_2$, $z_3$). Once you get the $\\\\\\\\hat \\\\\\\\beta_i$ it is not difficult to find the intercept of the classical regression.</p>\\n\\n<p>Mathematical explications will be find in page 54-55 of last edition of <a href=\"http://www-stat.stanford.edu/~tibs/ElemStatLearn/\" rel=\"nofollow\">The elements of statiscal learning</a> — much clearer and accurate that anything I could write (available on line).</p>\\n',\n",
       " '<p>The perspective Ralu is using is basically p is the probability of A and for the binomial he\\'s saying you have the events A and not A which for you is B and that\\'s your event space. Since you don\\'t know your actual value for P(A) and assuming you don\\'t have a good guess for it you\\'ll want to use a conservative estimate of .5 plugging that into the equation in the other answer is going to imply you need 16 observations in your sample. However I\\'m not sure binomial is the best choice in this case.</p>\\n\\n<p>When determining sample size there are two things you\\'re going to want to decide. First is your confidence level (which you have as 95%) the next is you\\'re going to want to decide what margin of error is acceptable for your analysis.</p>\\n\\n<p>It might be worth considering <a href=\"http://books.google.com/books?id=ZvPKTemPsY4C&amp;lpg=PP1&amp;dq=wackerly%20mathematical%20statistics&amp;pg=PA423#v=onepage&amp;q=Example%208.10&amp;f=false\" rel=\"nofollow\">example 8.10 in Wackerly et al</a>. Since it actually looks at determining sample size for two sample groups which is your situation.</p>\\n\\n<p>The explanation in the example seems thorough enough (if you have questions though please ask), but in case you don\\'t click to take a look it will result ( 2n = 1/[$(1/1.96){}^{2}$/8] = 31 so n = 62 ) in requiring 31 in each group so 62 in total. Notice that this is nearly four times the size of what the other method suggested it is also larger than the 40 samples usually cited for the Central Limit Theorem which gives it good properties. </p>\\n\\n<p>However that has a fairly large margin of error (1 = 100% margin of error). Let\\'s say instead you wanted a very small margin of error such as 4% </p>\\n\\n<p>2n = 1/[$(.04/1.96){}^{2}$/8] = 19208 so n = 38416</p>\\n\\n<p>Remember though in this example we assumed the range was 8 and used 4*Sigma is approximately equal to the range. Which may or may not make sense for your problem. Range is Max - Min. So if in your current data you see a very different range you may want to use that value instead to recalculate accordingly.</p>\\n\\n<p>As for determining whether there was a difference you\\'re going to want to use a hypothesis test. In particular you\\'re going to want to use a <a href=\"http://www.acastat.com/Statbook/ttest2.htm\" rel=\"nofollow\">two sample T-Test</a>. Your null hypothesis is that the means are equal. In your case you want to know if the new one is greater than the old one so you\\'ll want a one tail (also called a directional hypothesis) alternative hypothesis. Once you\\'ve calculated the T statistic using the formulas in that link you\\'ll need to find the corresponding critical value <a href=\"http://www.statsoft.com/textbook/distribution-tables/#t\" rel=\"nofollow\">from a table</a>. For your confidence level you\\'ll want to know if the test statistic is greater than this value: 1.644854 if it is than the new mean which we\\'re going to consider as mu1 is greater than the old one mu2. If it is not greater than that value you fail to reject because your evidence isn\\'t strong enough.</p>\\n\\n<p>Hopefully this helps!</p>\\n',\n",
       " '<p>Have you normalized the data? If not, and you have features measured on very different scales, then your zero coefficients in the equation of the SVM hyperplane may not be zeros, but merely some small numbers, so that corresponding features still contribute to the classification decision. You can remove these features, rebuild the classifier, and check the accuracy of the new classifier.</p>\\n\\n<p>Theoretically, if certain coefficients in the equation of the SVM hyperplane are equal to zero, then the corresponding features do not contribute to the classification decision.</p>\\n\\n<p>If you think that certain features are informative, but they are not used by your classifier, that means that the classifier can get good enough separation using other features, so that other features may also be quite informative.</p>\\n',\n",
       " '<p>There is no clear answer in the literature on how to deal with the fact that RT data do not conform to the traditional Gaussian models of error. Some folks will say that one need not worry about non-Gaussian error within cells of the experimental design because traditional analysis approaches typically collapse these observations to a mean, and we know that the sampling distribution of the mean will tend to conform to the Gaussian assumption of error. Such arguments miss the point, now well established in the RT literature, that experimental variables can affect not only the central tendency of the RT distribution but also the scale and shape of the distribution. Indeed, a number of reports show that the traditional \"collapse to a mean\" approach can miss phenomena when a variable decreases the central tendency but increases skew.</p>\\n\\n<p>For those interested in characterizing the full nature of the RT distribution (or at least, features of the distribution beyond central tendency), there are three general approaches:</p>\\n\\n<ol>\\n<li>Quantify the distribution via quantiles (typically <code>seq(.1,.9,.2)</code>) and add quantile to the list of fixed effect variables in your analysis. This requires that you have some method for dealing with the continuous-yet-likely-nonlinear nature of quantile as a variable; I like generalized additive mixed effects modelling for this purpose.</li>\\n<li>Choose an a priori distribution form (ex. Ex-Gaussian, Wald, Weibull, etc) and attempt to estimate the best fitting parameters of this distribution (and the effect of your experimental variables thereon) given the data. While some folks will obtain parameter estimates per cell of the experimental design then submit the estimates to ANOVA, but where the assumption of Gaussian error may not be appropriate for these parameter estimates, I\\'d say that hierachircal modelling (as in <a href=\"http://pcl.missouri.edu/apps?q=node/36\" rel=\"nofollow\">Rouder et al 2005</a>) is a better approach.</li>\\n<li>Choose an a priori process model (ex. diffusion, linear ballistic accumulator, etc) and fit it repeatedly to the data, comparing the quality of the resulting fits as a function of whether you let certain parameters of interest to vary as a function of the experimental design. Again, this is best done in a hierarchical manner.</li>\\n</ol>\\n\\n<p>Personally, I like #3 but suggest that #1 should always be done as well to guard against the possibility that the process model isn\\'t appropriate.</p>\\n\\n<p>Now, how to do any of the above and account for outliers (fast and slow) is another matter, though Trisha Van Zandt had a neat talk at the SCiP meeting last year where she showed an approach where fast and slow outliers were explicitly modelled with a priori distributions (her modelling also accounted for serial correlations in RTs nicely). </p>\\n',\n",
       " \"<p>I am conducting a 2x2 within-subjects design. When I plot my results it appears that there will be an interaction between my variables but unfortunately none is emerging p=0.08. I find it a shame that I can't explore this further with simple main effects. Does anyone have any suggestions?</p>\\n\",\n",
       " 'Intervention analysis estimates the effect of an external intervention on a time-series.',\n",
       " '<p>If you are using R, SPSS or Stata, you can look at the <a href=\"http://home.wanadoo.nl/john.hendrickx/statres/perturb/\" rel=\"nofollow\"><code>perturb</code></a> package. It diagnoses collinearity by adding random noise to continuous variables; for categorical variables, some are changed to different categories.</p>\\n\\n<p>In the documentation for <code>perturb</code> in R, it notes that the model need not be <code>lm</code>, implying that any model (including ones built with optimal scaling or ordinal logistic) could be used. </p>\\n',\n",
       " '<p>There is a significant 3-way interaction in a data-set I\\'m working with.</p>\\n\\n<p>The interaction involves both categorical and quantitative variables.</p>\\n\\n<p>I have been pointed towards simple slopes and this <a href=\"http://www.jeremydawson.co.uk/slopes.htm\" rel=\"nofollow\">website</a> but I find the explanations lacking. I only have a basic background in statistics, and Googling for other examples has not been very helpful. </p>\\n\\n<p>Any insight as to how to begin to understand this interaction will be very welcome, especially using R rather than SPSS. Thank you very much in advance, from a lower-year college student who has suddenly found himself over his head!</p>\\n',\n",
       " '<p>Starting with a Binomial Distribution with parameters $n=1000, p=0.5$ and measured successes of 300, I would like to test whether there is a significant difference between success and failure.</p>\\n\\n<p>The obvious solution (using R):</p>\\n\\n<pre><code>n1 &lt;- 300\\nn2 &lt;- 700\\np &lt;- 0.5\\nbinom.test(n1, n1+n2, p, alternative=\\'two.sided\\')\\n</code></pre>\\n\\n<p>I also want to use the similarity of the Binomial and Normal Distribution for \"large\" numbers of observations. A trivial solution may be this:</p>\\n\\n<pre><code>t.test(c(rep(0, n1), rep(1, n2)), mu=p, alternative=\\'two.sided\\')\\n</code></pre>\\n\\n<p>Properties of a Normal Distribution based on a Binomial Distribution can be calculated directly:</p>\\n\\n<pre><code>mu &lt;- (n1+n2)*p\\nsig2 &lt;- p*(1-p)*(n1+n2)\\n</code></pre>\\n\\n<p>Therefore it should be possible to simply apply a one-sample t-test. After some trial-and-error I got this solution:</p>\\n\\n<pre><code>t &lt;- (n2-mu)/sqrt(sig2)\\np.value &lt;- 2*abs(1-pt(abs(t), n1+n2-1))\\n</code></pre>\\n\\n<p>Luckily the results are rather similar. </p>\\n\\n<p>I do not understand why the t-test stated for example in Wikipedia, where an additional $\\\\\\\\sqrt{n_1+n_2}$ is used, does not produce the right result:</p>\\n\\n<pre><code>t.wrong &lt;- sqrt(n1+n2)*(n2-mu)/sqrt(sig2)\\n</code></pre>\\n\\n<p>Why do I have to omit this part of the tests formula?</p>\\n',\n",
       " '<p>In fact there are two types of factors -- ordered (like Tiny &lt; Small &lt; Medium &lt; Big &lt; Huge) and unordered (Cucumber, Carrot, Fennel, Aubergine).<br>\\nFirst class is the same as continuous ones -- there is only easier to check all pivots, there is also no problem with extending levels list.<br>\\nFor the second class, you have to make a set of elements that will be directed in one branch, leaving the rest into the other -- in this case you can either:</p>\\n\\n<ol>\\n<li>throw error </li>\\n<li>assume that unseen class goes into your favorite branch</li>\\n<li>treat this as NA and select branch in more-less random way.</li>\\n</ol>\\n\\n<p>Now, direct treating of unordered factors is tricky so many algorithms \"cheat\" and claim unordered factors as ordered ones, so they don\\'t even touch this problem*. The rest usually use int mask, i.e. optimize an integer number from $1$ to $2^{\\\\\\\\text{#categories}+1}-2$ (or $0$ to $2^{\\\\\\\\text{#categories}+1}-1$) and treat $i$-th bit as a branch selection for a factor level $i$. (Ever wondered why there is frequent limit to 32 levels?) In this setting, it is quite natural that unseen levels go silently into \"0\" branch. Yet this does not seem too \"right\", because why really should we do this? And what about the number of levels required to entropy leverage in attribute selection?</p>\\n\\n<p>I would say the the most sensible idea is to make the user define the full set of factors (for instance R does this organically, preserving levels through subset operations) and use option 1. for not-declared levels and option 2. for declared ones. Option 3. may make sense if you already have some NA handling infrastructure.</p>\\n\\n<p>*) There is also side strategy to do some non-trivial re-coding of levels into numbers, like for instance Breiman encoding -- yet this generates even more problems.</p>\\n',\n",
       " '<p>I have two vectors $x,y \\\\\\\\in \\\\\\\\mathbb{R}.$ Based on the count and vector length I can compute $p(x)$ and $p(y)$ but I have no information on the joint density. How can I calculate mutual information in this case?</p>\\n',\n",
       " '<h3>Context</h3>\\n\\n<p>I was talking to a researcher in the following situation.</p>\\n\\n<ul>\\n<li>Participants (n = 500) were sampled from schools.</li>\\n<li>Participants came from around 50 different schools.</li>\\n<li>The number of participants per school varied with some schools supplying 20 or 30 participants, but a few schools only supplying 3 or 4 or 5 participants.</li>\\n<li>The researcher was trying to assess whether there was a substantial violation of the independence of observations assumption. Thus, they were looking at the intra-class correlation of core outcome variables based on the effect of school.</li>\\n</ul>\\n\\n<h3>Question</h3>\\n\\n<ul>\\n<li>Is it problematic to include groups in an intra-class correlation analysis with small numbers per group? If so, what are the implications of this? What might a rule of thumb regarding a minimum number for inclusion be?</li>\\n<li>To take it to the extreme, what is some groups only supplied a single participant?</li>\\n<li>If there are groups with samples sizes below a given threshold, what is a good subsequent course of action? remove group from analysis? collapse small groups into an \"other\" group?</li>\\n<li>How would any recommendations given relate to the assessment of the independence of observations assumption?</li>\\n</ul>\\n',\n",
       " \"<p>Some of the features I am working with has more than 300 factor levels. I tried to reduce their number of levels with a 'fake dummy' method. For example, I replaced one 600-level predictor with 4 predictors (as $600&lt;5^4$). Is this a valid approach?</p>\\n\",\n",
       " \"<p>I started to play with <code>naiveBayes</code> function from e1071 package. It looks simple on small test examples. But it performs poor on my actual task. I have ~4000 observation belonging to 2 classes and described by ~19000 numeric variables. The entire dataset was split on training and test sets, and naiveBayes model have been produced for the training set.</p>\\n\\n<p>The prediction performance of the model on training and test sets was poor (~55%), much less than Random Forest model for example (~80%). Almost all observation were classified as class 2, while the dataset is balanced and ratio of class 1 to class 2 is almost 1:1.</p>\\n\\n<p>What can be the reason for such a bad performance? How can I improve the model and perdiction results? As I'm relatively new in Bayes approach any suggestion will be helpful.</p>\\n\\n<p>As I understood initial assumption of Bayes approach is independence of variables. But I'm sure that many variables in my dataset are highly correlated. Can it be the source of the problem? Should I use variable selection with Naive Bayes?</p>\\n\",\n",
       " '<p>Failing to look at (plot) the data.</p>\\n',\n",
       " \"<p>I have 31 observations for 9 subjects undergoing therapy (4 session for 4 subjects, and 3 session for the rest).  I have implemented a mixed model to assess relationships between the observations using Fisher-transformed correlation coefficients.  </p>\\n\\n<p>For example, to assess the effect of finger tapping on correct responses, I correlated the number of finger taps with the percentage of correct responses for each subject (3 pairs of observations for some subjects, and 4 for others).  I then perform a Fisher-transform on the 9 resulting correlation coefficients to get 9 z-scores.  Then it's a simple matter of testing whether or not those z-scores differ from 0.</p>\\n\\n<p>What I would like to do is control for subjects' age.  However, I'm not clear on the best way to do this.  It's easy to calculate the covariance between the z-scores and subject age, but how do I get the effect of finger taps on correct responses, controlling for age?  </p>\\n\\n<p>(As a side note, I've implemented this as a MIXED model in SPSS, but it fails to converge, so I can't trust the results).  </p>\\n\",\n",
       " \"<p>I'd like to use Bayesian updating to form price expectations to be used in another model. I'm very new to this area, so your help would be highly appreciated. </p>\\n\\n<p>I'm not sure which distribution to use for the prior. When I use normal distribution, I do not get much variability between prior and posterior. However it would be better if there is more variability.</p>\\n\\n<p>Would it better to use beta or Gaussian distributions for the prior? If so, can I use java for coding? Again because of the model limitations, I cannot use winbugs. </p>\\n\\n<p>Many thanks! </p>\\n\\n<p>Erin </p>\\n\",\n",
       " '<p>You are probably over impression from the classical modelling, which is vulnerable to the <a href=\"http://en.wikipedia.org/wiki/Runge%27s_phenomenon\" rel=\"nofollow\">Runge paradox</a>-like problems and thus require some parsimony tuning in post-processing.<br>\\nHowever, in case of machine learning, the idea of including robustness as an aim of model optimization is just the core of the whole domain (often expressed as accuracy on unseen data). So, well, as long as you know your model works good (for instance from CV) there is probably no point to bother. </p>\\n\\n<p>The real problem with $p\\\\\\\\gg n$ in case of ML are the irrelevant attributes -- mostly because some set of them may become more usable for regenerating decision than the truly relevant ones due to some random fluctuations. Obviously this issue has nothing to do with parsimony, but, same as in classical case, ends up in terrible loss of generalization power. How to solve it is a different story, called feature selection -- but the general idea is to pre-process the data to kick out the noise rather than putting constrains on the model.</p>\\n',\n",
       " '<p>The <i>Nguyen-Widrow</i> initialization algorithm is the following :</p>\\n\\n<ol>\\n<li>Initialize all weight of hidden layers with (ranged) random values<br></li>\\n<li>For each hidden layer<br>\\n2.1 calculate beta value, 0.7 * Nth(#neurons of input layer) root of\\n#neurons of current layer <br>\\n2.2 for each synapse<br>\\n2.1.1 for each weight <br>\\n2.1.2 Adjust weight by dividing by norm of weight for neuron and\\nmultiplying by beta value</li>\\n</ol>\\n\\n<p><a href=\"http://code.google.com/p/encog-java/source/browse/branches/2.5.0/encog-core/src/org/encog/mathutil/randomize/NguyenWidrowRandomizer.java\" rel=\"nofollow\">Encog Java Framework</a></p>\\n',\n",
       " '<p>Python will give you all the flexibility you need. With the NumPy and <a href=\"http://docs.scipy.org/doc/scipy/reference/cluster.html\" rel=\"nofollow\">SciPy cluster module</a> you have the tools you need, and the datatypes of NumPy give you a good insight in how much memory you will use.</p>\\n',\n",
       " \"<p>I have some non Gaussian distributed variable and I need to check if there are significant differences between the values of this variable in 5 different groups. I have performed Kruskal-Wallis one way analysis of variance (which came up significant) and after that I had to check which groups were significantly different. Since the groups are kind of sorted (the values of the variable in the first group are supposed to be lower than the values of the variable in the second group which are supposed to be lower than the values of the variable in the third group and so on) I only performed 4 test:</p>\\n\\n<pre><code>Group 1 vs Group 2\\nGroup 2 vs Group 3\\nGroup 3 vs Group 4\\nGroup 4 vs Group 5\\n</code></pre>\\n\\n<p>I have performed this analysis with two different methods. I started by using the Dunn's Multiple Comparison Test but nothing came up significant. On the other hand if I use a Mann-Whitney test and correct for the number of tests (4) using Bonferroni, 3 tests come up significant. What does it mean? Which results should I trust?</p>\\n\\n<p>Any help is greatly appreciated.</p>\\n\",\n",
       " '<p>Mathematical statistics concentrates on theorems and proofs and mathematical rigor, like other branches of math. It tends to be studied in math departments, and mathematical statisticians often try to derive new theorems.</p>\\n\\n<p>\"Statistics\" includes mathematical statistics, but the other parts of the field tend to concentrate on more practical problems of data analysis and so on. </p>\\n',\n",
       " '<p>I have a data set of about 3,000 field observations. </p>\\n\\n<p>The data collected is divided into 20 variables (real numbers), 30 boolean variables, and 10 or so look up variables and one \"answer\" variable</p>\\n\\n<p>We have about 20,000 objects in the field, and i\\'m trying to produce an \"answer\" for the 20,000 objects based on the 3,000 observations.</p>\\n\\n<p>What are some of the available methods that incorporate booleans and look up tables?</p>\\n\\n<p>any suggestions on how i should proceed?</p>\\n\\n<p><strong>EDIT</strong></p>\\n\\n<p>the answer variable is a boolean as well</p>\\n\\n<p><strong>EDIT 2</strong></p>\\n\\n<p>a sample of the variable data:</p>\\n\\n<ul>\\n<li>Age of specimen</li>\\n<li>length, area, volume</li>\\n<li>time since last inspection</li>\\n<li>height</li>\\n<li>design life</li>\\n</ul>\\n\\n<p>Lookup table</p>\\n\\n<ul>\\n<li>material type </li>\\n<li>coating type</li>\\n<li>design standard</li>\\n<li>design effectiveness</li>\\n</ul>\\n\\n<p>a sample of the boolean</p>\\n\\n<ul>\\n<li>is it inspected?</li>\\n<li>is it in bad shape</li>\\n<li>does it need repairs soon</li>\\n</ul>\\n\\n<p>the answer variable which is my f(x) is:</p>\\n\\n<ul>\\n<li>is it useable</li>\\n</ul>\\n',\n",
       " '<p>The simplest approach to this problem is probably simulation. I would sample player skills $S_i$ from a standard normal distribution, then model individual match-ups by testing $S_1 - S_2 &gt; X$, where $X$ is a normal random variate with mean zero and variance $v$. The value of the parameter $v$ will determine the likelihood of weaker players beating stronger players, which you\\'ll have to tweak until the results seem reasonable for your scenario. If you have a large body of match records, you can probably use this to guide your estimate, either in an ad hoc way, or by using a system like <a href=\"http://research.microsoft.com/en-us/projects/trueskill/\" rel=\"nofollow\">TrueSkill</a>.</p>\\n\\n<p>You can then set up a number of different tournament forms, and run a couple of thousand simulations on each. That should give you a rough idea of how the various different forms compare. Intuitively though, I think your initial ideas are probably right, but how much accuracy you \\'sacrifice\\' in the Swiss/double-elimination system will probably depend strongly on the value of $v$.</p>\\n',\n",
       " '<p>I\\'m presuming the rows in the way you\\'ve presented data don\\'t necessarily mean anything ie there is no <em>necessary</em> link between the third yawn, third whisper, and third stretch.  What you are interested in with the third yawn is \"how close is this in time to <em>any</em> whisper - not just the third whisper\".</p>\\n\\n<p>For each yawn I would calculate the time to the nearest whisper and the time to the nearest stretch.  And similarly for each whisper (calculate time to nearest stretch and time to nearest yawn); and for each stretch.  Then I would calculate some kind of indicator statistics of the proximity of each behaviour to each of the other two - something like the trimmed mean distance in time to the nearest behaviour of the other type.  (There will be six of these indicators, not just three, because the average time from a yawn to its nearest stretch is not the same as the average time from a stretch to its nearest yawn.)</p>\\n\\n<p>This already will give you some sense of which behaviours are clustered together, but you also should check that this isn\\'t plausibly due just to chance.</p>\\n\\n<p>To check that, I would create simulated data generated by a model under the null hypothesis of no relation.  Doing this would require generating data for each behaviour\\'s time from a plausible null model, probably based on resampling the times between each event (eg between each yawn) to create a new set of time stamps for hypothetical null model events.  Then calculate the same indicator statistic for this null model and compare to the indicator from your genuine data. By repeating this simulation a number of times, you could find out whether the indicator from your data is sufficiently different from the null model\\'s simulated data (smaller average time from each yawn to the nearest stretch, for example) to count as statistically significant evidence against your null hypothesis.</p>\\n',\n",
       " '<p>I\\'m going to assume the quality control team just make a correct/incorrect judgement about each text box.  If so, this looks like a good candidate for a straightforward logistic regression model, with text box as one explanatory variable with ten levels, and team member as a second explanatory variable with 150 levels.  The response will be one if the entry is wrong, zero if it is right.</p>\\n\\n<p>The estimates for the coefficient for each of the various explanatory variables can then be used to identify which team members and which text boxes have higher error rates.  This should then be fed into an algorithm for determining weighting for the future sampling.  Weighted sampling is a straightforward technique.  You will need some limits on the weighting - you don\\'t want any text box to get below say 1/20 chance of being sampled, or any individual team member to get below 1/500 - so you choose an algorithm that sets those (or some other arbitrary levels) as the minimum for the weighting.</p>\\n\\n<p><em>Addition</em></p>\\n\\n<p>If the quality control team make a more nuanced judgement - say a rating on a four point scale from \"poor\" to \"perfect\" - then the procedure would be just the same as above but you could use <a href=\"http://en.wikipedia.org/wiki/Ordered_logit\" rel=\"nofollow\">ordinal logistic regression</a> instead.  The response variable would be this rating; the explanatory variables would still be the factors for team members and for text boxes.  You would still then need to use the estimated sizes of the effects for each level of the factor in an arbitrary algorithm that weights the chance of those text boxes and those users being included in the next sample.</p>\\n',\n",
       " '<p>I am having difficulties understanding the underlying logic in setting the <a href=\"http://en.wikipedia.org/wiki/Null_hypothesis\">null hypothesis</a>. In this <a href=\"http://stats.stackexchange.com/questions/12461/how-to-specify-the-null-hypothesis-in-hypothesis-testing\">answer</a> the obviously generally accepted proposition is stated that the null hypothesis is the hypothesis that there will be no effect, everything stays the same, i.e. nothing new under the sun, so to speak.</p>\\n\\n<p>The alternative hypothesis is then what you try to prove, that e.g. a new drug delivers on its promises.</p>\\n\\n<p>Now coming form science theory and general logic we know that we can only falsify propositions, we cannot prove something (no number of white swans can prove that all swans are white but one black swan can disprove it). This is why we try to disprove the null hypothesis, which is not equivalent to proving the alternative hypothesis - and this is where my skepticism starts - I will give an easy example:</p>\\n\\n<p>Let\\'s say I want to find out what kind of animal is behind a curtain. Unfortunately I cannot directly observe the animal but I have a test which gives me the number of legs of this animal. Now I have the following logical reasoning:</p>\\n\\n<blockquote>\\n  <p>If the animal is a dog then it will have 4 legs.</p>\\n</blockquote>\\n\\n<p>If I conduct the test and find out that it has 4 legs this is <em>no</em> proof that it is a dog (it can be a horse, a rhino or any other 4-legged animal). But if I find out that it has <em>not</em> 4 legs this is a definite proof that it can <em>not</em> be a dog (assuming a healthy animal).</p>\\n\\n<p>Translated into drug effectiveness I want to find out if the drug behind the curtain is effective. The only thing I will get is a number that gives me the effect. If the effect is positive, nothing is proved (4 legs). If there is no effect, I disprove the effectiveness of the drug.</p>\\n\\n<p>Saying all this I think - contrary to common wisdom - the only valid null hypothesis must be</p>\\n\\n<blockquote>\\n  <p>The drug is effective (i.e.: if the drug is effective you will see an effect).</p>\\n</blockquote>\\n\\n<p>because this is the only thing that I can disprove - up to the next round where I try to be more specific and so on. So it is the null hypothesis that states the effect and the alternative hypothesis is the default (<em>no effect</em>).</p>\\n\\n<p><strong>Why is it that statistical tests seem to have it backwards?</strong></p>\\n\\n<p><strong>P.S.</strong>: You cannot even negate the above hypothesis to get a valid equivalent hypothesis, so you <em>cannot</em> say \"The drug is <em>not</em> effective\" as a null hypothesis because the only logically equivalent form would be \"if you see <em>no</em> effect the drug will <em>not</em> be effective\" which brings you nowhere because now the conclusion is what you want to find out!</p>\\n\\n<p><strong>P.P.S.</strong>: Just for clarification after reading the answers so far: If you accept scientific theory, that you can only falsify statements but not prove them, the only thing that is logically consistent is choosing the null hypothesis as the new theory - which can then be falsified. Because if you falsify the status quo you are left empty handed (the status quo is disproved but the new theory far from being proved!). And if you fail to falsify it you are in no better position either.</p>\\n',\n",
       " '<p>See Theorem 1 given in <a href=\"http://www.ism.ac.jp/editsec/aism/pdf/037_3_0541.pdf\">Moschopoulos</a> (1985) for the distribution of a sum of independent gamma variables.  You can extend this result using the <a href=\"http://en.wikipedia.org/wiki/Gamma_distribution#Scaling\">scaling property</a> for linear combinations.</p>\\n',\n",
       " \"<p>I don't know of any papers on this.  I've used this approach, for descriptive purposes.  DFA provides a nice way to summarize group differences and dimensionality with respect to the original variables.  One might more easily just profile the groups on the original variables, however, this loses the inherently multivariate nature of the clustering problem.  DFA allows you to describe the groups while keeping the multivariate character of the problem intact.  So, it can assist with the interpretation of the clusters, where that is a goal.  This is particularly ideal when there is a close relationship between your clustering method and your classification method--e.g., DFA and Ward's method. </p>\\n\\n<p>You are right about the testing problem.  I published a paper using the Cluster Analysis with DFA follow-up to describe the clustering solution.  I presented the DFA results with no test statistics.  A reviewer took issue with that.  I conceded and put the test statistics and p values in there, with the disclaimer that these p-values should not be interpreted in the traditional manner.  </p>\\n\",\n",
       " '<p>Fun question.</p>\\n\\n<p>Someone found out I work in biostatistics, and they asked me (basically) \"Isn\\'t statistics just a way of lying?\"</p>\\n\\n<p>(Which brings back the Mark Twain quote about Lies, Damn Lies, and Statistics.)</p>\\n\\n<p>I tried to explain that statistics allows us to say with 100 percent precision that, given assumptions, and given data, that the probability of such-and-so was exactly such-and-such.</p>\\n\\n<p>She wasn\\'t impressed.</p>\\n',\n",
       " '<p>The Steam site uses a Flash plugin to draw graphs. If you want to draw using statistical data, use R software. If you are expert at Javascript, you\\'d better use <a href=\"http://www.sencha.com/\" rel=\"nofollow\">extjs</a> to draw your graph.</p>\\n',\n",
       " '<p>Using sparse matrix objects in svm training in e1071 returns different results than running on the same data represented as standard matrix:</p>\\n\\n<pre><code>library(e1071)\\nlibrary(Matrix)\\nlibrary(SparseM)\\nm=10\\nn=800\\n#means &lt;- colMeans(myData)\\n#stdDevs &lt;- apply(myData, 2, sd)\\nmeans &lt;- c(0.042154664, 0.010474473, 0.106408354, 0.002237226, 0.089791084, \\n0.072792224, 0.001884146, 0.002925725, 0.010788693, 0.151466160)\\nstdDevs &lt;- c(0.017026132, 0.012720986, 0.026721416, 0.004810966, 0.026454962, \\n0.025349870, 0.004165095, 0.005573776, 0.009063219, 0.036368062)\\nclss = rep(0,n)\\nclss[sample(1:n,n/2)] &lt;- 1\\nclss &lt;- factor(clss)\\nftrs &lt;- matrix(nrow=n,ncol=m)\\nfor (i in seq(m)){ftrs[,i] &lt;- rnorm(n,means[i],stdDevs[i])}\\nftrs[ftrs&lt;0] &lt;- 0\\nfor (i in seq(m)){ftrs[sample(1:n,(n/10)),i]&lt;-0}\\nftrs.csr &lt;- as.matrix.csr(ftrs)\\nftrs.Mtrx &lt;- Matrix(ftrs, sparse=TRUE)\\nmod1 &lt;- svm(ftrs,clss, kernel=\\'linear\\')\\nmod2 &lt;- svm(ftrs.csr,clss, kernel=\\'linear\\')\\nmod3 &lt;- svm(ftrs.Mtrx,clss, kernel=\\'linear\\')\\nprf(data.frame(fitted(mod1),clss))\\nprf(data.frame(fitted(mod2),clss))\\nprf(data.frame(fitted(mod3),clss))\\n</code></pre>\\n\\n<p>The above code simulates a subset of one of my feature sets pretty closely, and if you run it repeatedly, the prf results will sometimes be very close and sometimes be very different, i.e. but they are never matching:</p>\\n\\n<pre><code>&gt; prf(data.frame(fitted(mod1),clss))\\n         Acc   P_0       R_0       F_0    P_1       R_1       F_1\\n[1,] 0.52125 0.675 0.5162524 0.5850488 0.3675 0.5306859 0.4342688\\n&gt; prf(data.frame(fitted(mod2),clss))\\n        Acc   P_0       R_0       F_0  P_1       R_1       F_1\\n[1,] 0.5025 0.775 0.5016181 0.6090373 0.23 0.5054945 0.3161512\\n&gt; prf(data.frame(fitted(mod3),clss))\\n        Acc   P_0       R_0       F_0  P_1       R_1       F_1\\n[1,] 0.5025 0.775 0.5016181 0.6090373 0.23 0.5054945 0.3161512\\n\\n&gt; prf(data.frame(fitted(mod1),clss))\\n       Acc  P_0      R_0     F_0  P_1       R_1      F_1\\n[1,] 0.545 0.61 0.539823 0.57277 0.48 0.5517241 0.513369\\n&gt; prf(data.frame(fitted(mod2),clss))\\n         Acc    P_0       R_0       F_0   P_1    R_1       F_1\\n[1,] 0.50375 0.2225 0.5085714 0.3095652 0.785 0.5024 0.6126829\\n&gt; prf(data.frame(fitted(mod3),clss))\\n         Acc    P_0       R_0       F_0   P_1    R_1       F_1\\n[1,] 0.50375 0.2225 0.5085714 0.3095652 0.785 0.5024 0.6126829\\n</code></pre>\\n\\n<p>Is this something I\\'m doing incorrectly? Or is training on sparse matrix representations just more inexact?</p>\\n\\n<p>Thanks,\\nRob</p>\\n\\n<p>prf Function:</p>\\n\\n<pre><code>prf &lt;- function(predAct){\\n    ## predAct is two col dataframe of pred,act\\n    preds = predAct[,1]\\n    trues = predAct[,2]\\n    xTab &lt;- table(preds, trues)\\n    clss &lt;- as.character(sort(unique(preds)))\\n    r &lt;- matrix(NA, ncol = 7, nrow = 1, \\n        dimnames = list(c(),c(\\'Acc\\',\\n        paste(\"P\",clss[1],sep=\\'_\\'), \\n        paste(\"R\",clss[1],sep=\\'_\\'), \\n        paste(\"F\",clss[1],sep=\\'_\\'), \\n        paste(\"P\",clss[2],sep=\\'_\\'), \\n        paste(\"R\",clss[2],sep=\\'_\\'), \\n        paste(\"F\",clss[2],sep=\\'_\\'))))\\n    r[1,1] &lt;- sum(xTab[1,1],xTab[2,2])/sum(xTab) # Accuracy\\n    r[1,2] &lt;- xTab[1,1]/sum(xTab[,1]) # Miss Precision\\n    r[1,3] &lt;- xTab[1,1]/sum(xTab[1,]) # Miss Recall\\n    r[1,4] &lt;- (2*r[1,2]*r[1,3])/sum(r[1,2],r[1,3]) # Miss F\\n    r[1,5] &lt;- xTab[2,2]/sum(xTab[,2]) # Hit Precision\\n    r[1,6] &lt;- xTab[2,2]/sum(xTab[2,]) # Hit Recall\\n    r[1,7] &lt;- (2*r[1,5]*r[1,6])/sum(r[1,5],r[1,6]) # Hit F\\n    r}\\n</code></pre>\\n',\n",
       " '<p>I am trying to work out how to propagate standard deviations in a biological experiment, but I have some difficulties. </p>\\n\\n<p>I have the following (semi)fictitious data originating from an image analysis resulting in optical density (OD) measurements:</p>\\n\\n<pre><code>ds &lt;- data.frame(\\n    Sample = c(rep(seq(1,6),2)),\\n    Control.OD = c(7000, 7100, 5600, 5200, 5900, 7000, \\n                   8100, 7700, 6100, 5500, 6600, 7500), \\n    Target.OD  = c(1000,  330,   35, 9300, 5570, 8700, \\n                   1300,  400,   62, 9100, 5817, 9000),\\n    Group = c(1,1,1,2,2,2,1,1,1,2,2,2)\\n)\\n\\nprint(ds)\\n\\n   Sample Control.OD Target.OD Group\\n1       1       7000      1000     1\\n2       2       7100       330     1\\n3       3       5600        35     1\\n4       4       5200      9300     2\\n5       5       5900      5570     2\\n6       6       7000      8700     2\\n7       1       8100      1300     1\\n8       2       7700       400     1\\n9       3       6100        62     1\\n10      4       5500      9100     2\\n11      5       6600      5817     2\\n12      6       7500      9000     2\\n</code></pre>\\n\\n<p>The data represents duplicate measurements from one experiment with 6 biological samples from two different groups.</p>\\n\\n<p>I do the following:</p>\\n\\n<pre><code># Aggregate technical duplicates\\nds.Agg &lt;- aggregate(cbind(Control.OD, Target.OD) ~ Sample + Group, \\n                    data = ds, mean)\\n\\n# Rename columns for easier reading\\nnames(ds.Agg)[names(ds.Agg)==\"Control.OD\"] &lt;- \"Control.mean\"\\nnames(ds.Agg)[names(ds.Agg)==\"Target.OD\"] &lt;- \"Target.mean\"\\n\\n# Calculate initial ratio between the target and the control\\n# This is the value that represents each biological sample\\nds.Agg$Ratio &lt;- ds.Agg$Target.mean / ds.Agg$Control.mean \\n\\n# Calculate the group means and normal sd (no propagation)\\nds.group &lt;- aggregate(Ratio ~ Group, data = ds.Agg, mean)\\nds.group$sd &lt;- aggregate(Ratio ~ Group, data = ds.Agg, sd)$Ratio\\nds.group$sd.rel &lt;- ds.group$sd / ds.group$Ratio * 100\\n\\n# Print results for classical way of doing this (no propagation)\\nformat(ds.group, digits=3)\\n\\n  Group Ratio     sd sd.rel\\n1     1  0.07 0.0742  106.0\\n2     2  1.28 0.4080   31.8    \\n</code></pre>\\n\\n<p>A final read-out would be:</p>\\n\\n<pre><code>library(ggplot2) # version 0.9.0   \\n\\npng(type = \"windows\", file = \"c:/temp/propagation1.png\",width=140,height=200)\\nggplot(ds.group, mapping = aes(x = factor(Group), y = Ratio)) +\\n  geom_bar(width = 0.8, position=\"dodge\") +\\n  geom_errorbar(mapping = aes(ymin = Ratio - sd, ymax = Ratio + sd), \\n                width = 0.40, position=position_dodge(width = 0.8)) +\\n  scale_x_discrete(\"Group\") +\\n  scale_y_continuous(limits = c(-0.1, 1.7)) +\\n  theme_bw()\\nmessage(dev.off())\\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/RLQzm.png\" alt=\"No propagation\"></p>\\n\\n<p>Then an attempt on propagation:</p>\\n\\n<pre><code># Calculate sd for technical duplicates which will be propagated\\nsdTmp &lt;- aggregate(cbind(Control.OD, Target.OD) ~ Sample + Group, data=ds, sd) \\nds.Agg$Control.sd &lt;- sdTmp$Control.OD\\nds.Agg$Target.sd &lt;- sdTmp$Target.OD\\n\\n# Calculate the relative propagated sd associated with the calculated Ratio\\nfltRelativeSd &lt;- with(ds.Agg, \\n    sqrt( (Control.sd / Control.mean)^2 + (Target.sd / Target.mean)^2 ) \\n)\\nds.Agg$Ratio.sd &lt;- ds.Agg$Ratio * fltRelativeSd\\nds.Agg$Ratio.sd.rel &lt;- ds.Agg$Ratio.sd / ds.Agg$Ratio * 100\\n</code></pre>\\n\\n<p><strong>The follow code is my biggest concern when it comes to calculation of the propagated standard deviation:</strong></p>\\n\\n<pre><code># Calculate propagated sd for the groups\\nds.group$sd.prop &lt;- aggregate(Ratio.sd ~ Group, data = ds.Agg, \\n    function(x) { \\n        sqrt( sum((x)^2) ) / length(x);\\n    })$Ratio\\n</code></pre>\\n\\n<p><strong>to here.</strong></p>\\n\\n<pre><code>ds.group$sd.prop.rel &lt;- ds.group$sd.prop / ds.group$Ratio * 100\\n</code></pre>\\n\\n<p>Re-print the results with the propagated standard deviations:</p>\\n\\n<pre><code>format(ds.group, digits=3)\\n\\n  Group Ratio     sd sd.rel sd.prop sd.prop.rel\\n1     1  0.07 0.0742  106.0  0.0111       15.79\\n2     2  1.28 0.4080   31.8  0.0418        3.26\\n</code></pre>\\n\\n<p>And a graph:</p>\\n\\n<pre><code>png(type = \"windows\", file = \"c:/temp/propagation2.png\",width=140,height=200)\\nggplot(ds.group, mapping = aes(x = factor(Group), y = Ratio)) +\\n  geom_bar(width = 0.8, position=\"dodge\") +\\n  geom_errorbar(mapping = aes(ymin = Ratio - sd.prop, ymax = Ratio + sd.prop), \\n                width = 0.40, position=position_dodge(width = 0.8)) +\\n  scale_x_discrete(\"Group\") +\\n  scale_y_continuous(limits = c(-0.1, 1.7)) +\\n  theme_bw()\\nmessage(dev.off())\\n</code></pre>\\n\\n<p>So a read-out for the propagation:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/1vFrk.png\" alt=\"With propagation\"></p>\\n\\n<p>I would have expected standard deviations in the same neighborhood as seen on the first graph, however they seem a factor 10 off.</p>\\n\\n<p>So my thoughts/questions at this point:</p>\\n\\n<ol>\\n<li>Am I doing the calculations right?</li>\\n<li>Does it make sense to do this calculation (it seems that the biological variance is ignored and only the initial duplicate sd is actually propagated and presented)?  </li>\\n</ol>\\n\\n<p>Any comments are most welcome.</p>\\n\\n<p>Additional info: The experiment uses a technique called <a href=\"http://en.wikipedia.org/wiki/Western_blot\" rel=\"nofollow\">Western Blotting</a> and the image analysis was done in ImageJ in a similar fashion as described <a href=\"http://lukemiller.org/index.php/2010/11/analyzing-gels-and-western-blots-with-image-j/\" rel=\"nofollow\">here</a>.</p>\\n',\n",
       " '<p>@whuber gave an exhaustive explanation, I just want to point out that there is a standard statistical distribution corresponding to this scenario: the <em>hypergeometric</em> distribution. So you can obtain any such probabilities directly in, say, R:</p>\\n\\n<p>Probability of exactly 2 out of 12 selected:</p>\\n\\n<pre><code>   &gt; dhyper(2, 12, 363-12, 232)\\n   [1] 0.0008498838\\n</code></pre>\\n\\n<p>Probability of 2 or fewer out of 12 selected:</p>\\n\\n<pre><code>   &gt; phyper(2, 12, 363-12, 232)\\n   [1] 0.000934314\\n</code></pre>\\n',\n",
       " '<blockquote>\\n  <p>Good statistics involves principled argument that conveys an interesting and credible point.</p>\\n</blockquote>\\n\\n<p>-- Robert P. Abelson, (1995) <em>\"Statistics as Principled Argument\"</em></p>\\n\\n<blockquote>\\n  <p>We left in our mathematical model a gap for the exercise of a more intuitive process of personal judgement</p>\\n</blockquote>\\n\\n<p>-- Egon Pearson, quoted in Abelson (1995).</p>\\n',\n",
       " \"<p>The Behrens-Fisher distribution is defined by $t_2\\\\\\\\cos\\\\\\\\theta - t_1\\\\\\\\sin\\\\\\\\theta$ where $\\\\\\\\theta$ is a real number and $t_2$ and $t_1$ are independent $t$-distributions with degrees of freedom $\\\\\\\\nu_2$ and $\\\\\\\\nu_1$ respectively.</p>\\n\\n<p>Behrens and Fisher's solution of the Behrens-Fisher problem involves the Behrens-Fisher distribution with $\\\\\\\\theta$ depending on the observations because it is a pseudo-Bayesian (in fact, a fiducial) solution: this data-depending distribution is a posterior-like distribution of $\\\\\\\\tau$ (with $\\\\\\\\delta$ the only random part in the definition of $\\\\\\\\tau$ because the data are fixed).</p>\\n\",\n",
       " '<p>I\\'ve used a wide array of tests for my thesis data, from parametric ANOVAs and t-tests to non-parametric Kruskal-Wallis tests and Mann-Whitneys, as well as rank-transformed 2-way ANOVAs, and GzLMs with binary, poisson and proportional data. Now I need to report everything as I write all of this up in my results.</p>\\n\\n<p>I\\'ve already asked <a href=\"http://stats.stackexchange.com/questions/11381/how-to-report-asymmetrical-confidence-intervals-of-a-proportion\">here</a> how to report asymmetrical confidence intervals for proportion data. I know that standard deviation, standard error or confidence intervals are appropriate for means, which is what I\\'d report if all my tests were nicely parametric. However, for my non-parametric tests, should I be reporting medians and not means? If so, what error would I report with it?  </p>\\n\\n<p>Associated with this is how best to present non-parametric test results graphically. Since I largely have continuous or interval data within categories, I\\'m generally using bar graphs, with the top of the bar being the mean and error bars showing 95% CI. For NP tests, can I still use bar graphs, but have the top of the bar represent the median?</p>\\n\\n<p>Thanks for your suggestions!</p>\\n',\n",
       " '<p>In an AR model the coefficients on the lags can be solved for using least squares.  How is the MA part of ARMA solved for?  Since the MA part is a sum of white noise terms I imagine that it is not solved using least squares.  </p>\\n',\n",
       " '<h3>Solution</h3>\\n\\n<p>When you assume the residuals (vertical deviations in a graph of $n$ data) are independently and identically distributed with some normal distribution of zero mean, the estimate of the slope will have a Student t distribution with $n-2$ degrees of freedom, scaled by the standard error.  Because the theoretical value has essentially zero error, we can ignore this complication and treat the theoretical value as a constant.  Therefore we refer the ratio</p>\\n\\n<p>$$t = (0.0106623 - 0.0075) / 0.0011 = 2.88$$</p>\\n\\n<p>to <a href=\"http://en.wikipedia.org/wiki/Student%27s_t-distribution#Table_of_selected_values\" rel=\"nofollow\">Student\\'s t distribution</a> (as a <em>two sided test,</em> because in principle the slope could have been greater or less than the theoretical value and you just want to see whether the difference could be attributed to chance).</p>\\n\\n<p>Whether this deviation is \"significant\" depends on your criterion for significance and on the degrees of freedom.  For example, if you want 95% or greater significance, then this difference will be significant if and only if you have six or more data values.  This conclusion follows from noting that the 95% two-sided critical value with $5-2 = 3$ degrees of freedom is $3.182$, greater than $2.88$, and the critical value with $6-2 = 4$ d.f. is $2.776$, less than $2.88$.</p>\\n\\n<h3>Discussion</h3>\\n\\n<p><em>If</em> the uncertainty in the theoretical value were appreciable compared to the standard error of the slope ($0.0011$) <em>and</em> you had relatively few data points (perhaps 10 or fewer), the problem would become more difficult:</p>\\n\\n<ul>\\n<li><p>First, you don\\'t know the distribution of the theoretical error.  </p></li>\\n<li><p>Second, you probably don\\'t know for sure that it is a standard error (people often report confidence limits or two or three standard errors or even standard deviations without clearly specifying what they have computed).</p></li>\\n<li><p>Third, the sum of a t-distributed value (your error) and another distribution (the theoretical error) can have a mathematically less tractable distribution.</p></li>\\n</ul>\\n\\n<p>Mitigating these complications, though, is a simple consideration: if the theoretical uncertainty were largish, then it would add to the overall uncertainty in the difference between the theoretical and estimated values, thereby lowering the t-statistic.  In some cases such a semi-quantitative result might be good enough.  (The addition is in terms of variances: you sum the squares of the two standard errors, obtaining the square of the standard error of the difference, and (therefore) take its square root.)</p>\\n\\n<p>For instance, if the theoretical uncertainty were equal to the uncertainty of the estimate, the t-statistic would be reduced to $2.03$.  The distribution of the difference would be approximately Normal, but with slightly longer tails, so referring the value of $2.03$ to a standard Normal distribution would slightly overestimate the significance.  Well, we can compute that $4.2\\\\\\\\%$ of the standard Normal distribution is more extreme than $\\\\\\\\pm 2.03$.  Thus--still in this hypothetical situation with a largish standard error for the theoretical result--you would <em>not</em> conclude the difference is significant if your criterion for significance exceeds $100 - 4.2 = 95.8\\\\\\\\%$.  Otherwise, the picture is murky and the determination depends on the resolution of the difficulties enumerated above.</p>\\n',\n",
       " '<p>A random variable usually has a definition like \"a function that maps from the original sample space to a new sample space, typically a set of real numbers\". Is it ever useful to map into a new space that isn\\'t a set of real numbers?</p>\\n',\n",
       " '<p>I think that \"periods\" in history are closely related to people and their developments. Of course one can expect \"waves\" in Toffler\\'s sense, but even those waves are related to persons. </p>\\n\\n<p>Anyway, wikipedia has an <a href=\"http://en.wikipedia.org/wiki/Timeline_of_probability_and_statistics\" rel=\"nofollow\">article</a> in this regard. </p>\\n',\n",
       " \"<p>For a very thorough explanation, see David Belsley's books, either (with Kuh and Welsch) <em>Regression Diagnostics</em> or <em>Conditioning Diagnostics</em>.  The latter book is more concentrated on condition indices, but it's older and out of print.  The former covers more types of problems in regression, but should give you plenty of info on conditioning indices.</p>\\n\",\n",
       " '<blockquote>\\n  <p><strong>Possible Duplicate:</strong><br>\\n  <a href=\"http://stats.stackexchange.com/questions/11691/how-to-tell-if-data-is-clustered-enough-for-clustering-algorithms-to-produce-m\">How to tell if data is &ldquo;clustered&rdquo; enough for clustering algorithms to produce meaningful results?</a>  </p>\\n</blockquote>\\n\\n\\n\\n<p>I have used hierarchical clustering, e.g, Ward\\'s method, single,complete, etc. I have the same problem, I do not know how to assess my clustering algorithm. How would I know that the clusters are meaningful?</p>\\n\\n<p>I do not know what FPC software is, can any one use FPC, and, what is the programming language of this software?</p>\\n\\n<p>If I\\'m not familiar with FPC, is there another means of assessment?</p>\\n',\n",
       " '<p>Salford Analytics and Data Mining Conference 2012\\n<a href=\"http://www.salforddatamining.com/index.php\" rel=\"nofollow\">http://www.salforddatamining.com/index.php</a></p>\\n',\n",
       " '<p>It doesn\\'t make sense to both include tank as a random effect and nest tank within the pop/temp fixed effect.  You only need one of these, depending on how tank is coded.</p>\\n\\n<p>If tank is coded 1-8, you only need the tank random effect.  Nesting it within the pop/temp fixed effect results in the same 8 units, so is not necessary.</p>\\n\\n<p>If tank is coded 1-2 (that is, which rep it was), you only need to nest tank within the pop/temp fixed effect, because that gives you your 8 unique tanks.  Including the tank random effect is only desired if the tanks were first divided into two groups and then randomized to treatment; if the eight tanks were completely randomized to treatment, this is not necessary. </p>\\n\\n<p>You could do this with likelihood based solutions such those in <code>nlme</code> and <code>lme4</code> but if everything is balanced, it might be simpler to use the traditional ANOVA approach using <code>aov</code>.</p>\\n\\n<p>Creating some sample data:</p>\\n\\n<pre><code>set.seed(5)\\nd &lt;- within(expand.grid(pop=factor(c(\"A\",\"B\")),\\n                        temp=factor(c(\"warm\", \"cold\")),\\n                        rep=1:2,\\n                        fish=1:100), {\\n                          tank &lt;- factor(paste(pop, temp, rep, sep=\".\"))\\n                          tanke &lt;- round(rnorm(nlevels(tank))[unclass(tank)],1)\\n                          e &lt;- round(rnorm(length(pop)),1)\\n                          m &lt;- 10 + 2*as.numeric(pop)*as.numeric(temp)\\n                          growth &lt;- m + tanke + e\\n                        })\\n</code></pre>\\n\\n<p>Using <code>aov</code> like this:</p>\\n\\n<pre><code>a0 &lt;- aov(growth ~ pop*temp + Error(tank), data=d)\\nsummary(a0)\\n</code></pre>\\n\\n<p>or <code>lme</code> like this:</p>\\n\\n<pre><code>library(nlme)\\nm1 &lt;- lme(growth ~ pop*temp, random=~1|tank, data=d)\\nanova(m1)\\n</code></pre>\\n',\n",
       " '<p>Random assignment is valuable because it ensures independence of treatment from potential outcomes.  That is how it leads to unbiased estimates of the average treatment effect.  But other assignment schemes can also systematically ensure independence of treatment from potential outcomes.  So why do we need random assignment?  Put another way, what is the advantage of random assignment over nonrandom assignment schemes that also lead to unbiased inference?</p>\\n\\n<p>Let $\\\\\\\\mathbf{Z}$ be a vector of treatment assignments in which each element is 0 (unit not assigned to treatment) or 1 (unit assigned to treatment).  In a JASA article, <a href=\"http://people.ucsc.edu/~cdobkin/Classes/Reza/Identification%20of%20Causal%20Effects%20Using%20Instrumental%20Variables%20%28Angrist%29.pdf\">Angrist, Imbens, and Rubin (1996, 446-47)</a> say that treatment assignment $Z_i$ is random if $\\\\\\\\Pr(\\\\\\\\mathbf{Z} = \\\\\\\\mathbf{c}) = \\\\\\\\Pr(\\\\\\\\mathbf{Z} = \\\\\\\\mathbf{c\\'})$ for all $\\\\\\\\mathbf{c}$ and $\\\\\\\\mathbf{c\\'}$ such that $\\\\\\\\iota^T\\\\\\\\mathbf{c} = \\\\\\\\iota^T\\\\\\\\mathbf{c\\'}$, where $\\\\\\\\iota$ is a column vector with all elements equal to 1.</p>\\n\\n<p>In words, the claim is that assignment $Z_i$ is random if any vector of assignments that includes $m$ assignments to treatment is as likely as any other vector that includes $m$ assignments to treatment.</p>\\n\\n<p>But, to ensure independence of potential outcomes from treatment assignment, it suffices to ensure that each unit in the study has equal probability of assignment to treatment.  And that can easily occur even if most treatment assignment vectors have <em>zero</em> probability of being selected.  That is, it can occur even under nonrandom assignment.  </p>\\n\\n<p>Here is an example. We want to run an experiment with four units in which exactly two are treated.  There are six possible assignment vectors:</p>\\n\\n<ol>\\n<li>1100</li>\\n<li>1010</li>\\n<li>1001</li>\\n<li>0110</li>\\n<li>0101</li>\\n<li>0011</li>\\n</ol>\\n\\n<p>where the first digit in each number indicates whether the first unit was treated, the second digit indicates whether the second unit was treated, and so on.</p>\\n\\n<p>Suppose that we run an experiment in which we exclude the possibility of assignment vectors 3 and 4, but in which each of the other vectors has equal (25%) chance of being chosen.  This scheme is not random assignment in the AIR sense.  But in expectation, it leads to an unbiased estimate of the average treatment effect.  And that is no accident.  Any assignment scheme that gives subjects equal probability of assignment to treatment will permit unbiased estimation of the ATE.</p>\\n\\n<p>So: why do we need random assignment in the AIR sense?  My argument is rooted in randomization inference; if one thinks instead in terms of model-based inference, does the AIR definition seem more defensible?</p>\\n',\n",
       " '<p>Generally, by not allowing for assymetry, you expect the effect of shocks to last longers: i.e. the half-life increases (the half life is the number of units of time, after a 1 S.D. shock to $\\\\\\\\epsilon_{t-1}$ for $\\\\\\\\hat{\\\\\\\\sigma}<em>t|I</em>{t-1}$ to come back to the its unconditional value.) </p>\\n\\n<p>Here is a code snipped that downloads stock data, fits (e)Garch and computes half lifes, in R:</p>\\n\\n<pre><code>install.packages(\"rgarch\",repos=\"http://R-Forge.R-project.org\")\\ninstall.packages(\"fGarch\")\\ninstall.packages(\"fImport\")\\nlibrary(rgarch)\\nlibrary(fImport)\\nlibrary(fGarch)\\nd1&lt;-yahooSeries(symbols=\"ibm\",nDaysBack=1000,frequency=c(\"daily\"))[,4]\\ndprice1&lt;-diff(log(as.numeric(d1[length(d1):1])))\\nspec1&lt;-ugarchspec(variance.model=list(model=\"eGARCH\",garchOrder=c(1,1)),mean.model=list(armaOrder=c(0,0),include.mean=T))\\nspec2&lt;-ugarchspec(variance.model=list(model=\"fGARCH\",submodel=\"GARCH\",garchOrder=c(1,1)),mean.model=list(armaOrder=c(0,0),include.mean=T))\\nfit1&lt;-ugarchfit(data=dprice1,spec=spec1)\\nfit2&lt;-ugarchfit(data=dprice1,spec=spec2)\\nhalflife(fit1)\\nhalflife(fit2)\\n</code></pre>\\n\\n<p>The reason for this is that generally speaking, negative spells tend to be more persistent.\\nIf you don\\'t control for this, you will generally bias the $\\\\\\\\beta$ (i.e. persistance parameters) downwards.</p>\\n',\n",
       " '<p>When you say \\'a absolute loss function\\', do you mean you\\'re using <a href=\"http://en.wikipedia.org/wiki/Least_absolute_deviations\" rel=\"nofollow\">least absolute deviations</a> (LAD) instead of the more usual ordinary least squares (OLS)? As that wikipedia article says, although LAD is more robust to outliers than OLS it can be unstable and even have multiple solutions, so it doesn\\'t seem that surprising if it\\'s harder to find the minimum of the objective function even when there\\'s only one.</p>\\n\\n<p>If you\\'re trying this because you\\'re after some sort of <a href=\"http://en.wikipedia.org/wiki/Robust_regression\" rel=\"nofollow\">robust regression</a>, I think there are several more attractive alternatives than LAD.</p>\\n',\n",
       " '<p>If it\\'s on Linux, then the most straight-forward is <a href=\"http://cran.r-project.org/web/packages/multicore/index.html\"><strong>multicore</strong></a>.  Beyond that, I suggest having a look at <a href=\"http://www.stats.uwo.ca/faculty/yu/Rmpi/\">MPI</a> (especially with the <a href=\"http://cran.r-project.org/web/packages/snow/index.html\"><strong>snow</strong></a> package).</p>\\n\\n<p>More generally, have a look at:</p>\\n\\n<ol>\\n<li>The <a href=\"http://cran.r-project.org/web/views/HighPerformanceComputing.html\">High-Performance Computing view</a> on CRAN.</li>\\n<li><a href=\"http://cran.r-project.org/web/views/HighPerformanceComputing.html\">\"State of the Art in Parallel Computing with R\"</a></li>\\n</ol>\\n\\n<p>Lastly, I recommend using the <a href=\"http://cran.r-project.org/web/packages/foreach/index.html\">foreach</a> package to abstract away the parallel backend in your code.  That will make it more useful in the long run.</p>\\n',\n",
       " '<p>What is the simplest script for generating a beanplot (see <a href=\"http://www.jstatsoft.org/v28/c01/paper\">article</a> published in the <em>Journal of Statistical Software</em>) in Matlab?</p>\\n',\n",
       " '<p>I\\'ve often found the Engineering Statistics Handbook useful. It can be found <a href=\"http://www.itl.nist.gov/div898/handbook/\">here</a>.  </p>\\n\\n<p>Although I\\'ve never read it myself, I hear <a href=\"http://www.lulu.com/items/volume_68/8123000/8123594/3/print/IPSUR.pdf\">Introduction to Probability and Statistics Using R</a> is very good. It\\'s a full ~400 page ebook (also available as an actual book). As a bonus, it also teaches you R, which of course you want to learn anyways.</p>\\n',\n",
       " \"<p>There are several nonparametric methods for discriminant analysis: rank methods, classifiers based on robust estimators of location and scale (M-estimators or MCD-estimators, for instance), and so on. Have you decided what kind of method that you want to use?</p>\\n\\n<p>As for the impact of non-normality on LDA and QDA, I'd recommend that you have a look at the following paper:</p>\\n\\n<p>Lachenbruch, P. A., Sneeringer, C., and Revo, L. T. (1973). Robustness of the linear and quadratic discriminant function to certain types of non-normality, <em>Comm. Statist.</em>, 1(1):39–56.</p>\\n\",\n",
       " \"<p>I've run a Tobit regression but I'm not sure of how I should interpret the coefficients. Do you have any suggestions? Thank you very much!</p>\\n\",\n",
       " '<p>N itself is often set in an arbitrary manner that makes sense for the application. Frankly the alpha = 2/(N+1) is also somewhat arbitrary, <a href=\"http://www.itl.nist.gov/div898/handbook/pmc/section4/pmc431.htm\" rel=\"nofollow\">http://www.itl.nist.gov/div898/handbook/pmc/section4/pmc431.htm</a> suggests instead doing an optimization that minimizes the mean squared error. (Nevertheless, alpha = 2/(N+1) seems to be fairly standard practice.)</p>\\n\\n<p>So as to how n should depend on m, or vice versa, I say try to find something that makes sense given your particular data and that will help with interpreting your results. I wouldn\\'t worry about overlap too much, as it\\'s not like the data is independent--that\\'s why you do the EMA in the first place. </p>\\n',\n",
       " '<p>I conducted a computer-based assessment of different methods of fitting a particular type of model used in the palaeo sciences. I had a large-ish training set and so I randomly (stratified random sampling) set aside a test set. I fitted $m$ different methods to the training set samples and using the $m$ resulting models I predicted the response for the test set samples and computed a RMSEP over the samples in the test set. This is a single <strong>run</strong>.</p>\\n\\n<p>I then repeated this process a large number of times, each time I chose a different training set by randomly sampling a new test set.</p>\\n\\n<p>Having done this I want to investigate if any of the $m$ methods has better or worse RMSEP performance. I also would like to do multiple comparisons of the pair-wise methods.</p>\\n\\n<p>My approach has been to fit a linear mixed effects (LME) model, with a single random effect for <strong>Run</strong>. I used <code>lmer()</code> from the <strong>lme4</strong> package to fit my model and functions from the <strong>multcomp</strong> package to perform the multiple comparisons. My model was essentially</p>\\n\\n<pre><code>lmer(RMSEP ~ method + (1 | Run), data = FOO)\\n</code></pre>\\n\\n<p>where <code>method</code> is a factor indicating which method was used to generate the model predictions for the test set and <code>Run</code> is an indicator for each particular <strong>Run</strong> of my \"experiment\".</p>\\n\\n<p>My question is in regard to the residuals of the LME. Given the single random effect for <strong>Run</strong> I am assuming that the RMSEP values for that run are correlated to some degree but are uncorrelated between runs, on the basis of the induced correlation the random effect affords.</p>\\n\\n<p>Is this assumption of independence <em>between</em> runs valid? If not is there a way to account for this in the LME model or should I be looking to employ another type of statical analysis to answer my question?</p>\\n',\n",
       " \"<p>I've proposed using an anomaly detection algorithm in a project. </p>\\n\\n<p>The algorithm would consist of choosing some features we think might be indicative of anomalous examples.  Then using a training set of anomalous and non-anomalous examples to fit parameters to a Gaussian distribution.  We would then use these parameters to create a probability function p where p(x) &lt; epsilon for some epsilon would indicate an anomaly.</p>\\n\\n<p>How would I answer this challenge?</p>\\n\\n<blockquote>\\n  <p>“We love this concept, but how does this differ from us just doing some statistics on a set of results?”</p>\\n</blockquote>\\n\\n<p>By just doing some statistics, I assume the challenger means detecting the anomalies manually in old data and coming up with a set of conditions that would indicate anomalies in future data.</p>\\n\",\n",
       " \"<p>I have a sequence of observations and I would like to determine if the observations in the sequence are mutually independent. Wald-Wolfowitz is a non-parametric test that can be used to check for randomness in the sequence. I think I have a general idea of how to formulate the problem, but don't want to sprain my brain if it's been done before. [My literature searches and Google searches didn't turn up anything, but I may not be asking the question properly.] \\nThanks in advance. </p>\\n\",\n",
       " '<p>I have tested male and female participants in one visual search task. </p>\\n\\n<p>The task included 3 different types of displays (1 factor, 3 levels); meaning I have 3 means for each male or female participant. So the design is 2 (gender: f/m) x 3 (RT: M1/M2/M3). Is this analysis called a repeated measure ANOVA? </p>\\n',\n",
       " '<p>You could try:</p>\\n\\n<p>Kohn R., Schimek M.G., Smith M. (2000) Spline and kernel regression for dependent data. In Schimekk M.G. (Ed) (2000) Smoothing and Regression: approaches, computation and application. John Wiley &amp; Sons, Inc.</p>\\n\\n<p>I used an example from that chapter to illustrate one of the <a href=\"http://wp.me/pZRQ9-30\" rel=\"nofollow\">problems of smoothing correlated data</a> on my blog a while back.</p>\\n',\n",
       " \"<p>First off, I apologize if it seems like I'm flooding the board, but I'm new to modelling and a lot of the questions that are coming up aren't closely related to one another, so it seems better to put them in their own threads. </p>\\n\\n<p>I'm trying to predict win percentage of a particular team in a particular season in baseball. I have three independent variables. One of the independent variables is the win percentage of the team of interest in the previous season. The other two independent variables are quite similar to this one in that they are in their own right reasonable predictions. </p>\\n\\n<p>Call the target $Y$ and the independent variables $X1$, $X2$, $X3$. </p>\\n\\n<p>I've run simple linear regressions <code>Y ~ X1</code>, <code>Y ~ X2</code>, and <code>Y ~ X3</code>. I want to compare the predictive abilities of these models. Is $R^2$ a good method to do so? If I have, say, that </p>\\n\\n<p>$$R^2(\\\\\\\\text{Y ~ X1}) &gt; R^2(\\\\\\\\text{Y ~ X2}) &gt; R^2(\\\\\\\\text{Y ~ X3})$$</p>\\n\\n<p>is it reasonable to say that $X1$ provides the best prediction, then $X2$, then $X3$? </p>\\n\\n<p>I've read a lot of conflicting things about the value of $R^2$, so I'm not sure. </p>\\n\\n<p>Since I can't get a very coherent model by multilinear regression of $Y$ against $X1$, $X2$, and $X3$, since they are all highly correlated, I've instead decided I would try a stacked regression to combine the simple linear regressions.</p>\\n\\n<p>As far as I understand, this requires me to choose one subset of my data $S_o$ to use in  performing the simple linear regressions for each predictor, then choose another, disjoint subset of my data $S_1$ to use in performing a multilinear regression for $Y$ with the simple linear regressions as my independent variables, with the restriction that the regression coefficients be non-negative. This multilinear regression is the stacked regression.</p>\\n\\n<p>What are your thoughts on using stacked regression in this situation? </p>\\n\\n<p>How do I compare the predictive ability of the stacked regression to that of the simple linear regressions? I know that $R^2$ can depend on model complexity, so I don't know how reasonable it would be to simple compare the $R^2$ value of the stacked regression to those of the simple linear regressions. Is there a reasonably simple way to figure out a range for the actual amount each of the predictions will err with some confidence? </p>\\n\\n<p>There is also the problem of deciding what proportion of my data should be in $S_o$ and what proportion should be in $S_1$ when I perform the stacked regression. Opinions? </p>\\n\\n<p>EDIT: I'm starting to think I may have misunderstood what's involved in the stack process, because after doing what I described, the resulting predictor is worse than the originals. Any ideas as to the problem?  </p>\\n\",\n",
       " \"<p>Let $X_1,X_2,\\\\\\\\cdots ,X_j \\\\\\\\cdots $ be i.i.d. $\\\\\\\\mathcal N(0, 1)$ random variables. Show that\\nfor any $a &gt; 0$, $$\\\\\\\\lim_{n\\\\\\\\to \\\\\\\\infty}P\\\\\\\\left( \\\\\\\\sum_{i=1}^n X_i^2\\\\\\\\leq a\\\\\\\\right)=0$$ \\nIt is clear that $\\\\\\\\sum_{i=1}^n X_i^2 \\\\\\\\sim \\\\\\\\chi ^2_n$; then, I can't proceed. Please help.</p>\\n\",\n",
       " '<p>I would like to manipulate the work done by the loess function in R.  However, the main workhorse of this function is written in C.  Is there some pure R implementation of the code?</p>\\n\\n<p>Thanks.</p>\\n',\n",
       " '<p>If you are using SPSS, then use the GLM under the \"Analyse\" tab. GLM allows you to see the main effects and the interaction of your predictor variables on your response variable. </p>\\n',\n",
       " '<p>\"Bootstrap validation\"/\"resampling cross-validation\" is new to me, but was discussed by the answer to <a href=\"http://stats.stackexchange.com/questions/11602\">this question</a>. I gather it involves 2 types of data: the real data and simulated data, where a given set of simulated data is generated from the real data by resampling-with-replacement until the simulated data has the same size as the real data. I can think of two approaches to using such data types: (1) fit the model once, evaluate it many times on many simulated data sets; (2) fit the model many times using each of many simulated data sets, each time evaluate it against the real data. Which (if either) is best?</p>\\n',\n",
       " '<p>As I see it there are two survival analysis paradigms that could be used. The Cox regression framework allows time varying covariates and would produce an estimate for the  risk of cancellation conditioned on any particular set of covariates relative to the mean level of cancellation. The glm framework with Poisson errors is also a proportional hazards model and is particularly suited to discrete intervals. JVM has pointed out that there is potential error in using incomplete data in the current month, but the sense I get is that you want an estimate that is conditional on the latest value of a co-variate or set of covariates. Better description of the data situation could yield better worked examples....</p>\\n',\n",
       " \"<p>I don't know if I could list them all.  The exponential, normal and binomial come to mind and they all fall into the class of exponential families.  The exponential family has its sufficient statistic in the exponent and the mle is often a nice function of this sufficient statistic.</p>\\n\",\n",
       " '<p><em>Maximum Likelihood Estimation</em> of Weibull parameters may be a good idea in your case. A form of Weibull distribution looks like this:</p>\\n\\n<p>$$(\\\\\\\\gamma / \\\\\\\\theta) (x)^{\\\\\\\\gamma-1}\\\\\\\\exp(-x^{\\\\\\\\gamma}/\\\\\\\\theta)$$</p>\\n\\n<p>Where $\\\\\\\\theta, \\\\\\\\gamma &gt; 0$ are parameters. Given observations $X_1, \\\\\\\\ldots, X_n$, the log-likelihood function is </p>\\n\\n<p>$$L(\\\\\\\\theta, \\\\\\\\gamma)=\\\\\\\\displaystyle \\\\\\\\sum_{i=1}^{n}\\\\\\\\log f(X_i| \\\\\\\\theta, \\\\\\\\gamma)$$</p>\\n\\n<p>One \"programming based\" solution would be optimize this function using constrained optimization. Solving for optimum solution:</p>\\n\\n<p>$$\\\\\\\\frac {\\\\\\\\partial \\\\\\\\log L} {\\\\\\\\partial \\\\\\\\gamma} = \\\\\\\\frac{n}{\\\\\\\\gamma} + \\\\\\\\sum_1^n \\\\\\\\log x_i - \\\\\\\\frac{1}{\\\\\\\\theta}\\\\\\\\sum_1^nx_i^{\\\\\\\\gamma}\\\\\\\\log x_i = 0 $$\\n$$\\\\\\\\frac {\\\\\\\\partial \\\\\\\\log L} {\\\\\\\\partial \\\\\\\\theta} = -\\\\\\\\frac{n}{\\\\\\\\theta} + \\\\\\\\frac{1}{\\\\\\\\theta^2}\\\\\\\\sum_1^nx_i^{\\\\\\\\gamma}=0$$ </p>\\n\\n<p>On eliminating $\\\\\\\\theta$ we get:</p>\\n\\n<p>$$\\\\\\\\Bigg[ \\\\\\\\frac {\\\\\\\\sum_1^n x_i^{\\\\\\\\gamma} \\\\\\\\log x_i}{\\\\\\\\sum_1^n x_i^{\\\\\\\\gamma}} - \\\\\\\\frac {1}{\\\\\\\\gamma}\\\\\\\\Bigg]=\\\\\\\\frac{1}{n}\\\\\\\\sum_1^n \\\\\\\\log x_i$$</p>\\n\\n<p>Now this can be solved for ML estimate $\\\\\\\\hat \\\\\\\\gamma$. This can be accomplished with the aid of standard iterative procedures which solve are used to find the solution of equation such as -- Newton-Raphson or other numerical procedures.</p>\\n\\n<p>Now $\\\\\\\\theta$ can be found in terms of $\\\\\\\\hat \\\\\\\\gamma$ as:</p>\\n\\n<p>$$\\\\\\\\hat \\\\\\\\theta = \\\\\\\\frac {\\\\\\\\sum_1^n x_i^{\\\\\\\\hat \\\\\\\\gamma}}{n}$$</p>\\n',\n",
       " '<p>I am learning about <a href=\"http://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis\" rel=\"nofollow\">pLSA (Probabilistic Latent Semantic Analysis)</a> right now, in the hopes of being able to apply it to biomolecular annotation prediction.</p>\\n\\n<p>I have a very simple question: <strong>How do you choose the number of topics / classes to use in the algorithm?</strong> I\\'ve searched also literature but I did not find anything enough useful.</p>\\n',\n",
       " '<p>When reporting a two way 2 x 3 mixed ANOVA I have recently heard that the interaction effect should only be reported if the two main effects are significant (APA style). Is this correct? </p>\\n',\n",
       " '<p>It seems that the only solution is to normalize all label values to 0...1 and then add all the label values. The biggest sum is the one that we will use to find the attribute to split the node.</p>\\n',\n",
       " '<p><a href=\"http://www.mathworks.com/products/matlab/\" rel=\"nofollow\">MATLAB</a> is a high-level language and interactive programming environment developed by <a href=\"http://www.mathworks.com/\" rel=\"nofollow\">MathWorks</a>. It is the foundation for a number of other tools related to statistical analysis, including <a href=\"http://www.mathworks.com/products/statistics\" rel=\"nofollow\">Statistics Toolbox</a>, <a href=\"http://www.mathworks.com/products/optimization\" rel=\"nofollow\">Optimization Toolbox</a>, <a href=\"http://www.mathworks.com/products/curvefitting\" rel=\"nofollow\">Curve Fitting Toolbox</a> and <a href=\"http://www.mathworks.com/products/neural-network\" rel=\"nofollow\">Neural Network Toolbox</a>.</p>\\n\\n<p>Some informative links that show up quite often in answers:</p>\\n\\n<ul>\\n<li><a href=\"http://www.mathworks.com/access/helpdesk/help/techdoc/\" rel=\"nofollow\">Online MATLAB documentation</a> for the most recent version (R2011b)</li>\\n<li><a href=\"http://www.mathworks.com/matlabcentral/fileexchange/\" rel=\"nofollow\">MATLAB Central File Exchange</a>: a repository of user-contributed MATLAB tools</li>\\n<li><a href=\"http://blogs.mathworks.com/\" rel=\"nofollow\">MATLAB Central blogs</a>: usually updated weekly by a number of MathWorks employees</li>\\n<li><a href=\"http://www.mathworks.com/matlabcentral/answers/\" rel=\"nofollow\">MATLAB Answers</a>: a collaborative environment for getting answers to questions about MathWorks products, similar to Stack Overflow</li>\\n<li>comp.soft-sys.matlab Usenet group: can be accessed online through the\\n<a href=\"http://www.mathworks.com/matlabcentral/newsreader/\" rel=\"nofollow\">MATLAB Central Newsreader</a> hosted by MathWorks, or using <a href=\"http://groups.google.com/group/comp.soft-sys.matlab\" rel=\"nofollow\">Google Groups</a></li>\\n</ul>\\n',\n",
       " '<p>Hello and thanks in advance for your help! </p>\\n\\n<p>I have conducted and EFA for a scale I made that includes 13 items.  Of these items, 2 are recoded.  Looking at the rotated component matrix I see that this scale has 2 factors, one with 11 items and one with the 2 recoded items.  What does this mean and how do I decribe this?  Does anyone know of a journal article or citation that deals with this sort of issue.</p>\\n',\n",
       " \"<p>I am conducting an experiment where I will measure the strength of a material used for roads. Ideally, I would like to use a large surface area to measure this but this takes too long and is really expensive. I therefore will scale it down to one area which is $50cm^2$ and one which is $12cm^2$. I then want to compare the data I get from both these samples and see how (if) they differ. For the smaller sample, I'll also be doing less observations, i.e I'll get around 30 values for the $50cm^2$ one and only like 3 or 4 for the $12cm^2$ due to how much it costs.</p>\\n\\n<p>I was going to use the Normal Distribution, but as I won't have a lot of data, I can't justify using the Normal Distribution through the Central Limit Theorem.</p>\\n\\n<p>Would I be able to use a non-parametric test like the Sign test? Or, would it actually be something like the Wilcoxon signed rank test as the data is paired isn't it? How would I compare the data from there?</p>\\n\\n<p>Thank you</p>\\n\",\n",
       " '<p>I am trying to apply the idea of mutual information to feature selection, as described in <a href=\"http://cs229.stanford.edu/notes/cs229-notes5.pdf\">these lecture notes</a> (on page 5).</p>\\n\\n<p>My platform is Matlab. One problem I find when computing mutual information from empirical data is that the number is always biased upwards. I found about 3~4 different files to calculate MI on Matlab Central and they all give big numbers (like > 0.4) when I feed in independent random variables. </p>\\n\\n<p>I am not an expert, but the problem seems to be that if you simply use joint and marginal densities to compute MI, bias is introduced in the process because MI is by definition positive. Does anyone have practical advice on how to estimate mutual information accurately?</p>\\n\\n<p>A related question is, in practice, how do people actually use MI to select features? It is not obvious to me how to come up with a threshold value since MI is in theory unbounded. Or do people just rank the features by MI and take the top k features?</p>\\n',\n",
       " '<p>EDIT: I have solved this problem myself. The problem with the simulation below is that the omitted variable should not be included in the \\'true model\\'. I have written a blog post with a more detailed analysis <a href=\"http://diffuseprior.wordpress.com/2012/08/15/probit-models-with-endogeneity/\" rel=\"nofollow\">here</a>.</p>\\n\\n<p>I am trying to calculate the Average Structural Function (ASF) for a binary response regression model with an endogenous variable. The ASF is known as the policy relevant result obtained from these models because it shows how the conditional probability of the outcome (one or zero) changes in response to changes in any of the explanatory variables.</p>\\n\\n<p>To estimate the regression model, I have used a two-step control function approach, wherein the first stage regression residuals ($\\\\\\\\textbf{v}_{i}$) are included as a right-hand-side variable in the second stage probit regression à la Rivers and Vuong (1988). </p>\\n\\n<p>Based on my reading of a paper by Blundell and Powell (2004) (and also <a href=\"http://www.cemfi.es/~arellano/binary-endogeneity.pdf\" rel=\"nofollow\">these lecture notes</a>) the ASF can be calculated as follows:</p>\\n\\n<p>$P(y|\\\\\\\\bar{\\\\\\\\textbf{X}},v)=\\\\\\\\widehat{ASF}=\\\\\\\\frac{1}{N}\\\\\\\\sum^{N}_{i} \\\\\\\\Phi(\\\\\\\\bar{\\\\\\\\textbf{X}}\\\\\\\\boldsymbol{\\\\\\\\hat{\\\\\\\\beta}}+\\\\\\\\rho \\\\\\\\hat{\\\\\\\\textbf{v}_{i}}) $</p>\\n\\n<p>where the $\\\\\\\\textbf{X}$ values are held at a constant level (say their mean), and we average over all of the first-stage residuals (multiplied by the second stage coefficient $\\\\\\\\rho$). In effect, this formalization will allow one to calculate how the probability of the outcome varies as the one of the x-variables changes, while all of the other values are (typically) held at their means.</p>\\n\\n<p>Or so you would think. However, I have attempted this calculation on a simple simulation with R and have not been able to replicate the ASF. My R code is below. Basically, this is a simple setup where we want to measure the effect of y1 on y2 (the binary outcome). There is one omitted variable (x1) that renders y1 endogenous the regression equation of interest.</p>\\n\\n<p>A picture of my attempt is:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/OZBA8.jpg\" alt=\"enter image description here\"></p>\\n\\n<p>When $x_1$ is available, everything should be fine. Just estimate a standard probit of $y_2$  on $x_1$ and $y_1$. The ASF for this is just the normal CDF for changes in $y_1$. When $x_1$ is not observed, it becomes necessary to instrument $y_1$. </p>\\n\\n<p>From the IV regression I have calculated the ASF as in the above, and plotted this with comparisons to the model where $x_1$ is observed (the blue line in the picture), and also where $x_1$ is not observed and $y_1$ is not instrumented (the green line).</p>\\n\\n<p>The red line is my attempt to construct the ASF from the method described in the above. It is clear that this line is not matching the blue line as it should. I have gone wrong somewhere here but I am not sure where. Would somebody be able to help me with this please? </p>\\n\\n<pre><code>rm(list=ls())\\nx1 &lt;- rnorm(10000)\\nx2 &lt;- rnorm(10000)\\ny1 &lt;- 1 + 0.5*x1 + x2 + rnorm(10000)\\ny2 &lt;- ifelse(0.5 + 0.5*y1 - 1.5*x1 + rnorm(10000) &gt; 0, 1, 0)\\n\\n# true\\nr1 &lt;- glm(y2~y1+x1,binomial(link=\"probit\"))\\ndata &lt;- data.frame(cbind(seq(-4,6,0.2),mean(x1)))\\nnames(data) &lt;- c(\"y1\",\"x1\")\\nasf1 &lt;- cbind(data$y1,pnorm(predict(r1,data)))\\nplot(asf1,type=\"l\",col=\"blue\",xlab=\"y1\",ylab=\"P(y2)\")\\n\\n# no endog correction\\nr2 &lt;- glm(y2~y1,binomial(link=\"probit\"))\\ndata &lt;- data.frame(cbind(seq(-4,6,0.2)))\\nnames(data) &lt;- c(\"y1\")\\nasf2 &lt;- cbind(data$y1,pnorm(predict(r2,data)))\\nlines(asf2,type=\"l\",col=\"green\")\\n\\n# control function approach\\nv1 &lt;- (residuals(lm(y1~x2)))/sd(residuals(lm(y1~x2)))\\nr3 &lt;- glm(y2~y1+v1,binomial(link=\"probit\"))\\n# proceedure to get asf\\nasf3 &lt;- cbind(seq(-4,6,0.2),NA)\\nfor(i in 1:dim(asf3)[1]){\\n    dat2 &lt;- data.frame(cbind(asf3[i,1],v1))\\n    names(dat2) &lt;- c(\"y1\",\"v1\")\\n    asf3[i,2] &lt;- mean(pnorm(predict(r3,dat2)))\\n}\\nlines(asf3,type=\"l\",col=\"red\")\\n</code></pre>\\n',\n",
       " \"<p>As I understand it, you've generally discretized to create a set of $n$ points, $x_1, \\\\\\\\dots, x_n$, with probability $p_1, \\\\\\\\dots, p_n$, and you then calculate the cumulative probabilities, say $c_i = \\\\\\\\sum_{j=1}^i p_j$.  So you can draw $U \\\\\\\\sim Uniform(0,1)$ and then take $X = x_{i^*}$ where $i^* = \\\\\\\\min_i \\\\\\\\{i:c_i \\\\\\\\ge U\\\\\\\\}$, or something like that.</p>\\n\\n<p>But your current problem is that the $p_i$ are so small that you want to just work with $a_i = \\\\\\\\log p_i$.  </p>\\n\\n<p>One approach would be to sort the $a_i$ from largest to smallest and then calculate partial sums using something like the following <code>addlog</code> function, which calculates $\\\\\\\\log(f + g)$ on the basis of $a = \\\\\\\\log(f)$ and $b = \\\\\\\\log(g)$.  </p>\\n\\n<pre><code>addlog(a, b, THRESH=200.0)\\n{\\n  if(b &gt; a + THRESH) return(b);\\n  else if(a &gt; b + THRESH) return(a);\\n  else return(a + log1p(exp(b-a)));\\n}\\n</code></pre>\\n\\n<p>where <code>log1p(x)</code> returns <code>log(1+x)</code>.</p>\\n\\n<p>But, really, I would think that you should focus on the $x_i$ for which $p_i$ is large enough that you don't need to worry about underflow, and neglect the $x_i$ with exceedingly small $p_i$.  If <em>all</em> of the $p_i$ are small, then it seems that you should grid more coarsely.  In most applications, it should be sufficient to discretize to 1000 or so values, I would think.</p>\\n\",\n",
       " '<p>For your two specific examples:</p>\\n\\n<p><strong>Linear Regression</strong>\\nThe paper <a href=\"http://research.yahoo.com/pub/2293\">\"Online Linear Regression and Its Application to Model-Based Reinforcement Learning\"</a> by Alexander Strehl and Michael Littman describes an algorithm called \"KWIK Linear Regression\" (see algorithm 1) which provides an approximation to the linear regression solution using incremental updates. Note that this is <a href=\"http://en.wikipedia.org/wiki/Tikhonov_regularization\"><em>not regularised</em></a> (i.e. it is not Ridge Regression). I\\'m pretty sure that the method of Strehl &amp; Littman cannot extend to that setting.</p>\\n\\n<p><strong>Logistic Regression</strong></p>\\n\\n<p><a href=\"http://www.talkstats.com/showthread.php/15743-Incremental-update-to-%2aexisting%2a-Logistic-Regression-model\">This thread</a> sheds some light on the matter. Quoting:</p>\\n\\n<blockquote>\\n  <p>Even without a regularization constraint, logistic regression is a nonlinear optimization problem. Already this does not have an analytic solution, which is usually a prerequisite to deriving an update solution. With a regularization constraint, it becomes a constrained optimization problem. This introduces a whole new set of non-analytic complications on top of the ones that the unconstrained problem already had.</p>\\n</blockquote>\\n\\n<p>There are however other online (or incremental) methods for regression that you might want to look at, for example <a href=\"http://wcms.inf.ed.ac.uk/ipab/slmc/research/software-lwpr\">Locally Weighted Projection Regression (LWPR)</a></p>\\n',\n",
       " \"<p>The ASA has a section on statistics in sports.  It cosponsors conferences like the one where you found Hwang's paper.  There are sessions and a luncheon talk every year at the JSM. There is now a journal on quantitative analysis in sports.  Serious articles for the layperson can be found in Chance Magazine and Significance Magazine.  I was chair of the sections program at the JSM in 2002.  Michael Schell has written two very serious books on baseball both published by Princeton University Press.  One is about greatest hitters of all time (adjusting for era and ballpark effects) and the other is about home run hitting (where the ballpark effect is taken int account).  Wizardry by Humphreys is an analysis of fielding skill in baseball.  Michael Lewis' Moneyball that was very popular and made into a movie is about how usign sabermetrics allowed a low market team like the Oakland As get talent to form a winning team without paying out the big bucks.</p>\\n\\n<p>Baseball is rich with data especially in the major leagues and now data is being collected and analyzed about every pitch and every park of the ballpark where a ball is hit.</p>\\n\\n<p>You will also find another statistician Jim Albert has written popular books on baseball including one he coauthored called Curve Ball.  There are now jobs in major league baseball for people like Bill James as well as for professionally trained statisticians.\\nBud Goode was one of the pioneers to consult with the Dallas Cowboys and other NFL teams years ago on football strategy based on simple regression methods.</p>\\n\\n<p>There are hired statisticians in the NBA, NFL and MLB and the use of statistics in sports is growing.  I have seen papers on tennis, golf, soccer and even cricket.  I have spanned the gamut of my knowledge on this.  I hope this has addressed your question.</p>\\n\",\n",
       " '<p>Both <em>x</em> and <em>y</em> will be correlated with <em>xy</em> (unless you have taken a specific measure to prevent this by using centering).  Thus if you obtain a substantial interaction effect with your approach, it will likely amount to one or more main effects masquerading as an interaction.  This is not going to produce clear, interpretable results.  What is desirable is instead to see how much the interaction can explain over and above what the main effects do, by including <em>x</em>, <em>y</em>, and (preferably in a subsequent step) <em>xy</em>.</p>\\n\\n<p>As to terminology:  yes, β 0 is called the \"constant.\"  On the other hand, \"partial\" has specific meanings in regression and so I wouldn\\'t use that term to describe your strategy here.</p>\\n\\n<p>Some interesting examples that will arise once in a blue moon are described <a href=\"http://stats.stackexchange.com/questions/11009/including-the-interaction-but-not-the-main-effects-in-a-model\">at this thread</a>.</p>\\n',\n",
       " '<p>\"One death is a tragedy, 100,000 deaths are statistics.\"</p>\\n\\n<p>Albert Szent-Gyorgyi </p>\\n',\n",
       " '<p>Say you run a regression (Y = Beta_0 + Beta_1 + GDP + ... ), and obtain a coefficient estimate for \"GDP\" of -6 with a standard error = 4.  Assume sample size is 250.</p>\\n\\n<p>How do you answer the question:</p>\\n\\n<blockquote>\\n  <p>What is the probability that the true (population) coefficient for GDP is greater\\n  than +1?</p>\\n</blockquote>\\n\\n<p>Normally, we set up a null hypothesis that the true coefficient is equal to zero and reject/fail to reject -- the \"significance test\" -- however, this approach doesn\\'t directly answer the question I posed above.  All p-values from a significance test are only relevant when you first assume that the null hypothesis is true, which I do not want to do.</p>\\n\\n<p>Any/all code welcome.</p>\\n',\n",
       " '<p>I\\'m trying to report the results of my chi-square test for independence in APA format but these results don\\'t resemble the examples I\\'ve seen especially the 4 digits before the period in the chi-square test results i.e. 9355.19 and 9556.44.  Am I doing this right?  </p>\\n\\n<blockquote>\\n  <p>\"the χ2 tests showed significant\\n  results in the association of this\\n  knowledge with years professional of\\n  experience χ2  (4, N = 302) = 9355.19,\\n  p = 0.83 and with professional level\\n  χ2 (4, N = 302) = 9556.44, p = 0.30.\"</p>\\n</blockquote>\\n\\n<p><strong>These are the values:</strong></p>\\n\\n<pre><code>         I agree I disagree I don\\'t know |Total\\nLevel 1      141         26           26 |  193\\nLevel 2       29          5           12 |   46 \\nLevel 3       43         10           10 |   63 \\n-----------------------------------------------\\nTotal        213         41           48 |  302\\n</code></pre>\\n\\n<p>Thank you very much for your help.  </p>\\n',\n",
       " '<p>The general sense in optimization is that if you have a convex function and no constraints, you want to use the \"powerful stuff\", gradient descent, Newton, etc. Without constraints interior point methods are not very good (competitive). </p>\\n\\n<p>In particular for the problem you\\'re studying (binary logistic regression) you should consider trying simple stochastic gradient descent.</p>\\n\\n<p>Nothing really stops you from applying constrained optimization techniques to unconstrained problems. The same way nothing stops from pushing (instead of riding) your car to work. But you should definitely try interior point methods w/o constraints and convince yourself about it.</p>\\n\\n<p>Finally you mention that you want to try linear programming-based methods presumably without constraints, what you plan to do in this case I don\\'t quite understand.</p>\\n',\n",
       " '<p>The issue is mainly that Bayesian analysis involves <em>integrals</em>, often multidimensional ones in realistic problems, and it\\'s these integrals that are typically intractable analytically (except in a few special cases requiring the use of conjugate priors). </p>\\n\\n<p>By contrast, much of non-Bayesian statistics is based on <a href=\"http://en.wikipedia.org/wiki/Maximum_likelihood\" rel=\"nofollow\">maximum likelihood</a> -- finding the maximum of a (usually multidimensional) function, which involves knowledge of its <a href=\"http://en.wikipedia.org/wiki/Derivative\" rel=\"nofollow\">derivatives</a>, i.e. differentiation. Even so numerical methods are used in many more complex problems, but it\\'s possible to get further more often without them, and the numerical methods can be simpler (even if less simple ones may perform better in practice).</p>\\n\\n<p>So I\\'d say it comes down to the fact that differentiation is more tractable than integration. </p>\\n',\n",
       " '<p>To get the factors, use:</p>\\n\\n<pre><code>cut(dataset, quantile(dataset))\\n</code></pre>\\n\\n<p>From the help:</p>\\n\\n<pre><code>## Default S3 method:\\ncut(x, breaks)\\n\\nx          a numeric vector which is to be converted to a factor by cutting.\\nbreaks   either a numeric vector of two or more cut points or a single number (greater than or equal to 2) giving the number of intervals into which x is to be cut.\\n</code></pre>\\n',\n",
       " '<p>Inter-market analysis is a method of modeling market behavior by means of finding relationships between different markets. Often times, a correlation is computed between two markets, say S&amp;P 500 and 30-Year US treasuries. These computations are more often than not based on price data, which is obvious to everyone that it does not fit the definition of stationary time series. </p>\\n\\n<p>Possible solutions aside (using returns instead), is the computation of correlation whose data is non-stationary even a valid statistical calculation?</p>\\n\\n<p>Would you say that such a correlation calculation is somewhat unreliable, or just plain nonsense?</p>\\n',\n",
       " '<p>We\\'ll start with two definitions: </p>\\n\\n<ul>\\n<li><p>A <a href=\"http://en.wikipedia.org/wiki/Probability_density_function\">probability density function (pdf)</a> is a non-negative function that integrates to $1$. </p></li>\\n<li><p>The likelihood is defined as the joint density of the observed data as a function of the parameter. But, as pointed out by the reference to Lehmann made by @whuber in a comment below, <strong>the likelihood function is a function of the parameter only, with the data held as a fixed constant.</strong> So the fact that it is a density as a function of the data is irrelevant. </p></li>\\n</ul>\\n\\n<p>Therefore, the likelihood function is not a pdf because <strong>its integral with respect to the parameter does not necessarily equal 1</strong> (and may not be integrable at all, actually, as pointed out by another comment from @whuber). </p>\\n\\n<p>To see this, we\\'ll use a simple example. Suppose you have a single observation, $x$, from a ${\\\\\\\\rm Bernoulli}(\\\\\\\\theta)$ distribution. Then the likelihood function is </p>\\n\\n<p>$$ L(\\\\\\\\theta) = \\\\\\\\theta^{x} (1 - \\\\\\\\theta)^{1-x} $$ </p>\\n\\n<p>It is a fact that $\\\\\\\\int_{0}^{1} L(\\\\\\\\theta) d \\\\\\\\theta = 1/2$. Specifically, if $x = 1$, then $L(\\\\\\\\theta) = \\\\\\\\theta$, so $$\\\\\\\\int_{0}^{1} L(\\\\\\\\theta) d \\\\\\\\theta = \\\\\\\\int_{0}^{1} \\\\\\\\theta \\\\\\\\  d \\\\\\\\theta = 1/2$$ </p>\\n\\n<p>and a similar calculation applies when $x = 0$. Therefore, $L(\\\\\\\\theta)$ cannot be a density function.</p>\\n\\n<p>Perhaps even more important than this technical example showing why the likelihood isn\\'t a probability density is to point out that <strong>the likelihood is <em>not</em> the probability of the parameter value being correct</strong> or anything like that - <strong>it is the probability (density) <em>of the data given the parameter value</em></strong>, which is a completely different thing. Therefore one should not expect the likelihood function to behave like a probability density. </p>\\n',\n",
       " '<p>The log model fails the lack of fit test by looking at the ANOVA, Pr(>F) > 0.05. By comparing it to the categorical model, you are basically doing a one way ANOVA (comparing SS_lack.of.fit to SS_pure.error[which is the categorical model]). The R^2 of the categorical model is naturally better because it consumes more degrees of freedom.</p>\\n',\n",
       " '<p>If you want to see if the first group fits a N(0,1) distribution, why not just do a goodness of fit test instead of creating an artificial N(0,1) data set to compare it with?  Although what you suggest doing is unconventional it is possible to do the two sample t test to see if the means are equal (essentially both 0).  Of course you might say that if the goal is to test for 0 mean why not just do the one sample test that the mean is zero rather than create the artificial second sample (which induces additional uncertainty and reduces the power of the test)?  It seems that alternative approaches provide better choices.</p>\\n',\n",
       " \"<p>I have two logistic regression models, using the same data set and same dependent binary variable but with different sample sizes due to different IV's. How would I go about comparing the two models aside from using a classification matrix?</p>\\n\",\n",
       " \"<p>Standard panel data representation assumes $Y=[y_{it}]_{i=1,...N,t=1,...,T}$ this is a $NT  $ rows vector. So your representation is an orthodox one. As long as your matrix representation retrieves your NT equations, then it's ok.</p>\\n\",\n",
       " \"<p>Posterior predictive checks, outlined in Gelman et al (1996), are an obvious starting point.  Given how simple the model is, it probably makes sense to use graphical checks.  Plot the histogram of $(y_1,...,y_n)$ against histograms of several posterior predictive replications $(y_1^{rep},...,y_n^{rep})$.  If you spot a feature that doesn't fit, you can formalize things by defining an appropriate discrepancy statistic and computing the posterior predictive p-value of your model against that statistic. </p>\\n\",\n",
       " '<p>When would one prefer to use a Conditional Autoregressive model over a Simultaneous Autoregressive model when modelling autocorrelated geo-referenced areal data?</p>\\n',\n",
       " '<p>These answers about selection of variables all assume that the cost of the observation of variables is 0.</p>\\n\\n<p>And that is not true. </p>\\n\\n<p>While the issue of selection of variables for a given model may or may not involve selection, the implications for future behavior DOES involve selection. </p>\\n\\n<p>Consider the problem of predicting which college lineman will do best in the NFL. You are a scout. You must consider which qualities of the current linemen in the NFL are most predictive of their success. You measure 500 quantities, and begin the task of the selection of the quantities which will be needed in the future. </p>\\n\\n<p>What should you do? Should you retain all 500? Should some (astrological sign, day of the week born on) be eliminated?</p>\\n\\n<p>This is an important question, and is not academic. There is a cost to the observation of data, and the framework of cost-effectiveness suggests that some variables NEED NOT be observed in the future, since their value is low.</p>\\n',\n",
       " \"<p>There are good answers here, but I am surprised that one thing has not been emphasized.  CART does not make any distributional assumptions about the data, particularly the response variable.  In contrast, OLS regression (for continuous response variables) and logistic regression (for certain categorical response variables), for example, <em>do</em> make strong assumptions; specifically, OLS regression assumes the response is normally distributed, and logistic assumes the response is binomial or multinomial.  </p>\\n\\n<p>CART's lack of such assumptions is a double-edged sword.  When those assumptions are not warranted, this gives the approach a relative advantage.  On the other hand, when those assumptions hold, more information can be extracted from the data by taking those facts into account.  That is, standard regression methods can be more informative than CART when the assumptions are true.  </p>\\n\",\n",
       " '<p>In addition to specifying $\\\\\\\\alpha$ (probability of a type I error), you need a fully specified hypothesis pair, i.e., $\\\\\\\\mu_{0}$, $\\\\\\\\mu_{1}$ and $\\\\\\\\sigma$ need to be known. $\\\\\\\\beta$ (probability of type II error) is $1 - \\\\\\\\textrm{power}$. I assume a one-sided $H_{1}: \\\\\\\\mu_{1} &gt; \\\\\\\\mu_{0}$. In R:</p>\\n\\n<pre><code>&gt; sigma &lt;- 15    # theoretical standard deviation\\n&gt; mu0   &lt;- 100   # expected value under H0\\n&gt; mu1   &lt;- 130   # expected value under H1\\n&gt; alpha &lt;- 0.05  # probability of type I error\\n\\n# critical value for a level alpha test\\n&gt; crit &lt;- qnorm(1-alpha, mu0, sigma)\\n\\n# power: probability for values &gt; critical value under H1\\n&gt; (pow &lt;- pnorm(crit, mu1, sigma))\\n[1] 0.36124\\n\\n# probability for type II error: 1 - power\\n&gt; (beta &lt;- 1-pow)\\n[1] 0.63876\\n</code></pre>\\n\\n<p>Edit: visualization</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/rxoDS.jpg\" alt=\"enter image description here\"></p>\\n\\n<pre><code>xLims &lt;- c(50, 180)\\nleft  &lt;- seq(xLims[1],   crit, length.out=100)\\nright &lt;- seq(crit, xLims[2],   length.out=100)\\nyH0r  &lt;- dnorm(right, mu0, sigma)\\nyH1l  &lt;- dnorm(left,  mu1, sigma)\\nyH1r  &lt;- dnorm(right, mu1, sigma)\\n\\ncurve(dnorm(x, mu0, sigma), xlim=xLims, lwd=2, col=\"red\", xlab=\"x\", ylab=\"density\",\\n      main=\"Normal distribution under H0 and H1\", ylim=c(0, 0.03), xaxs=\"i\")\\ncurve(dnorm(x, mu1, sigma), lwd=2, col=\"blue\", add=TRUE)\\npolygon(c(right, rev(right)), c(yH0r, numeric(length(right))), border=NA,\\n        col=rgb(1, 0.3, 0.3, 0.6))\\npolygon(c(left,  rev(left)),  c(yH1l, numeric(length(left))),  border=NA,\\n        col=rgb(0.3, 0.3, 1, 0.6))\\npolygon(c(right, rev(right)), c(yH1r, numeric(length(right))), border=NA,\\n        density=5, lty=2, lwd=2, angle=45, col=\"darkgray\")\\nabline(v=crit, lty=1, lwd=3, col=\"red\")\\ntext(crit+1,  0.03,  adj=0, label=\"critical value\")\\ntext(mu0-10,  0.025, adj=1, label=\"distribution under H0\")\\ntext(mu1+10,  0.025, adj=0, label=\"distribution under H1\")\\ntext(crit+8,  0.01,  adj=0, label=\"power\", cex=1.3)\\ntext(crit-12, 0.004,  expression(beta),  cex=1.3)\\ntext(crit+5,  0.0015, expression(alpha), cex=1.3)\\n</code></pre>\\n',\n",
       " '<p>The issue with attractive pairwise interactions is that the integral of the intensity of the process is infinite as each new point always increases the intensity for others in the neighbourhood.</p>\\n\\n<p>If your process is mildly attractive on medium distances and strongly repulsive on very short ones (like physical particles), you can consider a pairwise interaction like Lennard-Jones or similar. See Van Lieshout (2000) Markov point processes and their applications or in a more applied fashion using R: Baddeley (2010) <a href=\"http://www.csiro.au/resources/pf16h\" rel=\"nofollow\">Analysing spatial point patterns in \\'R\\'</a>.</p>\\n\\n<p>Else, the best way out is to use higher-order interactions such as area-interaction process (e.g. Baddeley and van Lieshout, 1995) or one of the processes in Geyer (1999) Likelihood inference for spatial point processes. Higher order interactions allow for both attractive and repulsive behaviour, e.g. points could be pairwise attractive, but triplets of points could be repulsive.</p>\\n\\n<p>Depending on your application, the inhomogeneity of space could play a very important role. However, whether the intensity depends on the properties of underlying space or on the presence of other points should be dictated by theory. Even if these effects potentially could be hard to tell apart empirically.</p>\\n',\n",
       " '<p><a href=\"http://en.wikipedia.org/wiki/Reliability_%28statistics%29\" rel=\"nofollow\">Reliability</a> and <a href=\"http://en.wikipedia.org/wiki/Validity_%28statistics%29\" rel=\"nofollow\">validity</a> are both multifaceted concepts, so be careful about using them in such general terms.</p>\\n\\n<p>Your friend is probably asking about the internal consistency (reliability) of the survey. This is commonly measured using <a href=\"http://en.wikipedia.org/wiki/Cronbach%27s_alpha\" rel=\"nofollow\">Cronbach\\'s alpha</a>. However, Cronbach\\'s alpha is typically used on single-factor surveys; the TEQ has four factors. You could compute four reliability coefficients, one for each factor, or a confirmatory factor analysis might be more appropriate.</p>\\n\\n<p>Truth be told, I would consider most of this overkill. The beauty of using a standardized test is that its reliability has been confirmed many times before, hence standardization. That is, of course, assuming the TEQ is a commonly used standardized test-- I haven\\'t used it myself. If this were a write-up, citing an authoritative source in regards to the reliability of the TEQ would probably be sufficient to appease most readers. If not, calculating Cronbach\\'s alpha for each factor and saying something like \"all alpha > .7\" should be sufficient (provided of course, all alpha <strong>are</strong> greater than .7).</p>\\n\\n<p>The validity of the survey depends on what you\\'re doing with the data. The test is valid to the extent that it measures what you\\'re trying to measure, or predicts what you\\'re trying to predict. As long as the connection between the test and your use of it is plausible, you probably don\\'t need to compute any statistics for validity.</p>\\n',\n",
       " '<p>John Kruschke released a book in mid 2011 called <a href=\"http://rads.stackoverflow.com/amzn/click/0123814855\">Doing Bayesian Data Analysis: A Tutorial with R and BUGS</a>. It is truly introductory. If you want to walk from frequentist stats into Bayes though, especially with multilevel modelling, I recommend Gelman and Hill.</p>\\n\\n<p>John Kruschke also has a <a href=\"http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/\">website for the book</a> that has all the examples in the book in BUGS and JAGS. His <a href=\"http://doingbayesiandataanalysis.blogspot.com.au/\">blog</a> on Bayesian statistics also links in with the book.</p>\\n',\n",
       " \"<p>You can do the following:</p>\\n\\n<pre><code>proc logistic data=thedata;\\n  model outcome = age1 age2 age3;\\n  estimate 'age1 vs age2' age1 1 age2 -1,\\n           'age1 vs age3' age1 1 age3 -1,\\n           'age2 vs age3' age2 1 age3 -1 / CL;\\n  run;\\n</code></pre>\\n\\n<p>where outcome is a binary variable, and age1, age2, and age3 are binary indicator variables. </p>\\n\\n<p>In the output:</p>\\n\\n<pre><code>                             Estimates \\nLabel          Estimate   Standard Error  z Value  Pr &gt; |z|  Alpha  Lower   Upper \\nage1 vs age2   0.1122     0.1022          1.10     0.2725    0.05  -0.08819 0.3125 \\nage1 vs age3   0.2705     0.1088          2.49     0.0129    0.05   0.05725 0.4837 \\nage2 vs age3   0.1583     0.1094          1.45     0.1480    0.05  -0.05619 0.3728 \\n</code></pre>\\n\\n<p>take $exp(Estimate)$ for the OR, take $exp(Lower)$ for the lower bound of the 95% CI, and $exp(Upper)$ for the upper bound. </p>\\n\",\n",
       " '<p>There are two time sequences in my system, one of them represents IN events, and the other OUT events. Each IN event would be released by the nearest following OUT event or reaching the deadline. I want to get the pdf of the interval between the time the IN event is triggered until the time it is released (OUT event occurs or censored at deadline).</p>\\n\\n<p>As the following figure shows, $t_a$, $t_r$, $t_r^\\'$ represent the IN event time, the nearest OUT event time and last OUT event time prior to $t_r$. The nearest OUT event can be represented as $t_r&gt;t_a$ and $t_r^\\' &lt; t_a$. I extend the situation to statistics, so that $t_a$ is a sample of random variable $T_a \\\\\\\\sim U(0,T)$. $t_r$ obey $T_r \\\\\\\\sim U(0,2T)$. The interval of two OUT event, $t_r-t_r^\\'$, should be treated as $T_i \\\\\\\\sim Exp(\\\\\\\\lambda)$.  </p>\\n\\n<p><img src=\"http://i.stack.imgur.com/RfuzT.png\" alt=\"enter image description here\"></p>\\n\\n<p>How to model this complex multivariate random variable and get its pdf? </p>\\n\\n<p>Furthermore, deadline of event $t_a$ is $2T$, which means that if there is no $t_r$ between $(t_a, 2T)$, it would be released at time $2T$. This condition could introduce more difficulty of modeling.</p>\\n',\n",
       " '<p>Averaging is just a low pass filter so if you want to filter out the high frequency component of your signal, it will do that.</p>\\n\\n<p>Obviously different averaging techniques and different parameters will filter out high frequency components in a different way. See for instance <a href=\"http://ptolemy.eecs.berkeley.edu/eecs20/week12/freqResponseRA.html\" rel=\"nofollow\">this illustration of the frequency response of a simple moving average</a>.</p>\\n',\n",
       " '<p>One thought is that you should\\'ve mentioned the regularity conditions for asymptotic normality of the estimates and the $\\\\\\\\chi^2$ performance of the likelihood ratio test statistic. These conditions include, informally speaking,</p>\\n\\n<ol>\\n<li>the true parameter being in the interior of the parameter space;</li>\\n<li>the log-likelihood really affording Taylor series expansion;</li>\\n<li>i.i.d. data;</li>\\n<li>conditions to interchange some of the derivatives and the integrals/expectations (some sort of uniform boundedness);</li>\\n</ol>\\n\\n<p>and such. See <a href=\"http://www.stat.unc.edu/postscript/rs/ISI89.pdf\">http://www.stat.unc.edu/postscript/rs/ISI89.pdf</a> and <a href=\"http://www.jstor.org/stable/2346086\">http://www.jstor.org/stable/2346086</a> concerning violations of these conditions. (The simplest example is estimation when the support depends on the parameter value, e.g., $U[0,\\\\\\\\theta]$. The MLE $\\\\\\\\hat\\\\\\\\theta_n=x_{(n)}$ is not asymptotically normal, and an estimator that has a greater asymptotic efficiency in terms of MSE can be constructed.) These are worthy papers to read if you are serious about statistical theory, and many courses on asymptotics do not really wander off far enough into this elephant\\'s graveyard of ML elegance.</p>\\n\\n<p>Another thought is that may be they really did want you to mention both Wald and score tests. <a href=\"http://www.citeulike.org/user/ctacmo/article/890474\">Buse (1982)</a> provides a wonderful review of the relation between the three tests.</p>\\n',\n",
       " '<p>Ha.  Okay, I\\'m kicking myself.  The problem is in the data generation.  I\\'m using uniform distributions instead of normal distributions.  Since these have mean=.5 instead of mean=0, and I didn\\'t bother to include intercept terms, the model is giving me garbage answers.</p>\\n\\n<p>Switch all those \"runif\"s to \"rnorm\"s and it works like a charm.</p>\\n',\n",
       " \"<p>Given your data:</p>\\n\\n<pre><code>cp &lt;- c(5, 2, 4, 1, 9, 2, 9, 2, 10, 1)\\n</code></pre>\\n\\n<p>then the ranks, with ties being given average of the ranks, are:</p>\\n\\n<pre><code>&gt; rank(cp)\\n [1]  7.0  4.0  6.0  1.5  8.5  4.0  8.5  4.0 10.0  1.5\\n</code></pre>\\n\\n<p>What is being done here? If you sort the data in increasing order, then we have a <code>1</code> in both rank order positions 1 <em>and</em> 2. We could assign rank 1 to both <code>1</code>s, or rank 2, or as stated above, the average of the rank orders (1/2) / 2 = 1.5. This is why the two <code>1</code>s have been given rank of 1.5 in the above output from R.</p>\\n\\n<p>Now look at the next values in the rank order, the <code>2</code>s. The <code>2</code>'s are in rank order positions 3, 4, and 5, therefore they all get rank 4 from (3+4+5) / 3 = 4, as this is the average of the tied ranks for these values.</p>\\n\\n<p>If we initiate $S_0 = 0$, i.e. the zeroth cumulative sum is 0, we compute the $i$th cumulative sum ($S_i$) as the previous cumulative sum ($S_{i-1}$) plus the difference between the rank of the $i$th data point ($r_i$) and the average over all ranks $\\\\\\\\bar{r}$.</p>\\n\\n<p>For the above data, the average rank is:</p>\\n\\n<pre><code>&gt; rcp &lt;- rank(cp)\\n&gt; mean(rcp)\\n[1] 5.5\\n</code></pre>\\n\\n<p>The values $r_i - \\\\\\\\bar{r}$ for this set of data are:</p>\\n\\n<pre><code>&gt; rcp - mean(rcp)\\n [1]  1.5 -1.5  0.5 -4.0  3.0 -1.5  3.0 -1.5  4.5 -4.0\\n</code></pre>\\n\\n<p>and the cumulative sums are:</p>\\n\\n<pre><code>&gt; cumsum(rcp - mean(rcp))\\n [1]  1.5  0.0  0.5 -3.5 -0.5 -2.0  1.0 -0.5  4.0  0.0\\n</code></pre>\\n\",\n",
       " \"<p>user1149913 has an excellent answer (+1), but it looks to me that your data collection fell apart in late 2011, so you'd have to cut that part of your data off, and then still run things a few times with different random starting coefficients to see what you get.</p>\\n\\n<p>One straightforward way to do things would be to separate your data into two sets by eye, then use whatever linear model technique you're used to. In R, it would be the <code>lm</code> function.</p>\\n\\n<p>Or fit two lines by eye. In R you would use <code>abline</code> to do this.</p>\\n\\n<p>The data's jumbled, has outliers, and falls apart at the end, yet by-eye has two fairly obvious lines, so I'm not sure a fancy method is worth it.</p>\\n\",\n",
       " '<p>Firstly a caveat. In clustering there is often no one \"correct answer\" - one clustering may be better than another by one metric, and the reverse may be true using another metric. And in some situations two different clusterings could be equally probable under the same metric.</p>\\n\\n<p>Having said that, you might want to have a look at <a href=\"http://en.wikipedia.org/wiki/Dirichlet_process\">Dirichlet Processes</a>. Also see this <a href=\"http://www.cs.cmu.edu/~kbe/dp_tutorial.pdf\">tutorial</a>.</p>\\n\\n<p>If you begin with a Gaussian Mixture model, you have the same problem as with k-means - that you have to choose the number of clusters. You could use model evidence, but it won\\'t be robust in this case. So the trick is to use a Dirichlet Process prior over the mixture components, which then allows you to have a potentially infinite number of mixture components, but the model will (usually) automatically find the \"correct\" number of components (under the assumptions of the model).</p>\\n\\n<p>Note that you still have to specify the concentration parameter $\\\\\\\\alpha$ of the Dirichlet Process prior. For small values of $\\\\\\\\alpha$, samples from a DP are likely to be composed of a small number of atomic measures with large weights. For large values, most samples are likely to be distinct (concentrated). You can use a hyper-prior on the concentration parameter and then infer its value from the data, and this hyper-prior can be suitably vague as to allow many different possible values. Given enough data, however, the concentration parameter will cease to be so important, and this hyper-prior could be dropped.</p>\\n',\n",
       " '<p>There are way too many measures starting from chi-square based (<a href=\"http://en.wikipedia.org/wiki/Phi_coefficient\">phi coefficient</a>) going to less commonly used (<a href=\"http://en.wikipedia.org/wiki/Goodman_and_Kruskal%27s_lambda\">Goodman and Kruskal\\'s lambda</a>).</p>\\n\\n<p>There is a series of four articles <a href=\"http://www.jstor.org/stable/2281536\">starting here</a> on the issue.</p>\\n',\n",
       " '<p>I am currently working on some time series data, I know I can use\\nLOESS/ARIMA model. </p>\\n\\n<p>The  data is written to a vector whose length is 1000, which is a queue,\\nupdating every 15 minutes,  </p>\\n\\n<p>Thus the old data will pop out while the new data push in the vector.</p>\\n\\n<p>I can rerun the whole model on a scheduler, e.g. retrain the model every 15\\nminutes, that is,  Use the whole 1000 value to train the LOESS model, However it is very inefficient, as every time only one\\nvalue is insert while another 999 vlaues still same as last time.</p>\\n\\n<p>So how can I achieve better performance?</p>\\n\\n<p>Many thanks</p>\\n',\n",
       " '<p>Functional Data Analysis (FDA) can model phase variation (differences in timing), whereas the alternatives that you mention cannot. An example of phase variation is the variability in timing in the onset of puberty in children. Ignoring phase variation (which is standard practice) mismodels puberty. FDA models phase variation by time warping, where the time axis is locally stretched or compressed to fit a target. In this way, FDA can give a realistic and useful description of the process. FDA requires relatively dense data, but nowadays we see these more and more. In my opinion, FDA has great potential and is vastly underused. </p>\\n',\n",
       " '<p>In addition to the other answers:\\nIf you have 1,000,000 observations and when your event comes up only a few times, you are likely to want to look at a lot of different events.\\nIf you look at 100 different events you will run into problems if you work with p&lt;0.05 as criteria for significance. </p>\\n',\n",
       " '<p>For a nice intro into stats, check out O\\'Reilly <a href=\"http://greenteapress.com/thinkstats/\" rel=\"nofollow\">Think Stats by Allen B. Downey</a>. It\\'s a freely-available ebook from the author.</p>\\n',\n",
       " \"<p>The GLM approach is pretty painful here, but your GLM is really doing a regression analysis, so if the variables have a different scale, you would reject equality just because of that.  By standardizing the variables, the coefficients are equivalent to correlations, so you are doing the test on equality of correlations, and scale differences don't count.</p>\\n\",\n",
       " \"<p>To do element-wise multiplication you can just make a for loop in those languages and that's it! I've used for loops in WinBUGS with no problems.</p>\\n\",\n",
       " '<p>Predicting <code>counts</code> using the fixed-effects part of your model means that you set to zero (i.e. their mean) the random effects. This means that you can \"forget\" about them and use standard machinery to calculate the predictions and the standard errors of the predictions (with which you can compute the confidence intervals).</p>\\n\\n<p>This is an example using Stata, but I suppose it can be easily \"translated\" into R language:</p>\\n\\n<pre><code>webuse epilepsy, clear\\nxtmepoisson seizures treat visit || subject: visit\\npredict log_seiz, xb\\ngen pred_seiz = exp(log_seiz)\\npredict std_log_seiz, stdp\\ngen ub = exp(log_seiz+invnorm(.975)*std_log_seiz)\\ngen lb = exp(log_seiz-invnorm(.975)*std_log_seiz)\\n\\ntw (line pred_seiz ub lb visit if treat == 0, sort lc(black black black) ///\\n lp(l - -)), scheme(s1mono) legend(off) ytitle(\"Predicted Seizures\") ///\\n xtitle(\"Visit\")\\n</code></pre>\\n\\n<p>The graph refers to <code>treat == 0</code> and it\\'s intended to be an example (<code>visit</code> is not a really continuous variable, but it\\'s just to get the idea). The dashed lines are 95% confidence intervals.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/VcSms.png\" alt=\"enter image description here\"></p>\\n',\n",
       " '<p>One possibility is a 2-step model.</p>\\n\\n<ol>\\n<li>Find a trend/Cycle/seasonality model for the data ignoring businessdays/weekedays</li>\\n<li>Then perform a time series regression with dummy variables indicating the issues above (business days, holidays, ...). </li>\\n</ol>\\n\\n<p>Step 2 can be extended to include more predictors. This model is \"easy\" and I will test its performance.</p>\\n',\n",
       " '<p>Generally tables are handled as matrices or arrays and matrix indexing allows two column arguments as arrays so:</p>\\n\\n<pre><code>&gt; inp.mtx &lt;- as.matrix(inp)\\n&gt; mat[inp.mtx[,1:2] ]&lt;- inp.mtx[,3]\\n&gt; mat\\n     [,1] [,2]  [,3]\\n[1,]   NA 0.05 0.040\\n[2,]   NA   NA 0.001\\n[3,]   NA   NA    NA\\n</code></pre>\\n',\n",
       " '<p>As far as I know, the Pitman test is formulated as :</p>\\n\\n<p>$$F=\\\\\\\\frac{SD_2}{SD_1} ~with~ SD_2 > SD_1$$</p>\\n\\n<p>$$T=\\\\\\\\frac{(F-1)\\\\\\\\sqrt{n-2}}{2\\\\\\\\sqrt{F(1-r^2)}} $$ with $r$ the correlation between the scores in sample 1 and sample 2. This is not equivalent to the formula you use and mentioned in the paper. I\\'m not positive about my formula either, I got it from a course somewhere (alas no reference...)</p>\\n\\n<p>Apart from that, it might be interesting to take a look at an alternative approach to dealing with regression to the mean. I found the <a href=\"http://ije.oxfordjournals.org/content/34/1/215.full\" rel=\"nofollow\">tutorial paper of Barnett et al</a> on regression to the mean very enlightening.</p>\\n\\n<p>Now let\\'s get back a moment to the 2-sided versus 1-sided p-values. Regardless of the formula you use, the sign of T is only dependent on the order of the SD\\'s. (In fact, how I know the pitman test, T is always positive.) Hence, the underlying distribution is -as far as I\\'m concerned- not the T distribution but half the T distribution, meaning you have to put the cutoff at $T_{0.975, df}$, but the related p-value is originating from one tail only. This is equivalent the standard F test for comparing variances.</p>\\n',\n",
       " '<p>Suppose I have 20 people matched in 10 pairs.</p>\\n\\n<p>Suppose two doctors are assigned at random to diagnose one person in each pair, so in each pair each person is diagnosed by a different doctor.  </p>\\n\\n<p>Suppose that each doctor reports a vector of symptoms for each patient.  That is, the response for patient $i$ diagnosed by doctor $j$ is a report $x_{ij}=(a,b,c,)^T$ where each element reports the number of instances of condition a, b, or c in patient $i$  like $x_{ij}=(2,5,0)$.  </p>\\n\\n<p>Since the doctors are assigned to patients at random we would expect the average report $x_{.j}=(\\\\\\\\bar{a},\\\\\\\\bar{b},\\\\\\\\bar{c})^T$ across doctors  to be identical <em>if</em> both doctors have the same diagnostic ability.</p>\\n\\n<p>How do I test the null that both doctors have the same diagnostic ability against the alternative that they differ in some respect?</p>\\n',\n",
       " '<p>Consider 1000 samples drawn from an unknown distribution. Whats the difference between the following two ways of calculating sample mean and sample variance?</p>\\n\\n<ol>\\n<li><p>find sample mean and sample variance over all 1000 samples.</p></li>\\n<li><p>find sample mean over first 500 samples and sample variance over remaining 500 samples.</p></li>\\n</ol>\\n\\n<p>What is the difference and which method is preferred?</p>\\n',\n",
       " '<p>Chemical analyses of environmental samples are often censored below at reporting limits or various detection/quantitation limits.  The latter can vary, usually in proportion to the values of other variables.  For example, a sample with a high concentration of one compound might need to be diluted for analysis, resulting in proportional inflation of the censoring limits for all other compounds analyzed at the same time in that sample.  As another example, sometimes the presence of a compound can alter the response of the test to other compounds (a \"matrix interference\"); when this is detected by the laboratory, it will inflate its reporting limits accordingly.</p>\\n\\n<p>I am seeking a practical way to estimate the entire variance-covariance matrix for such datasets, especially when many of the compounds experience more than 50% censoring, which is often the case.  A conventional distributional model is that the logarithms of the (true) concentrations are multinormally distributed, and this appears to fit well in practice, so a solution for this situation would be useful.</p>\\n\\n<p>(By \"practical\" I mean a method that can reliably be coded in at least one generally available software environment like R, Python, SAS, etc., in a way that executes quickly enough to support iterative recalculations such as occur in multiple imputation, and which is reasonably stable [which is why I am reluctant to explore a BUGS implementation, although Bayesian solutions in general are welcome].)</p>\\n\\n<p>Many thanks in advance for your thoughts on this matter.</p>\\n',\n",
       " '<p>I have a logistic regression model: $softmax(WX)$ where $W$ is my parameter matrix and $X$ is my input. I want a density function over the outputs of that model.</p>\\n\\n<p>Say I know that my $X$ are distributed according to some density $p$. From the <a href=\"http://en.wikipedia.org/wiki/Probability_density_function#Dependent_variables_and_change_of_variables\" rel=\"nofollow\">change of variables</a> I know that the density of the outputs will be something like $p&#39;(x) = p(x) det|J^{-1}|$ where $J = \\\\\\\\frac{\\\\\\\\partial{softmax(WX)}}{\\\\\\\\partial{X}}$.</p>\\n\\n<p>However, this holds only for a bijective function $softmax(WX)$. </p>\\n\\n<p>What can I do if this is not the case - especially if $W$ is not square?</p>\\n',\n",
       " \"<p>One solution to this problem is to assume that the number of events (like flare-ups) is proportional to time. If you denote the individual level of exposure (length of follow-up in your case) by $t$, then $\\\\\\\\frac{E[y \\\\\\\\vert x]}{t}=\\\\\\\\exp\\\\\\\\{x'\\\\\\\\beta\\\\\\\\}.$ Here a follow-up that is twice as long would double the expected count, all else equal. This can be algebraically equivalent to a model where $E[y \\\\\\\\vert x]=\\\\\\\\exp\\\\\\\\{x'\\\\\\\\beta+\\\\\\\\log{t}\\\\\\\\},$ which is just the Poisson model with the coefficient on $\\\\\\\\log t$ constrained to $1$. You can also test the proportionality assumption by relaxing the constraint and testing the hypothesis that $\\\\\\\\beta_{log(t)}=1$.</p>\\n\\n<p>However, it does not sound like you observe the number of events, since your outcome is binary (or maybe it's not meaningful given your disease). This leads me to believe a logistic model with an logarithmic offset would be more appropriate here. </p>\\n\",\n",
       " '<p>So the simple answer is yes: Metropolis-Hastings and its special case Gibbs sampling :) General and powerful; whether or not it scales depends on the problem at hand. </p>\\n\\n<p>I\\'m not sure why you think sampling an arbitrary discrete distribution is more difficult than an arbitrary continuous distribution. If you can calculate the discrete distribution and the sample space isn\\'t huge then it\\'s much, much easier (unless the continuous distribution is standard, perhaps). Calculate the likelihood $f(k)$ for each category, then normalise to get the probabilities $P(\\\\\\\\tilde k = k) = f(k)/\\\\\\\\sum f(k)$ and use inverse transform sampling (imposing an arbitrary order on $k$).</p>\\n\\n<p>Have you got a particular model in mind? There are all sorts of MCMC approaches to fitting mixture models, for example, where the latent component assignments are discrete parameters. These range from very simple (Gibbs) to quite complex.</p>\\n\\n<p>How big is the parameter space? Is it potentially enormous (eg in the mixture model case, it\\'s N by the number of mixture components)? You might not need anything more than a Gibbs sampler, since conjugacy is no longer an issue (you can get the normalizing constant directly so you can compute the full conditionals). In fact griddy Gibbs used to be popular for these cases, where a continuous prior is discretized to ease computation.</p>\\n\\n<p>I don\\'t think there is a particular \"best\" for all problems having a discrete parameter space any more than there is for the continuous case. But if you tell us more about the models you\\'re interested in perhaps we can make some recommendations.</p>\\n\\n<p>Edit: OK, I can give a little more information in re: your examples.</p>\\n\\n<p>Your first example has pretty long history, as you might imagine. A recent-ish review is in [1], see also [2]. I\\'ll try to give some details here: A relevant example is stochastic search variable selection. The initial formulation was to use absolutely continuous priors like $p(\\\\\\\\beta)\\\\\\\\sim \\\\\\\\pi N(\\\\\\\\beta; 0, \\\\\\\\tau) + (1-\\\\\\\\pi) N(\\\\\\\\beta, 0, 1000\\\\\\\\tau)$. That actually turns out to work poorly compared to priors like $p(\\\\\\\\beta)\\\\\\\\sim \\\\\\\\pi \\\\\\\\delta_0 (\\\\\\\\beta) + (1-\\\\\\\\pi) N(\\\\\\\\beta, 0, \\\\\\\\tau)$ where $\\\\\\\\delta_0$ is a point mass at 0. Note that both fit into your original formulation; an MCMC approach would usually proceed by augmenting $\\\\\\\\beta$ with a (discrete) model indicator (say $Z$). This is equivalent to a model index; if you have $Z_1\\\\\\\\dots, Z_p$ then obviously you can remap the $2^p$ possible configurations to numbers in $1:2^p$.</p>\\n\\n<p>So how can you improve the MCMC? In a lot of these models you can sample from $p(Z, \\\\\\\\beta|y)$ by composition, ie using that $p(Z, \\\\\\\\beta|y) = p(\\\\\\\\beta | Y, Z)p(Z|Y)$. Block updates like this can tremendously improve mixing since the correlation between $Z$ and $\\\\\\\\beta$ is now irrelevant to the sampler</p>\\n\\n<p>SSVS embeds the whole model space in one big model. Often this is easy to implement but gives works poorly. Reversible jump MCMC is a different kind of approach which lets the dimension of the parameter space vary explicitly; see [3] for a review and some practical notes. You can find more detailed notes on implementation in different models in the literature, I\\'m sure.</p>\\n\\n<p>Oftentimes a complete MCMC approach is infeasible; say you have a linear regression with $p=1000$ variables and you\\'re using an approach like SSVS. You can\\'t hope for your sampler to converge; there\\'s not enough time or computing power to visit all those model configurations, and you\\'re especially hosed if some of your variables are even moderately correlated. You should be especially skeptical of people trying to estimate things like variable inclusion probabilities in this way. Various stochastic search algorithms used in conjunction with MCMC have been proposed for such cases. One example is BAS [4], another is in [5] (Sylvia Richardson has other relevant work too); most of the others I\\'m aware of are geared toward a particular model.</p>\\n\\n<p>A different approach which is gaining in popularity is to use absolutely continuous shrinkage priors that mimic model averaged results. Typically these are formulated as scale mixtures of normals. The Bayesian lasso is one example, which is a special case of normal-gamma priors and a limiting case of normal-exponential-gamma priors. Other choices include the horseshoe and the general class of normal distributions with inverted beta priors on their variance. For more on these, I\\'d suggest starting with [6] and walking back through the references (too many for me to replicate here :) )</p>\\n\\n<p>I\\'ll add more about outlier models later if I get a chance; the classic reference is [7]. They\\'re very similar in spirit to shrinkage priors. Usually they\\'re pretty easy to do with Gibbs sampling. </p>\\n\\n<p>Perhaps not as practical as you were hoping for; model selection in particular is a hard problem and the more elaborate the model the worse it gets. Block update wherever possible is the only piece of general advice I have. Sampling from a mixture of distributions you will often have the problem that membership indicators and component parameters are highly correlated. I also haven\\'t touched on label switching issues (or lack of label switching); there is quite a bit of literature there but it\\'s a little out of my wheelhouse.</p>\\n\\n<p>Anyway, I think it\\'s useful to start with some of the references here, to get a feeling for the different ways that others are approaching similar problems.</p>\\n\\n<p>[1] Merlise Clyde and E. I. George. Model Uncertainty Statistical Science 19 (2004): 81--94.\\n<a href=\"http://www.isds.duke.edu/~clyde/papers/statsci.pdf\" rel=\"nofollow\">http://www.isds.duke.edu/~clyde/papers/statsci.pdf</a></p>\\n\\n<p>[2]http://www-personal.umich.edu/~bnyhan/montgomery-nyhan-bma.pdf</p>\\n\\n<p>[3] Green &amp; Hastie Reversible jump MCMC (2009)\\n<a href=\"http://www.stats.bris.ac.uk/~mapjg/papers/rjmcmc_20090613.pdf\" rel=\"nofollow\">http://www.stats.bris.ac.uk/~mapjg/papers/rjmcmc_20090613.pdf</a></p>\\n\\n<p>[4] <a href=\"http://www.stat.duke.edu/~clyde/BAS/\" rel=\"nofollow\">http://www.stat.duke.edu/~clyde/BAS/</a></p>\\n\\n<p>[5] <a href=\"http://ba.stat.cmu.edu/journal/2010/vol05/issue03/bottolo.pdf\" rel=\"nofollow\">http://ba.stat.cmu.edu/journal/2010/vol05/issue03/bottolo.pdf</a></p>\\n\\n<p>[6] <a href=\"http://www.uv.es/bernardo/Polson.pdf\" rel=\"nofollow\">http://www.uv.es/bernardo/Polson.pdf</a></p>\\n\\n<p>[7] Mike West Outlier models and prior distributions in Bayesian linear regression (1984) JRSS-B</p>\\n',\n",
       " \"<p>For a linear regression, the fitted slope is going to be the correlation (which, when squared, gives the coefficient of determination, the $R^2$) times the empirical standard deviation of the regressand (the $y$) divided by the empirical standard deviation of the regressor (the $x$).  Depending on the scaling of the $x$ and $y$, you can have a fit slope equal to one but an arbitrarily small $R^2$ value. </p>\\n\\n<p>In short, the slope is not a good indicator of model 'fit' unless you are certain that the scales of the dependent and independent variables must be equal to each other. </p>\\n\",\n",
       " '<p>I know that <a href=\"http://gephi.org/\">Gephi</a> can process undirected weighted graph, but I seem to remember it has to be stored in <a href=\"http://gephi.org/users/supported-graph-formats/gdf-format/\">GDF</a>, which is pretty close to CSV, or Ucinet <a href=\"http://gephi.org/users/supported-graph-formats/ucinet-dl-format/\">DL</a>. Be aware that it\\'s still an alpha release.\\nNow, about clustering your graph, Gephi seems to lack clustering pipelines, except for the MCL algorithm that is now available in the latest version. There was a <a href=\"http://code.google.com/p/google-summer-of-code-2009-gephi/\">Google Code Project</a> in 2009, <a href=\"http://web.ecs.syr.edu/~pjmcswee/gephi.pdf\">Gephi Network Statistics</a> (featuring e.g. Newman’s modularity metric), but I don\\'t know if something has been released in this direction. Anyway, it seems to allow some kind of modularity/clustering computations, but see also <a href=\"http://www.rcasts.com/2010/04/social-network-analysis-using-r-and.html\">Social Network Analysis using R and Gephi</a> and <a href=\"http://www.r-bloggers.com/data-preparation-for-social-network-analysis-using-r-and-gephi/\">Data preparation for Social Network Analysis using R and Gephi</a> (Many thanks to @Tal).</p>\\n\\n<p>If you are used to Python, it is worth trying <a href=\"http://networkx.lanl.gov/\">NetworkX</a> (Here is an example of a <a href=\"http://networkx.lanl.gov/examples/drawing/weighted_graph.html\">weighted graph</a> with the corresponding code). Then you have many ways to carry out your analysis.</p>\\n\\n<p>You should also look at <a href=\"http://www.insna.org/software/index.html\">INSNA - Social Network Analysis Software</a> or Tim Evans\\'s webpage about <a href=\"http://155.198.210.128/~time/networks/\">Complex Networks and Complexity</a>.</p>\\n',\n",
       " \"<p>Correct me if I'm wrong, but I think this can be done in R using this command</p>\\n\\n<pre><code>&gt; chisq.test(c(15,13,10,17))\\n\\n    Chi-squared test for given probabilities\\n\\ndata:  c(15, 13, 10, 17) \\nX-squared = 1.9455, df = 3, p-value = 0.5838\\n</code></pre>\\n\\n<p>This assumes proportions of 1/4 each. You can modify expected values via argument <code>p</code>. For example, you think people may prefer (for whatever reason) one color over the other(s).</p>\\n\\n<pre><code>&gt; chisq.test(c(15,13,10,17), p = c(0.5, 0.3, 0.1, 0.1))\\n\\n    Chi-squared test for given probabilities\\n\\ndata:  c(15, 13, 10, 17) \\nX-squared = 34.1515, df = 3, p-value = 1.841e-07\\n</code></pre>\\n\",\n",
       " '<p>I\\'m trying to run a zero-inflated regression for a continuous response variable in R. I\\'m aware of a gamlss implementation, but I\\'d really like to try out this algorithm by Dale McLerran that is conceptually a bit more straightforward. Unfortunately, the code is in SAS and I\\'m not sure how to re-write it for something like nlme. </p>\\n\\n<p>Would very much appreciate your help!</p>\\n\\n<p>Mike</p>\\n\\n<p>The code is as follows:</p>\\n\\n<pre><code>proc nlmixed data=mydata;\\n  parms b0_f=0 b1_f=0 \\n        b0_h=0 b1_h=0 \\n        log_theta=0;\\n\\n\\n  eta_f = b0_f + b1_f*x1 ;\\n  p_yEQ0 = 1 / (1 + exp(-eta_f));\\n\\n\\n  eta_h = b0_h + b1_h*x1;\\n  mu    = exp(eta_h);\\n  theta = exp(log_theta);\\n  r = mu/theta;\\n\\n\\n  if y=0 then\\n     ll = log(p_yEQ0);\\n  else\\n     ll = log(1 - p_yEQ0)\\n          - lgamma(theta) + (theta-1)*log(y) - theta*log(r) - y/r;\\n\\n\\n  model y ~ general(ll);\\n  predict (1 - p_yEQ0)*mu out=expect_zig;\\n  predict r out=shape;\\n  estimate \"scale\" theta;\\nrun;\\n</code></pre>\\n\\n<p>From: <a href=\"http://listserv.uga.edu/cgi-bin/wa?A2=ind0805A&amp;L=sas-l&amp;P=R20779\" rel=\"nofollow\">http://listserv.uga.edu/cgi-bin/wa?A2=ind0805A&amp;L=sas-l&amp;P=R20779</a></p>\\n\\n<p><strong>ADD:</strong></p>\\n\\n<p>Note: There are no mixed effects present here - only fixed.</p>\\n\\n<p>The advantage to this fitting is that (even though the coefficients are the same as if you separately fit a logistic regression to P(y=0) and a gamma error regression with log link to E(y | y>0)) you can estimate the combined function E(y) which includes the zeroes. One can predict this value in SAS (with a CI) using the line <code>predict (1 - p_yEQ0)*mu</code> .</p>\\n\\n<p>Further, one is able to write custom contrast statements to test the significance of predictor variables on E(y). For example, here is another version of the SAS code I have used:</p>\\n\\n<pre><code>proc nlmixed data=TestZIG;\\n      parms b0_f=0 b1_f=0 b2_f=0 b3_f=0\\n            b0_h=0 b1_h=0 b2_h=0 b3_h=0\\n            log_theta=0;\\n\\n\\n        if gifts = 1 then x1=1; else x1 =0;\\n        if gifts = 2 then x2=1; else x2 =0;\\n        if gifts = 3 then x3=1; else x3 =0;\\n\\n\\n      eta_f = b0_f + b1_f*x1 + b2_f*x2 + b3_f*x3;\\n      p_yEQ0 = 1 / (1 + exp(-eta_f));\\n\\n      eta_h = b0_h + b1_h*x1 + b2_h*x2 + b3_h*x3;\\n      mu    = exp(eta_h);\\n      theta = exp(log_theta);\\n      r = mu/theta;\\n\\n      if amount=0 then\\n         ll = log(p_yEQ0);\\n      else\\n         ll = log(1 - p_yEQ0)\\n              - lgamma(theta) + (theta-1)*log(amount) -                      theta*log(r) - amount/r;\\n\\n      model amount ~ general(ll);\\n      predict (1 - p_yEQ0)*mu out=expect_zig;\\n      estimate \"scale\" theta;\\n    run; \\n</code></pre>\\n\\n<p>Then to estimate \"gift1\" versus \"gift2\" (b1 versus b2) we can write this estimate statement:</p>\\n\\n<pre><code>estimate \"gift1 versus gift 2\" \\n (1-(1 / (1 + exp(-b0_f -b1_f))))*(exp(b0_h + b1_h)) - (1-(1 / (1 + exp(-b0_f -b2_f))))*(exp(b0_h + b2_h)) ; \\n</code></pre>\\n\\n<p>Can R do this?</p>\\n',\n",
       " '<p>This is data from Maxwell &amp; Delaney (2004), artificially extended to include a second between-subjects IV, yielding a 3x2 design. Using <code>multcomp</code>\\'s <code>glht()</code> function is easier once you switch to the associated one-factorial design by combining your two IVs into one with <code>interaction()</code>.</p>\\n\\n<p>DV is depression scores pre-treatment and post-treatment. Treatment is one of SSRI, Placebo or Waiting List. Pre-treatment score is the covariate. I included a second IV.</p>\\n\\n<pre><code>P        &lt;- 3     # number of groups in IV1\\nQ        &lt;- 2     # number of groups in IV2\\nNjk      &lt;- 5     # cell size\\nSSRIpre  &lt;- c(18, 16, 16, 15, 14, 20, 14, 21, 25, 11)\\nSSRIpost &lt;- c(12,  0, 10,  9,  0, 11,  2,  4, 15, 10)\\nPlacPre  &lt;- c(18, 16, 15, 14, 20, 25, 11, 25, 11, 22)\\nPlacPost &lt;- c(11,  4, 19, 15,  3, 14, 10, 16, 10, 20)\\nWLpre    &lt;- c(15, 19, 10, 29, 24, 15,  9, 18, 22, 13)\\nWLpost   &lt;- c(17, 25, 10, 22, 23, 10,  2, 10, 14,  7)\\nIV1      &lt;- factor(rep(1:3,  each=Njk*Q), labels=c(\"SSRI\", \"Placebo\", \"WL\"))\\nIV2      &lt;- factor(rep(1:2, times=Njk*P), labels=c(\"A\", \"B\"))\\n\\n# combine both IVs into 1 to get the associated one-factorial design\\nIVi      &lt;- interaction(IV1, IV2)\\nDVpre    &lt;- c(SSRIpre,  PlacPre,  WLpre)\\nDVpost   &lt;- c(SSRIpost, PlacPost, WLpost)\\n</code></pre>\\n\\n<p>Now do the ANCOVA with <code>IVi</code> as between-subjects factor and <code>DVpre</code> as covariate. Using the associated one-factorial design is possible since it has the same Error-MS as the two-factorial design.</p>\\n\\n<pre><code>&gt; aovAncova1 &lt;- aov(DVpost ~ IVi     + DVpre, data=dfAncova)  # one-factorial design\\n&gt; aovAncova2 &lt;- aov(DVpost ~ IV1*IV2 + DVpre, data=dfAncova)  # two-factorial design\\n&gt; summary(aovAncova1)[[1]][[\"Mean Sq\"]][3]  # Error MS one-factorial design\\n[1] 32.84399\\n\\n&gt; summary(aovAncova2)[[1]][[\"Mean Sq\"]][5]  # Error MS two-factorial design\\n[1] 32.84399\\n</code></pre>\\n\\n<p>Next comes the matrix defining 3 cell comparisons with sum-to-zero coefficients. The coefficients follow the order of 2*3 levels of <code>IVi</code>.</p>\\n\\n<pre><code>&gt; levels(IVi)\\n[1] \"SSRI.A\" \"Placebo.A\" \"WL.A\" \"SSRI.B\" \"Placebo.B\" \"WL.B\"\\n\\n&gt; cntrMat &lt;- rbind(\"SSRI-Placebo\"  = c(-1, 1, 0, -1,  1,  0),\\n+                  \"SSRI-0.5(P+WL)\"= c(-2, 1, 1, -2,  1,  1),\\n+                  \"A-B\"           = c( 1, 1, 1, -1, -1, -1))\\n\\n&gt; library(multcomp)\\n&gt; summary(glht(aovAncova1, linfct=mcp(IVi=cntrMat), alternative=\"greater\"),\\n+         test=adjusted(\"none\"))\\n\\nSimultaneous Tests for General Linear Hypotheses\\nMultiple Comparisons of Means: User-defined Contrasts\\nFit: aov(formula = DVpost ~ IVi + DVpre, data = dfAncova)\\n\\nLinear Hypotheses:\\n                    Estimate Std. Error t value  Pr(&gt;t)   \\nSSRI-Placebo &lt;= 0      8.887      5.135   1.731 0.04846 * \\nSSRI-WL &lt;= 0          12.878      5.129   2.511 0.00976 **\\nA-B &lt;= 0               1.024      6.492   0.158 0.4380\\n---\\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 \\n(Adjusted p values reported -- none method)\\n</code></pre>\\n',\n",
       " '<p>\"deviations in the data are the devil\" is just not true I think - well I don\\'t agree with it at least.  I\\'d say its more like \"chilli\" than the \"devil\" - as much as you can reasonably handle is good, but it can get nasty if there is too much.</p>\\n\\n<p>The most general procedure I know of to \"choose a statistic\" to report your data is a combination of two things</p>\\n\\n<ol>\\n<li>Bayesian inference (describing what is known)</li>\\n<li>Decision theory (taking actions under uncertainty)</li>\\n</ol>\\n\\n<p>However, both of these methods are only partially \"algorithmic\" so to speak.  You have to supply the inputs though.  Perhaps the most important part of this stage is that <em>you have to ask a question</em> that your procedure is going to answer.  Naturally, different questions get different answers.  As the saying goes \"I have just derived a very elegant and beautiful answer.  All I have to do now is figure out the question.\"  This is a common problem that I have seen with many statistical procedures, is that there is not always a clear statement of the class of problems that it is the best procedure to use.</p>\\n\\n<p>Bayesian inference requires you to specify your prior information in a mathematical framework.  This involves</p>\\n\\n<ol>\\n<li>Specifying the hypothesis space - what possibilities am I going to consider?</li>\\n<li>Assigning probabilities to each part of the space</li>\\n<li>Using the rules of probability theory to manipulate the assigned probabilities</li>\\n</ol>\\n\\n<p>this is basically an open-ended problem (you can always analyse a given English statement more deeply, to extract more or different information from it).  Decision theory also requires you to specify a loss function - and there are basically no rules or principles by which to do this, at least as far as I know (computational simplicity is a key driver).</p>\\n\\n<p>One useful question to ask yourself though is \"what information about the sample do I convey by presenting this statistic?\"  or \"how much of the complete data set can I recover from using just this set of statistics?\"</p>\\n\\n<p>One way you could use Bayesian statistics to help you here is to propose a hypothesis:</p>\\n\\n<p>$$\\\\\\\\begin{array}{l l}\\\\\\\\n  H_{mean}:\\\\\\\\text{The mean is the best statistic}\\\\\\\\n\\\\\\\\\\\\\\\\H_{med}:\\\\\\\\text{The median is the best statistic}\\\\\\\\n\\\\\\\\\\\\\\\\H_{IQR}:\\\\\\\\text{The IQR is the best statistic}\\\\\\\\n\\\\\\\\end{array}\\\\\\\\n$$</p>\\n\\n<p>Now these are not \"mathematically well posed\" hypothesis, but if we use them anyway, and see what parts of the maths are required to make it well posed.  The first part is the prior probabilities, without any data, how likely is each hypothesis?  The usual answer is equal probabilities (but not always - may have some theoretical reason to support one hypothesis being more likely - the CLT is perhaps one for $H_{mean}$ being higher than the others).</p>\\n\\n<p>So we use Bayes theorem to update each probability ($I$=prior information, $D$= data set):</p>\\n\\n<p>$$P(H_{i}|D,I)=P(H_{i}|I)\\\\\\\\frac{P(D|H_{i},I)}{P(D|I)}\\\\\\\\implies \\\\\\\\frac{P(H_{i}|D,I)}{P(H_{j}|D,I)}=\\\\\\\\frac{P(H_{i}|I)}{P(H_{j}|I)}\\\\\\\\frac{P(D|H_{i},I)}{P(D|H_{j},I)}$$</p>\\n\\n<p>So if the prior probabilities are equal, then the relative probabilities are given by the likelihood ratio.  So you also need to specify a probability distribution for what type of data sets you would be likely to see if the mean was the best statistic, etc.  Note that each hypothesis doesn\\'t actually state what the specific value of the mean, median, or IQR actually is.  Therefore, the probability cannot depend on the exact value of the mean.  Hence in the likelihoods these must have been \"integrated out\" using the sum and product rules</p>\\n\\n<p>$$P(D|H_{i},I)=\\\\\\\\int P(\\\\\\\\theta_{i}|H_{i},I)P(D|\\\\\\\\theta_{i},H_{i},I)d\\\\\\\\theta_{i}$$</p>\\n\\n<p>So you have the prior $P(\\\\\\\\theta_{i}|H_{i},I)$ which can be interpreted for i=mean as \"given that the mean is the best statistic, and prior to seeing the data, what values of the mean are we likely to see?\" and the likelihood $P(D|\\\\\\\\theta_{i},H_{i},I)$ can be similarly interpreted as \"given the mean is best, and equal to $\\\\\\\\theta_{mean}$ how likely is the data that was observed?\".  This may help you come up with some kinds of features that your distribution should have.</p>\\n\\n<p>This describes the inference - now it is time to apply decision theory.  This is particularly simple because your decision doesn\\'t influence the state of nature - the statistic won\\'t change if you do or don\\'t use it.  So we can describe the decisions ($A$ for \"action\" because $D$ is already taken):</p>\\n\\n<p>$$\\\\\\\\begin{array}{l l}\\\\\\\\n  A_{mean}:\\\\\\\\text{The mean is the reported statistic}\\\\\\\\n\\\\\\\\\\\\\\\\A_{med}:\\\\\\\\text{The median is the reported statistic}\\\\\\\\n\\\\\\\\\\\\\\\\A_{IQR}:\\\\\\\\text{The IQR is the reported statistic}\\\\\\\\n\\\\\\\\end{array}\\\\\\\\n$$</p>\\n\\n<p>And now you need to specify a loss matrix $L_{ij}$ which relates the action/decision $A_{i}$ to the state of nature $H_{j}$ - what is the loss if I report the mean, but the median is actually the best statistic?  In most cases the diagonal elements will be zero - taking the correct action means no loss.  You may also have that all non-diagonal elements are equal - how you are wrong doesn\\'t matter, only whether or not you are wrong.</p>\\n\\n<p>You then proceed by calculating the average loss for each action, weighted by their probabilities:</p>\\n\\n<p>$$L_{i}=\\\\\\\\sum_{j}L_{ij}P(H_{j}|D,I)$$</p>\\n\\n<p>And you then choose the action with the smallest average loss.</p>\\n',\n",
       " '<p>It may be an overshoot, but you may train an unsupervised Random Forest on the data and use the object proximity measure to detect outliers. More details <a href=\"http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#outliers\" rel=\"nofollow\">here</a>.</p>\\n',\n",
       " '<p>I am trying to estimate the following unobserved components model using the MARSS package </p>\\n\\n<p>$y_t = \\\\\\\\mu_t + \\\\\\\\varepsilon_t $</p>\\n\\n<p>$\\\\\\\\mu_t = \\\\\\\\mu_{t-1} + \\\\\\\\beta_{t-1}$</p>\\n\\n<p>$\\\\\\\\beta_t = \\\\\\\\beta_{t-1} + \\\\\\\\zeta_{t}$</p>\\n\\n<p>with a restriction on the variance of the errors: $\\\\\\\\sigma^2_{\\\\\\\\zeta} = \\\\\\\\frac{1}{\\\\\\\\lambda}\\\\\\\\sigma^2_{\\\\\\\\varepsilon}$ for a given $\\\\\\\\lambda$. $y_t$ are the observed series and $\\\\\\\\mu_t$ and $\\\\\\\\beta_t$ are the states. Those of you familiar with these models will recognize this as the Hodrick-Prescott filter.</p>\\n\\n<p>MARSS handles models in this form</p>\\n\\n<p>$y_t = Z x_t + v_t , v_t \\\\\\\\sim MVN(A,R) $</p>\\n\\n<p>$x_t = B x_{t-1} + w_t, w_t \\\\\\\\sim MVN(U,Q) $</p>\\n\\n<p>$x_0 \\\\\\\\sim MVN(x0,V0)$</p>\\n\\n<p>So I tried this</p>\\n\\n<pre><code>lambda=1600\\nB1 = matrix(c(1,0,1,1),2,2)\\nU1 = matrix(0,2,1)\\nQ1= matrix(list(),2,2)\\nQ1[1,1]=0\\nQ1[1,2]=0\\nQ1[2,1]=0\\nQ1[2,2]=\"q11*1/lambda\"\\nZ1 = matrix(c(1,0),1,2)\\nA1 = as.matrix(0)\\nR1 = as.matrix(\"q11\")\\npi1 = matrix(0,2,1)\\nV1 = diag(1,2)\\nmodel.list = list(B = B1, U = U1, Q = Q1, Z = Z1, A = A1, R = R1, x0 = pi1, V0 = V1)\\nfit = MARSS(data, model = model.list, control = list(kf.x0 = \"x00\"))\\n</code></pre>\\n\\n<p>But the specification for Q[2,2] is wrong and MARSS reads it as a new parameter to estimate. I know I can restrict parameters to be the same within the Q matrix and R matrix, but I don\\'t know how to impose the variance restriction that affects parameters of both matrices. Does anyone know a way around this? Maybe using substitute() ?</p>\\n\\n<p>I know there are packages to implement the HP filter as well as other state space estimation packages but I want to use MARSS since it uses the EM algorithm to estimate the parameters and I want to extend this model to others where convergence is harder to achieve.</p>\\n\\n<p>Thank you.</p>\\n',\n",
       " '<p><a href=\"http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.multil.jpg\"><strong>Feed-forward</strong></a> ANNs allow signals to travel one way only: from input to output. There are no feedback (loops); <em>i.e.</em>, the output of any layer does not affect that same layer. Feed-forward ANNs tend to be straightforward networks that associate inputs with outputs. They are extensively used in pattern recognition. This type of organisation is also referred to as bottom-up or top-down.</p>\\n\\n<p><img src=\"http://www.cs.iusb.edu/~danav/teach/c463/forward_nn.gif\" alt=\"A feed-forward network\"></p>\\n\\n<p><a href=\"http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.neural2.jpg\"><strong>Feedback</strong></a> (or recurrent or interactive) networks can have signals traveling in both directions by introducing loops in the network. Feedback networks are powerful and can get extremely complicated. Computations derived from earlier input are fed back into the network, which gives them a kind of memory. Feedback networks are dynamic; their \\'state\\' is changing continuously until they reach an equilibrium point. They remain at the equilibrium point until the input changes and a new equilibrium needs to be found.</p>\\n\\n<p><img src=\"http://www.cs.iusb.edu/~danav/teach/c463/rec_nn.gif\" alt=\"feedback architecture\"></p>\\n\\n<p>Feedforward neural networks are ideally suitable for modeling relationships between a set of predictor or input variables and one or more response or output variables. In other words, they are appropriate for any functional mapping problem where we want to know how a number of input variables affect the output variable. The multilayer feedforward neural networks, also called <a href=\"http://en.wikipedia.org/wiki/Multilayer_perceptron\">multi-layer perceptrons</a> (MLP), are the most widely studied and used neural network model in practice.</p>\\n\\n<p>As an example of feedback network, I can recall <a href=\"http://en.wikipedia.org/wiki/Hopfield_net\">Hopfield’s network</a>. The main use of Hopfield’s network is as associative memory. An associative memory is a device which accepts an input pattern and generates an output as the stored pattern which is most closely associated with the input. The function of the associate memory is to recall the corresponding stored pattern, and then produce a clear version of the pattern at the output. Hopfield networks are typically used for those problems with binary pattern vectors and the input pattern may be a noisy version of one of the stored patterns. In the Hopfield network, the stored patterns are encoded as the weights of the network.</p>\\n\\n<p><strong>Kohonen’s self-organizing maps</strong> (SOM) represent another neural network type that is markedly different from the feedforward multilayer networks. Unlike training in the feedforward MLP, the SOM training or learning is often called unsupervised because there are no known target outputs associated with each input pattern in SOM and during the training process, the SOM processes the input patterns and learns to cluster or segment the data through adjustment of weights (that makes it an important neural network model for dimension reduction and data clustering). A two-dimensional map is typically created in such a way that the orders of the interrelationships among inputs are preserved. The number and composition of clusters can be visually determined based on the output distribution generated by the training process. With only input variables in the training sample, SOM aims to learn or discover the underlying structure of the data.</p>\\n\\n<p>(The diagrams are from Dana Vrajitoru\\'s <a href=\"http://www.cs.iusb.edu/~danav/teach/c463/12_nn.html\">C463 / B551 Artificial Intelligence web site</a>.)</p>\\n',\n",
       " \"<p>To do this well takes not so much a programming background but significant training in multivariable modeling.  There are many aspects of the problem that need to be addressed first, including the causal pathway, appropriateness of predictors in other ways, appropriateness of the outcome variable, reliability of measurements, variable redundancy and data reduction, relaxing the linearity assumption, dealing with the additivity and proportional hazards assumptions, overfitting, mastering indexes of predictive accuracy, model validation, model interpretation.  [I'm the author of the R <code>rms</code> package]</p>\\n\",\n",
       " '<p>Since you ask for a reference, <a href=\"http://rads.stackoverflow.com/amzn/click/0387975179\" rel=\"nofollow\">Statistics methods: the geometric approach</a> explains this and many other properties by visualization in geometric space. For teaching purposes, it can also be useful because you can explain deep concepts to people with a weak background in calculus.</p>\\n\\n<p>@caracal\\'s answer is very comprehensive. All I will add is a little bit of background. Formally, angles and norms (distances) are defined from a scalar product. So all you need is a scalar product $\\\\\\\\langle x ; y \\\\\\\\rangle$.</p>\\n\\n<ul>\\n<li>The norm of a vector $x$ is then defined as $\\\\\\\\|x\\\\\\\\|^2 = \\\\\\\\langle x ; x \\\\\\\\rangle$.</li>\\n<li>The angle $\\\\\\\\theta$ between $x$ and $y$ is such that $\\\\\\\\cos \\\\\\\\theta = \\\\\\\\frac{\\\\\\\\langle x ; y \\\\\\\\rangle}{\\\\\\\\|x\\\\\\\\| \\\\\\\\cdot \\\\\\\\|y\\\\\\\\|}$.</li>\\n</ul>\\n\\n<p>What is a scalar product? By definition, it is a function that </p>\\n\\n<ol>\\n<li>is linear and symmetric in both of its arguments,</li>\\n<li>such that $\\\\\\\\langle x ; x \\\\\\\\rangle \\\\\\\\geq 0$,</li>\\n<li>and such that $\\\\\\\\langle x ; x \\\\\\\\rangle = 0$ if and only if $x = 0$.</li>\\n</ol>\\n\\n<p>We easily check that the covariance is a scalar product.</p>\\n\\n<ol>\\n<li>$Cov(X + \\\\\\\\lambda \\\\\\\\cdot Y ; Z) = Cov(X ; Z) + \\\\\\\\lambda \\\\\\\\cdot Cov(Y ; Z)$, and $Cov(X;Z) = Cov(Z;X)$,</li>\\n<li>$Cov(X ; X) = Var(X) \\\\\\\\geq 0$,</li>\\n<li>... but we don\\'t have $Var(X) = 0 \\\\\\\\iff X = 0$.</li>\\n</ol>\\n\\n<p>Actually, a random variable has zero variance if it is equal to a constant $a$. The difficulty is addressed by re-defining the space vector as the random variables, up to an additive constant (we say that two random variables are the same if they differ by a constant). With this new definition, we have a scalar product and we can interpret the coefficient of correlation (second bullet point above) as the cosine between two random variables. It is then obvious that maximizing the correlation is like minimizing the angle.</p>\\n\\n<p>This all comes from the properties of space vectors, which capture the essential features of geometry in simple formulas.</p>\\n',\n",
       " '<p>I\\'ll add some paper references when I\\'m back at a computer, but here are some simple suggestions:</p>\\n\\n<p>Definitely start by working with returns. This is critical to deal with the irregular spacing where you can naturally get big price gaps (especially around weekends). Then you can apply a simple filter to remove returns well outside the norm (eg. vs a high number of standard deviations). The returns will adjust to the new absolute level so large real changes will result in the loss of only one tick. I suggest using a two-pass filter with returns taken from 1 step and <em>n</em> steps to deal with clusters of outliers.</p>\\n\\n<p><em>Edit 1:</em> Regarding the usage of prices rather than returns: asset prices tend to not be stationary, so IMO that can pose some additional challenges. To account for the irregularity and power law effects, I would advise some kind of adjustment if you want to include them in your filter. You can scale the price changes by the time interval or by volatility.  You can refer to the \"realized volatility\" literture for some discussion on this.  Also discussed in Dacorogna et. al.   </p>\\n\\n<p>To account for the changes in volatility, you might try basing your volatility calculation from the same time of the day over the past week (using the seasonality).  </p>\\n',\n",
       " \"<p>I have data from patients treated with 2 different kinds of treatments during surgery.\\nI need to analyze its effect on heart rate. \\nThe heart rate measurement is taken every 15 minutes. </p>\\n\\n<p>Given that the surgery length can be different for each patient, each patient can have between 7 and 10 heart rate measurements. \\nSo an unbalanced design should be used. \\nI'm doing my analysis using R. And have been using the ez package to do repeated measure mixed effect ANOVA. But I do not know how to analyse unbalanced data. Can anyone help?</p>\\n\\n<p>Suggestions on how to analyze the data are also welcomed.</p>\\n\\n<p>Update:<br>\\nAs suggested, I fitted the data using the <code>lmer</code> function and found that the best model is:</p>\\n\\n<pre><code>heart.rate~ time + treatment + (1|id) + (0+time|id) + (0+treatment|time)\\n</code></pre>\\n\\n<p>with the following result:</p>\\n\\n<pre><code>Random effects:\\n Groups   Name        Variance   Std.Dev. Corr   \\n id       time        0.00037139 0.019271        \\n id       (Intercept) 9.77814104 3.127002        \\n time     treat0      0.09981062 0.315928        \\n          treat1      1.82667634 1.351546 -0.504 \\n Residual             2.70163305 1.643665        \\nNumber of obs: 378, groups: subj, 60; time, 9\\n\\nFixed effects:\\n             Estimate Std. Error t value\\n(Intercept) 72.786396   0.649285  112.10\\ntime         0.040714   0.005378    7.57\\ntreat1       2.209312   1.040471    2.12\\n\\nCorrelation of Fixed Effects:\\n       (Intr) time  \\ntime   -0.302       \\ntreat1 -0.575 -0.121\\n</code></pre>\\n\\n<p>Now I'm lost at interpreting the result. \\nAm I right in concluding that the two treatments differed in affecting heart rate? What does the correlation of -504 between treat0 and treat1 means?</p>\\n\",\n",
       " '<p>Simply, single ANOVA tests between-groups for a single independent variable (<code>col1</code>) for a response variable (<code>base</code>). Multiway ANOVA tests between-groups for two or more independent variables (<code>col1</code> and <code>col2</code>). The choice to use one or the other depends on your experimental design. </p>\\n\\n<p><code>base~col1</code> is testing a single \"main\" effect</p>\\n\\n<p><code>base~col1+col2</code> would be testing two \"main\" effects only</p>\\n\\n<p><code>base~col1*col2</code> would be testing two \"main\" effects along with the \"interaction\" effect between independent variables.</p>\\n\\n<p><a href=\"http://people.richland.edu/james/lecture/m170/ch13-2wy.html\" rel=\"nofollow\">Some more info about two-way ANOVA</a></p>\\n\\n<p><strong>Edit:</strong> Given your updated post:</p>\\n\\n<p>I suspect you made a mistake in the code you gave as example, but here are some explanations:</p>\\n\\n<pre><code>aov.ex2 &lt;- aov(base~col1+col2,data=data1) \\nsummary(aov.ex2)\\n</code></pre>\\n\\n<p>This would test for only main effects of col1 and main effects of col2. In other words, are there any differences between groups for col1 or col2?</p>\\n\\n<pre><code>aov.ex2 &lt;- aov(base~col1,data=data1) \\nsummary(aov.ex2)\\n</code></pre>\\n\\n<p>This would test for only the main effect of col1. In other words, are there any differences between groups for col1?</p>\\n\\n<p>It seems like your basic problem is that with the inclusion of a second independent variable you are getting different results. <a href=\"http://stats.stackexchange.com/questions/7620/main-effect-of-the-first-independent-variable-in-two-way-anova-lost-depending-on\">This post may be of relevance.</a></p>\\n',\n",
       " \"<p>The <em>reason</em> that we calculate standard deviation instead of absolute error is that we are <strong>assuming error to be normally distributed</strong>.  It's a part of the model.</p>\\n\\n<p>Suppose you were measuring very small lengths with a ruler, then standard deviation is a bad metric for error because you know you will never accidentally measure a negative length.  A better metric would be one to help fit a Gamma distribution to your measurements:</p>\\n\\n<p>$E(\\\\\\\\log(x)) - \\\\\\\\log(E(x))$</p>\\n\\n<p>Like the standard deviation, this is also non-negative and differentiable, but it is a better error statistic for this problem.</p>\\n\",\n",
       " \"<p>One rule per answer ;-)</p>\\n\\n<p>Talk to the statistician <em>before</em> conducting the study. If possible, before applying for the grant. Help him/her understand the problem you are studying, get his/her input on how to analyze the data you are about to collect and think about what that means for your study design and data requirements. Perhaps the stats guy/gal suggests doing a hierarchical model to account for who diagnosed the patients - then you need to track who diagnosed whom. Sounds trivial, but it's far better to think about this before you collect data (and fail to collect something crucial) than afterwards.</p>\\n\\n<p>On a related note: do a power analysis before starting. Nothing is as frustrating as not having budgeted for a sufficiently large sample size. In thinking about what effect size you are expecting, remember publication bias - the effect size you are going to find will probably be smaller than what you expected given the (biased) literature.</p>\\n\",\n",
       " '<p>Try using the keyword <code>TRUNC</code>, e.g. <code>TRUNC(MEAN(var1,var2,var3))</code>.</p>\\n',\n",
       " '<p>This problem is encountered in quality control/<a href=\"http://en.wikipedia.org/wiki/Statistical_process_control\" rel=\"nofollow\">statistical process control</a> settings.  There\\'s a large literature, as you have hinted, because different parameters as estimated in various ways from different forms of sampling different distributions can be expected to vary in different ways.  The purpose is to detect that variation on-line as soon as possible after it occurs without triggering too many false detections along the way.  Consider using a control chart (<a href=\"http://p://en.wikipedia.org/wiki/Control_chart\">1</a>, <a href=\"http://www.itl.nist.gov/div898/handbook/pmc/section3/pmc31.htm\" rel=\"nofollow\">2</a>).  In your concrete situation a good choice is a combined Shewhart-CUSUM control chart.</p>\\n',\n",
       " '<p><a href=\"http://flowingdata.com/2010/02/11/an-easy-way-to-make-a-treemap/\" rel=\"nofollow\">Flowing Data has a tutorial</a> on how to use the <code>map.market</code> function in the <code>portfolio</code> package in R.</p>\\n',\n",
       " \"<p>I need to fit a gamma distribution that is shifted to the left and truncated at zero (so that for example, my data may only come from the right tail of the full distribution, and I don't have any observations less than zero).  </p>\\n\\n<p>I can find the alpha and beta parameters to fit a regular old two parameter gamma no problem by MLE, but I can't find a good reference on how to fit the parameters for the shifted (and truncated) gamma case.  In the shifted-truncated Gamma case I have now three parameters: alpha, beta, and lambda, where lambda is the shifting parameter.</p>\\n\\n<p>Does anyone knows of a paper that shows how to fit a shifted gamma using MLE?  I'd also be open to hear about other methods for fitting such distribution...</p>\\n\\n<p>Suggestions are welcome!</p>\\n\\n<p>Thanks!</p>\\n\",\n",
       " '<p>If you use simple regression (say OLS), then $\\\\\\\\beta$ can be converted to a t-stat, which then can be converted to a p-value, which ranges between $0$ and $1$; a p-value of $0$ for $\\\\\\\\beta_i$ means a significant contribution of $X_i$ to $y$, whereas a p-value of 1 means no contribution of $X_i$ to $y$.\\nDoes this help?</p>\\n',\n",
       " \"<p>It the case of <code>gplot</code> it means what it says, that the graph in question is limited to observations where <code>var1=.</code>, i.e. where <code>var1</code> is missing. I tried but was unable to break this behavior, and as far as I can see the manual doesn't specify any other behavior.</p>\\n\\n<p>If these graphs are unwanted then a simple <code>where</code> statement at any of several places would remove them. E.g. place this somewhere before <code>proc gplot</code>:</p>\\n\\n<pre><code>data;\\n    set;\\n    where not missing (var1);\\nrun;\\n</code></pre>\\n\\n<p>If these graphs are unexpected then I advise you to look through your data and programs to see if something went wrong somewhere. For me personally unexpected missing data tends to indicate I misspelled a variable name somewhere.</p>\\n\\n<p>For a more specific answer you'll have to give more specific information.</p>\\n\",\n",
       " '<p>Joshua Epstein wrote a paper titled \"Why Model?\" available at <a href=\"http://www.santafe.edu/media/workingpapers/08-09-040.pdf\">http://www.santafe.edu/media/workingpapers/08-09-040.pdf</a> in which gives 16 reasons:</p>\\n\\n<ol>\\n<li>Explain (very distinct from predict)</li>\\n<li>Guide data collection</li>\\n<li>Illuminate core dynamics</li>\\n<li>Suggest dynamical analogies</li>\\n<li>Discover new questions</li>\\n<li>Promote a scientific habit of mind</li>\\n<li>Bound (bracket) outcomes to plausible ranges</li>\\n<li>Illuminate core uncertainties.</li>\\n<li>Offer crisis options in near-real time</li>\\n<li>Demonstrate tradeoffs / suggest efficiencies</li>\\n<li>Challenge the robustness of prevailing theory through perturbations</li>\\n<li>Expose prevailing wisdom as incompatible with available data</li>\\n<li>Train practitioners</li>\\n<li>Discipline the policy dialogue</li>\\n<li>Educate the general public</li>\\n<li>Reveal the apparently simple (complex) to be complex (simple)</li>\\n</ol>\\n\\n<p>(Epstein elaborates on many of the reasons in more detail in his paper.)</p>\\n\\n<p>I would like to ask the community:</p>\\n\\n<ul>\\n<li>are there are additional reasons that Epstein did not list?</li>\\n<li>is there a more elegant way to conceptualize (a different grouping perhaps) these reasons?</li>\\n<li>are any of Epstein\\'s reasons flawed or incomplete?</li>\\n<li>are their clearer elaborations of these reasons?</li>\\n</ul>\\n',\n",
       " '<p>Parsimony is your enemy.  Nature does not act parsimoneously, and datasets do not have enough information to allow one to choose the \"right\" variables.  It doesn\\'t matter very much which method you use or which index you use as a stopping rule.  Variable selection without shrinkage is almost doomed.  However limited backwards stepdown (with $\\\\\\\\alpha=0.50$) can sometimes be helpful.  It works simply because it will not delete many variables.</p>\\n',\n",
       " \"<p>I typically like to work with normalized data.  Imagine I have data on height of Americans (in cm) and I have predictor variables Weight (in kg), and Age (in years).  If I don't standardize the data, I might get a regression such as age Height (in cm) = 2 * Weight(in kg) + 3*Age(in years).  The physical units don't make sense to me.  How do I take a combination of kg and years to create cm?  A physicist would complain.  </p>\\n\\n<p>By standardizing the data, which typically refers to subtracting the mean and dividing by the standard deviation, we are able to remove the confusing units.  If weight is in kg, its standard deviation will also be in kg.  Dividing some number in kg with another number in kg yields a number with no units.  Doing this is essentially the same as working with each variable's Z score.  Now, in the end I might see </p>\\n\\n<p>Unitless measurement for Height = Unitless measurement for Weight + 0.5 * Unitless measurement for Age</p>\\n\\n<p>Later on if you decide to use other techniques for modeling like PCA, it will become very important to work with normalized data.  In the PCA case, it is important to have mean 0 data, but having standard deviation 1 is not so important.</p>\\n\",\n",
       " '<p>I am analyzing data with a binary outcome and a variety of continuous and categorical (including dichotomous) predictor variables. My approach is to perform a binary logistic regression and to treat any predictor with more than 20 unique values as continuous. Several arguments against categorization, especially well-documented on <a href=\"http://biostat.mc.vanderbilt.edu/wiki/Main/CatContinuous\" rel=\"nofollow\">Frank Harrell\\'s site</a> are a good reason to not categorize.</p>\\n\\n<p>However, at a recent meeting where I discussed my analyses approach, a faculty suggested that I would get more accurate risk estimates for the data if I categorized the variables which have a skewed distribution and outliers. Their logic was that the tail of the skewed distribution and the outliers in that tail will have a detrimental effect on the risk estimates generated by the logistic regression and that categorization will address this by erasing the effect of the tail and the skew.</p>\\n\\n<p>I have several predictor variables that definitely have skewed distributions and some outliers. Is the claim that a variable with a skewed distribution (with outliers) is more likely to produce inaccurate risk estimates compared to the categorized version of the same variable, true? How do skews and outliers in the tail affect logistic regression estimates? </p>\\n',\n",
       " '<p>I really hope I\\'m not asking this question on the wrong stackexchange site, but seen as this question has a lot to do with statistics I\\'m guessing I might be in the right place.</p>\\n\\n<p>There are certain rules in medical statistics as I understand, for example results that show below 7% difference between the test group and control group are(if I\\'m correct?) normally discarded as statistically irrelevant.</p>\\n\\n<p>Now tried I googling for an hour the source of a statement I once heard(I do not like to say something without checking/evaluating it\\'s source). </p>\\n\\n<p>The statement was something like any \\'totally negative genetic affliction/purely bad gene\\' has lower than 2%(or 1%?) prevalence in the general population as natural selection quickly weeds it out. ADHD for example has about 4-5% prevalence so it\\'s highly unlikely that any genetic material connected to it is purely bad, it\\'s more likely something that was or is useful in some other way/situation. (The term might of been \"deleterious alleles\", but I\\'m not entirely sure)</p>\\n\\n<p><strong>Any idea how this is called within medical statistics or where I can find this statement I\\'m referring to?</strong></p>\\n\\n<p>(P.S.: Did I use the right tags?)</p>\\n',\n",
       " '<p>Software:\\nI advice to use R because it is free and designed for data analysis.\\n<a href=\"http://cran.r-project.org/\" rel=\"nofollow\">http://cran.r-project.org/</a></p>\\n\\n<p>About your first question, the R code to answer it, is the following (I suppose that your data are in the file \"data.csv\")</p>\\n\\n<pre><code># load the file \"data.csv\"\\nd &lt;- read.table(\"data.csv\",header=T,sep=\",\")\\n\\n# create a data frame for the results\\nres &lt;- data.frame(component=paste(\"op\",1:(ncol(d)-2),sep=\"\"),timeMax=rep(0,ncol(d)-2),timeMin=rep(0,ncol(d)-2),timeAverage=rep(0,ncol(d)-2),timeSD=rep(0,ncol(d)-2),samples=rep(0,ncol(d)-2))\\n\\n# loop over the operations\\nfor (ind in 3:ncol(d))\\n    {\\n    res[ind-2,2:6]&lt;-c(max(d$time*d[,ind]),min(d$time*d[,ind]),mean(d$time*d[,ind]),sd(d$time*d[,ind]),sum(d[,ind]))\\n    }\\n</code></pre>\\n\\n<p>For the question about the combinations, you could use the same trick by replacing\\n<code>d$time*d[,ind]</code>\\nby\\n<code>d$time*d[,ind1]*d[,ind2]</code>\\nif you want to obtain statistics about the combination of op1 and op2. But if you have many op, this is maybe not suited to your case as the number of combinations is equal to 2^N... Is N large? Do you want statistics about all the possible combinations or only about some of them?</p>\\n',\n",
       " '<p>I have data on 26 participants (13 from computing and remaining 13 from non computing) who have participated in my research. Each participant is treated with a lab module (Hands on Robotics Session). Now each participant will be evaluated using a rubric on scale of 1 to 4. This experiment has both pre and post test.</p>\\n\\n<p>For my research i want to evaluate the following questions:</p>\\n\\n<h3>Research question 1</h3>\\n\\n<ul>\\n<li>Null Hypothesis: students do not learn about computational thinking (programming basics and algorithmic thinking) with the help of robotics.</li>\\n<li>Alternate Hypothesis: Students learn about computational thinking (programming basics and algorithmic thinking) with the help of robotics.</li>\\n</ul>\\n\\n<p>To evaluate the above question, the categories i will be considering are Plan, Implementation and Knowledge gained on a scale of Excellent, Good, Fair and Poor. </p>\\n\\n<p>I think i should use DEPENDENT T-TEST FOR PAIRED SAMPLES. Am i correct?</p>\\n\\n<h3>Research question 2:</h3>\\n\\n<ul>\\n<li>Null Hypothesis: Participants background (computing and non computing) has no effect in learning about algorithmic thinking with help of robotics</li>\\n<li>Alternate Hypothesis:Participants background (computing and non computing) has an effect in learning about algorithmic thinking with help of robotics </li>\\n</ul>\\n\\n<p>I will also evaluate question 2 on a scale of Excellent, Good, Fair and Poor, but with respect to background.</p>\\n\\n<h3>My Statistical Question</h3>\\n\\n<p><strong>Which T test should I use for each of my research questions?</strong></p>\\n',\n",
       " '<p>You could test the differences between the two models by fitting one logistic regression over both data sets using a dummy variable to distinguish observations from each population. Then you could use interaction terms to simultaneously fit the two models.  </p>\\n\\n<p>Create a dummy variable <em>Pop</em>, setting the value equal to zero for observations from the first data set and to one for the second data set. Then, fit a logistic regression including the population dummy as a predictor and include interactions between the population dummy and all other predictors. </p>\\n\\n<p>For example if you had one predictor $X_1$, you would fit the following model:</p>\\n\\n<p>$$\\\\\\\\nlogit(p_i) = \\\\\\\\beta_0 + \\\\\\\\beta_1 \\\\\\\\cdot Pop + \\\\\\\\beta_2 \\\\\\\\cdot X_1 + \\\\\\\\beta_3 \\\\\\\\cdot Pop \\\\\\\\cdot X_1  \\\\\\\\n$$</p>\\n\\n<p>Then you can test whether constraining $\\\\\\\\beta_1$ and $\\\\\\\\beta_3$ to zero significantly worsens the fit of the model using a likelihood ratio test (LRT). These two variables give the difference in the intercept and slope coefficients in the linear prediction equation when <em>Pop</em> is one versus zero; if you can set them to zero and not significantly worsen the fit of your model, it suggests you can use the same regression model for both data sets.</p>\\n\\n<p>You could also just check individual significance of the interaction term coefficients for example by Wald test stats, but I think that comparing models with and without all interaction terms at once via an LRT sounds closer to what you want. Also, likelihood ratio tests are <a href=\"http://stats.stackexchange.com/questions/3586/inconsistency-between-chi-sq-and-ci-estimation-using-wald-test\">generally preferred to Wald tests</a>. </p>\\n',\n",
       " \"<p>Could anyone advise if the following makes sense:</p>\\n\\n<p>I am dealing with an ordinary linear model with 4 predictors. I am in two minds whether to drop the least significant term. It's $p$-value is a little over 0.05. I have argued in favor of dropping it along these lines:\\nMultiplying the estimate of this term by (for example) the interquartile range of the sample data for this variable, gives some meaning to the clinical effect that keeping this term has on the overall model. Since this number is very low, approximately equal to typical intra-day range of values that the variable can take when measuring it in a clinical setting, I see it as not clinically significant and could therefore be dropped to give a more parsimonious model, even though dropping it reduces the adjusted $R^2$ a little.</p>\\n\",\n",
       " '<p>As people pointed out in the comments, your description doesn\\'t say whether the number of balls which are white and labelled $a$ is $60$ (all white are labelled $a$), $24$ (the label is independent of the color), or $0$ (all white are not labelled $a$). However, what you can say is that it is quite unlikely to get $20$ out of $30$ white balls labelled $a$ if the balls are chosen uniformly. This would be most probable with $60$ white balls labelled $a$, and it\\'s still <strong>astronomically unlikely</strong>.</p>\\n\\n<p>The relevant distribution is a <a href=\"http://en.wikipedia.org/wiki/Hypergeometric_distribution\" rel=\"nofollow\">hypergeometric distribution</a>. If you assume that there are $60$ white balls labelled $a$ in the population of size $1000$, then the chance to get exactly $20$ out of $30$ is </p>\\n\\n<p>$$\\\\\\\\frac{{60 \\\\\\\\choose 20}{940 \\\\\\\\choose 10}}{1000 \\\\\\\\choose 30} \\\\\\\\approx 2.44 \\\\\\\\times 10^{-19}. $$</p>\\n\\n<p>However, instead of the exact chance to get $20$, you should consider the chance to get $20$ or more, which is $2.49 \\\\\\\\times 10^{-19}$. This is an upper bound for the chance that $20$ out of $30$ will be both white and labelled with $a$.</p>\\n',\n",
       " '<p><a href=\"http://stats.stackexchange.com/questions/11887/is-this-design-a-one-way-repeated-measures-anova-or-not\">Here</a> I described my general situation. How to calculate 95%CI between means in R?</p>\\n',\n",
       " '<p>Is there a \"rule\" to determine the minimum sample size required for a t-test to be valid?</p>\\n\\n<p>For example, a comparison needs to be performed between the means of 2 populations.</p>\\n\\n<p>There are 7 data points from one population and only 2 data points from the other.</p>\\n\\n<p>Unfortunately, the experiment is very expensive and time consuming, and obtaining more data is not feasible.</p>\\n\\n<p>Can a t-test be used? Why or why not? Please provide details (the population variances and distributions are not known).</p>\\n\\n<p>If a t-test can not be used, can a non parametric test (Mann Whitney) be used? Why or why not?</p>\\n\\n<p>Thanks a lot for your help! I know very little about stats, and have been searching around the internet for a detailed answer for some time.</p>\\n',\n",
       " \"<p>I'm not sure where to post this, so I thought I'd post it here. If there is another better place to ask this question then please could you let me know. </p>\\n\\n<p>Let\\n$\\\\\\\\beta$  = rate of infection, and $\\\\\\\\gamma$ = rate of recovery.</p>\\n\\n<p>I don't understand how the basic reproduction number (BRN) is $\\\\\\\\frac{\\\\\\\\beta}{\\\\\\\\gamma}$?</p>\\n\\n<p>I know there is something to do with the exponential distribution, and the fact that the mean of the exponential distribution is $ \\\\\\\\frac{1}{\\\\\\\\lambda}$, so clearly here $ \\\\\\\\lambda = \\\\\\\\gamma$, but I don't understand why.</p>\\n\\n<p>Is it: BRN is the number of secondary infections caused by one infection, so I thought it would be $\\\\\\\\beta$, I don't get why the $\\\\\\\\frac{1}{\\\\\\\\gamma}$ comes into it.</p>\\n\\n<p>Please could someone explain this?</p>\\n\",\n",
       " '<p>I think SPSS, like most modern software, uses the <a href=\"http://en.wikipedia.org/wiki/Mersenne_twister\">Mersenne Twister</a>. Its period is $2^{19937} − 1$ so you’re pretty safe from this point of view.</p>\\n\\n<p>Up to 623 successive outcomes are uncorrelated, so you can safely consider a few consecutive outcomes as independent (this would not be the case with a classical <a href=\"http://en.wikipedia.org/wiki/Linear_congruential_generator\">Linear congruential generator</a>).</p>\\n\\n<p>To summarize: modern random number generators are performant enough for all ordinary applications in statistics... don’t worry.</p>\\n',\n",
       " '<p>I assume the question refers to the error on the parameter estimates. To assess the linear relationship between two variables x and y we use linear regression to estimate the two parameters intercept and slope. </p>\\n\\n<p>It is easy to demonstrate that the last two options are identical, because during linear regression we minimize the sum of squared residuals which in this particular case amounts to the same as averaging all y at a particular x value.</p>\\n\\n<p>However, there is a slight difference between the first option and the last two. We have the same number of data points or measurements, but in the first case we sample y at more x locations; in the second case we sample at any particular x value more often.</p>\\n\\n<p>The following R code simulates the first and second scenario.</p>\\n\\n<pre><code>for (i in 1:1e3) {\\n#1000 different values of x, get a single y for each x\\nx&lt;-runif(1e3);\\nnoise&lt;-rnorm(length(x),sd=0.1);\\ny&lt;-x+noise;\\np1&lt;-rbind(p1,as.array(lm(y~x)$coeff));\\n\\n#100 different values of x, run 10 experiments for each x\\nx_rep&lt;-rep(runif(1e2),times=1e1);\\nnoise&lt;-rnorm(length(x_rep),sd=0.1);\\ny_rep&lt;-x_rep+noise;\\np2&lt;-rbind(p2,as.array(lm(y_rep~x_rep)$coeff));\\n}; \\n\\n# differences between standard deviations of intercepts and slopes\\napply(p1,2,sd)[[1]]-apply(p2,2,sd)[[1]]\\napply(p1,2,sd)[[2]]-apply(p2,2,sd)[[2]]\\n</code></pre>\\n\\n<p>The for loop repeats the two scenarios many times, running linear regression each time. At the end we calculate the standard deviation of intercept and slope across repeats. The first scenario might be slightly better as its standard deviation seems to be consistently smaller.</p>\\n',\n",
       " '<p>An alternative solution is to use the <code>seqgranularity</code> function provided by the <code>TraMineRextras</code> package.\\nThis function changes the time granularity using different methods, currently either <code>\"first\"</code> state or <code>\"last\"</code> state, but other methods such as choosing the most frequent state in the aggregated spell should be implemented in the future.</p>\\n\\n<p>For the example above, you would just use </p>\\n\\n<pre><code>mvadg.seq.year &lt;- seqgranularity(mvad.seq, tspan=12, method = \"first\")\\n</code></pre>\\n\\n<p><code>TraMineRextras</code> should be made available on the CRAN in the near future. In the meantime,  if you have the latest version of R, you can just install it from R-Forge with</p>\\n\\n<pre><code>install.packages(\"TraMineRextras\", repos=\"http://R-Forge.R-project.org\")\\n</code></pre>\\n',\n",
       " '<p>If the errors are not normally distributed, asymptotic results can be used. Suppose your model is</p>\\n\\n<p>$$y_i=x_i&#39;\\\\\\\\beta+\\\\\\\\varepsilon_i$$</p>\\n\\n<p>where $(y_i,x_i&#39;,\\\\\\\\varepsilon_i)$, $i=1,...,n$ is an iid sample. Assume</p>\\n\\n<p>\\\\\\\\begin{align*}\\nE(\\\\\\\\varepsilon_i|x_i)&amp;=0  \\\\\\\\\\\\\\\\\\nE(\\\\\\\\varepsilon_i^2|x_i)&amp;=\\\\\\\\sigma^2\\n\\\\\\\\end{align*}</p>\\n\\n<p>and </p>\\n\\n<p>$$rank(Ex_ix_i&#39;)=K,$$</p>\\n\\n<p>where $K$ is the number of coefficients. Then usual OLS estimate $\\\\\\\\hat\\\\\\\\beta$ is asymptoticaly normal:</p>\\n\\n<p>$$\\\\\\\\sqrt{n}(\\\\\\\\hat\\\\\\\\beta-\\\\\\\\beta)\\\\\\\\to N(0,\\\\\\\\sigma^2E(x_ix_i&#39;))$$</p>\\n\\n<p>Practical implications of this result are that the usual t-statistics become z-statistics, i.e. their distribution is normal instead of Student. So you can interpret t-statistics as usual, only p-values should be adjusted for normal distribution. </p>\\n\\n<p>Note that since this result is asymptotic, it does not hold for small sample sizes. Also the assumptions used can be relaxed. </p>\\n',\n",
       " '<p>This may be overkill, but <a href=\"http://www.cs.toronto.edu/~hinton/\" rel=\"nofollow\">Geoffrey Hinton</a> has been doing something like this using <a href=\"http://www.cs.toronto.edu/~ilya/pubs/2011/LANG-RNN.pdf\" rel=\"nofollow\">artificial neural networks and backpropagation in time</a>.  </p>\\n\\n<p>He trains his model using blocks of text from wikipedia or NYTimes, the network learns the chained statistical relationships you mention in your question.  Then the network can iteratively generate characters from a short prompt.  </p>\\n\\n<blockquote>\\n  <p>The sample below was obtained by running the MRNN less than 10 times\\n  and selecting the most intriguing sample. The beginning\\n  of the paragraph and the parentheses near the end are par-\\n  ticularly interesting. The MRNN was initialized with the\\n  phrase “<strong>The meaning of life is</strong>”:</p>\\n  \\n  <p><strong>The meaning of life is</strong> the tradition of the ancient human reproduction: it is less favorable to the good boy for when to remove\\n  her bigger. In the show’s agreement unanimously resurfaced. The\\n  wild pasteured with consistent street forests were incorporated\\n  by the 15th century BE. In 1996 the primary rapford undergoes\\n  an effort that the reserve conditioning, written into Jewish cities,\\n  sleepers to incorporate the .St Eurasia that activates the popula-\\n  tion. Mar??a Nationale, Kelli, Zedlat-Dukastoe, Florendon, Ptu’s\\n  thought is. To adapt in most parts of North America, the dynamic\\n  fairy Dan please believes, the free speech are much related to the</p>\\n</blockquote>\\n\\n<p>The take-home from this paper is that the model was able to learn the fact that characters are organized into words, to learn many of those words, and to learn some of the rules of how words are strung together grammatically.  That may be much more structure than the ABC character sequence you had in mind, so I think user4581 \\'s answer is more appropriate.  But this one is a lot of fun.</p>\\n',\n",
       " '<p>If this is truly a scale, and the 11 observed items are endogenous, then your score contains a measurement error, and putting it into a regression model leads to biases: your estimates will be shrunk towards zero (see <a href=\"http://www.citeulike.org/user/ctacmo/article/2663962\" rel=\"nofollow\">http://www.citeulike.org/user/ctacmo/article/2663962</a>). This is a poor man\\'s strategy for somebody who has SPSS, but does not have AMOS. If you have AMOS, there is little excuse in putting together a model that uses composite scores, rather than a full SEM that incorporates the measurement model for these 11 items. Besides improvements in accuracy of the estimates, you will also get the overall fit test that would allow you to judge how well your model fits the data.</p>\\n',\n",
       " '<p>A few weeks back I used the following command:</p>\\n\\n<p><strong>accuracy(train,test)</strong></p>\\n\\n<p>where train and test are training and test data respectively. Last night I updated R and the forecast package and used the same command and I got error. After trying a little I used the following command</p>\\n\\n<p><strong>accuracy(train,test[1:30])</strong></p>\\n\\n<p>and it worked. (I was checking the accuracy of 30 forecasted values). \\nIs there some change in the forecast package or did I do something wrong?</p>\\n\\n<p>regards\\nLeo</p>\\n',\n",
       " '<p>I\\'m helping my father to translate a text from my own native Latvian language to English. Even though I\\'m a fair English speaker and have a bit of mathematics in my past, I never did have a firm grasp of statistics. :(</p>\\n\\n<p>The term I\\'m looking for has something to do with error calculation. A GPS device is measuring coordinates and then trying to estimate the error. In wikipedia I\\'ve come across two different likely terms - <a href=\"http://en.wikipedia.org/wiki/Mean_squared_error\" rel=\"nofollow\">Mean squared error</a> and <a href=\"http://en.wikipedia.org/wiki/Root_mean_square_deviation\" rel=\"nofollow\">Root mean square error</a>. From what I can understand, they are not one and the same thing, but what each of them means is beyond me.</p>\\n\\n<p>A direct translation from Latvian language would be \"average (mean?) squared error estimate\". No idea what that means either, I\\'m afraid.</p>\\n\\n<p>Perhaps someone here has a better idea?</p>\\n',\n",
       " '<p>It can only be the case that variables are correlated but independent if you mean something different by independence than statistical independence because if two variables are statistically indepedent they must be uncorrelated.  In regression, ANOVA or ANCOVA it is okay for the so-called \"independent\" variables to have some correlation with each other (and hence not be statistically independent).  Problems arise when the variables are highly correlated because for example in regression the regression coefficients can be unstable due to near multicollinearity.</p>\\n',\n",
       " \"<p>I want to test the effect of a set of predictors (ecological and morphological factors) on a categorical response variable (an animal behaviour). \\nAs far as I've read, random forests do not make assumptions about data independence. Therefore, can I use species in my analysis disregarding their relatedness, that is, not using phylogenetic signals (which measure tendency of related species to resemble each other more than species drawn at random from the same tree)??? Is it correct?\\nOtherwise, I could find for example, that one behaviour is well explained by body size, not because body size has some kind of relationship with that behaviour, rather because most species that show that trait are indeed of the same taxonomic family, and that family is mainly composed by same size species.</p>\\n\\n<p>If this is the case, can I use a taxonomic level as explanatory variables (e.g. genus, family or order)??? Would it give me the real importance of relatedness on that behaviour? (e.g.if in my importance list a get that my taxonomic level is the most important factor, can I interpret that the behaviour is mostly dependent on phylogenetic reasons rather than eco-morphological factors???)</p>\\n\\n<p>What about classification trees on this regard?</p>\\n\\n<p>I haven't found anything on papers and books in this regard.\\nThank you in advance for the help!</p>\\n\",\n",
       " \"<p>What are some approaches for classifying data with a variable number of features?</p>\\n\\n<p>As an example, consider a problem where each data point is a vector of x and y points, and we don't have the same number of points for each instance. Can we treat each pair of x and y points as a feature? Or should we just summarize the points somehow so each data point has a fixed number of features?</p>\\n\",\n",
       " \"<p>You can add </p>\\n\\n<p>Drift: ARIMA(0,1,0) with constant.</p>\\n\\n<p>Damped Holt's: ARIMA(0,1,2)</p>\\n\\n<p>Additive Holt-Winters: SARIMA(0,1,$m+1$)(0,1,0)$_m$.</p>\\n\\n<p>However, HW uses only three parameters and that (rather strange) ARIMA model has $m+1$ parameters. So there are a lot of parameter constraints.</p>\\n\\n<p>The ETS (exponential smoothing) and ARIMA classes of models overlap, but neither is contained within the other. There are a lot of non-linear ETS models that have no ARIMA equivalent, and a lot of ARIMA models that have no ETS equivalent. For example, all ETS models are non-stationary.</p>\\n\",\n",
       " \"<p>I wonder whether it is possible to perform within R a clustering of mixed data variables. In other words I have a data set containing both numerical and categorical variables within and I'm finding the best way to cluster them. In SPSS I would use two - step cluster. I wonder whether in R can I find a similar techniques. Thanks in advance. I was told about poLCA package, but I'm not sure...</p>\\n\",\n",
       " \"<p>I keep hearing my professor try to explain that we can use robust standard errors when we run a regression to confront the issue of heteroskedasticity. However I don't quite understand how telling Stata to use the robust standard errors is different than using regular standard errors. If the regular standard errors have a risk of being a problem wouldn't we always want to use robust standard errors then?</p>\\n\",\n",
       " '<p>What will be the easiest way to compare the derivations (clinical parameters) from two contingency tables? I want to test two 2x2 tables with their respective True Positive, True Negative, False Positive and False Negative entries ($H_0$: Table 1 = Table 2 <em>vs.</em> $H_1$: Table 1 ≠ Table 2). I want to compare statistically the sensitivities, specificities, negative and positive predictive values of the two tables. Sample size for the two tables is different. </p>\\n',\n",
       " '<p>One situation you would want to avoid $R^2$ is multiple regression, where adding irrelevant predictor variables to the model can in some cases increase $R^2$. This can be addressed by using the <a href=\"http://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2\">adjusted $R^2$</a> value instead, calculated as</p>\\n\\n<p>$\\\\\\\\bar{R}^2 = 1 - (1-R^2)\\\\\\\\frac{n-1}{n-p-1}$ where $n$ is the number of data samples, and $p$ is the number of regressors not counting the constant term.</p>\\n',\n",
       " '<p>This can be done using relational database. R has a nice implementation of this (see this post on <a href=\"http://stackoverflow.com/questions/1169551/sql-like-functionality-in-r\">sqldf</a>). MS Access (or even Excel) will work just as well.</p>\\n\\n<p>The idea here is you want to create a table that maps a number (as you say, of 5/6 digits) to a geographical region (75 or however many you have). Then, you join your table of 10000 records onto your reference table.</p>\\n\\n<p>Let table <code>mydata</code> contain your 10000 records and holds at least 1 column:</p>\\n\\n<ol>\\n<li>ID - contains your \\'cell number\\'</li>\\n</ol>\\n\\n<p>Let table <code>myreftable</code> contain your reference table, which should have exactly 1 row for each geographic region, and holds 2 columns:</p>\\n\\n<ol>\\n<li>ID - contains the relevent 5/6 digits of your \\'cell number\\'</li>\\n<li>Geo - contains the description of the geographic region</li>\\n</ol>\\n\\n<p>The table you\\'d want would be generate by the following SQL:</p>\\n\\n<pre><code>select\\n   m.ID as cell_number\\n   ,r.Geo as geo_region\\nfrom mydata m\\ninner join myreftable r on left(m.ID,6)=r.ID\\n</code></pre>\\n\\n<p>... where \\'left()\\' is any function that takes the first \\'n\\' characters of a string. Each database has different text/string functions you can use for this purpose. </p>\\n',\n",
       " '<p>Considering a simple linear regression model Y= beta0+beta1 x X, with beta0 and beta1 computed, I have to estimate the expected X given a new Y and 95% confidence intervals. \\nI used the formula X=(Y-beta0)/slope. How can I compute in R the 95% interval for the calculated value of ind, given a new value of the height?</p>\\n\\n<blockquote>\\n  <p>head(df)</p>\\n</blockquote>\\n\\n<pre><code>   ind height\\n1 4.27    174\\n2 3.60    159\\n3 3.61    175\\n\\nsummary(lm(df$ind~df$height))\\n\\nCall:\\nlm(formula = df$ind ~ df$height)\\n\\nResiduals:\\n Min       1Q   Median       3Q      Max \\n\\n-0.56263 -0.27596  0.03866  0.26632  0.55440 \\n\\nCoefficients:\\n         Estimate Std. Error t value Pr(&gt;|t|)   \\n(Intercept) -0.968895   1.512371  -0.641  0.52903   \\n\\ndf$height    0.027871   0.008985   3.102  0.00562 **\\n\\nResidual standard error: 0.3287 on 20 degrees of freedom\\nMultiple R-squared: 0.3248,     Adjusted R-squared: 0.2911 \\nF-statistic: 9.622 on 1 and 20 DF,  p-value: 0.005621 \\n</code></pre>\\n\\n#\\n\\n<p>I tried:</p>\\n\\n<pre><code>pred.frame &lt;- data.frame(ind=seq(3,5,0.25)) \\npredict(bclm,int=\"c\",level=0.95,data=pred.frame)\\n    fit      lwr      upr\\n1  174.5780 169.3146 179.8414\\n2  166.7696 163.6419 169.8973\\n3  166.8862 163.7806 169.9917\\n...............\\n</code></pre>\\n',\n",
       " \"<p>Pearson's chi-squared test for association can be used for this sort of problem.  For tables with low expected values, like your one, Fisher's Exact Test is better.  But you don't need to do any statistical test to see that you need more data to tell whether A or B has the highest breakage rate.</p>\\n\",\n",
       " \"<p>I plan to do a simulation study where I compare the performance of several robust correlation techniques with different distributions (skewed, with outliers, etc.). With <em>robust</em>, I mean the ideal case of being robust against a) skewed distributions, b) outliers, and c) heavy tails.</p>\\n\\n<p>Along with the Pearson correlation as a baseline, I was thinking to include following more robust measures:</p>\\n\\n<ul>\\n<li>Spearman's $\\\\\\\\rho$</li>\\n<li>Percentage bend correlation (Wilcox, 1994, [1])</li>\\n<li>Minimum volume ellipsoid, minimum covariance determinant (<code>cov.mve</code>/ <code>cov.mcd</code> with the <code>cor=TRUE</code> option)</li>\\n<li>Probably, the winsorized correlation</li>\\n</ul>\\n\\n<p>Of course there are many more options (especially if you include robust regression techniques as well), but I want to restrict myself to the mostly used/ mostly promising approaches.</p>\\n\\n<p><strong>Now I have three questions (feel free to answer only single ones):</strong></p>\\n\\n<ol>\\n<li><strong>Are there other robust correlational methods I could/ should include?</strong></li>\\n<li><strong>Which robust correlation techniques are</strong> <em><strong>actually</em></strong>  <strong>used in your field?</strong>\\n<sub>(Speaking for psychological research: Except Spearman's $\\\\\\\\rho$, I have never seen any robust correlation technique outside of a technical paper. Bootstrapping is getting more and more popular, but other robust statistics are more or less non-existent so far).</sub></li>\\n<li><strong>Are there already systematical comparisons of multiple correlation techniques that you know of?</strong></li>\\n</ol>\\n\\n<p>Also feel free to comment the list of methods given above.</p>\\n\\n<hr>\\n\\n<p>[1] Wilcox, R. R. (1994). The percentage bend correlation coefficient. <em>Psychometrika</em>, 59, 601-616.</p>\\n\",\n",
       " '<p>First let me tell you that I am not going to explain exactly all the measures here, but I am going to give you an idea about how to compare how good the clustering is (let\\'s assume we are comparing 2 clustering methods with the same number of clusters.</p>\\n\\n<ol>\\n<li>For example the bigger the diameter of the cluster, the worst the clustering, because the points that belong to it are more scattered. </li>\\n<li>The higher the average distance of each clustering, the worst the clustering method. (Let\\'s assume that the average distance is the average of the distances from each point in the cluster to the center of the cluster.)</li>\\n</ol>\\n\\n<p>These are the 2 metrics that are the most used, check the links to understand what they stand for:</p>\\n\\n<ul>\\n<li><em>inter-cluster distance</em> (the higher the better, is the summatory of the distance between the different cluster centroids)</li>\\n<li><em>intra-cluster distance</em> (the lower the better, is the summatory of the distance between the cluster members to the center of the cluster)</li>\\n</ul>\\n\\n<p>To better understanding the metrics above, check <a href=\"http://www.stanford.edu/~maureenh/quals/html/ml/node82.html\" rel=\"nofollow\">this</a>.</p>\\n\\n<p>Then you should read the manual of the library and functions you are using to understand which measures represent each of these, or if it is not here what is the meaning of it, but I wouldn\\'t bother I would stick with the ones I stated here).</p>\\n\\n<p>Let\\'s go on with the questions you made:</p>\\n\\n<ol>\\n<li>Regarding scaling data: Yes you should always scale the data for clustering, otherwise the different scales of the different dimensions (variables) will have different influences in how the data are clustered, with the higher the values in the variable, the more influential that variable will be in how the clustering is done, while indeed they should all have the same influence (unless for some particular strange reason you do not want it that way). </li>\\n<li><p>The distance functions compute all the distances from one point (instance) to another. The most common distance measure is Euclidean, so for example, let\\'s suppose you want to measure the distance from instance 1 to instance 2 (let\\'s assume you only have 2 instances for the sake of simplicity). Also let\\'s assume that each instance has 3 values <code>(x1, x2, x3)</code>, so <code>I1=0.3, 0.2, 0.5</code> and <code>I2=0.3, 0.3, 0.4</code> so the Euclidean distance from I1 and I2 would be: <code>sqrt((0.3-0.2)^2+(0.2-0.3)^2+(0.5-0.4)^2)=0.17</code>, hence the distance matrix will result in:  </p>\\n\\n<pre><code>    i1    i2\\ni1  0     0.17\\ni2  0.17  0\\n</code></pre></li>\\n</ol>\\n\\n<p>Notice that the distance matrix is always symmetrical.</p>\\n\\n<p>The Euclidean distance formula is not the only one that exists; there are many other distances that can be used to calculate this matrix. Check for example in Wikipedia <a href=\"http://en.wikipedia.org/wiki/Manhattan_distance\" rel=\"nofollow\">Manhattain Distance</a> and how to calculate it. At the end of the Wikipedia page for <a href=\"http://en.wikipedia.org/wiki/Euclidean_distance\" rel=\"nofollow\">Euclidean Distance</a> you can also check its formula and at the end check which other distances exist.</p>\\n',\n",
       " '<p>Usually the <a href=\"http://en.wikipedia.org/wiki/Intraclass_correlation\" rel=\"nofollow\">intraclass-coefficient</a> is calculated in this situation. It is sensitive both to profile as well as to elevation differences between raters. If all raters rate throughout the study, report ICC(2, k); if only one rater rates everything and the other only rate, say, 20% to check the interrater agreement, then you should report ICC(2, 1).</p>\\n',\n",
       " '<p>You should note that <code>T</code> is none of your model\\'s a random effects terms, but a fixed effect. Random effects are only those effects that appear after the <code>|</code> in a <code>lmer</code> formula!</p>\\n\\n<p>A more thorough discussion of what this specification does you can find in this <a href=\"http://stats.stackexchange.com/q/13166/442\">lmer faq question</a>.</p>\\n\\n<p>From this questions your model should give the following (for your fixed effect <code>T</code>):</p>\\n\\n<ul>\\n<li>A global slope</li>\\n<li>A random slopes term specifying the deviation from the overall slope for each level of <code>Site</code></li>\\n<li>The correlation between the random slopes.</li>\\n</ul>\\n\\n<p>And as said by @mark999 this indeed is a common specification. In repeated measures designs, you generally want to have random slopes and correlations for all repeated measures (within-subjects) factors.</p>\\n\\n<p>See the following paper for some examples (which I tend to always cite here):</p>\\n\\n<p>Judd, C. M., Westfall, J., &amp; Kenny, D. A. (2012). Treating stimuli as a random factor in social psychology: A new and comprehensive solution to a pervasive but largely ignored problem. <em>Journal of Personality and Social Psychology</em>, 103(1), 54–69. doi:10.1037/a0028347</p>\\n',\n",
       " '<p>Can someone help me find a way to estimate the variance of the Kaplan-Meier estimate with dependent observations? Specifically, I have failure time data from patients with several different observations for each patient (and different patients may have different number of observations). The observations for different patients are assumed to be independent but observations from the same patient are expected to be dependent.</p>\\n\\n<p>I was suggested two publications, <a href=\"http://jdr.sagepub.com/content/80/11/2016.short\" rel=\"nofollow\">\"Kaplan-Meier Analysis of Dental Implant Survival: A Strategy for Estimating Survival with Clustered Observations\"</a> and <a href=\"http://www.sciencedirect.com/science/article/pii/S0047259X84710311\" rel=\"nofollow\">\"The Kaplan-Meier Estimate for Dependent Failure Time Observations\"</a>, the second of which is cited by the first.</p>\\n\\n<p>However I was unable to make sense of these. The first has errors and the second seems far more rigorous but the equation for the variance does not make sense (double integration on a 1-form).</p>\\n',\n",
       " '<p>We mostly use:</p>\\n\\n<ul>\\n<li>ggplot - for charts</li>\\n<li>stats</li>\\n<li>e1071 - for <a href=\"http://en.wikipedia.org/wiki/Support_vector_machine\" rel=\"nofollow\">SVMs</a></li>\\n</ul>\\n',\n",
       " '<p>While it is focussed on statistical pattern recognition, rather than time series forecasting, I would strongly recommend Chris Bishop\\'s book <a href=\"http://www.oup.com/us/catalog/general/subject/Medicine/Neuroscience/?view=usa&amp;ci=9780198538646\">Neural Networks for Pattern Recognition</a> becuase it is the best introduction to neural networks in general, and I think it would be a good idea to get to grips with the potential pitfalls in the use of neural networks in a more simple context, where the problems are more easily visualised an understood.  Then move on to the book on recurrent neural networks by <a href=\"http://rads.stackoverflow.com/amzn/click/0471495174\">Mandic and Chambers</a>.  The bishop book is a classic, nobody should use neural nets for anything until they feel confident that they understand the material contained in that book; ANN make it all too easy to shoot yourself in the foot!</p>\\n\\n<p>I also disagree with mbq, nn are not obsolete, while many problems are better solved with linear models or more modern machine learning techniques (e.g. kernel methods), there are some problems where they work well and other methods don\\'t.  It is still a tool that should be in our toolboxes.</p>\\n',\n",
       " '<p>I.e. I want to put in Baseball and get out Sports &amp; Outdoors. </p>\\n',\n",
       " '<p>Dimensionality reduction via something like PCA would be helpful to get an idea of the number of dimensions that are critical to represent your data.</p>\\n\\n<p>To check for misclassified instances, you can do a rudimentary k-means clustering of your data to get an idea of how well your raw data would fit your proposed categories.  While not automatic, visualizing at this stage would be helpful, as your visual brain is a powerful classifier in and of itself.  </p>\\n\\n<p>In terms of data that are outright missing, statistics has <a href=\"http://en.wikipedia.org/wiki/Missing_data\">numerous techniques</a> to deal with that situation already, including imputation, taking data from the existing set or another set to fill in the gaps.</p>\\n',\n",
       " '<p>@Tal: Might I suggest <a href=\"http://rads.stackoverflow.com/amzn/click/007310874X\" rel=\"nofollow\">Kutner et al</a> as a fabulous source for linear models. </p>\\n\\n<p>There is the distinction between 1) a prediction of Y from an individual new observation X_vec, 2) the expected value of a Y conditioned on X_vec, $E(Y|X_{vec})$ and 3) Y from several instances of x_vec  - all covered in detail in the text. </p>\\n\\n<p>I think you are looking for the formula for the confidence interval around $E(Y|X_{vec})$ and that is $\\\\\\\\hat{Y}$ $\\\\\\\\pm$ t(1-$\\\\\\\\alpha$ /2)s{$\\\\\\\\hat{Y}$} where t has n-2 d.f. and s{$\\\\\\\\hat{Y}$} is the standard error of $\\\\\\\\hat{Y}$, $\\\\\\\\frac{\\\\\\\\sigma^{2}}{n}$+($X_{vec}-\\\\\\\\bar{X})^{2}\\\\\\\\frac{\\\\\\\\sigma^{2}}{\\\\\\\\sum(X_{i}-\\\\\\\\bar{X})^{2}}$</p>\\n',\n",
       " 'Refers to an estimator of a population parameter that \"hits the true value\" on average. That is, a function of the observed data $\\\\\\\\hat{\\\\\\\\theta}$ is an unbiased estimator of a parameter $\\\\\\\\theta$ if $E(\\\\\\\\hat{\\\\\\\\theta}) = \\\\\\\\theta$. The simplest example of an unbiased estimator is the sample mean as an estimator of the population mean. ',\n",
       " '<p>Two further references from <a href=\"http://www.kyb.mpg.de/~bs\">B. Schölkopf</a>:</p>\\n\\n<ul>\\n<li>Schölkopf, B. and Smola, A.J. (2002). <a href=\"http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;tid=8684\">Learning with kernels</a>. The MIT Press.</li>\\n<li>Schölkopf, B., Tsuda, K., and Vert, J.-P. (2004). <a href=\"http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;tid=10338\">Kernel methods in computational biology</a>. The MIT Press.</li>\\n</ul>\\n\\n<p>and a website dedicated to <a href=\"http://www.kernel-machines.org/\">kernel machines</a>.</p>\\n',\n",
       " '<p>A <a href=\"http://en.wikipedia.org/wiki/Recommender_system\">Recommender System</a> would measure the correlation between ratings of different users and yield recommendations for a given user about the items which may be of interest to him.</p>\\n\\n<p>However, tastes change over time so <em>old ratings</em> might not reflect <em>current preferences</em> and vice versa. You may once have put \"excellent\" to a book you would now rate as \"not too disgusting\" and so on. Moreover, the interests themselves do change as well. </p>\\n\\n<p><strong>How should recommender systems work in a changing environment?</strong></p>\\n\\n<ol>\\n<li>One option is to cut off the \"old\" ratings, which may work just fine assuming you correctly define \"old\" (you can even say ratings never expire and pretend that the problem doesn\\'t exist). But it\\'s not the best possible option: of course tastes evolve, it\\'s a normal life flow, and there\\'s no reason why we cannot use the extra knowledge of once correct past ratings. </li>\\n<li>Another option is to somehow accommodate this extra knowledge. Thus we could not just find an \"instant match\" for your current interests but suggest you the things you may like <em>next</em> (as opposed to the things you may like <em>now</em>).</li>\\n</ol>\\n\\n<p>I\\'m not sure if I\\'m explaining this well enough. Basically I\\'m in favor of the second approach and am talking about a Recommender System which would measure the correlations of taste <em>trajectories</em> and yield recommendations which will cater for.. well, let\\'s call it personal growth - because they will be coming from people whose \"tastes trajectory\" (and not just \"tastes snapshot\") is similar to yours.</p>\\n\\n<p><strong>Now the question:</strong> I wonder if something similar to the \"option 2\" already exists and, if it does, I wonder how it works. And if it doesn\\'t exist, you\\'re welcome to discuss how it should work! :)</p>\\n',\n",
       " '<p>First, an AR(1) plus white noise is equivalent to an ARMA(1,1) process. So unless there is a good reason to formulate your model with a latent AR(1) process observed with error, I would suggest you use the simpler and equivalent model with ARMA(1,1) errors.</p>\\n\\n<p>Then you can write\\n$$  d_t = f(t,\\\\\\\\theta) + r_t$$\\nwhere\\n$$ r_t = \\\\\\\\phi r_{t-1} + \\\\\\\\gamma e_{t-1} + e_t$$\\nand $e_t$ is white noise. The residuals are \\n$$e_t = r_t - \\\\\\\\phi r_{t-1} - \\\\\\\\gamma e_{t-1}$$\\nwhich can easily be found recursively beginning with $r_0=e_0=0$ so that $e_1=r_1$.</p>\\n\\n<p>If you wish to persist with the AR(1) + WN formulation, you can compute the errors via a Kalman filter.</p>\\n',\n",
       " '<p>With regards to common control groups, you may want to check out <a href=\"http://www.mrc-bsu.cam.ac.uk/cochrane/handbook/chapter_16/16_5_4_how_to_include_multiple_groups_from_one_study.htm\" rel=\"nofollow\">16.5.4 of the Cochrane Handbook</a>. To quote a subset of this page:</p>\\n\\n<blockquote>\\n  <p>Approaches to overcoming a unit-of-analysis error for a study that\\n  could contribute multiple, correlated, comparisons include the\\n  following. </p>\\n  \\n  <ul>\\n  <li>Combine groups to create a single pair-wise comparison\\n  (recommended). </li>\\n  <li>Select one pair of interventions and exclude the\\n  others. </li>\\n  <li>Split the ‘shared’ group into two or more groups with smaller\\n  sample size, and include two or more (reasonably independent)\\n  comparisons. </li>\\n  <li>Include two or more correlated comparisons and account\\n  for the correlation. </li>\\n  <li>Undertake a multiple-treatments meta-analysis\\n  (see Section 16.6).   </li>\\n  </ul>\\n  \\n  <p>The recommended method in most situations is to\\n  combine all relevant experimental intervention groups of the study\\n  into a single group, and to combine all relevant control intervention\\n  groups into a single control group. As an example, suppose that a\\n  meta-analysis of ‘acupuncture versus no acupuncture’ would consider\\n  studies of either ‘acupuncture versus sham acupuncture’ or studies of\\n  ‘acupuncture versus no intervention’ to be eligible for inclusion.\\n  Then a study comparing ‘acupuncture versus sham acupuncture versus no\\n  intervention’ would be included in the meta-analysis by combining the\\n  participants in the ‘sham acupuncture’ group with participants in the\\n  ‘no intervention’ group. This combined control group would be compared\\n  with the ‘acupuncture’ group in the usual way. For dichotomous\\n  outcomes, both the sample sizes and the numbers of people with events\\n  can be summed across groups. For continuous outcomes, means and\\n  standard deviations can be combined using methods described in Chapter\\n  7 (Section 7.7.3.8).</p>\\n</blockquote>\\n\\n<p>With regards to pooling effect sizes for performing moderator meta analysis. There is a list of <a href=\"http://stats.stackexchange.com/questions/5220/how-to-do-meta-regression-analysis-with-spss\">SPSS resources here</a>. You might want to get a book like <em>Introduction to Meta-Analysis</em> to provide an overview of some of the many issues and calculations involved.</p>\\n\\n<h3>References</h3>\\n\\n<ul>\\n<li>Borenstein, M., Hedges, L.V., Higgins, J.P.T. &amp; Rothstein, H.R. (2011). Introduction to meta-analysis. John Wiley \\\\\\\\&amp; Sons</li>\\n</ul>\\n',\n",
       " '<p>I bought this book:</p>\\n\\n<p><a href=\"http://rcm.amazon.com/e/cm?lt1=_blank&amp;bc1=000000&amp;IS2=1&amp;bg1=FFFFFF&amp;fc1=000000&amp;lc1=0000FF&amp;t=justibozon-20&amp;o=1&amp;p=8&amp;l=as1&amp;m=amazon&amp;f=ifr&amp;md=10FE9736YVPPT7A0FBG2&amp;asins=0470539399\">How to Measure Anything: Finding the Value of Intangibles in Business</a> </p>\\n\\n<p>and </p>\\n\\n<p><a href=\"http://rcm.amazon.com/e/cm?lt1=_blank&amp;bc1=000000&amp;IS2=1&amp;bg1=FFFFFF&amp;fc1=000000&amp;lc1=0000FF&amp;t=justibozon-20&amp;o=1&amp;p=8&amp;l=as1&amp;m=amazon&amp;f=ifr&amp;md=10FE9736YVPPT7A0FBG2&amp;asins=0596153937\">Head First Data Analysis: A Learner\\'s Guide to Big Numbers, Statistics, and Good Decisions</a></p>\\n\\n<p>What other books would you recommend?</p>\\n',\n",
       " '<p>You are on the right track; think about what the z-statistic means. I am not sure what you mean by \"count the p-value\" but that may be just a linguistic problem. </p>\\n',\n",
       " '<p>The relevant lines in the code file <code>xtcsd.ado</code>, to view which type</p>\\n\\n<pre><code>viewsource xtcsd.ado\\n</code></pre>\\n\\n<p>are <code>163</code>-<code>165</code>, which read</p>\\n\\n<pre><code>    di in gr \"Pesaran\\'s test of cross sectional independence = \"/*\\n    */in ye %9.3f `pesaran\\' in gr \", Pr = \" %6.4f /*\\n    */ in ye 2*(1-norm(abs(`pesaran\\'))) \\n</code></pre>\\n\\n<p>The last line of code is the one that produces the p-value, and it behaves as expected under version control, with the value of the statistic that you get</p>\\n\\n<pre><code>. version 9: di 2*(1-norm(abs(-2.673)))\\n.00751763\\n</code></pre>\\n\\n<p>So, there is no ostensible reason why your computed statistic should result in an abnormal p-value. I suggest you <code>set trace on</code> and check what is going on, or show us the output.</p>\\n\\n<p>By the way, you should check what version of the <code>xtcsd</code> package you have. You can check using</p>\\n\\n<pre><code>. which xtcsd\\n*! version 1.1.1        R.E. De Hoyos and V. Sarafidis 16may2006\\n</code></pre>\\n',\n",
       " '<p>I wanted to refer you to the Statistical Learning book but I wanted to be sure that it wasn\\'t your text.  It was written by Stanford Statistics faculty Trevor Hastie, Jerome Friedman and Rob Tibshirani. <a href=\"http://rads.stackoverflow.com/amzn/click/0387848576\" rel=\"nofollow\">Here</a> is a link to it on amazon.</p>\\n',\n",
       " '<p>One generally examines the conformity with regression assumptions by examining the <em>residuals</em> since most of the regression assumptions have to do with the distribution of residuals rather than with the distribution of the outcome variable or covariates. In models with interactions or spline terms, the analysis may be more complex, but basically, the answer is yes, you would be checking the homoschedasticity and presence of autocorrelation and lack of linearity within each covariate.</p>\\n',\n",
       " \"<p>A paired t-test assumes that the <em>differences</em> are normal: the original values could have any distribution. More precisely, just like with a t-test, the differences don't even have to be normal, just the sampling distribution of the mean. This usually means that with a large enough sample you can use a t-test even without normality because the central limit theorem will kick in.</p>\\n\\n<p>On the other hand, one can always use a non-parametric test with not too much loss in efficiency.</p>\\n\",\n",
       " '<p>With continuous data, a linear regression $Y=\\\\\\\\beta_1+\\\\\\\\beta_2X_2+u$ assumes that the error term is distributed N(0,$\\\\\\\\sigma^2$)</p>\\n\\n<p>1) Do we assume that Var(Y|x) is likewise ~N(0,$\\\\\\\\sigma^2$)?</p>\\n\\n<p>2) What is this error distribution in logistic regression? When the data is in the form of 1 record per case, where the \"Y\" is 1 or 0, is the error term distributed Bernoulli (i.e. variance is  p(1-p) )) and when the data is in the form #successes out of #of trials, is it assumed binimial (i.e. variance is np(1-p)), where p is the probability that Y is 1?</p>\\n',\n",
       " \"<p>I have been investigating the possibility of using the interval between uncommon events to test for changes in the frequency of such events over time.<br>\\nAs an example, say that the event is breaking a record in some sporting competition. This might occur at most a few times a year, and the data segmentation problem (whether an event falls within a certain interval of observation) causes the usual events-per-year analysis, usually a GLM with a Poisson link, to be strongly affected by even a single event without a very long series.<br>\\nI first thought of sampling different observation intervals, and then decided to discard the intervals entirely and see if the intervals between events were associated with their serial position. This seemed to be a more powerful test in the data I had. Apparently this was an accepted procedure some 50-60 years ago, but I have not been able to find much in the recent literature. I'm pretty sure that this is related to analyzing the frequency of extreme weather events and the like, but I am not familiar with that field.<br>\\nAnyone out there know whether this type of test has been superceded by something? </p>\\n\",\n",
       " \"<p>If you are absolutely certain that nobody will ever critically review your analysis, consider it skeptically, or just need to be convinced of your results, then go ahead and remove the outliers (but if this is the case then there is probably no need to do the analysis at all).</p>\\n\\n<p>If you remove outliers then you open yourself to accusations of cherry-picking only those data points that confirm your preconceptions. </p>\\n\\n<p>If you are concerned about the normality assumptions (and note that it is the residuals, not the response that need to be normal) then you should consider a method that uses all the data, but does not depend on normality rather than deleting points (e.g. data tranformations, permutation tests, bootstrap, or other non-parametric approaches).</p>\\n\\n<p>If you have points that are true outliers without obvious explanation, then examining why they are different may lead to more interesting findings than the original analysis.</p>\\n\\n<p>Note that the common tests of normality generally have poor power to find important differences when they matter and high power to find minor differences when they don't matter.  Decisions about using normal based inference or not should come from your knowledge (and that of other researchers) of the science that produces the data.  While the canned normality tests were developed by people who were probably smarter than me and possibly smarter than you, it cannot be expected of them that they knew (at the time they developed the test) more about your data and your questions than you do.  </p>\\n\",\n",
       " '<p>If you\\'re asking about Simpson\\'s Paradox, the <a href=\"http://en.wikipedia.org/wiki/Simpson%27s_paradox\" rel=\"nofollow\">Wikipedia entry</a> is actually a fairly decent one. <a href=\"http://online.wsj.com/article/SB125970744553071829.html\" rel=\"nofollow\">Here</a> is another example.</p>\\n',\n",
       " \"<p>Yes, in practice those values are skipped. In your description in terms of a Frobenius norm, this corresponds to minimising the components of the norm which can be measured, i.e. those which have known ratings. The regularisation term can be seen as a Bayesian prior on the components of the feature vectors, with the SVD calculating the maximum likelihood estimator, subject to this prior and the known values.</p>\\n\\n<p>It's probably best to think of the SVD as a method for inferring the missing values. If you've already got a better way of doing this, why do you need the SVD? If you don't, then the SVD will happily fill in the gaps for you.</p>\\n\",\n",
       " '<p>This is called <a href=\"http://en.wikipedia.org/wiki/Three-point_estimation\" rel=\"nofollow\">Three Point Estimation</a> sometimes used in project management.  It is a rule of thumb used to translate simple estimates from non-statisticians into helpful statistics.</p>\\n\\n<p>Rather strangely, the estimate of the mean comes from assuming a so-called <a href=\"http://www.mhnederlof.nl/doubletriangular.html\" rel=\"nofollow\">double-triangular distribution</a>, where $m$ is both the median and the mode, so with density </p>\\n\\n<ul>\\n<li>$f(x) = \\\\\\\\frac{x-a}{(m-a)^2} \\\\\\\\text{ when } a \\\\\\\\lt x \\\\\\\\lt m$</li>\\n<li>$f(x) = \\\\\\\\frac{b-x}{(b-m)^2} \\\\\\\\text{ when } m \\\\\\\\lt x \\\\\\\\lt b$</li>\\n</ul>\\n\\n<p>but the estimate of the standard deviation is rather less than this distribution might suggest.</p>\\n',\n",
       " \"<p>I think the key is consistency. The expert has not only particular knowledge, but a system within which that knowledge operates. They have a persona, an overall strategy, within which their tactics reside and evolve.</p>\\n\\n<p>In some sense, a computer program playing chess is a Frankenstein's Monster created from a mashup of various bodies (programmers, experts, etc). So it's not surprising that any one expert's advice would not fit well with the system that exists.</p>\\n\\n<p>I agree with other comments that experts may not know how they do what they do. In which case, being human, their conscious mind makes up a plausible story as to why they reached a particular decision. But I still think that expert advice to the programming team is always out of context (i.e. inconsistent with the context of the program's design and history).</p>\\n\",\n",
       " \"<p>I am not sure I understand what you mean by difference/distance/dissimilarity matrix. Assuming that $D_{i,j}^2 = (v_i - v_j)^{\\\\\\\\top}(v_i - v_j)$ for some vectors $v_i, v_j$, if you can accept a transformation to the crossproduct matrix $G_{i,j} = -2 v_i^{\\\\\\\\top}v_j$ (say for example the vectors are normalized so $v_i^{\\\\\\\\top}v_i = 1 = v_j^{\\\\\\\\top}v_j$, and so you can subtract out the 2 from $D_{i,j}^2$). Then you can compare the two cross product matrices, $G$ and $H$ call them, by a Wilks' lambda test, I think. I'm not sure, but I think they would both have to be the same rotation away from Wishart matrices. The Wilks' lambda distribution would then describe the ratio of the products of the eigenvalues of the two matrices under the null.</p>\\n\\n<p>This may not be applicable to your problem, though...</p>\\n\",\n",
       " '<p>The data below encodes an acquaintance network. The challenge is to divide it into three groups of similar sizes (not necessarily identical) aiming to maximise familiarity among groups. The idea is that it is very hard to do it from just being presented with a list of edges such as below; it becomes easier when visualising it, especially with a force directed layout. This can be tied to a much larger network, for which visualisation becomes less helpful again. The list of edges is also intended to impress on the audience that that is what computer algorithms deal with; the input is raw unstructured data. Especially with clustering, it can be deceptive to explain the difficulty, as any visualisation of cluster structure suggests the problem is easy. So my approach is to use visualisation as an aid, but stress that such perception is not available to algorithms in the first place and that it does not generally scale in the second place. This data is basically a toy example I have used in many places to explain mcl clustering (e.g. Figure 2 page 9 of <a href=\"http://micans.org/mcl/lit/INS-R0010.ps.Z\" rel=\"nofollow\">http://micans.org/mcl/lit/INS-R0010.ps.Z</a>). The best grouping is (using the first letter of each name) {a, b, f, g, j}, {b, c, e}, {d, h, i, k, l}, leading to only four \\'cut\\' acquaintance pairs. Data:</p>\\n\\n<pre><code>ann bob\\nann fred\\nann gillian\\nann john\\nbob ann\\nbob charlie\\nbob esme\\ncharlie bob\\ncharlie daniel\\ncharlie esme\\ndaniel  charlie\\ndaniel  harriet\\ndaniel  indy\\ndaniel  kim\\nesme    bob\\nesme    charlie\\nesme    gillian\\nesme    harriet\\nfred    ann\\nfred    john\\ngillian ann\\ngillian esme\\ngillian john\\nharriet daniel\\nharriet esme\\nharriet indy\\nharriet kim\\nindy    daniel\\nindy    harriet\\nindy    kim\\nindy    lucy\\njohn    ann\\njohn    fred\\njohn    gillian\\nkim daniel\\nkim harriet\\nkim indy\\nkim lucy\\nlucy    indy\\nlucy    kim\\n</code></pre>\\n',\n",
       " '<p>If the goal of the standard deviation is to summarise the spread of a symmetrical data set (i.e. in general how far each datum is from the mean), then we need a good method of defining how to measure that spread.</p>\\n\\n<p>The benefits of squaring include:</p>\\n\\n<ul>\\n<li>Squaring always gives a positive\\nvalue, so the sum will not be zero.</li>\\n<li>Squaring emphasizes larger differences - a feature that turns out to be both good and bad (think of the effect outliers have).</li>\\n</ul>\\n\\n<p>Squaring however does have a problem as a measure of spread and that is that the units are all squared, where as we\\'d might prefer the spread to be in the same units as the original data (think of squared pounds or squared dollars or squared apples). Hence the square root allows us to return to the original units.</p>\\n\\n<p>I suppose you could say that absolute difference assigns equal weight to the spread of data where as squaring emphasises the extremes. Technically though, as others have pointed out, squaring makes the algebra much easier to work with and offers properties that the absolute method does not (for example, the variance is equal to the expected value of the square of the distribution minus the square of the mean of the distribution)</p>\\n\\n<p><em><strong>It\\'s important to note</em></strong> <em><strong>however</em></strong> that there\\'s no reason you couldn\\'t take the absolute difference if that is your preference on how you wish to view \\'spread\\' (sort of how some people see 5% as some magical thresh hold for p-values, when in fact it\\'s situation dependent). Indeed, there are in fact several competing methods for measuring spread.</p>\\n\\n<p>My view is to use the squared values because I like to think of how it relates to the Pythagorean Theorem of Statistics: c = sqrt(a^2 + b^2)  ...this also helps me remember that when working with independent random variables, variances add, standard deviations don\\'t. But that\\'s just my personal subjective preference.</p>\\n\\n<p>An much more indepth analysis can be read <a href=\"http://www.leeds.ac.uk/educol/documents/00003759.htm\">here</a>.</p>\\n',\n",
       " '<p><strong>Context:</strong></p>\\n\\n<p>I\\'ve recently adopted version control as part of my data analysis work (finally I may hear you saying: see my <a href=\"http://stackoverflow.com/questions/2712421/r-and-version-control-for-the-solo-data-analyst\">earlier question on SO</a>). This prompted me to think more about repositories and the directory structure I use for my projects. </p>\\n\\n<p>My typical research work involves one or more studies (i.e., data that I have collected) which gets written up as one or more publications (journal articles, book chapters, presentations, reports, etc.).  Analyses and reports are typically produced using a combination of R, LaTeX, Sweave, textual data files and so on. I really like the idea of being able to upload a single self-contained repository that can be used to analyse data and reproduce a publication.</p>\\n\\n<p>In particular, I\\'ve been thinking about publications, studies, data, and common code, and how these entities map on to repositories. For example, is it better to have a separate repository for each publication or is it better to have each publication as an individual folder within the larger repository. I\\'m evolving a few thoughts on this, but I was keen to hear other options.</p>\\n\\n<p><strong>Question:</strong></p>\\n\\n<ul>\\n<li>What strategies do people use to map studies, publications, and analyses onto repositories?</li>\\n<li>When should related entities (e.g., publications, studies, etc.) be split into multiple repositories?</li>\\n</ul>\\n',\n",
       " '<p>Sure you can use Naive Bayes. You just have to specify what form the conditional distribution will have.</p>\\n\\n<p>I can think of a few options:</p>\\n\\n<ol>\\n<li>Binary distribution: Binarize your data using a threshold, and you revert to the problem that you were already solving.</li>\\n<li>Parametric distribution: If there is some reasonable parametric distribution, e.g. Gaussian, you can use that.</li>\\n<li>Non-parametric distribution: Decide on bins for the numerical data and use those to construct an empirical non-parametric distribution.</li>\\n</ol>\\n',\n",
       " \"<p>Perhaps you could do a preliminary analysis on a random subset of the data to form hypotheses, and then test those few hypotheses of interest using the rest of the data. That way you would not have to correct for nearly as many multiple tests. (I think...)</p>\\n\\n<p>Of course, if you use such a procedure you will be reducing the size of the dataset used for the final analysis and so reduce your power to find real effects. However, corrections for multiple comparisons reduce power as well and so I'm not sure that you would necessarily lose anything.</p>\\n\",\n",
       " '<p>I observed 21 times a response that could be one out of 8 categories (multinomial response). I computed the proportions for each category.\\nI did it independently 12 times. Hence I have 12 replicates for each proportion.\\nIs bootstrapping the mean proportion over the 12 replicates for getting the confidence interval a good idea?</p>\\n\\n<p>I tried (see codes below) and got some very large confidence intervals. Actually, according to the bootstrapped CI, there are no significant differences in proportions, while a chi-square test on pooled proportions (over replicates) confirms that there are differences.</p>\\n\\n<p>So, do I have to pool my proportions or not?</p>\\n\\n<p>If I have to, do you know an R function for computing Simultaneous confidence intervals for multinomial proportions?</p>\\n\\n<p>If I am able to keep replicates, is bootstrapping as done below OK?</p>\\n\\n<pre><code># generation of a data set: multinomial réponse variable repeatedly measured\\nsub&lt;-c(\"s1\",\"s2\",\"s3\",\"s4\")#subject\\nper&lt;-c(\"p1\",\"P2\",\"p3\")#period, RM within subject\\ntim&lt;-paste(\"t\",1:21,sep=\"\")#tim, RM within period\\nmydata&lt;-expand.grid(tim=tim,per=per,sub=sub)\\n#for simplicity, let\\'s consider that period is not a repeated measure\\n#within subject. Hence, we have 12 trials (\"independent\") of 21 observations\\n\\n#random generation of the response variable\\nrequire(plyr)\\nposl&lt;-paste(\"z\",1:8,sep=\"\")\\nmydata$pos&lt;-factor(NA,levels=posl)\\n    n=nrow(mydata)\\n    mydata$pos&lt;-replicate(n,sample(posl,1,prob=c(0.05,0.05,0.05,0.4,0.3,0.05,0.05,0.05)))\\n\\n#strong preference for z3 and z4\\n\\n##pre-treatment of the above row data, number of time (out of 21) each position was observed\\nmydata2=ddply(mydata,.(sub,per,pos),summarize,nobs=length(pos),.drop=F)\\nmydata2[,1:3]=catcolwise(function(x)as.factor(x))(mydata2)\\nsummary(mydata2)\\n\\n# boxplot of frequencies of occpupancy\\nrequire(ggplot2)\\nmydata2$fobs=mydata2$nobs/21\\nggplot(mydata2)+geom_boxplot(aes(pos,fobs))\\n\\n###chi2 test\\nnobsT=ddply(mydata,.(pos),summarize,T=length(pos))\\nnobsT=nobsT[,2]\\nchisq.test(nobsT)\\n\\n#bootstrapping of the mean proportions over the 12 replicates\\nrequire(boot)\\nmaf=function(data,index){o=ddply(data[index,],.(pos),summarize,mmea=mean(fobs),.drop=F);as.vector(o[,2])}\\nbootobj=boot(mydata2,maf,R=9999,stype=\"i\")\\n\\nconfint=ddply(mydata2,.(pos),summarize,mean.prop=mean(fobs))\\nconfint$boot.LOW=-999\\n    confint$boot.UP=-999\\nfor (posi in 1:8){\\n  try({confint$boot.LOW[posi]&lt;-boot.ci(bootobj,0.95,index=posi,type=\"bca\")$bca[4]},silent=T)\\n  try({confint$boot.UP[posi]&lt;-boot.ci(bootobj,0.95,index=posi,type=\"bca\")$bca[5]},silent=T)\\n}\\n\\n####################### plotting boostraped CI\\nggplot(mydata2)+geom_boxplot(aes(pos,fobs))+\\n  geom_errorbar(data=confint,aes(pos,ymin=mean.prop-boot.LOW,ymax=mean.prop+boot.UP),lty=2,width=0.5)+\\n  geom_point(data=confint,aes(pos,mean.prop),shape=24,size=2)\\n</code></pre>\\n\\n<p>References of the CI estimations I would like to run:</p>\\n\\n<p>Glaz, J. and C.P. Sison. 1999. Simultaneous confidence intervals for multinomial proportions. Journal of Statistical Planning and Inference 82:251-262.</p>\\n\\n<p>Sison, C.P and J. Glaz. 1995. Simultaneous confidence intervals and sample size determination for multinomial proportions. Journal of the American Statistical Association, 90:366-369. Paper available at <a href=\"http://tx.liberal.ntu.edu.tw/~purplewoo/Literature/!Methodology/!Distribution_SampleSize/SimultConfidIntervJASA.pdf\" rel=\"nofollow\">http://tx.liberal.ntu.edu.tw/~purplewoo/Literature/!Methodology/!Distribution_SampleSize/SimultConfidIntervJASA.pdf</a></p>\\n',\n",
       " '<p>Rather than test the significance of the differences in the deviances, why not plot the predicted probabilities from each model against each other in a scatterplot? You might also plot the differences between the two predicted probabilities against the predicted probability from the first model. Finally you might look at a boxplot of the differences.</p>\\n\\n<p>If the predicted values are similar, then the models are, for most purposes anyway, giving similar results.</p>\\n\\n<p>Another idea is to compare the parameter estimates on the independent variables in the two models, but this might require some rescaling. </p>\\n',\n",
       " '<p>The asymptotic results come from the general theory of extremum estimators which allows for the complications you bring up. However, the criterion function in this case is not smooth, so there might be issues with estimation of the derivatives, while the bootstrap is still reliable. Some modifications allow to speed it up while preserving validity which is relevant for huge data-sets or when you are interested in the band for the whole process.</p>\\n',\n",
       " '<p>In gene expression studies using microarrays, intensity data has to be normalized so that intensities can be compared between individuals, between genes. Conceptually, and algorithmically, how does \"quantile normalization\" work, and how would you explain this to a non-statistician?</p>\\n',\n",
       " 'Hierarchical Bayesian models specify priors on parameters and hyperpriors on the parameters of the prior distributions',\n",
       " '<p>You can -- and, generally, should always -- do some cross-validation and try out a set of different values for <code>k</code> and see what looks best and produces the best results.</p>\\n',\n",
       " '<p>I am wanting to examine whether religiosity moderates intervention effects on stigmatized attitudes.  Here are my variables:\\nX = group (1 = experimental; 0 = control)\\nZ = religiosity (14-item scale - using total score)\\nY = either post-test scores or pre-post change scores - Social Distance; Genderism/Transphobia</p>\\n\\n<p>I realize that I need to center my moderator variable and will also need to center my outcome variable (if I use post-test scores).</p>\\n\\n<p>My question is whether to use post-test scores or to use pre-post test change scores (and if I use the latter would I still center)</p>\\n',\n",
       " '<p>I am trying to programmatically identify an ARIMA model for a series of data and forecast values. </p>\\n\\n<p>Currently the problem i am facing is to find a way to evaluate partial autocorrelation. I have been looking for methods to calculate PACF for quite a long time now but in vain. </p>\\n\\n<p>Please provide some online resources which can help me in this matter.</p>\\n\\n<p>Thank you.</p>\\n',\n",
       " '<p>I am looking for a good tutorial on clustering data in <code>R</code> using hierarchical dirichlet process (HDP) (one of the recent and popular nonparametric Bayesian methods). </p>\\n\\n<p>There is <code>DPpackage</code> (IMHO, the most comprehensive of all the available ones) in <code>R</code> for nonparametric Bayesian analysis. But I am unable to understand the examples provided in <code>R News</code> or in the package reference manual well enough to code HDP.</p>\\n\\n<p>Any help or pointer is appreciated.</p>\\n\\n<p>A C++ implementation of HDP for topic modeling is available <a href=\"http://www.cs.princeton.edu/~blei/topicmodeling.html\">here</a> (please look at the bottom for C++ code)</p>\\n',\n",
       " '<p>I see no obvious reason not to do so. As far as I know, we usually make a distinction between two kind of effect size (ES) measures for qualifying the strength of an observed association: ES based on $d$ (difference of means) and ES based on $r$ (correlation). The latter includes Pearson\\'s $r$, but also Spearman\\'s $\\\\\\\\rho$, Kendall\\'s $\\\\\\\\tau$, or the multiple correlation coefficient.</p>\\n\\n<p>As for their interpretation, I think it mainly depends on the field you are working in: A correlation of .20 would certainly not be interpreted in the same way in psychological <em>vs.</em> software engineering studies. Don\\'t forget that Cohen\\'s three-way classification--small, medium, large--was based on behavioral data, as discussed in Kraemer et al. (2003), p. 1526. In their Table 1, they made no distinction about the different types of ES measures belonging to the $r$ family. There have by no way an absolute meaning and should be interpreted with reference to established results or literature review.</p>\\n\\n<p>I would like to add some other references that provide useful reviews of common ES measures and their interpretation.</p>\\n\\n<p><strong>References</strong></p>\\n\\n<ol>\\n<li>Helena C. Kraemer, George A. Morgan, Nancy L. Leech, Jeffrey A. Gliner, Jerry J. Vaske, and Robert J. Harmon (2003). <a href=\"http://psy6023.alliant.wikispaces.net/file/view/Kraemer2003.pdf\" rel=\"nofollow\">Measures of Clinical Significance</a>. <em>J Am Acad Child Adolesc Psychiatry</em>, 42(12), 1524-1529.</li>\\n<li>Christopher J. Ferguson (2009). <a href=\"http://www.tamiu.edu/~cferguson/Ferguson%20PPRP.pdf\" rel=\"nofollow\">An Effect Size Primer: A Guide for Clinicians and Researchers</a>. <em>Professional Psychology: Research and Practice</em>, 40(5), 532-538.</li>\\n<li>Edward F. Fern and Kent B. Monroe (1996). <a href=\"http://www.wisedata.hk/readings/Topic%2015_Secondary%20data/3.%20Effect%20size%20and%20significance%20testing/Effect-Size%20Estimates_%20Issues%20and%20Problems%20in%20Interpretation.pdf\" rel=\"nofollow\">Effect-Size Estimates: Issues and Problems in Interpretation</a>. <em>Journal of Consumer Research</em>, 23, 89-105.</li>\\n<li>Daniel J. Denis (2003). <a href=\"http://theoryandscience.icaap.org/content/vol4.1/02_denis.html\" rel=\"nofollow\">Alternatives to Null Hypothesis Significance Testing</a>. <em>Theory and Science</em>, 4(1).</li>\\n<li>Paul D. Ellis (2010). <em><a href=\"http://www.cambridge.org/gb/knowledge/isbn/item2704152/?site_locale=en_GB\" rel=\"nofollow\">The Essential Guide to Effect Sizes</a></em>. Cambridge University Press. -- just browsed the TOC</li>\\n</ol>\\n',\n",
       " '<p>My favorite is the <a href=\"http://www.burns-stat.com/pages/Tutor/R_inferno.pdf\" rel=\"nofollow\">R Inferno</a> by Patrick Burns.</p>\\n',\n",
       " '<p>You might be interested in the paper \"<a href=\"http://tables2graphs.com/doku.php\">Using Graphs Instead of Tables in Political Science</a>\". One section of the paper is devoted to \"<a href=\"http://tables2graphs.com/doku.php?id=04_regression_coefficients\">Plotting Regression Coefficients</a>\". Andrew Gelman likes this type of plot (<a href=\"http://andrewgelman.com/?s=coefplot\">coefplot</a>), too (the R code can be found in his <code>arm</code> package, see <a href=\"http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=arm%3acoefplot\">here</a> for some examples). </p>\\n\\n<p>However, I agree with @Macro that these plots are not easy to interpret when the coefficients are not on the same scale. </p>\\n',\n",
       " '<p>One at a somewhat lower level of mathematical sophistication than Wooldridge (less dense, more pictures), but a bit more up to date on some of the fast-moving areas:</p>\\n\\n<p>Murray, Michael P. <em>Econometrics: A Modern Introduction.</em> Addison Wesley, 2006. 976 pp. ISBN <a href=\"http://en.wikipedia.org/w/index.php?title=Special%3ABookSources&amp;isbn=9780321113610\" rel=\"nofollow\">9780321113610</a></p>\\n\\n<p>Seems that it\\'s not available for preview on the web and the publisher is out of stock, but you can view <a href=\"http://wps.aw.com/aw_murray_economtrcs_1/37/9551/2445250.cw/index.html\" rel=\"nofollow\">pdfs of 11 web extensions</a> to get an idea of its style.</p>\\n',\n",
       " '<p>If the random variable $H = H(X,Y,Z)$ is a function of three integer-valued random variables $X, Y$, and $Z$ that take on values in $[0, n]$\\nand whose joint probability mass function is \\n$$p_{X,Y,Z}(n_1,n_2,n_3) = P\\\\\\\\{X = n_1, Y = n_2, Z = n_3\\\\\\\\}, ~0 \\\\\\\\leq n_1,n_2,n_3 \\\\\\\\leq n,$$\\nthen \\n$$E[H] = \\\\\\\\sum_{n_1=0}^n\\\\\\\\,\\\\\\\\sum_{n_2=0}^n\\\\\\\\,\\\\\\\\sum_{n_3=0}^nH(n_1,n_2,n_3)p_{X,Y,Z}(n_1,n_2,n_3).$$\\nIt is <em>not</em> necessary that $H$ be expressible as a \"nice\" formula \\nsuch as $X+Y+Z$ in order to use the above formula.</p>\\n\\n<p>Here, \\n$$p_{X,Y,Z}(n_1,n_2,n_3) = \\\\\\\\n\\\\\\\\begin{cases}\\\\\\\\dfrac{n!}{n_1!n_2!n_3!}x^{n_1}y^{n_2}(1-x-y)^{n_3},\\\\\\\\n&amp; \\\\\\\\text{if}~ n_1 + n_2 + n_3 = n,\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\quad\\\\\\\\\\\\\\\\\\\\\\\\n0, &amp; \\\\\\\\text{otherwise},\\\\\\\\end{cases}$$\\nis a $(n+1)\\\\\\\\times(n+1)\\\\\\\\times(n+1)$ array with lots of zeroes in it,\\nas is $H(n_1,n_2,n_3)$ which is an array of zeroes and ones.  Thus,\\n$E[H]$ is actually the probability of an event.</p>\\n\\n<p>Similarly, $H(n_1,n_2,n_3)/S(n_1,n_2,n_3)$ is an array with lots of zeroes\\nin it but the nonzero entries are $1/S(n_1,n_2,n_3)$ and\\n$$E\\\\\\\\left [\\\\\\\\frac{H}{S}\\\\\\\\right]\\\\\\\\n= \\\\\\\\sum_{n_1=0}^n\\\\\\\\,\\\\\\\\sum_{n_2=0}^n\\\\\\\\,\\\\\\\\sum_{n_3=0}^n\\\\\\\\n\\\\\\\\frac{H(n_1,n_2,n_3)}{S(n_1,n_2,n_3)}p_{X,Y,Z}(n_1,n_2,n_3).$$</p>\\n',\n",
       " \"<p>This is a multi-response linear regression experiment. I generate a random matrix of gaussian noise $\\\\\\\\mathcal{N}(0,1)$ sized 10x6 called $Y_N$. I then generate an independent response $X$ via either uniform or gaussian noise, 10x1. I then regress $X$ on $Y_N$, generating $\\\\\\\\hat{Y}$. I obtain my residuals, $Y_{res}$, by subtracting $\\\\\\\\hat{Y}$ from $Y_N$. Standard linear regression. </p>\\n\\n<p>Since $Y_N$ is 10x6 we have 6 response vectors, therefore $Y_{res}$ also has 6 vectors each containing the residuals corresponding with each response in $Y_N$. I find that the correlation among the columns of $Y_{res}$ is greater than the correlations among the columns of $Y_N$. This is true for >95% of the random experiments performed. Indeed, it looks like the regression is introducing artificial correlation, but why? Is there any way to overcome this limitation? Extending this experiment to multivariate regression, (e.g. X 10x2), further exacerbates the issue.</p>\\n\\n<p>To be more specific, I compute the correlation among the columns of $Y_N$ ($Y_{res}$) as follows:</p>\\n\\n<p>Average Square Correlations:</p>\\n\\n<pre><code>C = covariance(YN)    % This is a square symmetric covariance matrix.\\n\\nD = C.^2 % Square each cell so that we only have positive numbers\\n\\nSC(i,j) = D(i,j)/var(i)*var(j) \\n\\n% i,j refer to respective cells of C, i,j selected so that only the upper triangle of matrix D (omitting diagonal) is included in computation. \\n</code></pre>\\n\\n<p>(Then take the average of all <code>SC(i,j)</code>'s to obtain average squared correlation.)</p>\\n\",\n",
       " '<p>This is count data.  That rules out uniform (continuous or discrete) as well as normal.  The only possibilities based on data type are the Poisson and the Binomial. The binomial does not seem appropriate because this is not the number of outcomes for a fixed number of independent experiments where each of n people can have their bone broken with the same probability.  The Poisson fits because it represents certain rare event hypotheses and is a number of broken bpne events observed over a given interval of time (ome football season).  It is not clear that the Poisson is the best model but it is better than the other choices.  The number of college football players in finite so there is a fixed finite limit to the number of events when technically the Poisson has no limit. </p>\\n\\n<p>If someone argued for the binomial because there is a fixed finite number of players available at the beginning of the season that are at risk for injury from a borken bone on any individula play and the plays are independent.  Then the trial could be considered the plays and with 22 players available on the play may indicate a constant probability that a broken bone will occur on the play.  The problem with that argument is that the number of plays in a season is not known and should be considered random (unless you look ay this conditional on the number of plays that actually occurred during the season).  The other issue is that plays differ in the injury risk.  Real life situations can always lead to arguments against a model but the shear randomness and rarity of bone breaks points to the Poisson as the best model of the 4 choices.</p>\\n',\n",
       " '<p>So basically 50%, well I will see if I can use this in the formula and get some sensible results.</p>\\n\\n<p>Thanks for your response, I appreciate it!</p>\\n',\n",
       " \"<p>WinBUGS has support for RJMCMC with an addon. I've used it for GLMs, including those with an intrinsic CAR component. Not R, obviously, but through R2WinBUGS you can patch them together.</p>\\n\",\n",
       " '<p>The stattrek site seems to have a much better description of outliers and influential points than your textbook but you\\'ve only quoted a short passage that may be misleading.  I don\\'t have that particular book so I cannot examine it in context.  Keep in mind though, that the textbook passage you quoted says, \"potentially\".  It\\'s not exclusive either.  Keeping those points in mind, stattrek and your book don\\'t necessarily disagree.  But it does appear that your book is misleading in the sense that it implies (from this short passage) that the only difference between outliers and influential points is whether they deviate on x or y axis.  That is incorrect.</p>\\n\\n<p>The \"rule\" for outliers varies depending on context.  The rule you cite is just a rule of thumb and yes, not really designed for regression.  There are a few ways to use it.  It might be easier to visualize if you imagine multiple y-values at each x and examining the residuals.  Typical textbook regression examples are too simple to see how that outlier rule might work, and in most real cases it is quite useless.  Hopefully, in real life, you collect much more data.  If it\\'s necessary that you may be applying the quantile rule for outliers to a regression problem then they should be providing data for which it is appropriate.</p>\\n',\n",
       " '<p>Consider the following model </p>\\n\\n<p>$Y_i = f(X_i) + e_i$</p>\\n\\n<p>from which we observe n iid data points $\\\\\\\\left( X_i, Y_i \\\\\\\\right)_{i=1}^n$. Suppose that $X_i \\\\\\\\in \\\\\\\\mathbb{R}^d$ is a $d$ dimensional feature vector. And suppose that a ordinary least squares estimate is fit to data, that is,</p>\\n\\n<p>$\\\\\\\\hat \\\\\\\\beta = {\\\\\\\\rm arg} \\\\\\\\min_{\\\\\\\\beta \\\\\\\\in \\\\\\\\mathbb{R}^d} \\\\\\\\sum_i (Y_i - \\\\\\\\sum_j X_{ij} \\\\\\\\beta_j)^2$</p>\\n\\n<p>Since a wrong model is estimated, what is the interpretation for the confidence interval around estimated coefficients? </p>\\n\\n<p>More generally, does it make sense to estimate confidence intervals around parameters in a misspecified model? And what does the confidence interval tell us in such a case?</p>\\n',\n",
       " \"<p>A simple way to find anomalous servers would be to assume they are identically distributed, estimate the population parameters, and sort them according to their likelihoods, ascending. Column likelihoods would be combined either with their product or their minimum (or some other T-norm). This works pretty well as long as outliers are rare. For outlier detection itself, stable population parameters are usually estimated iteratively by dropping any discovered outliers, but that's not vital as long as you're manually inspecting the list and thereby avoiding thresholding.</p>\\n\\n<p>For the likelihoods, you might try Beta for the proportions and Poisson for the rates.</p>\\n\\n<p>As pointed out by David, outlier detection is not quite the same as reliability analysis, which would flag all servers that exceed some threshold. Furthermore, some people would approach the problem trough loss functions - defining the pain you feel when some server is at 50% availability or 500 error rate, and then rank them according to that pain.</p>\\n\",\n",
       " \"<p><strong>Hats and tildes</strong></p>\\n\\n<p>The convention in (my end of) applied statistics is that $\\\\\\\\hat{\\\\\\\\beta}$ is an estimate of the true parameter value $\\\\\\\\beta$ and that $\\\\\\\\tilde{\\\\\\\\beta}$ is another, possibly competing estimate.     </p>\\n\\n<p>Following the Wolfram example, these can both be distinguished from a statistic (function of the data) that also happens to be an estimate, e.g. the sample mean $\\\\\\\\bar{x}$ could be an estimate of the population mean $\\\\\\\\mu$ so it it could also be called $\\\\\\\\hat{\\\\\\\\mu}$.  </p>\\n\\n<p>Contra Wolfram, I'd call $\\\\\\\\bar{X}$ the estimator (upper case roman letters denote random variables) and $\\\\\\\\bar{x}$ the estimate (lower case roman letters denote observations of random variables), but only if I was feeling pedantic or it mattered to the argument.</p>\\n\\n<p>Similarly, in the 'Reference for Stats Symbols' the thing that suggests to me that $\\\\\\\\tilde{u}$ is a random variable rather than a parameter is the fact that it's a roman letter not a greek one.  Again, this is why in the example above the sample mean involved the letter $x$ when it was a function of the data but $\\\\\\\\mu$ when it was considered as an estimator.  (And frankly, it's unclear to me what the tilde denotes on $u$.  The mean? the mode? the actual but unobserved value?  The surrounding text would have to say.)</p>\\n\\n<p><strong>Expectations</strong></p>\\n\\n<p>Re the expectation operator: I've never seen curly brackets used.  Maybe that's a mathematical statistics thing, in which case someone around here should recognize it.</p>\\n\\n<p><strong>The empirical approach to notation</strong></p>\\n\\n<p>One simple situation where estimators, random variables, and expectations collide in the notation is in the discussion of EM algorithms.  You might want to look at a few careful expositions to get a sense of the normal range of notational variation.  This is the empirical approach to notation, which always beats theory provided you are looking at variation from the right population, i.e. your discipline or expected audience.</p>\\n\\n<p><strong>The bottom line</strong></p>\\n\\n<p>Stay within the normal range described above, and anyway say what you mean by the symbols once in the text before using them.  It doesn't take much space and your readers will thank you.</p>\\n\",\n",
       " '<p>When I took courses in theoretical statistics as an undergrad 10 years ago, we used <em>Modern Mathematical Statistics</em> by Dudewicz and Mishra.  I find myself referring back to the book now and am reminded some of the code examples are in assembly for an IBM 370.  While quaint, I cannot help but feel this is somewhat dated.</p>\\n\\n<p>What high quality books exist of more recent vintage?</p>\\n',\n",
       " '<p>What sort of test should I use to get the p-value after getting a shannon diversity index? Also does anyone know if and how they can be done using PRIMER 5?</p>\\n',\n",
       " '<p>There\\'s a subtle issue here that is not mentioned in the question regarding estimation of the standard deviation of $\\\\\\\\overline{x}$\\'s sampling distribution. </p>\\n\\n<p>Suppose you have a sample from a population with mean $\\\\\\\\mu$ and variance $\\\\\\\\sigma^2$. When $\\\\\\\\sigma^2$ is known, $${\\\\\\\\rm SE}(\\\\\\\\overline{x}) = \\\\\\\\sigma/\\\\\\\\sqrt{n}$$ is exactly the standard deviation of the sample mean. In practice, you usually don\\'t know $\\\\\\\\sigma^2$, so you instead plug in the sample variance $\\\\\\\\hat\\\\\\\\sigma$ to use $${\\\\\\\\rm SE}(\\\\\\\\overline{x}) = \\\\\\\\hat\\\\\\\\sigma/\\\\\\\\sqrt{n}$$ to estimate the standard deviation of $\\\\\\\\overline{x}$. This distinction is actually important - when the variance is unknown, <em>this additional uncertainty must be incorporated into the hypothesis test</em>. This is why, even when the sample is normally distributed, the test statistic has a $t$-distribution (which has longer tails) instead of a normal distribution when $\\\\\\\\sigma$ is unknown. </p>\\n\\n<p><em><strong>Is it correct to say that $t=(\\\\\\\\overline{x}−μ)/SE(\\\\\\\\overline{x})$ follows a normal distribution for ANY population (not just normally distributed), as long as the samples sizes are significant in size (by means of the central limit theorem)</em></strong></p>\\n\\n<p>This is almost correct. The population must have finite variance (i.e. not have tails that are \"too long\") for this to be the case. Even when the population does have a finite variance, the population distribution can have a large effect on how long until the CLT \"kicks in\". For shorter tailed distributions this convergence is faster. For long-tailed distributions it can take quite a while (e.g. see <a href=\"http://stats.stackexchange.com/questions/29731/regression-when-the-ols-residuals-are-not-normally-distributed/29748#29748\">my example here</a>). </p>\\n\\n<p>Note that since we\\'re talking about a \"large sample\" result here, this is true regardless of whether or not you know $\\\\\\\\sigma$ since $\\\\\\\\hat \\\\\\\\sigma$ gets closer to the true $\\\\\\\\sigma$ as the sample size increases. </p>\\n\\n<p><em><strong>And, is it correct that t follows a t-distribution when the sample size is small, but then the population only if the population is normally distributed, because the central limit theorem does not apply?</em></strong> </p>\\n\\n<p>Again, assuming we\\'re in the \"$\\\\\\\\sigma$ is unknown\" world, $t$ only follows a $t$-distribution when the sample is normally distributed, which I think is what you\\'re saying here. Related to what I said in the beginning, if $\\\\\\\\sigma$ is known, then $t$ will have a (exact) normal distribution if the sample is normally distributed. </p>\\n\\n<p>To summarize: </p>\\n\\n<ul>\\n<li><p><strong>If $\\\\\\\\sigma$ is known, and the population is normally distributed:</strong> $t$ has a normal distribution. </p></li>\\n<li><p><strong>If $\\\\\\\\sigma$ is unknown, and the population is normally distributed:</strong> $t$ has a $t$-distribution. </p></li>\\n<li><p><strong>If the population is not normally distributed but meets the regularity requirements of the CLT:</strong> $t$ has an approximate normal distribution whether or not $\\\\\\\\sigma$ is known. That is, the distribution of $t$ <em>converges</em> to a normal distribution as the sample size increases. </p></li>\\n</ul>\\n',\n",
       " '<p>Here are a few resources for performing various types of mixed and factorial ANOVA in R:</p>\\n\\n<ul>\\n<li><a href=\"http://www.personality-project.org/r/r.anova.html\" rel=\"nofollow\">http://www.personality-project.org/r/r.anova.html</a></li>\\n<li><a href=\"http://www.statmethods.net/stats/anova.html\" rel=\"nofollow\">http://www.statmethods.net/stats/anova.html</a></li>\\n<li><a href=\"http://www.r-statistics.com/2010/04/repeated-measures-anova-with-r-tutorials/\" rel=\"nofollow\">http://www.r-statistics.com/2010/04/repeated-measures-anova-with-r-tutorials/</a></li>\\n</ul>\\n',\n",
       " '<p>Since the single time-coefficient model is nested in the multiple-time-coefficient model, you could fit both and do an F test. You will have 2df in the nominator (2=3-1). If it rejected, the single time coefficient assumption is implausible. </p>\\n',\n",
       " '<p>I was taught the stock phrase \"recognising the density of a <em>whatever</em> distribution,\" to use as part of a mathematical proof. I agree this \\'trick\\' is useful, but AFAIK it doesn\\'t have a name.</p>\\n',\n",
       " '<p>Following my question <a href=\"http://stats.stackexchange.com/questions/18378/effect-size-interpretation\">here</a>, <strong>when is it appropriate (or inappropriate) to report r squared for a bivariate linear correlation.</strong></p>\\n\\n<p>As explained in the earlier question, r = 0.74 which results in r squared of 55%.</p>\\n\\n<p>The size of r is an excellent outcome but when r squared is taken into account (especially when 45% of the variance in the dependent variable remains unexplained), it sort of dilutes the message. </p>\\n\\n<p>As pointed out in the answer to the earlier question, I do understand that it is not readily possible for a single factor to explain all the variation.</p>\\n\\n<p>I think all statistical textbooks that I have consulted (unless I am consulting the wrong books!) state that r squared is a better way of understanding r (or the effect size).</p>\\n\\n<p>I do understand though that r of at least 0.71 is needed to get r squared of 50%.</p>\\n\\n<p><strong>Questions</strong></p>\\n\\n<ol>\\n<li>Is reporting both r and r squared a good practice (in social science research)?</li>\\n<li>When can I not report r squared?</li>\\n</ol>\\n\\n<p>I have asked Q2 in order to to avoid a situation like the above, where my non statistical target audience may focus more on 45% of the unexplained variance, rather than on the 55% of the explained variance.</p>\\n',\n",
       " '<p>If a process is Gaussian, then any linear combination of its elements is Gaussian too. Take for example the AR(1) process: </p>\\n\\n<p>$$X_t=\\\\\\\\phi X_{t-1}+\\\\\\\\varepsilon_t$$</p>\\n\\n<p>Then </p>\\n\\n<p>$$\\\\\\\\varepsilon_t=X_t-\\\\\\\\phi X_{t-1},$$</p>\\n\\n<p>i.e. the innovation is a linear combination of the process elements, hence it should be Gaussian if $\\\\\\\\{X_t\\\\\\\\}$ is Gaussian.</p>\\n',\n",
       " '<p>What do you think of this?</p>\\n\\n<pre><code>&gt; lm(response ~ City+Sex+Rater+drink, data=x) -&gt; model\\n&gt; model\\n\\nCall:\\nlm(formula = response ~ City + Sex + Rater + drink, data = x)\\n\\nCoefficients:\\n(Intercept)       CityNY         SexM        Rater   drinkpepsi  drinksprite    \\n  8.150e+01   -7.500e-01   -6.750e+00    2.500e-01   -4.668e-15    4.000e+00  \\n\\n&gt; anova(model)\\nAnalysis of Variance Table\\n\\nResponse: response\\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \\nCity       1  36.75   36.75  4.0726 0.0500025 .  \\nSex        1 630.75  630.75 69.8984 1.766e-10 ***\\nRater      1   0.75    0.75  0.0831 0.7745397    \\ndrink      2 170.67   85.33  9.4565 0.0004068 ***\\nResiduals 42 379.00    9.02                      \\n---\\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 \\n</code></pre>\\n\\n<p>Note that there are only two cities in the data set you give, NY and LA. If you can describe the data, it can help to comment a little more...</p>\\n',\n",
       " '<p>Both are done.</p>\\n\\n<p>Least squares is easier, and the fact that for independent random variables \"variances add\" means that it\\'s considerably more convenient; for examples, the ability to partition variances is <em>particularly</em> handy for comparing nested models. It\\'s somewhat more efficient at the normal (least squares is maximum likelihood), which might seem to be a good justification -- however, some robust estimators with high breakdown can have surprisingly high efficiency at the normal.</p>\\n\\n<p>But L1 norms are certainly used for regression problems and these days relatively often.</p>\\n\\n<p>If you use R, you might find the discussion in section 5 here useful:</p>\\n\\n<p><a href=\"http://socserv.mcmaster.ca/jfox/Books/Companion/appendix/Appendix-Robust-Regression.pdf\" rel=\"nofollow\">http://socserv.mcmaster.ca/jfox/Books/Companion/appendix/Appendix-Robust-Regression.pdf</a></p>\\n\\n<p>(though the stuff before it on M estimation is also relevant, since it\\'s also a special case of that)</p>\\n',\n",
       " '<p>So let\\'s suppose there are <em>n</em> candidates running for office. A bunch of random people begin to selectively vote the candidates, each vote being a number from <em>1</em> to <em>n</em>. What is a good way to order the candidates based on their vote number and also the number of votes/frequency of votes?</p>\\n\\n<p>So for example:\\nIf candidate A was voted by a bunch of voters, but was the votes were fairly spaced out, he would probably be considered \"not as great\" as a candidate who was voted less/same amount/more but the votes were shortly after one another.</p>\\n\\n<p>I was thinking of something like Reddit\\'s Karma system, but that only takes into accounts +1\\'s and -1\\'s (and I\\'m not even sure if Reddit every gave information on how their Karma system works!)</p>\\n\\n<hr>\\n\\n<p>So for example:</p>\\n\\n<p>Candidates = [ Bob, Sally, Joe, Sam, Rob]</p>\\n\\n<p>Person A ranks Sally as #1 (best choice) and [Bob, Rob] as #5 (worst choice) and leaves (he doesn\\'t vote for others).</p>\\n\\n<p>Person B ranks Sam as #2, Joe as #4, and leaves.</p>\\n\\n<p>Person C ranks Sally as #1, Joe as #3, and Bob as #5.</p>\\n\\n<p>So in this case it may be compelling to say Sally is close to being #1, while Bob is probably closer to being #4 or #5. But Sam was only voted once as #2 and as such there\\'s not a whole lot of grounds to say \"yes he\\'s definitely #2\". Similarly, Rob was only voted once as being #5 but there isn\\'t sufficient votes to definitively say he\\'s worse than Bob.</p>\\n\\n<p>As such, the raking may become something like:</p>\\n\\n<p>Sally > Sam > Joe > Rob > Bob</p>\\n\\n<p>You can probably twist this example to have even more voters and create a few outliar, but I hope his gives you a general idea of what I\\'m looking for.</p>\\n',\n",
       " '<p>You might want to refer to this and the forecasted upper-lower limit plot in it.\\n<a href=\"http://stats.stackexchange.com/questions/24521/problem-in-discrete-valued-time-series-forecasting\">Problem in discrete valued time series forecasting</a></p>\\n',\n",
       " '<p>Suppose $X_1,\\\\\\\\ldots,X_n$ are i.i.d. $\\\\\\\\mathcal U(0,1)$. I am looking for the asymptotic distribution of $$T_n = \\\\\\\\prod_{i=1}^n [e{X_i}]^{1/\\\\\\\\sqrt{n}} \\\\\\\\&gt;.$$</p>\\n',\n",
       " '<p>I have 114 vectors with 6 boolean attributes. I saw that might be several distinct clusters in a simple visualization. K-means clustering on the transformed vectors (true = 1, false = 0) results in roughly the clusters that I had seen in the visualization.</p>\\n\\n<p>However, I am not sure what the most appropriate clustering method for this kind of data is, and how to determine the confidence in those factors (the k-means results change every time due to randomization). Should I treat the data as nominal or as numerical data?</p>\\n\\n<p>What would be the best way to do a cluster analysis on this kind of data in R?</p>\\n',\n",
       " \"<p>Background</p>\\n\\n<p>I would like to measure the performance of a model trained on 3k samples, because this number of samples might be feasible to obtain in practice. I have a larger set of samples to choose from (about 25k), but generating the model is expensive in terms of time and computational effort, so I can only do about 5 runs.</p>\\n\\n<p>In each run, I will pull 3k samples for training, and test on the remaining samples.</p>\\n\\n<p><strong>Question</strong></p>\\n\\n<p>Should the five sets of 3k samples used for training be disjoint? In other words, will this produce a better estimate of this model's performance than five sets of 3k samples selected independently at random?\\nNote that in neither case is it possible to use all samples for training at least once.</p>\\n\",\n",
       " '<p>You have two decent suggestions, but I don\\'t think either of them is optimal.  If you turn your individual, five-element vectors into a single, ordinal scalar, you will lose information.  This can be acceptable if it\\'s necessary, but if a better way is possible, you may want to avoid it.  <em>Multivariate</em> generalized linear models treat your response as a singular point in a multidimensional space, rather than five points ordered in time.  Multivariate methods are typically used / understood for cases where you have five different kinds of measurements (here binary) that are all related to each other, but I gather you have a sequence of five instances of the same kind of measurement.  It would be better to fit a model that is designed for that.  </p>\\n\\n<p>Fortunately, there are models that are designed exactly for this type of situation.  You will want to use a <a href=\"http://en.wikipedia.org/wiki/Generalized_linear_mixed_model\" rel=\"nofollow\">Generalized Linear Mixed effects Model</a> or use the <a href=\"http://en.wikipedia.org/wiki/Generalized_estimating_equation\" rel=\"nofollow\">Generalized Estimating Equations</a>.  Which you should choose depends on the question you want to ask, GLiMMs provide information on the effects of the covariates for the individual study units, whereas the GEE provides information on the effects of the covariates for the population average.  There are several threads on CV that discuss these:  </p>\\n\\n<ul>\\n<li>I provide a fairly conceptual explanation here: <a href=\"http://stats.stackexchange.com/questions/32419//32421#32421\">Difference between generalized linear models generalized linear mixed models in SPSS</a>,  </li>\\n<li>there is also an explanation here: <a href=\"http://stats.stackexchange.com/questions/17331/\">What is the difference between generalized estimating equations and GLMM</a>,  </li>\\n<li>and, a little more mathematical explanation here: <a href=\"http://stats.stackexchange.com/questions/16390/\">When to use generalized estimating equations vs. mixed effects models?</a>  </li>\\n</ul>\\n\\n<p>Regarding whether to use the logit link or the probit link, I discussed that fairly extensively here: <a href=\"http://stats.stackexchange.com/questions/20523//30909#30909\">Difference between logit and probit models</a>.  (Actually, the answer there is a little more fundamental in nature, so it may be worth reading that one first.)  </p>\\n',\n",
       " '<p>Not sure it gives a final answer to the question, but I would give a look at <a href=\"http://cscs.umich.edu/~crshalizi/weblog/491.html\" rel=\"nofollow\">this</a>. Especially point 2. See also the discussion in appendix A2 of the <a href=\"http://arxiv.org/abs/0706.1062\" rel=\"nofollow\">paper</a>.</p>\\n',\n",
       " '<p>Sounds like you\\'ll need a HMM to do that.\\nHave you read<br>\\nLawrence R. Rabiner (February 1989). <a href=\"http://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf\" rel=\"nofollow\">\"A tutorial on Hidden Markov Models and selected applications in speech recognition\"</a></p>\\n\\n<p>There are a few examples of models parameters discovery on page 259.</p>\\n\\n<p>The question is how do you plan to update the probabilites? Maybe you can use a kind of success ratio? (if the player has not managed to win much recently using one of the 3 strategies, he may use it less and try more the two others...)</p>\\n',\n",
       " '<p>you could use the Agresti-Caffo simultaneous confidence interval or (Simultaneous Score Intervals for Difference of Proportions) to compare differences in proportions (Agresti et al. 2008. Simultaneous confidence intervals for comparing binomial parameters, Biometrics 64, 1270-1275). </p>\\n\\n<p>The corresponding R code is available in <a href=\"http://www.stat.ufl.edu/~aa/cda/software.html\" rel=\"nofollow\">http://www.stat.ufl.edu/~aa/cda/software.html</a></p>\\n',\n",
       " '<p>Great question!  Your intuition was good.  There is indeed a strong relationship between methods of linear regression and metrics for measuring similarity between a line and a set of points.</p>\\n\\n<p><strong>The correspondence betwen linear regression and similarity metrics.</strong>  Here is the relationship.  Let $d(\\\\\\\\ell,S)$ be a similarity metric that measures the similarity between a line $\\\\\\\\ell$ and a set $S$ of points.  Then, linear regression is basically the following problem: given a set $S$ of points, find the line $\\\\\\\\ell$ that minimizes $d(\\\\\\\\ell,S)$.  Consequently, each different similarity metrics gives rise to a different flavor of linear regression.  And, dually, from any particular flavor of linear regression, you can extract a corresponding similarity metric that the regression method is minimizing.</p>\\n\\n<p>Notation: if $\\\\\\\\ell$ is a line, I will write $\\\\\\\\ell(x)$ for the $y$-coordinate on the linear corresponding to $x$.  In other words, I\\'ll treat $\\\\\\\\ell$ as a function of $x$.</p>\\n\\n<p><strong>Example 1.</strong> Ordinary least squares regression corresponds to the distance measure you mention: namely, $d(\\\\\\\\ell,S) = \\\\\\\\sum_{(x,y) \\\\\\\\in S} (\\\\\\\\ell(x) - y)^2$.  Thus, for each point in $S$, we consider the vertical distance from the point to the line $\\\\\\\\ell$, square it, and take the sum of these squared residuals.  Pictorially, we\\'re summing the squares of the lengths of the orange lines in this picture:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/iHA9s.png\" alt=\"OLS\"></p>\\n\\n<p><strong>Example 2.</strong> If you did ordinary least squares regression, but with the role of $x$ and $y$ swapped, then that would correspond to a slightly different similarity metric: namely, $d(\\\\\\\\ell,S) = \\\\\\\\sum_{(x,y) \\\\\\\\in S} (\\\\\\\\ell^{-1}(y) - x)^2$.  This corresponds to taking the horizontal distance from each point to the line $\\\\\\\\ell$, and summing these squared distances.  We\\'re summing the squared lengths the orange lines in this picture:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/pn4ua.png\" alt=\"OLS, with coordinates swapped\"></p>\\n\\n<p><strong>Example 3.</strong> If you did principal components analysis (PCA) (also known as orthogonal regression or total least squares (TLS)), that would correspond to a slightly different distance measure, namely, the orthogonal distance from each point to the line $\\\\\\\\ell$, squared and summed.  Now we\\'re summing the squared lengths of the orangle lines in this picture:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/9XTOT.png\" alt=\"PCA\"></p>\\n\\n<p><strong>Summary so far.</strong>  There are many possible similarity metrics.  Which one you choose will depend upon the nature of your application and your model for the data.</p>\\n\\n<p><strong>How to choose a similarity metric.</strong> OK, so there are many possible similarity metrics.  How should choose which one to use?  As already suggested, to answer this, you will need to have some model or hypothesis for how your data were generated.</p>\\n\\n<p>Ordinary least squares starts from the following model: it assumes that, for each point $(x_i,y_i)$, $y_i$ was generated as a probabilistic (stochastic) function of $x_i$.  It assumes the process went something like this: there\\'s some line $\\\\\\\\ell$ (not known to us a priori), and given the input $x_i$, we first calculate $\\\\\\\\ell(x_i)$ (the $y$-coordinate on the line corresponding to $x_i$), then we randomly generate some noise/error value $e_i$ from a normal distribution with mean 0 and standard deviation $\\\\\\\\sigma$ (where $\\\\\\\\sigma$ is not known a priori), and then we set $y_i = \\\\\\\\ell(x_i) + e_i$.  Since each $e_i$ is assumed to be iid Gaussian, a reasonable measure of how closely a hypothesized line $\\\\\\\\ell&#39;$ fits this model is to take the sum of squares of the inferred $e_i$ values.  For the correct line $\\\\\\\\ell$, we will have $d(\\\\\\\\ell, S) = \\\\\\\\sum_i e_i^2$, which has expectation $n \\\\\\\\sigma^2$.  For an incorrect line $\\\\\\\\ell&#39;$, we will have $d(\\\\\\\\ell&#39;, S) = \\\\\\\\sum_i (e_i \\\\\\\\pm \\\\\\\\ell&#39;(x_i)-\\\\\\\\ell(x_i))^2$, which has\\nexpectation $n \\\\\\\\sigma^2 + \\\\\\\\sum_i (\\\\\\\\ell&#39;(x_i)-\\\\\\\\ell(x_i))^2$, which you might notice is larger than the expectation for $d(\\\\\\\\ell, S)$.  In other words, starting from a particular stochastic model for how the data was generated, OLS uses a particular distance measure that is well-suited to testing how closely the observed data fits the presumed model.</p>\\n\\n<p>Of course, this is not the only plausible model for how your points might have been randomly generated (as a function of the line $\\\\\\\\ell$).  Given a different process, we might end up with a different similarity metric.</p>\\n\\n<p><strong>Hypothesis tests.</strong> Finally, let me mention one more topic that you didn\\'t bring up, but you could have.  Let\\'s say you have a stochastic model for how your points were randomly generated.  Let\\'s say that the line $\\\\\\\\ell$ is the parameter of this random process.  Then another interesting question we could ask is: given the observed set of points $S$, test the hypothesis that the points $S$ came from the aforementioned random process, using line $\\\\\\\\ell$ as parameter.  One could work out a way to compute a $p$-value for the likelihood that $S$ were generated according to that distribution.  If the $p$-value is very small, you might reject the hypothesis that the points were generated from this particular random process with parameter $\\\\\\\\ell$ (roughly speaking, this would amount to saying that the points $S$ don\\'t seem to correspond to the line $\\\\\\\\ell$).  If the $p$-value is not too small, you might proceed as though the hypothesis is potentially true (roughly speaking, this would amount to saying that the points $S$ are consistent with the hypothesis that they came from your random model with line $\\\\\\\\ell$).  So, hypothesis tests might be another way to, very roughly speaking, measure whether the points seem to be consistent with any particular line.</p>\\n\\n<p><strong>Credits.</strong> I gratefully acknowledge @JD Long for these excellent pictures.  See also <a href=\"http://stats.stackexchange.com/a/2700/2921\">his answer explaining different forms of regression</a> for more insight.</p>\\n',\n",
       " \"<p>I have some samples that have been treated with various chemical agents. Users score the treatment effect using categorical values/bins (i.e $&lt; 1, 2.5, 8, ... &gt; 100$). Within a range of say 1-100 they can get some fairly precise metric but outside of that range they input greater or less than some value. So the question is a user wants to know how similar one sample is to another... So I could do some sort of hierarchical clustering but I'm wondering with this mixed data type: </p>\\n\\n<ul>\\n<li>What could be an alternative approach? </li>\\n<li>What distance metric would be suitable?</li>\\n<li>Are there alternative methods to clustering?</li>\\n</ul>\\n\",\n",
       " \"<p>I worked out the details omitted from JMS's answer.</p>\\n\\n<p>Let $P_0 = \\\\\\\\Sigma_0^{-1}$ and $P_1 = \\\\\\\\Sigma_1^{-1}$</p>\\n\\n<p>We have</p>\\n\\n<p>$$KL(N_0||N_1) = \\\\\\\\int N_0(x) \\\\\\\\frac{1}{2}[\\\\\\\\ln |P_0| - \\\\\\\\ln|P_1| + (x-\\\\\\\\mu_0)^T P_0 (x-\\\\\\\\mu_0) - (x-\\\\\\\\mu_1)^T P_1 (x-\\\\\\\\mu_1)] dx$$\\n$$\\\\\\\\n= \\\\\\\\frac{1}{2}\\\\\\\\ln|P_0\\\\\\\\Sigma_1| - \\\\\\\\frac{1}{2} \\\\\\\\mathbb{E}[(x-\\\\\\\\mu_0)^T P_0 (x-\\\\\\\\mu_0) - (x-\\\\\\\\mu_1)^T P_1 (x-\\\\\\\\mu_1)]\\\\\\\\n$$\\n$$\\\\\\\\n= \\\\\\\\frac{1}{2}\\\\\\\\ln|P_0\\\\\\\\Sigma_1| + \\\\\\\\frac{1}{2}(\\\\\\\\mu_0-\\\\\\\\mu_1)^T P_1 (\\\\\\\\mu_0-\\\\\\\\mu_1)- \\\\\\\\frac{1}{2} \\\\\\\\mathbb{E}[x^T P_0 x - x^T P_1 x - \\\\\\\\mu_0^T (P_0 - P_1) \\\\\\\\mu_0]\\\\\\\\n$$\\n$$\\\\\\\\n= \\\\\\\\frac{1}{2}\\\\\\\\ln|P_0\\\\\\\\Sigma_1| + \\\\\\\\frac{1}{2}(\\\\\\\\mu_0-\\\\\\\\mu_1)^T P_1 (\\\\\\\\mu_0-\\\\\\\\mu_1)- \\\\\\\\frac{1}{2} \\\\\\\\mathbb{E}[(x-\\\\\\\\mu_0)^T (P_0-P_1) (x-\\\\\\\\mu_0)]\\\\\\\\n$$</p>\\n\\n<p>To simplify notation, assume $\\\\\\\\mu_0 = 0$. It remains to show that\\n$$\\\\\\\\mathbb{E}[x^T P_0x-x^T P_1 x] = k - tr(P_1\\\\\\\\Sigma_0)$$</p>\\n\\n<p>But $x^T P_0 x = x^T \\\\\\\\Sigma_0 x$ has a Chi-squared distribution and therefore has expectation $k$.  Meanwhile,</p>\\n\\n<p>$$\\\\\\\\mathbb{E}[x^T P_1 x] = \\\\\\\\mathbb{E}[tr(x^T P_1 x)] = \\\\\\\\mathbb{E}[tr(P_1 xx^T )]= tr(\\\\\\\\mathbb{E}(P_1 xx^T)) = tr(P_1 \\\\\\\\Sigma_0)$$</p>\\n\\n<p>where the second equality can be obtained by treating $x$ as a square matrix padded by zeroes.</p>\\n\",\n",
       " '<p>Here\\'s a nice review of the topic. It\\'s addressed to the Bioinformatics field, but the concepts generalize to other applications. <a href=\"http://bioinformatics.oxfordjournals.org/content/23/19/2507.abstract\" rel=\"nofollow\">http://bioinformatics.oxfordjournals.org/content/23/19/2507.abstract</a></p>\\n',\n",
       " '<p>I would like to quantify the amount of uncertainty in a given message, but the signal I work with is non-stationary and non-linear.</p>\\n\\n<p>Is it possible to apply Shannon entropy for such signal?</p>\\n',\n",
       " '<p>The following problem comes from a max likelihood calculation for gaussian families, but is of independent interest.</p>\\n\\n<p>Is it possible to find a closed-form approximation for small values of $x$ for</p>\\n\\n<p>$\\\\\\\\text{det}(B + xI)$</p>\\n\\n<p>where I is the identity matrix and B is hermitian rank-deficient positive semidefinite?</p>\\n',\n",
       " '<p>I have two raters who agree on 93% on the cases (two options: yes or no). However, when calculating cohens kappa through crosstabs in spss I get really strange outcomes like -0.42 with a sig. of 0.677.</p>\\n\\n<p>How can such a high agreement in percentage result in such a strange kappa?</p>\\n\\n<p>I don´t get it. </p>\\n',\n",
       " '<p>I have something like this:</p>\\n\\n<pre><code>Case I\\n\\n         A\\n|-----------------|\\n         |------------------|B\\n\\nCase II\\n                     A\\n            |-----------------|\\n|---------------------|\\n          B\\n</code></pre>\\n\\n<p>And the following numbers:</p>\\n\\n<pre><code>     Total generated by A            Total generated by B as seen by A\\nA            1000                                    139\\n\\n     Total generated by B            Total generated by A as seen by B\\nB            300                                      60\\n</code></pre>\\n\\n<p>I am trying to get the probability that the events generated by A and B will overlap (or rather I am trying to look at the probability that the pair A-B will generate events that overlap). I am doing it the following way:</p>\\n\\n<pre><code>    Total generated by B as seen by A + Total generated by A as seen by B\\n-------------------------------------------------------------------------------\\n                  Total generated by A + Total generated by B\\n</code></pre>\\n\\n<p>Is this correct or am I missing something? Or is there a better way to characterize the relation between A and B?</p>\\n\\n<p>Finally, if I have thousands of such pairs, what can I derive about the system?</p>\\n',\n",
       " '<p>Do you really need the maximum likelihood estimate?  Can you get away with the Bayesian estimator of the intensity of the Poisson process?  For $k$ observed events in a period of length $T$, the likelihood over the intensity is gamma-distributed with shape $k$ and rate $T$.  This is correct even if $k$ is one or zero (although you might want to introduce a gamma-distributed prior).</p>\\n',\n",
       " '<p>I have a dataset with observations form a population that have been described graphically as an annual trend. For example – male infection rate per year, female infection rate per year.</p>\\n\\n<p>The infections are aggregated count data and there are population based numerator data available.</p>\\n\\n<p>Which statistical test would I use to determine if the male infection temporal trend is different than the female infection temporal trend.</p>\\n\\n<p>Thanks.</p>\\n',\n",
       " \"<p>Simple (I hope) probability question I'd love it if someone could answer for me. </p>\\n\\n<p>You flip a coin 13 times (two outcomes: heads or tails).</p>\\n\\n<p>Before the coin tossing, I've written down the 'winning sequence', eg. a random sequence of 13 outcomes, heads or tails for each toss.</p>\\n\\n<p>For example: Heads, Heads, Tails, Heads, Tails, Tails, Tails, Heads, Tails, Heads, Heads, Tails, Heads.</p>\\n\\n<p>What are the chances of someone tossing the coin and getting the same sequence (heads or tails in the exact same sequence) as me?</p>\\n\",\n",
       " \"<p>I don't see any cause for concern here -- No assumptions are obviously violated.  But this is often difficult to confirm with so few data points.  I think you are ok.</p>\\n\",\n",
       " '<p>\"What the use of a p-value implies, therefore, is that a hypothesis that may be true may be rejected because it has not predicted observable results that have not occurred.\"</p>\\n\\n<p>Harold Jeffreys (Theory of Probability)</p>\\n',\n",
       " \"<p>For my thesis there's a big chance that I will need some sort of mixed-effects specification. I have some (non-syntax) experience with SPSS but feel that it won't suffice for my analysis. I have very basic knowledge in Stata and decided to experiment more with that package. </p>\\n\\n<p>I decided to try and replicate results from SPSS in Stata for a basic model. I have data on 4059 students in 65 schools, investigating the influence of entry level score (<code>standlrt</code>) of students on their final exam score (<code>normexam</code>).</p>\\n\\n<p>In a previously followed course which had a brief introduction to multilevel modeling, my teacher provided me with a syntax in SPSS.</p>\\n\\n<pre><code>MIXED\\n  normexam  WITH standlrt\\n  /FIXED = standlrt \\n  /PRINT = SOLUTION TESTCOV\\n /RANDOM INTERCEPT standlrt  | SUBJECT(school) COVTYPE(VC) .\\n</code></pre>\\n\\n<p>Now I tried replicating these results in Stata but the results are not consistent. Magnitude and sometimes even sign of the betas differ.</p>\\n\\n<p>First I use <code>xtset school</code> to indicate that my data is clustered. Then I use</p>\\n\\n<p><code>xtmixed normexam standlrt || school: standlrt</code> .</p>\\n\\n<p>What may be the cause of these inconsistent results?</p>\\n\\n<p>Thanks in advance!</p>\\n\\n<p>ps. this is not homework, and I hope I specified my first question properly.</p>\\n\\n<p>pps. a possibility may be that the 'problem' has multiple optima but I don't think this is the case in such a basic model, also because it's an uni-variate regression. Also, the iterative procedures performed while estimating may have different results, but I only think this would have big effects like sign changes.</p>\\n\\n<p>EDIT</p>\\n\\n<p>This is my Stata output</p>\\n\\n<pre><code>xtmixed normexam standlrt || school: standlrt\\n\\nMixed-effects REML regression                   Number of obs      =      4059\\nGroup variable: school                          Number of groups   =        65\\n\\n                                                Obs per group: min =         2\\n                                                               avg =      62.4\\n                                                               max =       198\\n\\n\\n                                                Wald chi2(1)       =    768.21\\nLog restricted-likelihood = -4667.8385          Prob &gt; chi2        =    0.0000\\n\\n------------------------------------------------------------------------------\\n    normexam |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\\n-------------+----------------------------------------------------------------\\n    standlrt |   .5570213   .0200971    27.72   0.000     .5176317    .5964108\\n       _cons |  -.0080944   .0400842    -0.20   0.840     -.086658    .0704691\\n------------------------------------------------------------------------------\\n\\n------------------------------------------------------------------------------\\n  Random-effects Parameters  |   Estimate   Std. Err.     [95% Conf. Interval]\\n-----------------------------+------------------------------------------------\\nschool: Independent          |\\n                sd(standlrt) |   .1214197   .0191066      .0891958    .1652852\\n                   sd(_cons) |   .3032317   .0309434      .2482638    .3703699\\n-----------------------------+------------------------------------------------\\n                sd(Residual) |   .7440605   .0083943      .7277885    .7606962\\n------------------------------------------------------------------------------\\nLR test vs. linear regression:       chi2(2) =   438.60   Prob &gt; chi2 = 0.0000\\n</code></pre>\\n\\n<p>And this is my SPSS output</p>\\n\\n<pre><code>-2 Restricted Log Likelihood    9335,677\\n\\nType III Tests of Fixed Effects(a)\\n|---------|------------|--------------|-------|----|\\n|Source   |Numerator df|Denominator df|F      |Sig.|\\n|---------|------------|--------------|-------|----|\\n|Intercept|1           |60,466        |,041   |,841|\\n|---------|------------|--------------|-------|----|\\n|standlrt |1           |56,936        |768,207|,000|\\n|---------|------------|--------------|-------|----|\\na. Dependent Variable: normexam = final exam scores.\\n\\n\\nEstimates of Fixed Effects(a)\\n|---------|--------|----------|------|------|----|-----------------------------------|\\n|Parameter|Estimate|Std. Error|df    |t     |Sig.|95% Confidence Interval            |\\n|         |        |          |      |      |    |-----------------------|-----------|\\n|         |        |          |      |      |    |Lower Bound            |Upper Bound|\\n|---------|--------|----------|------|------|----|-----------------------|-----------|\\n|Intercept|-,008094|,040084   |60,466|-,202 |,841|-,088262               |,072073    |\\n|---------|--------|----------|------|------|----|-----------------------|-----------|\\n|standlrt |,557021 |,020097   |56,936|27,717|,000|,516777                |,597266    |\\n|---------|--------|----------|------|------|----|-----------------------|-----------|\\na. Dependent Variable: normexam = final exam scores.\\n\\n\\nCovariance Parameters\\n\\nEstimates of Covariance Parameters(a)\\n|-------------------------------------|--------|----------|------|----|-----------------------------------|\\n|Parameter                            |Estimate|Std. Error|Wald Z|Sig.|95% Confidence Interval            |\\n|                            |--------|        |          |      |    |-----------------------|-----------|\\n|                                     |        |          |      |    |Lower Bound            |Upper Bound|\\n|----------------------------|--------|--------|----------|------|----|-----------------------|-----------|\\n|Residual                             |,553626 |,012492   |44,319|,000|,529676                |,578659    |\\n|----------------------------|--------|--------|----------|------|----|-----------------------|-----------|\\n|Intercept [subject = school]|Variance|,091949 |,018766   |4,900 |,000|,061635                |,137174    |\\n|----------------------------|--------|--------|----------|------|----|-----------------------|-----------|\\n|standlrt [subject = school] |Variance|,014743 |,004640   |3,177 |,001|,007956                |,027319    |\\n|----------------------------|--------|--------|----------|------|----|-----------------------|-----------|\\na. Dependent Variable: normexam = final exam scores.\\n</code></pre>\\n\\n<p>As you can see, the log likelihoods are the same. Additionally, the fixed effects tables are the same. However the random effects are different. I'm not very skilled in interpretation yet but the results seem to differ.</p>\\n\\n<p>These are the settings for the variance-covariance matrix</p>\\n\\n<pre><code> Model\\n      covariance(vartype)    variance-covariance structure of the random\\n                               effects\\n\\n    vartype                  Description\\n        -------------------------------------------------------------------------\\n        independent              one variance parameter per random effect, all\\n                                   covariances zero; the default unless a factor\\n                                   variable is specified\\n        exchangeable             equal variances for random effects, and one\\n                                   common pairwise covariance\\n        identity                 equal variances for random effects, all\\n                                   covariances zero; the default for factor\\n                                   variables\\n        unstructured             all variances and covariances distinctly\\n                                   estimated\\n</code></pre>\\n\\n<p>And I read online that <code>COVTYPE(VC) requests the default (variance component) structure for random effects, which assumes all random effects are independent.</code> </p>\\n\",\n",
       " \"<p>As Henry already said you can't say it's normal. Just try to run the following command in R several times:</p>\\n\\n<pre><code>shapiro.test(runif(9)) \\n</code></pre>\\n\\n<p>This will test the sample of 9 numbers from uniform distribution. Many times the p-value will be much larger than 0.05 - which means that you cannot conclude that the distribution is normal.</p>\\n\",\n",
       " '<p>Based on the information that you provide, I think you\\'re after a consensus-type measure, where the number of experts agreeing at each star level is weighted by how much you believe their ratings are accurate.</p>\\n\\n<p>So, for example, say your 6 raters have the following \"expert\" weights:</p>\\n\\n<ul>\\n<li>Raters A and B each have a weight of 1 </li>\\n<li>Rater C has a weight of 0.8</li>\\n<li>Rater D has a weight of 0.5 </li>\\n<li>Raters E and F each have a weight of 0.25</li>\\n</ul>\\n\\n<p>Now consider one bottle of wine. You receive the following ratings:</p>\\n\\n<pre><code>Rater     Rating\\nA         5\\nB         4\\nC         4\\nD         5\\nE         4\\nF         4\\n</code></pre>\\n\\n<p>Rating 5 is based basically on 1.5 \"reliable\" Raters <code>(1(A) + 0.5(D))</code>\\nRating 4 is based basically on 2.3 \"reliable\" Raters <code>(1(B)+0.8(C)+0.25(E)+0.25(F))</code>\\nso the consensus is 4 stars.</p>\\n\\n<p>We don\\'t adjust the rating based on rater reliability, we adjust the number of \"experts\" giving each rating based on rater reliability. Then we pick the rating based on the most number of experts.</p>\\n\\n<p>Update based on comments and the answer from @gung: if the experts are varying wildly across wines, and therefore you get an aggregate score that you don\\'t think is a good summary of the wine, the problem is likely to lie with your expert panel rather than the method used for aggregation. If the expert ratings are not reliable or are invalid for reasons other than unreliability, it won\\'t matter what aggregation method you use because the GIGO principle will apply.</p>\\n',\n",
       " '<p>Is a \"split plot\" ANOVA with two factors identical to two-way ANOVA with repeated measures in one factor? if not, what is the distinction?</p>\\n',\n",
       " '<p>I think I found it here:</p>\\n\\n<p><a href=\"http://quantitative-methoden.de/Dateien/Auflage3/Band_II/Kapitel_7_GPower_Ergaenzungen_A3.pdf\" rel=\"nofollow\">Link to book chapter, additional material</a></p>\\n\\n<p>Applied to my problem I guess it will be like this:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/YuYlG.png\" alt=\"enter image description here\"></p>\\n\\n<pre><code>F tests - ANOVA: Repeated measures, within factors\\nAnalysis:   Criterion: Compute required α \\nInput:  Effect size f   =   0.25\\n    Power (1-β err prob)    =   0.80\\n    Total sample size   =   28\\n    Number of groups    =   1\\n    Number of measurements  =   3\\n    Corr among rep measures =   0.3\\n    Nonsphericity correction ε  =   1\\nOutput: Noncentrality parameter λ   =   7.5000000\\n    Critical F  =   2.2129741\\n    Numerator df    =   2.0000000\\n    Denominator df  =   54.0000000\\n    α err prob  =   0.1191999\\n</code></pre>\\n',\n",
       " '<p>I am currently reading a <a href=\"http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6VG2-4Y648C8-1&amp;_user=10&amp;_coverDate=11/30/2009&amp;_rdoc=1&amp;_fmt=high&amp;_orig=gateway&amp;_origin=gateway&amp;_sort=d&amp;_docanchor=&amp;view=c&amp;_searchStrId=1672491428&amp;_rerunOrigin=google&amp;_acct=C000050221&amp;_version=1&amp;_urlVersion=0&amp;_userid=10&amp;md5=da95476f2c8535361fb031914a475b32&amp;searchtype=a\">paper</a> concerning voting location and voting preference in the 2000 and 2004 election. In it, there is a chart which displays logistic regression coefficients.  From courses years back and a little <a href=\"http://en.wikipedia.org/wiki/Logistic_regression\">reading up</a>, I understand logistic regression to be a way of describing the relationship between multiple independent variables and a binary response variable. What I\\'m confused about is, given the table below, because the South has a logistic regression coefficient of .903, does that mean that 90.3% of Southerners vote republican? Because of the logistical nature of the metric, that this direct correlation does not exist. Instead, I assume that you can only say that the south, with .903, votes Republican more than the Mountains/plains, with the regression of .506.  Given the latter to be the case, how do I know what is significant and what is not and is it possible to extrapolate a percentage of republican votes given this logistic regression coefficient.\\n<img src=\"http://i.stack.imgur.com/xsiyn.png\" alt=\"Table showing logistic regression coefficients\"></p>\\n\\n<p>As a side note, please edit my post if anything is stated incorrectly</p>\\n',\n",
       " '<p>There was too much snow on the highways, so the mayor of the town sent snowplows to spread some chemicals on them. There is a standard of how much of one specific substance should be present in the compound that is used for spreading...\\nWe measured how much of the substance was present in the compound in 30 different places of the town.\\nThese are the results: </p>\\n\\n<pre><code>0.91 1.08 0.72 1.07 1.14 0.62 1.06 1.20 0.76 1.19   \\n0.96 0.73 0.83 0.55 0.79 1.34 0.60 1.19 1.35 1.13   \\n0.67 0.77 0.48 0.83 1.78 2.25 1.21 0.89 0.83 1.07\\n</code></pre>\\n\\n<p>We expect that the values have normal distribution.\\nVerify with a reliability of 99% that the standard deviation is less than 0.4.\\n[Result: r = 24.546. Hypothesis H0 is not denied.]</p>\\n\\n<p>I calculated </p>\\n\\n<p>a)  $\\\\\\\\mu$  = 1.00.....and.....b)  $\\\\\\\\sigma$  = 0.367</p>\\n\\n<p>Now I set ...H0:  $\\\\\\\\sigma^{2} =  \\\\\\\\sigma^{2}_{o}$... versus...H1:  $\\\\\\\\sigma^{2} &lt;  \\\\\\\\sigma^{2}_{o}$</p>\\n\\n<p>I used this test:\\n   $ \\\\\\\\frac {(n-1) s_{n}^{2}} { \\\\\\\\sigma_{0}^{2}  }   \\\\\\\\leq  \\\\\\\\chi^{2} _{ \\\\\\\\alpha } (n-1) $</p>\\n\\n<p>Then, I calculated \\n$    \\\\\\\\frac {(n-1) s_{n}^{2}} { \\\\\\\\sigma_{0}^{2}  } $  = 24.54 and\\n$\\\\\\\\chi^{2} _{ \\\\\\\\alpha } (n-1) $ = 49.58</p>\\n\\n<p>Now, we see that the inequality holds good, so H0 should be denied! \\nHowever the result in the book says the opposite... </p>\\n',\n",
       " '<p>[A good place to find more on this stuff would be in many introductory college-level texts.]</p>\\n\\n<p>My biggest concern would be that with percentages, the variance is unlikely to be constant. This would be an issue because the t-test (for example) would be based on assuming that the differences are observations from a population with a common mean and common variance, when in fact the differences for the larger observations are likely to become \"squeezed up\"; the closer they get to 100%, if they can\\'t go over it. That is, the variance of the differences will depend on the sizes of the observations in the difference. The distribution of the test statistic will be impacted - to be specific, the nominal significance level will not reflect the actual rejection rate under the null hypothesis. The power, too, will be affected.</p>\\n\\n<p>--</p>\\n\\n<p>For the t-test:</p>\\n\\n<p>Let x = score on method 1, y = score on method 2</p>\\n\\n<p>To test the null hypothesis that the true mean difference is zero:</p>\\n\\n<ol>\\n<li><p>Calculate the difference ($d_i = y_i − x_i$) between the two observations on each pair, keeping the sign.</p></li>\\n<li><p>Calculate the mean of the differences, $\\\\\\\\bar{d}$.</p></li>\\n<li><p>Calculate the standard deviation of the differences, $s_d$, and use this to calculate the standard error of the mean difference, $s_{\\\\\\\\bar{d}} = \\\\\\\\frac{s_d}{\\\\\\\\sqrt{n}}$</p></li>\\n<li><p>Calculate the t-statistic, which is given by $T = \\\\\\\\frac{\\\\\\\\bar{d}}{s_{\\\\\\\\bar{d}}}$ . \\nUnder the null hypothesis,\\nthis statistic follows a $t$-distribution with $n−1$ degrees of freedom, where $n$ is the number of pairs.</p></li>\\n<li><p>Use tables of the $t$-distribution to compare your value for $T$ to the $t_{n−1}$ distribution. This will give the $p$-value for the paired $t$-test.</p></li>\\n</ol>\\n\\n<p>Uh, is this homework? What was the experiment about?</p>\\n\\n<p><em><strong>Edit:</em></strong> example calculations:</p>\\n\\n<p>In <strong>R</strong>:</p>\\n\\n<pre><code>Method1 &lt;-c(74.47,79.34,72.85,83.54,75.18,73.58,70.06,\\n           73.35,74.41,78.35,73.14,81.55,76.21,74.67)\\n\\nMethod2 &lt;- c(83.25,85.14,81.61,77.59,79.38,80.24,79.94,82.54,78.22,\\n           83.21,82.54,77.67,83.51,78.87)\\n\\n\\n&gt; t.test(Method1,Method2,paired=TRUE)\\n\\n    Paired t-test\\n\\ndata:  Method1 and Method2 \\nt = -4.071, df = 13, p-value = 0.001323\\nalternative hypothesis: true difference in means is not equal to 0 \\n95 percent confidence interval:\\n -7.982491 -2.447509 \\nsample estimates:\\nmean of the differences \\n                 -5.215 \\n</code></pre>\\n\\n<p>OR:</p>\\n\\n<pre><code>&gt; wilcox.test(Method1,Method2,paired=TRUE)\\n\\n    Wilcoxon signed rank test\\n\\ndata:  Method1 and Method2 \\nV = 9, p-value = 0.004028\\nalternative hypothesis: true location shift is not equal to 0 \\n</code></pre>\\n\\n<p>OR (sign test):</p>\\n\\n<pre><code>&gt; binom.test(sum(Method1&gt;Method2),length(Method1))\\n\\n    Exact binomial test\\n\\ndata:  sum(Method1 &gt; Method2) and length(Method1) \\nnumber of successes = 2, number of trials = 14, p-value = 0.01294\\nalternative hypothesis: true probability of success is not equal to 0.5 \\n95 percent confidence interval:\\n 0.01779452 0.42812916 \\nsample estimates:\\nprobability of success \\n             0.1428571 \\n</code></pre>\\n\\n<p>OR (permutation test):</p>\\n\\n<pre><code>diffs &lt;- Method1 - Method2\\nf3 &lt;- 2*outer(0:(2^14-1),2^(0:13),function(x,y) (x %/% y) %% 2) -1\\nres &lt;- rowMeans(f3*matrix(rep(diffs,each=2^14),nc=14))\\ndata.frame(pvalue = sum(abs(res)&gt;=abs(mean(diffs)))/2^14,row.names=\" \")\\n       pvalue\\n  0.003417969\\n</code></pre>\\n\\n<p>etc . . . </p>\\n',\n",
       " '<p>Probability distribution of two classes is given by  $N(5,1)$ and $N(6,1)$ where \\n$N(\\\\\\\\mu,\\\\\\\\sigma^2)$:</p>\\n\\n<p>$$f(x) = \\\\\\\\frac{1}{\\\\\\\\sqrt{2\\\\\\\\pi \\\\\\\\sigma^2}} e^{-\\\\\\\\frac{(x-\\\\\\\\mu)^2}{2\\\\\\\\sigma^2}} $$</p>\\n\\n<ul>\\n<li>How to classify them, and see error rate?</li>\\n</ul>\\n\\n<p>I am doing this in MATLAB</p>\\n\\n<p>Taking 500 samples of each distribution and tagging them depending where they came from.</p>\\n\\n<pre><code>sample1 =    \\n    0.8864   -1.0000\\n    0.1560   -1.0000\\n    0.8502   -1.0000\\n   -0.4059   -1.0000\\n    0.9298   -1.0000\\nsample2 =\\n   -0.0671    1.0000\\n    0.7057    1.0000\\n    0.3310    1.0000\\n   -0.7314    1.0000\\n   -0.4524    1.0000\\ndata =\\n    0.8864   -1.0000\\n    0.1560   -1.0000\\n    0.8502   -1.0000\\n   -0.4059   -1.0000\\n    0.9298   -1.0000\\n   -0.0671    1.0000\\n    0.7057    1.0000\\n    0.3310    1.0000\\n   -0.7314    1.0000\\n   -0.4524    1.0000\\n</code></pre>\\n\\n<p>Now I would like to classify them using Bayes, I am using <code>normpdf</code>, to make this easier I am taking prior probabilities equal so they are not important in creating the rule, but I do not know how to code this in MATLAB, any idea?</p>\\n\\n<pre><code>n=500;\\nsample1=[randn(n,1) -1*ones(n,1)];\\nsample2=[randn(n,1) ones(n,1)];\\ndata=[sample1; sample2];\\nmu1    =5;\\nsigma1 =1;\\nmu2    =6;\\nsigma2 =1;\\nx1=linspace(mu1-1*sigma1,mu1+1*sigma1,500);\\np1=normpdf(x1,mu1,sigma1);  \\nx2=linspace(mu2-1*sigma2,mu2+1*sigma2,500);\\np2=normpdf(x2,mu2,sigma2);\\nplot(x1,p1,x2,p2)\\n</code></pre>\\n\\n<p>Also Is it correct to label with <code>-1 and 1</code>?\\nIf I compute mean and variance of <code>sample1, and sample2</code>?\\n mean_S1=mean(sample1);\\n mean_S2=mean(sample2);\\n var_S1 = var(sample1);\\n var_S2 = var(sample2);\\n - What is next step?\\n - for error rate Im planning to do a comparison between the original class vector (-1,1) and the result of classifier like:</p>\\n\\n<pre><code>errorRate = mean(OriginalClasses ~= ResultOfClassifier);\\n</code></pre>\\n\\n<p><em><strong></em>**UPDATED*<em>*</em></strong></p>\\n\\n<pre><code>clear;\\nclc;\\nn = 500;\\nmu1    =5;\\nsigma1 =1;\\nmu2    =6;\\nsigma2 =1;\\nmu = [mu1,mu2];sigma = [sigma1,sigma2];  %group them\\n%suppose you get your test data from somewhere. \\n%for kicks, I put random data in:\\n%xtest = randn(2*n,1);     %OP example code has the labels in the data var; ack\\nsample1=[randn(n,1) -1*ones(n,1)];\\nsample2=[randn(n,1) ones(n,1)];\\ndata=[sample1; sample2];\\ndeviance = bsxfun(@minus,data,mu);  %tbc\\ndeviance = bsxfun(@rdivide,deviance,sigma); %tbc\\ndeviance = deviance .^ 2; %tbc\\ndeviance = bsxfun(@plus,deviance,2*log(abs(sigma))); %tbc\\ndeviance = deviance + log(2*pi);  %not necessary here, actually;\\n[dummy,mini] = min(deviance,[],2);  %find which class;\\n ResultOfClassifier = 2 * mini - 3;  %is now a -1/1\\n errorRate = mean(data(:,2) ~= ResultOfClassifier)\\n\\nerrorRate =\\n\\n    0.2680\\n</code></pre>\\n',\n",
       " '<p>Let me give you a very very intuitional insight.  Suppose you are tossing a coin 10 times and you get 8 heads and 2 tails. The question that would come to your mind is whether this coin is biased towards heads or not. </p>\\n\\n<p>Now if you go by conventional definitions or the frequentist approach of probability you might say that the coin is unbiased and this is an exceptional occurrence. Hence you would conclude that the possibility of getting a head next toss is also 50%. </p>\\n\\n<p>But suppose you are a Bayesian.  You would actually think that since you have got exceptionally high number of heads, the coin has a bias towards the head side. There are methods to calculate this possible bias. You would calculate them and then when you toss the coin next time, you would definitely call a heads.</p>\\n\\n<p>So, Bayesian probability is about the belief that you develop based on the data you observe. I hope that was simple enough.</p>\\n',\n",
       " '<p>I have a brief question. I am having trouble finding a good reference that explains what crisp logic is. </p>\\n\\n<p>What I think: </p>\\n\\n<p>I have two classification models, a decision tree and a ruleset, which I think are crisp models. Since they say that an instance is either class A or not. </p>\\n\\n<p>I have another classification model, a logistic regression, that is not crisp, since it gives the probability that an instance belongs to this class. </p>\\n\\n<p>I want to mention this in a presentation that I have later today. I tried looking up this subject, but it would be great if somebody could just confirm how I interpreted it. I could also use a good reference to use in my paper. </p>\\n\\n<p>Thanks, </p>\\n',\n",
       " '<p>I want to second @King\\'s points.  It is very intuitive to suspect that regressing $y$ onto $x$ (\\'direct regression\\') and regressing $x$ onto $y$ (\\'reverse regression\\') ought to be the same.  <a href=\"http://www.jstor.org/pss/1391254\" rel=\"nofollow\">However</a>, this is neither true mathematically nor with respect to how the regression is related to the situation you\\'re analyzing.  If you plot $y$ on the vertical axis of a graph and $x$ on the horizontal axis, you can see what\\'s happening.  Direct regression finds the line that minimizes the vertical distances between the data points and the line, whereas reverse regression minimizes the horizontal distances.  The line that minimizes the one will only minimize the other if $r_{xy}=1.0$.  You need to decide what you want to explain, and what you want to use to explain it.  The answer to that question gives you which variable is $y$ and $x$ and specifies your model.  Also, (again following @King), I disagree with trying to say $x_{max}=f^{-1}(y_{max})$, for the same reasons.  </p>\\n\\n<p>Regarding the issue of a bounded variable, typically it is conceivable that the \\'real\\' amount could go higher, but that you just can\\'t measure it.  For example, an outside thermometer out my window goes up to 120, but it could be 140 outside in some places, and you would only have 120 as your measurement.  Thus, the variable would have an upper bound, but the thing you really wanted to think about doesn\\'t.  If this is the case, <a href=\"http://en.wikipedia.org/wiki/Tobit_model\" rel=\"nofollow\">tobit</a> models exist for just such situations.  </p>\\n\\n<p>Another approach would be to use something more robust like loess, which may be perfectly adequate for your needs.</p>\\n',\n",
       " '<p>Suppose you have an $n$-vector $X$. For a fixed real number, $r$ between $-1$ and $1$, can one generate a random permutation of the integers $1,2,\\\\\\\\ldots,n$, call it $i_1,i_2,\\\\\\\\ldots,i_n$ such that the vector $X$ and the vector $\\\\\\\\tilde{X}$ defined by $\\\\\\\\tilde{X_j} = X_{i_j}$ have expected sample correlation of $r$?  I am looking for a process that generates such permutations. Without loss of generality, I believe one may assume $X$ has zero sample mean, and unit sample standard deviation. </p>\\n',\n",
       " '<p>You can, for each list, convert the time to \"number of seconds from 12:00:00.00\" (fractional values allowed). Then plot the 3 data series over the common numerical X-axis. </p>\\n',\n",
       " '<p>Usually, forecasting is applied to time series data where future values of a series are estimated based on past observations.</p>\\n',\n",
       " \"<p>There's no way to specify an error function for the standard <code>randomForest</code> implementation. It is possible to supply a custom splitting function to the function <code>rpart</code> which generates a single regression tree, and then to use the <code>ipred</code> package to perform bootstrap aggregation, giving a bagged regression forest, but this is still not a full random forest, as there is no random feature selection taking place.</p>\\n\\n<p>Unfortunately, your best bet here is probably to write your own random forest code, or possibly to modify the existing implementation, if you're happy digging around in C code.</p>\\n\",\n",
       " '<ol>\\n<li>Is the central-limit-theorem true for all distributions?</li>\\n<li>For what sample size is it true (what is meant with \"large\")?</li>\\n</ol>\\n',\n",
       " '<p>I open a .sav file in <a href=\"http://www.gnu.org/software/pspp/\" rel=\"nofollow\">PSPP</a> (an open-source alternativ to SPSS) with </p>\\n\\n<blockquote>\\n  <p>get file=...sav</p>\\n</blockquote>\\n\\n<p>Now I would like to see how many rows there exist.</p>\\n',\n",
       " '<p>I would like to compare before-and-after \"scores\" on five quality of life questions across two groups. What I would like to know are:</p>\\n\\n<ol>\\n<li>Are the baseline scores across the two groups similar?</li>\\n<li>Is the change in scores significant (before and after) within each group?</li>\\n<li>Is the change in scores significant across both groups?</li>\\n</ol>\\n\\n<p>Will an independent t-test suffice to answer question number 1?</p>\\n\\n<p>Will it be appropriate to perform a Wilcoxon signed rank test to answer question number 2? If not, what will be the more appropriate test?</p>\\n\\n<p>On question 3, is Mann-whitney appropriate?</p>\\n\\n<p>Thank you so much for the help.</p>\\n',\n",
       " '<p>In principle:</p>\\n\\n<p>Make your predictions using a single model trained on the entire dataset (so there is only one set of features).  The cross-validation is only used to estimate the predictive performance of the single model trained on the whole dataset.  It is VITAL in using cross-validation that in each fold you repeat the entire procedure used to fit the primary model, as otherwise you can end up with a substantial optimistic bias in performance.</p>\\n\\n<p>To see why this happens, consider a binary classification problem with 1000 binary features but only 100 cases, where the cases and features are all purely random, so there is no statistical relationship between the features and the cases whatsoever.  If we train a primary model on the full dataset, we can always achieve zero error on the training set as there are more features than cases.  We can even find a subset of \"informative\" features (that happen to be correlated by chance).  If we then perform cross-validation using only those features, we will get an estimate of performance that is better than random guessing.  The reason is that in each fold of the cross-validation procedure there is some information about the held-out cases used for testing as the features were chosen because they were good for predicting, all of them, including those held out.  Of course the true error rate will be 0.5.</p>\\n\\n<p>If we adopt the proper procedure, and perform feature selection in each fold, there is no longer any information about the held out cases in the choice of features used in that fold.  If you use the proper procedure, in this case, you will get an error rate of about 0.5 (although it will vary a bit for different realisations of the dataset).</p>\\n\\n<p>Good papers to read are:</p>\\n\\n<p>Christophe Ambroise, Geoffrey J. McLachlan, \"Selection bias in gene extraction on the basis of microarray gene-expression data\", PNAS <a href=\"http://www.pnas.org/content/99/10/6562.abstract\" rel=\"nofollow\">http://www.pnas.org/content/99/10/6562.abstract</a></p>\\n\\n<p>which is highly relevant to the OP and</p>\\n\\n<p>Gavin C. Cawley, Nicola L. C. Talbot, \"On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation\", JMLR 11(Jul):2079−2107, 2010 <a href=\"http://jmlr.csail.mit.edu/papers/v11/cawley10a.html\" rel=\"nofollow\">http://jmlr.csail.mit.edu/papers/v11/cawley10a.html</a></p>\\n\\n<p>which demonstrates that the same thing can easily ocurr in model selection (e.g. tuning the hyper-parameters of an SVM, which also need to be repeated in each iteration of the CV procedure).</p>\\n\\n<p>In practice:</p>\\n\\n<p>I would recommend using Bagging, and using the out-of-bag error for estimating performance.    You will get a committee model using many features, but that is actually a good thing.  If you only use a single model, it will be likely that you will over-fit the feature selection criterion, and end up with a model that gives poorer predictions than a model that uses a larger number of features.</p>\\n\\n<p>Alan Millers book on subset selection in regression (Chapman and Hall monographs on statistics and applied probability, volume 95) gives the good bit of advice (page 221) that if predictive performance is the most important thing, then don\\'t do any feature selection, just use ridge regression instead.  And that is in a book on subset selection!!! ;o)</p>\\n',\n",
       " \"<p>You have not correctly interpreted user697473's claim. He is not talking about failing to include any data from brand C. He was talking about giving a particular vector of assignemnts $0$ probability. He was not saying that you can magically determine the value of some variable while never testing it. He wants to be able to use a balanced random subset, so that each point is included in the random subset with the right probability, but not a uniformly random one. </p>\\n\\n<p>For example, if the set is $\\\\\\\\lbrace x_1,x_2,x_3,x_4 \\\\\\\\rbrace$, then the following random subsets of uniform size $2$ all have the property that the probability that $x_i$ is included is $1/2$:</p>\\n\\n<p>$S_1 = 1/6\\\\\\\\lbrace x_1,x_2\\\\\\\\rbrace + 1/6\\\\\\\\lbrace x_1,x_3\\\\\\\\rbrace  + 1/6\\\\\\\\lbrace x_1,x_4\\\\\\\\rbrace  + 1/6\\\\\\\\lbrace x_2,x_3\\\\\\\\rbrace  + 1/6\\\\\\\\lbrace x_2,x_4\\\\\\\\rbrace  + 1/6\\\\\\\\lbrace x_3,x_4\\\\\\\\rbrace $</p>\\n\\n<p>$S_2 =    1/4\\\\\\\\lbrace x_1,x_3\\\\\\\\rbrace  + 1/4\\\\\\\\lbrace x_1,x_4\\\\\\\\rbrace  + 1/4\\\\\\\\lbrace x_2,x_3\\\\\\\\rbrace  + 1/4\\\\\\\\lbrace x_2,x_4\\\\\\\\rbrace $</p>\\n\\n<p>$S_3 = 1/2\\\\\\\\lbrace x_1,x_2\\\\\\\\rbrace + 1/2\\\\\\\\lbrace x_3,x_4\\\\\\\\rbrace $</p>\\n\\n<p>These are all balanced in the sense that if you compute the average value of some function $f$ over the random set, the expected value is $1/4(f(x_1) + f(x_2) + f(x_3)+f(x_4))$. In the third random subset, the probability of the subset $\\\\\\\\lbrace x_1,x_3 \\\\\\\\rbrace$ is $0$.  </p>\\n\\n<p>That said, the point of the experiment should not be to produce an unbiased estimate. That is just one consideration. Another goal is to provide useful information. If you know that you may want to estimate $f$ on a subset $T$ (say $\\\\\\\\lbrace x_1,x_2 \\\\\\\\rbrace$) and its complement and to subtract the two, the quality of your estimate depends on $\\\\\\\\#(T \\\\\\\\cap S)$ and  $\\\\\\\\#(T^c \\\\\\\\cap S)$. Then not all balanced subsets have the same quality. For that task, $S_3$ is worse than random assignment ($S_1$), while $S_2$ is better than random assignment. </p>\\n\",\n",
       " \"<p>I think that for the covariance between $T2$ and $T3$ you could try substituting in $T2 = X1$ and $T3 = X1 + X2$.</p>\\n\\n<p>So:\\n\\\\\\\\begin{equation} Cov(T2,T3) = Cov(X1,X1+X2) = Cov(X1,X1) + Cov(X1,X2) \\\\\\\\end{equation} </p>\\n\\n<p>If you assume that $X1$ and $X2$ are independent and remember that $Cov(A,A) = Var(A)$ then you might get what you need here.  Then follow this blueprint to fill out the rest of the terms in $\\\\\\\\Sigma$.</p>\\n\\n<p>For the second part of the question ($P(T4&gt;70)$) I think you've got the right idea.</p>\\n\\n<p>For the last bit, note that you are estimating $T3$ conditional on knowing $T2$, i.e. $T3|T2 = 26.1$ which could be written $X1+X2|X1 = 26.1$.  The randomness associated with $X1$ is basically removed from the problem, because you know it.  Try thinking along those lines.  Similarly if you know $X1+X2+X3 = 65.234$, how can you use that knowledge to estimate $X1+X2$? (try rearranging the equation to start with).  Hope it helps.</p>\\n\",\n",
       " '<p>I think the best way to takle the problem is to take a Bayesian approach.</p>\\n\\n<p>Your scoring metric $S$ is going to be the expectation of the parameter of a Bernouilli distribution which represents the intrinsic accuracy of the player.\\nAs a prior distribution, you can take the conjugate prior that is the <a href=\"http://en.wikipedia.org/wiki/Beta_distribution\" rel=\"nofollow\">beta distribution</a> with parameter $\\\\\\\\alpha$ and $\\\\\\\\beta$. You choose them such as expectation and variance are equal to the sample mean and sample variance of the data. </p>\\n\\n<p>The final posterior expectation of the accuracy becomes (using <a href=\"http://en.wikipedia.org/wiki/Conjugate_prior\" rel=\"nofollow\">wikipedia</a>):</p>\\n\\n<p>$S=E(\\\\\\\\Theta) = \\\\\\\\frac{\\\\\\\\alpha + n_{acc}}{\\\\\\\\alpha + \\\\\\\\beta + n_{tot}}$</p>\\n\\n<p>where $n_{acc}$ (resp. $n_{tot}$) is the number of accurate (resp. total) passes of the player considered.</p>\\n\\n<p>The formula behaves quite nicely if the number of passes $n_{tot}$ is low / high.</p>\\n',\n",
       " '<p>I read <a href=\"http://www.evanmiller.org/how-not-to-run-an-ab-test.html\" rel=\"nofollow\">this article</a> on \"how not to run an A/B test\".</p>\\n\\n<p>And I still don\\'t understand what exactly the author\\'s reasoning is. Can someone dumb it down for me?</p>\\n\\n<p>I think what it might be saying is that reading the results of my split tests over time misleads me. I want to be able to understand this well enough that I can explain it to others, though.</p>\\n\\n<p>Any help?</p>\\n',\n",
       " '<p>My friend and I want to do a hands on tutorial on Bayes theorem for the Seattle <a href=\"http://lesswrong.com/\">LessWrong</a> group. Neither of us have done this before, so we\\'re searching for prior art; techniques that other people have tried before and descriptions of how they turned out. </p>\\n\\n<p>What are good techniques and resources for teaching bayes theorem? Reports of both successes and failures are useful, I\\'d like to know what <em>not</em> to do in addition to what to do.</p>\\n\\n<p>The audience is a group of ~8 programmers and natural science students. They\\'ll be smart and capable but not necessarily used to doing much math.</p>\\n',\n",
       " '<p>This term appears frequently in the <a href=\"http://www.kaggle.com/c/DontGetKicked/forums/t/1227/congratulations-to-the-winners\">method-related threads</a>.</p>\\n\\n<p>Is <strong>blending</strong> a specific method in data-mining and statistical learning? I cannot get a relevant result from google.</p>\\n\\n<p>It seems blending is mixing up outcomes from many models and resulting in a better result. Is there any resource that helps me knowing more about it?</p>\\n',\n",
       " '<p>I am not sure if this is another way of putting your answer but I just learned about the link function in glimmix...the default distribution in glimmix is gaussian so in order to use a different distribution I need to use the link statment...</p>\\n\\n<p>Thanks for expedient input.</p>\\n',\n",
       " \"<p>Computationally this is a correct calculation of a p value for a two sided test of whether there is significant evidence the parameter behind your t value is different from zero, assuming the t statistic does indeed have a t distribution under the null hypothesis.</p>\\n\\n<p>I'm surprised the summary() method doesn't itself calculate this p value.  This makes me wonder whether the author has reason to doubt whether these statistics really do have t distributions and this is a valid test to use.  I would investigate this further if I were you.</p>\\n\\n<p>I would also round your p values (and probably everything else in that table) to two decimal places to avoid giving a false sense of precision.</p>\\n\",\n",
       " '<p>I understand that there are degrees of trust one can have in the output of ANOVA. However, naturally I want to maximize the amount I can trust my results.</p>\\n\\n<p><strong>Question:</strong> If I have data that violate the assumption of normality (of residuals), how robust is a factorial ANOVA when cell sizes are unequal? My cell sizes range from 42 to 55, for a 2x3 between-subjects ANOVA. </p>\\n',\n",
       " \"<p>I'm trying to fit a multilevel model for a repeated mesures design with three levels:\\nSujects - conditions - trials. Each subject pass the test in two conditions, and there are 21 trials in each condition. Each trial is expected to be correlated in some manner with previous and following trial. </p>\\n\\n<p>My problem is that I don't know which covariance type for repeated measures should I select to the model... Anyone can tell me how can I find out which is the correct? (By the way, I'm using SPSS and not familiar with other softwares...)</p>\\n\\n<p>Thanks!</p>\\n\",\n",
       " '<p>I have used quadrat analysis only on regular grids. It was helpful in regard to the purpose, which was to compare the dispersion of sampling data with a known process, e.g., random. Therefore a regular grid worked well.<br>The method you developed and described is not sure to be quadrat counting. For example in the moving average method, one option is to count the number of neighbors for the process, i.e., averaging, which is simply done by searching within a circle (in 2D) or sphere (in 3D). Your method looks similar with a slightly different use of those selected samples.</p>\\n',\n",
       " '<p>You did not really describe what you mean with model share, i.e. what the purpose of the modelling is, but the <a href=\"http://robjhyndman.com/software/hts/\" rel=\"nofollow\">hts R package</a> and hierarchical time series provide a way to create forecasts of time series which sum up to 100%, etc. Prof. Hyndman has at least two publications on using them: <a href=\"http://robjhyndman.com/papers/hierarchical-tourism/\" rel=\"nofollow\">Hierarchical forecasts for Australian domestic tourism</a> and <a href=\"http://robjhyndman.com/papers/hierarchical/\" rel=\"nofollow\">Optimal combination forecasts for hierarchical time series</a>. Besides documentation of the package I don\\'t know other sources about hierarchical time series than the papers and the references therein.</p>\\n',\n",
       " '<p>This is an upcoming regular conference, that is being held in Sydney, Australia in July 2012, normally the meetings are in Europe: <a href=\"http://conference.acspri.org.au/index.php/rc33/2012/index\" rel=\"nofollow\">8th international conference on social science methodology</a>.</p>\\n\\n<p>The <a href=\"http://www.acspri.org.au/\" rel=\"nofollow\">Australian Consortium for Social and Political Research Inc</a> has a conference based in Australia every two years, the one for this year happens to be run combined with the conference I linked above.</p>\\n',\n",
       " '<p>There are formulae for computing the leave-one-out cross-validation error in closed form for many models, including least-squares regression, but as far as I am aware there isn\\'t a general formula for k-fold cross-validation (or at least it may be possible but the computational advantage is too small to be worthwhile).</p>\\n\\n<p>The formula in the book isn\\'t saying very much it is just saying that the cross-validation error is the average of the loss function (L) evaluated using models trained on different subsets of the data.  The superscript $-\\\\\\\\kappa(i)$ just means \"model $f$ is trained without the training patterns in the same partition of the dataset as pattern $i$\".  Sometimes writing things in formal mathematical notation makes things less ambiguous, but it doesn\\'t necessarily make it any easier to understand than the text - I think this is one of those occasions.</p>\\n',\n",
       " \"<p>I have several streams of time-series data representing sensor readings that are usually closely correlated (at least in the casual usage of the term) though with different scalar multipliers applied. For example, say I have time-series A, B, and C representing historical sensor readings over time. B might be consistently around 75% of A, and C might be consistently around 50% of A. The absolute values change frequently, but the ratios between A / B / C remain fairly constant.</p>\\n\\n<p>My goal is to detect points in time when one of these variables deviates significantly from the others. For example, I want to be able to detect if B's readings drop significantly in relation to A's and C's. The number of variables being compared might be anywhere between 3 and the low hundreds.</p>\\n\\n<p>A sketch of what I'm thinking (which may be totally off-base): build a model based on historical data that allows me to predict the values of the target variable based on the values of the other variables. Use the model to predict what the value of the target variable should be for a certain time period, and compare to the actual value for that time period. If the delta between prediction based on historical data and actual reading is large enough, mark the time range as anomalous.</p>\\n\\n<p>While that sounds reasonable to me, I have no idea how to go about:\\n1 - Building the predictive model from historical data, or\\n2 - Comparing the predicted and actual values and applying a test of some kind to determine whether the deviation is significant</p>\\n\\n<p>Can anyone recommend approaches for either of these issues? </p>\\n\",\n",
       " '<p>What \"fat-tailed distributions\" $p(x)$, symmetric about zero, have the property\\n$$\\\\\\\\newcommand{\\\\\\\\e}{\\\\\\\\mathbb{E}}\\\\\\\\newcommand{\\\\\\\\rd}{\\\\\\\\mathrm{d}}\\\\\\\\n\\\\\\\\e e^X = \\\\\\\\int_{-\\\\\\\\infty}^{\\\\\\\\infty} e^x p(x) \\\\\\\\rd x &lt; \\\\\\\\infty \\\\\\\\&gt; ?\\\\\\\\n$$</p>\\n\\n<p><strong>Context</strong></p>\\n\\n<p>I\\'m attempting to price financial options for $X$ without using the Black&ndash;Scholes formula. It is usually easier to work with the log-price $Y = \\\\\\\\log(X)$ and often it is assumed that $Y$ is normally distributed.</p>\\n\\n<p>Empirical observations (eg, the \"volatility smile\") suggest that $Y$ isn\\'t normal; the normal distribution decreases too rapidly away from 0. Thus, we need a fat-tailed distribution. </p>\\n\\n<p>The value of a call option increases exponentially as $Y$ increases linearly. Therefore, $p(x)$ must decline fast enough that $\\\\\\\\e e^X &lt; \\\\\\\\infty$. In other words, the distribution must decline slower than the normal distribution, but still fast enough that $\\\\\\\\e e^X$ converges. </p>\\n\\n<p>I tried the Cauchy and Student\\'s $t$ distributions, but $\\\\\\\\e e^X$ diverges for both, regardless of parameters. </p>\\n\\n<p>I also realize I can create arbitrary distributions meeting my conditions (though I\\'m not exactly sure how), but I\\'m looking for a well-known (parametrized family of) distribution. </p>\\n\\n<p>Even more details (for the masochist): \\n<a href=\"https://github.com/barrycarter/bcapps/blob/master/bc-imp-vol.m\" rel=\"nofollow\">https://github.com/barrycarter/bcapps/blob/master/bc-imp-vol.m</a> </p>\\n',\n",
       " '<p>Sam,</p>\\n\\n<p>I think I understood what you are after, so let me know if I\\'ve misinterpreted anything:</p>\\n\\n<ul>\\n<li>You want a separate <code>box_plot</code> for the ratio of each pairs of columns. There are 15 ratios we are interested in...(column 6 / column 7, column 8 / column 9, etc.)</li>\\n<li>This plot should have a separate \"window\" or facet for each annotation, for which there are 23 different annotations.</li>\\n</ul>\\n\\n<p>Assuming both of those are right, I think this will give you what you are after. First, we will make the 15 new ratio columns with a for-loop and some indexing. After we make these 15 new columns, we will <code>melt</code> the data into long format for easy plotting with <code>ggplot2</code>. Since we are only interested in the columns <code>annotation</code> and the new ratio columns, we\\'ll specify those in the call to <code>melt</code>. Then it is a relatively straight forward call to <code>ggplot</code> to specify the axes and faceting variable. </p>\\n\\n<p>These plots don\\'t make much sense with 10 rows of data, but I think it will look better with your full dataset.</p>\\n\\n<pre><code>library(ggplot2)\\n\\n#EDIT: this removes the call to cbind which should improve performance.\\nfor (i in seq(6, ncol(df), by = 2)) {\\n    df[,  paste(i, i+1, sep = \"_\", collapse = \"\")] &lt;- df[, i ] / df[, i + 1 ]\\n\\n}\\n\\ndf.m &lt;- melt(df, id.vars = \"annotation\", measure.vars = 36:ncol(df))\\n#Note that we use the column name for the id.vars and the column order for\\n#the measure.vars. In the case of the latter, this is simply to save on \\n#typing.\\n\\nggplot(data = df.m, aes(x = variable, y = value)) + \\n    geom_boxplot() + \\n    facet_wrap(~ annotation) +\\n    coord_flip()\\n</code></pre>\\n',\n",
       " '<p>Your data will give partial answers by means of the Hansen-Hurwitz or <a href=\"http://www.math.umt.edu/patterson/549/Horvitz-Thompson.pdf\" rel=\"nofollow\">Horvitz-Thompson</a> estimators.</p>\\n\\n<p>The model is this: represent this individual\\'s attendance as a sequence of indicator (0/1) variables $(q_i)$, $i=1, 2, \\\\\\\\ldots$.  You randomly observe a two-element subset out of each weekly block $(q_{5k+1}, q_{5k+2}, \\\\\\\\ldots, q_{5k+5})$.  (This is a form of systematic sampling.)</p>\\n\\n<ol>\\n<li><p><strong>How often does he train</strong>?  You want to estimate the weekly mean of the $q_i$.  The statistics you gather tell you the mean observation is 0.9.  Let\\'s suppose this was collected over $w$ weeks.  Then the Horvitz-Thompson estimator of the total number of the individual\\'s visits is $\\\\\\\\sum{\\\\\\\\frac{q_i}{\\\\\\\\pi_i}}$ = ${5\\\\\\\\over2} \\\\\\\\sum{q_i}$ = ${5\\\\\\\\over2} (2 w) 0.9$ = $4.5 w$ (where $\\\\\\\\pi_i$ is the chance of observing $q_i$ and the sum is over your actual observations.)  That is, <strong>you should estimate he trains 4.5 days per week.</strong>  See the reference for how to compute the standard error of this estimate.  As an extremely good approximation you can use the usual (Binomial) formulas.</p></li>\\n<li><p><strong>Does he train randomly</strong>?  There is no way to tell.  You would need to maintain totals by day of week.</p></li>\\n</ol>\\n',\n",
       " '<p>I\\'d probably say something like this:</p>\\n\\n<p>\"Anytime we want to talk about probabilities, we\\'re really integrating a density.  In Bayesian analysis, a lot of the densities we come up with aren\\'t analytically tractable: you can only integrate them -- if you can integrate them at all -- with a great deal of suffering.  So what we do instead is simulate the random variable a lot, and then figure out probabilities from our simulated random numbers.  If we want to know the probability that X is less than 10, we count the proportion of simulated random variable results less than 10 and use that as our estimate.  That\\'s the \"Monte Carlo\" part, it\\'s an estimate of probability based off of random numbers.  With enough simulated random numbers, the estimate is very good, but it\\'s still inherently random.</p>\\n\\n<p>\"So why \"Markov Chain\"?  Because under certain technical conditions, you can generate a memoryless process (aka a Markovian one) that has the same limiting distribution as the random variable that you\\'re trying to simulate.  You can iterate any of a number of different kinds of simulation processes that generate correlated random numbers (based only on the current value of those numbers), and you\\'re guaranteed that once you pool enough of the results, you will end up with a pile of numbers that looks \"as if\" you had somehow managed to take independent samples from the complicated distribution you wanted to know about.</p>\\n\\n<p>\"So for example, if I want to estimate the probability that a standard normal random variable was less than 0.5, I could generate ten thousand independent realizations from a standard normal distribution and count up the number less than 0.5; say I got 6905 that were less than 10000; my estimate for P(Z&lt;0.5) would be 0.6905, which isn\\'t that far off from the actual value.  That\\'d be a Monte Carlo estimate.</p>\\n\\n<p>\"Now imagine I couldn\\'t draw independent normal random variables, instead I\\'d start at 0, and then with every step add some uniform random number between -0.5 and 0.5 to my current value, and then decide based on a particular test whether I liked that new value or not; if I liked it, I\\'d use the new value as my current one, and if not, I\\'d reject it and stick with my old value.  Because I only look at the new and current values, this is a Markov chain.  If I set up the test to decide whether or not I keep the new value correctly (it\\'d be a random walk Metropolis-Hastings, and the details get a bit complex), then even though I never generate a single normal random variable, if I do this procedure for long enough, the list of numbers I get from the procedure will be distributed like a large number of draws from something that generates normal random variables.  This would give me a Markov Chain Monte Carlo simulation for a standard normal random variable.  If I used this to estimate probabilities, that would be a MCMC estimate.\"</p>\\n',\n",
       " '<p>We have already worked in predicting chemical compound activity using \"classical\" neural networks. Now there is all this hype about deep learning. I wonder if you know cases where predictive ability has dramatically increased going from classical to deep neural networks.</p>\\n',\n",
       " '<p>There are many ways to approach this - I identified three very different questions that would significantly change your approach. All of these approaches would require some of the underlying data the risk projections are based on, and not just the published statistics for each county.</p>\\n\\n<p>Can you provide some more information about the type of question you want to answer? Also, do you have access to or are you willing to compile historical data on hurricane risk by county?</p>\\n\\n<h2>Any specific combination of counties</h2>\\n\\n<p><em>Could be helpful for general portfolio risk analysis for real estate owners, insurers</em></p>\\n\\n<p><strong>General Model of Hurricane risk relations</strong> (a lot of effort for a good one, maybe not too much effort for a basic one)</p>\\n\\n<p><strong>Historical-data-driven correlation matrix</strong> (likely too sparse)</p>\\n\\n<h2>One, or small number of specific combinations of counties</h2>\\n\\n<p><em>Could be helpful for a specific property owner or insurer localized to one region</em></p>\\n\\n<p><strong>Individualized analysis</strong> (best able to account for locally varying factors like county size and geography)</p>\\n\\n<h2>How many counties are impacted in a year / in a hurricane</h2>\\n\\n<p><em>Could be helpful for government agencies trying to build cross-county communications for coordinating emergency management</em></p>\\n\\n<p><strong>Historical data</strong> (Very easy!)</p>\\n',\n",
       " \"<p>I 'm looking to find a general formula for the following problem:</p>\\n\\n<p>The events:</p>\\n\\n<ul>\\n<li>Let each event have two possible outcomes (e.g. success and failure)</li>\\n<li>The events are independent</li>\\n</ul>\\n\\n<p>Scenario: <br></p>\\n\\n<ul>\\n<li>Let the number of trials be Y</li>\\n<li>Let the number of successes be N</li>\\n</ul>\\n\\n<p>Here's the tough part:</p>\\n\\n<p>Over Y trials, what is the probability of there being X successes in a row given that the total number of successes is N?</p>\\n\",\n",
       " \"<p><strong>Correlation:</strong> The degree or extent to which variables are linearly related is called the <em>correlation</em> among variables.</p>\\n\\n<p><strong>MSE:</strong> If $T$ be an estimator of $\\\\\\\\theta$. <em>$MSE(T)$</em> is a measure of the spread of the values of $T$ around $\\\\\\\\theta$</p>\\n\\n<p>These two are different so don't be confused </p>\\n\",\n",
       " '<p>I would start with the <a href=\"http://en.wikipedia.org/wiki/Autocorrelation\" rel=\"nofollow\">autocorrelation</a> of the +1/-1 sequence with a lag of 1. It has a range of -1 to 1, but you can convert easily transform it to 0 to 1. Here is a quick example in R:</p>\\n\\n<p>(note: <code>head(x,-1)</code> drops the last value, <code>tail(x,-1)</code> drops the first)</p>\\n\\n<pre><code>&gt; x1 &lt;- c(1,1,1,1,1,1,-1,-1,-1,-1,-1)\\n&gt; x2 &lt;- c(1,-1,1,-1,1,-1,1,-1,1,-1,1)\\n&gt; x3 &lt;- c(1,1,-1,1,1,-1,1,1,-1,1,1)\\n&gt; cor(head(x1,-1), tail(x1,-1))\\n[1] 0.8164966\\n&gt; cor(head(x2,-1), tail(x2,-1))\\n[1] -1\\n&gt; cor(head(x3,-1), tail(x3,-1))\\n[1] -0.4285714\\n</code></pre>\\n',\n",
       " '<p>I have a CSV file with 4 million edges of a directed network representing people communicating with each other (e.g. John sends a message to Mary, Mary sends a message to Ann, John sends <em>another</em> message to Mary, etc.). I would like to do two things:</p>\\n\\n<ol>\\n<li><p>Find degree, betweeness and (maybe) eigenvector centrality measures for each person.</p></li>\\n<li><p>Get a visualization of the network.</p></li>\\n</ol>\\n\\n<p>I would like to do this on the command-line on a Linux server since my laptop does not have much power. I have R installed on that server and the statnet library. I found <a href=\"http://www.cybaea.net/Blogs/Data/SNA-with-R-Loading-your-network-data.html\" rel=\"nofollow\">this 2009 post</a> of someone more competent than me trying to do the same thing and having problems with it. So I was wondering if anyone else has any pointers on how to do this, preferably taking me step by step since I only know how to load the CSV file and nothing else. </p>\\n\\n<p>Just to give you an idea, this is how my CSV file looks like:</p>\\n\\n<pre><code>$ head comments.csv\\\\\\\\n    &quot;src&quot;,&quot;dest&quot;\\\\\\\\n    &quot;6493&quot;,&quot;139&quot;\\\\\\\\n    &quot;406705&quot;,&quot;369798&quot;\\\\\\\\n$ wc -l comments.csv \\n4210369 comments.csv\\n</code></pre>\\n',\n",
       " '<p>Suppose that you want to know how many likely voters plan to vote for the incumbant in your cities race for mayor this year so you take a simple random sample of likely voters and ask them if they plan to vote for the incumbant or the challenger.  The sampling distribution tells us the relationship between the proportion in our sample and the true proportion from the entire city.  Because of the sampling distribution we can make inference based only on the information about the proportion who said \"incumbant\" in our sample and the sample size, inference like hypothesis tests or confidence intervals.  If we used the joint distribution then we would have to use the information on how each individual answered the question instead of just the summary information (which is a lot simpler).</p>\\n',\n",
       " '<p>You might want to read about Box-Cox transformation:\\n$$ y \\\\\\\\mapsto \\\\\\\\left\\\\\\\\{ \\\\\\\\begin{eqnarray} \\\\\\\\frac{y^\\\\\\\\lambda-1}{\\\\\\\\lambda (\\\\\\\\dot y)^{\\\\\\\\lambda-1}}, &amp; \\\\\\\\lambda \\\\\\\\neq 0 \\\\\\\\\\\\\\\\ \\\\\\\\dot y \\\\\\\\ln y, &amp; \\\\\\\\lambda = 0 \\\\\\\\end{eqnarray} \\\\\\\\right.\\\\\\\\n$$\\nwhere $\\\\\\\\dot y$ is the geometric mean of the data. It generalizes both the square root and the log transformation, and admits a likelihood ratio test to select the best fitting parameter. Of course you must have the data that are always positive for the power and the log to be applicable. In general, you can shift your data $y\\\\\\\\mapsto y+a$. The shift parameter $a$, however, is difficult to identify, and usually has to be used as a fixed value in practice.</p>\\n\\n<p>In <a href=\"http://stata.com/help.cgi?boxcox\" rel=\"nofollow\">Stata\\'s Box-Cox model</a>, you can specify the same or different shape parameter $\\\\\\\\lambda$ to be applied to the dependent and explanatory variables, although I suspect it will only work with AR models specified explicitly with <a href=\"http://stata.com/help.cgi?tsvarlist\" rel=\"nofollow\">lags</a>, rather than the general <a href=\"http://stata.com/help.cgi?arima\" rel=\"nofollow\">ARIMA</a> models.</p>\\n',\n",
       " \"<p>I understand the rationale that underpins representative sampling (i.e. to avoid bias so that the sample 'broadly' represents the target population). </p>\\n\\n<p>Suppose the size of the target population is 600 and the population comprises of four types of units as follows:</p>\\n\\n<ul>\\n<li>100 white units</li>\\n<li>250 black units</li>\\n<li>50 green units</li>\\n<li>200 yellow units</li>\\n</ul>\\n\\n<p><strong>Questions</strong></p>\\n\\n<ol>\\n<li>What would be a representative sample (overall and subgroup) in this case?</li>\\n<li>Is the sample in (1) appropriate for only descriptive analysis or inferential analysis or both?</li>\\n<li>If you have a representative sample, is <em>a priori</em> power analysis needed?</li>\\n</ol>\\n\",\n",
       " '<p>You can use the free statistical program R, available at <a href=\"http://www.r-project.org/\">http://www.r-project.org/</a>, to analyze your data. You state that you want to determine how your data are distributed. You guess that your data either conform to a beta or a gamma distribution. You provide no information about your data and do not speculate that it could conform to one of the <a href=\"http://en.wikipedia.org/wiki/List_of_probability_distributions\">many other probability distribution types</a>, so I\\'ll just briefly discuss the two distributions that you mention in my answer. I suppose the most logical thing to do first would be to simply plot your data on a histogram or density plot to facilitate visual inspection.</p>\\n\\n<p><strong>Example 1: Beta Distribution</strong></p>\\n\\n<p>The beta distribution is a family of continuous probability distributions defined on the interval (0, 1).</p>\\n\\n<p>You provide no example data in your question, so we will have to make some up for this example. First, let\\'s make some randomly generated dummy data that conform to a beta distribution. We can use the <code>rbeta()</code> command to do this. Type <code>help(rbeta)</code> into R to learn how to use this function.</p>\\n\\n<pre><code>## Generate 10,000 data points and save them as an object called \"temp\".\\ntemp &lt;- rbeta(10000, shape1 = 1, shape2 = 1, ncp = 0)\\n</code></pre>\\n\\n<p>Now, let\\'s double check that our data are bound by zero and one. We can use the <code>summary()</code> command to do this.</p>\\n\\n<pre><code>## Find maximum and minimum value of the data.\\nsummary(temp)\\n</code></pre>\\n\\n<p>Now, let\\'s plot the data on a histogram for visual inspection.</p>\\n\\n<pre><code>## Plot the data on a histogram.\\nhist(temp)\\n</code></pre>\\n\\n<p>You could alternatively make a density plot of the data, which for practical purposes is the same thing.</p>\\n\\n<pre><code>## Make density plot.\\nplot(density(temp))\\n</code></pre>\\n\\n<p>I suppose you can take the mean and variance of your data and use them to calculate the parameters of a beta distribution. (Before jumping to this step, you should probably do more descriptive analyses, including looking at the qqplots as suggested by Seth, above). The below function, copied directly from <a href=\"http://stats.stackexchange.com/users/4812/max\">Max</a>\\'s example on <a href=\"http://stats.stackexchange.com/questions/12232/calculating-the-parameters-of-a-beta-distribution-using-the-mean-and-variance\">this page</a>, should provide estimates of the parameters.</p>\\n\\n<pre><code>## Calculate mean and variance of the data.\\nmu &lt;- mean(temp)\\nvar &lt;- var(temp)\\n\\n## Define function to estimate parameters of a beta distribution.\\nestBetaParams &lt;- function(mu, var) {\\n  alpha &lt;- ((1 - mu) / var - 1 / mu) * mu ^ 2\\n  beta &lt;- alpha * (1 / mu - 1)\\n  return(params = list(alpha = alpha, beta = beta))\\n}\\n\\n## Apply function to your data.\\nestBetaParams(mu, var)\\n</code></pre>\\n\\n<p><strong>Example 2: Gamma Distribution</strong></p>\\n\\n<p>The gamma distribution is a two-parameter family of continuous probability distributions.</p>\\n\\n<p>First, let\\'s make some randomly generated dummy data that conform to a gamma distribution. We can use the <code>rgamma()</code> command to do this.</p>\\n\\n<pre><code>## Generate 10,000 data points and save them as an object called \"temp\".\\ntemp &lt;- rgamma(10000, shape = 1, rate = 10, scale = 1/rate)\\n\\n## Plot the data on a histogram for visual inspection.\\nhist(temp)\\n</code></pre>\\n\\n<p>To estimate the gamma distribution parameters, you can use the <code>MASS</code> library, explained at this <a href=\"http://tolstoy.newcastle.edu.au/R/help/05/01/10043.html\">website</a>. After installing and loading the MASS library into R, type <code>help(fitdistr)</code> for more information.</p>\\n\\n<p><strong>Suggested Further Reading</strong></p>\\n\\n<p>The following document explains how to do everything in much more detail. Note that I am not a statistician so perhaps you should take my advice with a grain of salt. However, I think I am more or less leading you in the correct direction. The below document will show you how to make qqplots of your data against those densities, as suggested by Seth above.</p>\\n\\n<ol>\\n<li><a href=\"http://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf\">Ricci, V. Fitting distributions with R. February 2005.</a></li>\\n</ol>\\n',\n",
       " '<p>I really like the method used in the <code>caret</code> package: recursive feature elimination.   You can read more about it in the <a href=\"http://cran.r-project.org/web/packages/caret/vignettes/caretSelection.pdf\" rel=\"nofollow\">vignette</a>, but here\\'s the basic process:\\n<img src=\"http://i.stack.imgur.com/dXmNi.png\" alt=\"Variable Selection\"></p>\\n\\n<p>The basic idea is to use a criteria (such as t statistics) to eliminate unimportant variables and see how that improves the predictive accuracy of the model.  You wrap the entire thing in a resampling loop, such as cross-validation.  Here is an example, using a linear model to rank variables in a manner similar to what you\\'ve described:</p>\\n\\n<pre><code>#Setup\\nset.seed(1)\\np1 &lt;- rnorm(50)\\np2 &lt;- rnorm(50)\\np3 &lt;- rnorm(50)\\np4 &lt;- rnorm(50)\\np5 &lt;- rnorm(50)\\ny &lt;- 4*rnorm(50)+p1+p2-p5\\n\\n#Select Variables\\nrequire(caret)\\nX &lt;- data.frame(p1,p2,p3,p4,p5)\\nRFE &lt;- rfe(X,y, sizes = seq(1,5), rfeControl = rfeControl(\\n                    functions = lmFuncs,\\n                    method = \"repeatedcv\")\\n                )\\nRFE\\nplot(RFE)\\n\\n#Fit linear model and compare\\nfmla &lt;- as.formula(paste(\"y ~ \", paste(RFE$optVariables, collapse= \"+\")))\\nfullmodel &lt;- lm(y~p1+p2+p3+p4+p5,data.frame(y,p1,p2,p3,p4,p5))\\nreducedmodel &lt;- lm(fmla,data.frame(y,p1,p2,p3,p4,p5))\\nsummary(fullmodel)\\nsummary(reducedmodel)\\n</code></pre>\\n\\n<p>In this example, the algorythm detects that there are 3 \"important\" variables, but it only gets 2 of them.</p>\\n',\n",
       " '<p>I\\'m currently studying for an Experimental Design exam. I feel like I\\'m missing something quite fundamental...</p>\\n\\n<p>I\\'ve attached a minimal example to illustrate my confusion at <a href=\"https://gist.github.com/2942913\" rel=\"nofollow\">https://gist.github.com/2942913</a></p>\\n\\n<p>We were asked to compare the treatment and sum dummy coding systems. \\nIt was pointed out that with the treatment system the values in the \"contrast matrix\" [1] do not correspond with the observations in the data matrix (or the design matrix). My <code>cbind()</code> command in the example illustrates this. [2]</p>\\n\\n<p>Using the sum coding system, the contrasts are negative for one value of the factor and positive for the other (and they also don\\'t change when leaving out the interactions). This is supposed to result in \"interpretable parameters\" (as opposed to the treatment coding system?). My question is how I should interpret this last statement, i.e. why does having a correspondence between the contrast and data matrix with the sum coding result in interpretable parameters? Or: is there something I urgently need to revise?</p>\\n\\n<hr>\\n\\n<p>[1] $( X\\'  X)^{-1}  X\\' )$</p>\\n\\n<p>[2] Edit (replacing original footnote): There was a \\'bug\\' in the code I provided caused by the following. I used <code>cbind()</code> with a factor (essentially only for presentation purposes; to show the vectors side by side). This resulted in an implicit conversion of the numerical codes in the factor to numbers.</p>\\n',\n",
       " 'Refers to \"lumping together\" potentially inhomogeneous groups of data. The laws of total expectation and variance (http://en.wikipedia.org/wiki/Law_of_total_expectation and http://en.wikipedia.org/wiki/Law_of_total_variance) can be thought of as providing a way to calculate the mean and variance of an aggregated data set, if the variable being conditioned on ($Y$ in the wikipedia articles) is the grouping variable being aggregated over. ',\n",
       " '<p>Google is your friend\\nI have found<br>\\n<a href=\"http://users.ices.utexas.edu/~terejanu/files/tutorialUKF.pdf\" rel=\"nofollow\">http://users.ices.utexas.edu/~terejanu/files/tutorialUKF.pdf</a>\\nand<br>\\n<a href=\"http://signal.hut.fi/kurssit/s884221/ukf.pdf\" rel=\"nofollow\">http://signal.hut.fi/kurssit/s884221/ukf.pdf</a></p>\\n\\n<p>clear enough. </p>\\n\\n<p>Modify your question if you have something more precise to ask. What don\\'t you understand in the UKF?</p>\\n',\n",
       " '<p>You want the upper <em>q</em>-quantile point of a Binomial distribution $\\\\\\\\text{B}(K,p)$, i.e. $K$ trials each with success probability $p$. As $K$ is large and $p$ is small, <a href=\"http://en.wikipedia.org/wiki/Binomial_distribution#Poisson_approximation\" rel=\"nofollow\">you can approximate the Binomial distribution by a Poisson distribution</a> $\\\\\\\\text{Pois}(\\\\\\\\lambda)$with parameter $\\\\\\\\lambda = E = K p$.  </p>\\n\\n<p>For example in R, with $q=10^{-5}, E = 5$:</p>\\n\\n<pre><code>&gt; qpois(1e-5, 5, lower.tail=FALSE)\\n[1] 17\\n</code></pre>\\n\\n<p>The device should self-destruct if it fails 17 or more times. (17 or more, rather than more than 17, due to the precise way the quantile function is defined for a distribution which is only defined on the integers).</p>\\n\\n<p>To check if this is a good approximation, you can compare it with the exact result from the binomial distribution, e.g. for $K=10^4$ and $p=5\\\\\\\\times10^{-4}$ :  </p>\\n\\n<pre><code>&gt; qbinom(1e-5, 1e4, 5e-4, lower.tail=FALSE)\\n[1] 17\\n</code></pre>\\n',\n",
       " '<p><strong>Additional conditions are needed.</strong> (<em>A near-proof of this fact is that many incredibly smart individuals have been thinking deeply about these issues for over 100 years. It is highly unlikely that something like this would have escaped all of them.</em>)</p>\\n\\n<p>First of all, note that the formula for $V$ that you give is part of the <strong>conclusion</strong> of the associated central limit theorem. See, for example, <strong>Theorem 7.6</strong> on pages 416&ndash;417 of R. Durrett, <em>Probability: Theory and Examples</em>, 3rd. ed., which based on your link, you appear to have access to.</p>\\n\\n<p>At any rate, here is a simple counterexample to your claim.</p>\\n\\n<blockquote>\\n  <p>Let $X_0$ equal $+1$ with probability $1/2$ and $-1$ with probability $1/2$. Define $X_n = (-1)^n X_0$. Then $\\\\\\\\{X_n\\\\\\\\}$ is a stationary ergodic process with mean 0 and variance 1, but the Central Limit Theorem fails.</p>\\n</blockquote>\\n\\n<p>The properties of stationarity and ergodicity should be pretty easy to see as we can construct this process by defining a function over the states of a two-state Markov chain with stationary probability measure $\\\\\\\\pi(x) = 1/2$ for $x \\\\\\\\in \\\\\\\\{0,1\\\\\\\\}$. </p>\\n\\n<p>Observe that this process yields a sequence of the form $-X_0, X_0, -X_0, \\\\\\\\ldots$ and so </p>\\n\\n<ol>\\n<li>Even without appealing to any notions about ergodicity, it is easy to see that $\\\\\\\\newcommand{\\\\\\\\e}{\\\\\\\\mathbb{E}}\\\\\\\\bar{X}_n \\\\\\\\to \\\\\\\\e X_0 = 0$ almost surely, and,</li>\\n<li>$\\\\\\\\newcommand{\\\\\\\\Var}{\\\\\\\\mathbb{V}\\\\\\\\mathrm{ar}}\\\\\\\\Var(S_n) = 0$ if $n$ is even and $1$ if $n$ is odd. </li>\\n</ol>\\n\\n<p>This already is enough to conclude that there is <strong>no way</strong> that any rescaling of $S_n$ can make it converge in distribution to a normal random variable. In fact, for <strong>every</strong> function $f$ such that $f(n) \\\\\\\\to \\\\\\\\infty$, $S_n / f(n) \\\\\\\\to 0$ almost surely <em>no matter how slowly $f$ diverges</em>.</p>\\n\\n<p>Note also that this example should make it clear that the formula for $V$ is a conclusion of the theorem. Indeed, for the example above,\\n$$\\\\\\\\nV_n = 1 + 2 \\\\\\\\sum_{i = 1}^n \\\\\\\\e X_0 X_i = \\\\\\\\left\\\\\\\\{\\\\\\\\n\\\\\\\\begin{array}{rl}\\\\\\\\n-1, &amp; n \\\\\\\\text{ odd}, \\\\\\\\\\\\\\\\\\\\\\\\n1, &amp; n \\\\\\\\text{ even},\\\\\\\\n\\\\\\\\end{array}\\\\\\\\n\\\\\\\\right.\\\\\\\\n$$\\nwhich, of course, (a) makes no sense as a variance, (b) does not have a limit, and (c) is not asymptotically equivalent to $\\\\\\\\Var(S_n)$. (<strong>NB</strong>: I use a slightly different form for $V_n$ than you do where mine matches that given in Durrett.)</p>\\n',\n",
       " '<p>Is there an easy way to apply the trend line formula from a chart to any given X value in Excel?</p>\\n\\n<p>For example, I want to get the Y value for a given X = $2,006.00. I\\'ve already taken the formula and retyped it out be:</p>\\n\\n<pre><code>=-0.000000000008*X^3 - 0.00000001*X^2 + 0.0003*X - 0.0029\\n</code></pre>\\n\\n<p>I am continually making adjustments to the trend line by adding more data, and don\\'t want to retype out the formula every time.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/tgq8c.jpg\" alt=\"enter image description here\"></p>\\n',\n",
       " \"<p>A very simple option is dividing each number in your data by the largest number in your data.  If you have many small numbers and a few very large ones, this might not convey the information well.  But it's relatively easy; if you think meaningful information is lost when you graph the data like this, you could try one of the more sophisticated techniques that others have suggested.</p>\\n\",\n",
       " '<p>Instead of just finding the median, there is an algorithm that directly maintains an estimated histogram: \"<a href=\"http://www.cs.wustl.edu/~jain/papers/ftp/psqr.pdf\" rel=\"nofollow\">the P-Square Algorithm</a> for Dynamic Calculation  of Quantiles  and Histograms  Without  Storing Observations\".  This will probably be much more efficient that repeated binning for every quantile you want.</p>\\n',\n",
       " '<p>I have a chi squared of 0.1185 with 1 degree of freedom.  How can i determine the P-value? </p>\\n',\n",
       " '<p>I am working on a few algorithms where I have a list of $N$ samples. Currently I have plotted these into a histogram and have a view of how uniform the values are distributed within an interval, which is quite good as a visualization, although I need a comparable value of how uniform the dataset is, in order to measure how robust it is compared to my other algorithms.</p>\\n\\n<p>I have been looking at chi-squared test, but could not figure out how it would become helpful in my usecase?</p>\\n\\n<p>Sample from dataset:</p>\\n\\n<pre><code>8725\\n462\\n1492\\n972\\n9941\\n8235\\n8220\\n6949\\n1252\\n</code></pre>\\n\\n<p>Code for importing data and applying chi-squared in <code>R</code>:</p>\\n\\n<pre><code>mydata = read.csv2(\"/opt/doc/stat/uniform_test_1.csv\")\\nx &lt;- sapply(mydata, as.numeric)\\nchisq.test(x)\\n</code></pre>\\n\\n<p>Result: <code>X-squared = 1664769844, df = 999998, p-value &lt; 2.2e-16</code></p>\\n',\n",
       " '<p>It appears that \"equitable\" means that the expected values of the team points, conditional on $n$, need to be equal regardless of team size.  This answer explores some consequences of this interpretation.</p>\\n\\n<hr>\\n\\n<p>For a team of size $n$ that is awarded $x(n)$ points if it \"wins\" and zero points otherwise, the expectation reduces simply to the probability of winning multiplied by $x(n)$.  With the independent \"fair coin\" model of choosing colors, wherein the coin has chance $p$ of being black and (therefore) $n$ black values are found with probability $p\\\\\\\\times p \\\\\\\\times \\\\\\\\cdots \\\\\\\\times p = p^n$, this expectation equals $x(n)p^n$.  The condition $x(2)=10$ determines <strong>the unique solution</strong></p>\\n\\n<p>$$x(n) = 10 / p^n.$$</p>\\n\\n<p>Notice that this solution has nothing to do with $n$ being random or not.  However, the distribution of team points at the end of the \"competition\" will be strongly determined by the distribution of $n$. To explore this, <strong>we can simulate a competition.</strong>  (The code uses <code>R</code>.)</p>\\n\\n<p>First, we specify the number of teams <code>n.teams</code>, the number of trials during a competition <code>n.trials</code> (which is fixed and equal for all teams in this simulation), the parameters for a random distribution of team sizes, and the chance of black.  In this example, the distribution is Poisson (offset by $1$ to assure positive team sizes) with parameter <code>lambda</code>.  We also start the random number generator at a reproducible point.</p>\\n\\n<pre><code>lambda &lt;- 3\\nn.trials &lt;- 100\\nn.teams &lt;- 10000\\np.black &lt;- 1/2\\nset.seed(17)\\n</code></pre>\\n\\n<p>Create the teams, assign hat colors to their members for each trial, count up the total number of wins, and award the points accordingly (using the solution $x(n)$):</p>\\n\\n<pre><code>n &lt;- 1 + rpois(n.teams, lambda)\\nn.black &lt;- matrix(rbinom(n.trials * n.teams, size=n, p=p.black), nrow=n.teams) == n\\nwins &lt;- apply(n.black, 1, sum)\\npoints &lt;- wins * sapply(n, function(n) 10 / p.black^n)\\n</code></pre>\\n\\n<p>Here is a plot of points <em>versus</em> team size in this simulation involving 100 trials for each of 10,000 independent teams (averaging <code>lambda+1</code> = $4$ people per team) when <code>p.black</code> is $1/2$.  To make all the data visible, dots on the plot are randomly offset by up to 2.5% of the width and height of the plot.</p>\\n\\n<pre><code>jitter &lt;- 0.025\\nxr &lt;- jitter*(range(n) - mean(n))\\nyr &lt;- jitter*(range(points) - mean(points))\\nplot(n + runif(n.teams, xr[1], xr[2]), points + runif(n.teams, yr[1], yr[2]), \\n     xlab=\"Team size\", ylab=\"Total points\", \\n     main=sprintf(\"P = %0.2g, lambda = %0.2g\", p.black, lambda))\\nabline(coef(lm(points ~ n)), lwd=2, col=\"Blue\", lty=2)\\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/4MH7W.png\" alt=\"Plot\"></p>\\n\\n<p>The blue line is the least squares fit to the data.  Its horizontal trend attests to the \"egalitarian\" nature of the solution $x(n)$: the expected number of points is the same regardless of team size.  (In fact, inspecting the least squares fit with <code>summary(lm(points ~ n))</code> reveals it has an insignificant--and relatively small--slope.)</p>\\n\\n<p>Please notice (a) the extremely skewed distribution of points and (b) the strong tendency for larger teams either to get no points at all or so many points that a single win assures a high standing at the end of the competition. The uniqueness of $x(n)$ shows that  <em>this behavior is forced on us by the requirements of the problem</em>.  It nicely illustrates the tradeoff between risk (exhibited as the large ranges of results for larger teams) and reward (apparent as the numbers of points).</p>\\n\\n<p>It is instructive to re-run the simulation with different values of the input. When <code>lambda</code> grows, for instance, most teams are relatively large, so only a few--and not necessarily the largest--account for almost all the points.  When <code>p.black</code> grows, the skewness of the results decreases.  For instance, here are results of the same simulation with <code>p.black</code> changed from $1/2$ to $2/3$:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/SDTRQ.png\" alt=\"Plot for p=2/3\"></p>\\n',\n",
       " '<p>I have 3 points to say. First, what is \"good\" documentation for you? There is a lot of pages on DA and downloadable books on multivariate statistics (where DA is discussed) in the Internet. Some texts are superficial and easy, some are more sophisticated.</p>\\n\\n<p>Second, because you have just 2 groups your DA will be virtually equivalent to the <strong>multiple linear regression</strong> with \"Groups\" dependent variable and your 9 variables as independent variables. The results (coefficients) after DA and the regression will be proportional between the two analyses. So, if you know linear regression that will suffice to understand the results even if you don\\'t know DA.</p>\\n\\n<p>Third, whenever there is just two groups it is worth considering to prefer <strong>binary logistic regression</strong> to DA. The logistic regression is less hard-to-plese in regard to assumptions than DA is. Just to mention some: it doesn\\'t require multivariate normal distribution; it doesn\\'t require equality of variances-covariances between the groups; it is less sensitive to disproportion of groups sizes as well as to outliers. </p>\\n',\n",
       " '<p>This is a generic question. Let me put forth an example scenario. Say, I had 2 techniques for allocating my daily budget within 4 stocks. Upon allocation, I get the data about the stock\\'s performance the next day. (Am lazy, and go to a movie after allocation).</p>\\n\\n<p>Problem 1:I have to evaluate or come up with an experimental design to test the two techniques in this sequential experiment over a given number of days. </p>\\n\\n<p>Problem 2 with caveat: If I split my budget into half and have \"one\" of the two techniques allocate it amongst the four stocks in the same quantities in two parallel experiments at the same time-I find the following issue on the second day. Though everything from the budget to the allocation was the same. Both the experiments gave different performances -the next day- due to the randomness in the system. Under this situation- where performances are different even under \"one\" technique- How would I evaluate or design an experiment for  comparing two techniques over a given number of days?</p>\\n\\n<p>Problem 3: If instead of getting the performances the next day- I get minute by minute or hourly performances, and would like to evaluate the two technqiues-what would be your line of thought? </p>\\n',\n",
       " '<p>I am working on a small project with one time series which measures the customer visit data (daily). My covariates are a continuous variable <code>Day</code> to measure how many days have been elapsed since the first day of data collection, and some dummy variables, such as whether that day is Christmas, and which day of the week it is, etc. </p>\\n\\n<p>Part of my data looks like:</p>\\n\\n<pre><code>Date    Customer_Visit  Weekday Christmas       Day\\n11/28/11        2535       2        0            1   \\n11/29/11        3292       3        0            2   \\n11/30/11        4103       4        0            3   \\n12/1/11         4541       5        0            4   \\n12/2/11         6342       6        0            5  \\n12/3/11         7205       7        0            6   \\n12/4/11         3872       1        0            7   \\n12/5/11         3270       2        0            8   \\n12/6/11         3681       3        0            9   \\n</code></pre>\\n\\n<p>My plan is to use ARIMAX model to fit the data. This can be done in R, with the function <code>auto.arima()</code>. I understand that I have to put my covariates into the <code>xreg</code> argument, but my code for this part always returns an error. </p>\\n\\n<p>Here is my code:</p>\\n\\n<pre><code>xreg     &lt;- c(as.factor(modelfitsample$Christmas), as.factor(modelfitsample$Weekday), \\n              modelfitsample$Day)\\nmodArima &lt;- auto.arima(ts(modelfitsample$Customer_Visit, freq=7), allowdrift=FALSE, \\n                       xreg=xreg)\\n</code></pre>\\n\\n<p>The error message returned by R is:</p>\\n\\n<pre><code>Error in model.frame.default(formula = x ~ xreg, drop.unused.levels = TRUE) \\n :variable lengths differ (found for \\'xreg\\')\\n</code></pre>\\n\\n<p>I learned a lot from <a href=\"http://stats.stackexchange.com/questions/18375/how-to-fit-an-arimax-model-with-r\">How to fit an ARIMAX-model with R?</a> But I am still not very clear how to set up the covariates or dummies in the <code>xreg</code> argument in <code>auto.arima()</code> function. </p>\\n',\n",
       " '<p><strong>Background</strong>\\nI have a time series of structural loads, which are measured forces on a moored ocean buoy, and I need to obtain the return period value so that the structure can be designed to withstand the max load expected in a storm. The return period is estimated from the measured time series in the following way: First, I choose a threshold and identify the peaks above this threshold. Extreme events are expected to have a distribution known as the Generalized Pareto Distribution (GPD), so I fit the peaks to that distribution. Then the GPD fit is inverted to give the load associated with the probability of a chosen time period (3 hours). This load is called the return period value. I am using a program called <a href=\"http://www.maths.lth.se/matstat/wafo/\" rel=\"nofollow\">WAFO</a>, which takes extreme value analysis routines from S-Plus, to do this analysis.</p>\\n\\n<p><strong>Problem</strong>\\nThe problem is how to assess the quality of the return period estimate. WAFO produces quantile-quantile plots that show how well the peaks match the GPD, a p-value to check the quality of the fit, or confidence bounds on the return period estimate. But these 3 diagnostics seem to conflict. Sometimes the q-q plot shows that many peaks don\\'t fit the GPD (bad), but the p-value for the fit is high (good) and the confidence bounds are narrow (good). Sometimes the confidence bounds are very wide (bad) but the q-q plot shows the peaks lining up with the GPD (good). My gut sense is that the quality might be poor because we only measured data for a limited duration and there is likely measurement noise. </p>\\n\\n<p><strong>Question</strong>: How can I tell if the return period estimate is good vs bad?</p>\\n',\n",
       " '<p>Check out the following links. I\\'m not sure what exactly are you looking for.</p>\\n\\n<p><a href=\"http://videolectures.net/mlss08au_freitas_asm/\">Monte Carlo Simulation for Statistical Inference</a></p>\\n\\n<p><a href=\"http://videolectures.net/mlss08au_smola_ksvm/\">Kernel methods and Support Vector Machines</a></p>\\n\\n<p><a href=\"http://videolectures.net/epsrcws08_campbell_isvm/\">Introduction to Support Vector Machines</a></p>\\n\\n<p><a href=\"http://academicearth.org/lectures/monte-carlo-simulations-application-to-lattice-models\">Monte Carlo Simulations</a></p>\\n\\n<p><a href=\"http://freescienceonline.blogspot.com/2009_09_01_archive.html\">Free Science and Video Lectures Online!</a></p>\\n\\n<p><a href=\"http://videolectures.net/Top/Computer_Science/Machine_Learning/\">Video lectures on Machine Learning</a></p>\\n',\n",
       " \"<p>I'm currently working on a quasi-experimental research paper. I only have a sample size of 15 due to low population within the chosen area and that only 15 fit my criteria. Is 15 the minimum sample size to compute for t-test and F-test? If so, where can I get an article or book to support this small sample size?</p>\\n\\n<p>This paper was already defended last Monday and one of the panel asked to have a supporting reference because my sample size is too low.   He said it should've been at least 40 respondents.  </p>\\n\",\n",
       " '<p>This example was taken from Mathematical Statistics : A Unified Introduction (ISBN 9780387227696), page 58, under the section \\'The Principle of Least Squares\\'. I think my problem has more to do with algebra but since this is a statistics book, I thought I should post here...</p>\\n\\n<p>Anyway, I need to estimate k base on this equation:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/JQM6B.png\" alt=\"alt text\"></p>\\n\\n<p>The goal is to reach here:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/nqyzM.png\" alt=\"alt text\"></p>\\n\\n<p>I managed to follow the example until I got stuck here:\\n <img src=\"http://i.stack.imgur.com/1Xs42.png\" alt=\"alt text\"></p>\\n\\n<p>The explanation to reach to the goal from above is described as follows:\\n <img src=\"http://i.stack.imgur.com/j2UyX.png\" alt=\"alt text\"></p>\\n\\n<p>I know how to get k but I totally don\\'t understand how the \\'middle term\\' was eliminated. Please help. I will provide more details if needed. </p>\\n',\n",
       " '<p>anomaly detection requires the construction of an equation which describes the expectation. Intervention Detection is available in both a non-causal and causal setting . If one has a predictor series like price then things can get a little complicated. Other responses here don\\'t seem to take into account assignable cause attributable to user specified predictor series like price and thus might be flawed. Quantity sold may well depend on price , perhaps previous prices and perhaps quantity sold in the past. The basis for the anomaly detection ( pulses,seasonal pulses, level shifts and local time trends ) is found in <a href=\"http://www.unc.edu/~jbhill/tsay.pdf\" rel=\"nofollow\">http://www.unc.edu/~jbhill/tsay.pdf</a> . </p>\\n',\n",
       " '<p>How can I calculate the confidence interval of a mean in a non-normally distributed sample?</p>\\n\\n<p>I understand bootstrap methods are commonly used here, but I am open to other options. While I am looking for a non-parametric option, if someone can convince me that a parametric solution is valid that would be fine. The sample size is > 400.  </p>\\n\\n<p>If anyone could give a sample in R it would be much appreciated.</p>\\n',\n",
       " \"<p>I'm investigating the effect of a continuous variable A on a measurement variable M stratified by another factor variable C in an observational dataset. </p>\\n\\n<p>Due to heteroscedasticity I decided to use a bootstrapped regression analysis. However looking at the data, the background set of variables are not evenly distributed if I dichotomise A (present or not). I've just finished running another analysis where I do the same analysis after having matched the dataset for confounders (using CEM in R).</p>\\n\\n<p>Now the problem is which analysis to trust: the bootstrapped regression approach on the entire dataset or the bootstrapped version of the matched data? Under one of the factors in C the results diverge.</p>\\n\\n<p>Any ideas how this can be analyzed?</p>\\n\",\n",
       " \"<p>Okay this is my problem: </p>\\n\\n<ul>\\n<li>I have 12 participants.</li>\\n<li>Each participant spent 3 nights in my lab doing a reaction time task at four timepoints during the night(12, 1, 2 and 3 oclock), with one week between each of those nights.</li>\\n<li>Each night, each participants was exposed to one of three experimental conditions, such that each participant completed each condition at the end of the three weeks, but with the order of conditions fully balanced over subjects.</li>\\n</ul>\\n\\n<p>So in the end I have for each participant, for each condition, four mean reaction times.</p>\\n\\n<p>I wanted to use a linear mixed model with condition and timepoint during the night, and the interaction of condition*timepoint as fixed effects(since I expect reaction time to decrease less in one of the three conditions during the night). As a random effect I wanted to include subject, but I'm not confident of how I would be doing this in SPSS and whether a mixed model is the right way to go. </p>\\n\\n<p>Can someone give me a few hints on how to proceed? I'd really appreciate it!</p>\\n\\n<p>Thanks in advance</p>\\n\",\n",
       " '<p>I tried to specify a partial proportional odds regression in STATA using the <a href=\"http://www.stata-journal.com/article.html?article=st0097\" rel=\"nofollow\"><code>gologit2</code> command.</a> </p>\\n\\n<p>However, <code>gologit2</code> runs <a href=\"https://www3.nd.edu/~rwilliam/gologit2/tsfaq.html\" rel=\"nofollow\">extraordinarily slow</a> in my dataset (108k observations of 9 vars). For example,</p>\\n\\n<pre><code>ologit PfSt age if bund == 1\\n</code></pre>\\n\\n<p>runs in less than 1 sec, while</p>\\n\\n<pre><code>gologit2 PfSt age if bund == 1, pl\\n</code></pre>\\n\\n<p>which is <a href=\"http://fmwww.bc.edu/repec/bocode/g/gologit2.html\" rel=\"nofollow\">essentially the same</a>, as the <code>pl</code> option enforces parallel lines in all vars, i.e., the <code>ologit</code> specification, takes about a minute on my computer.</p>\\n\\n<p>Calculating the fully specified model is possible with <code>ologit</code> and <code>mlogit</code> (~ 1 min), but not with <code>gologit2</code>, even if I specify for which variables parallel lines should be assumed. \\nThe regression command is</p>\\n\\n<pre><code>ologit PfSt age age_sq gender i.bundesland tt tt2 [fw=size], cluster(bundesland)\\n</code></pre>\\n\\n<p>where <code>PfSt</code> ranges from 0 to 7 and <code>tt</code> capture time trends (<code>tt = year − 1996</code> and <code>tt2 = -1/tt +1</code>, a “phasing out” trend)</p>\\n\\n<p>Now, considering that relaxing the parallel lines assumption for some variables would be identical to adding interaction terms to my model specification, I could change my regression command to</p>\\n\\n<p><code>ologit PfSt age* gender i.bundesland#i.PfSt tt*, cluster(bundesland)</code> would yield me different coefficients per level of <code>PfSt</code> for the one variable for which I would relax the parallel lines assumption. <strong>Is that correct?</strong></p>\\n\\n<p><em>However</em>, the estimation process <strong>fails to converge</strong>, as now (1200th iteration) negative log pseudoelikelihood ratios rise (i.e., drop in absolute values, now &lt;&lt;0.0001), but they are <code>(non concave)</code>. I wonder <strong>why is this the case?</strong>, as both <code>ologit</code> and <code>mlogit</code> did produce results after a few iterations.</p>\\n',\n",
       " '<p><strong>It\\'s a fine method in theory.</strong>  To see why, we need to check two things.</p>\\n\\n<ol>\\n<li><p>Are all individuals selected with equal chances?  Yes, because the distributions of floats assigned to each individual are identical (they are uniform in $[0,1)$).</p></li>\\n<li><p>Are the selections independent?  Yes, because the floats assigned to the individuals are independent (presumably: that\\'s part of what it means to be a \"high quality\" random number generator).</p></li>\\n</ol>\\n\\n<p>However, <strong>this method tends to be inefficient</strong> both in terms of computation time and memory resources.  It takes $O(n\\\\\\\\log(n))$ time and $O(n)$ memory for selecting from a population of $n$.  Both can often be improved, sometimes greatly.</p>\\n\\n<p>A general-purpose algorithm starts by considering how much of the population you need to sample.  If it\\'s more than half the population, <em>then identify the individuals <strong>not</strong> in the sample</em> and select the rest (at a cost of $O(n)$ time).  This leaves us to identify no more than half the population, say $k$ out of $n$ individuals (with $2k \\\\\\\\le n$).  Let their identifiers be in an array <code>population[0..n-1]</code>:</p>\\n\\n<pre><code>i = 0\\nselection = new set\\nwhile (i &lt; k) {\\n    x = random float in [0,1)\\n    j = int(x * (n-i))\\n    adjoin population[j] to selection\\n    population[j] = population[n-1-i]\\n    i++\\n}\\nreturn selection\\n</code></pre>\\n\\n<p>The key step--copying the last individual at <code>population[n-i-i]</code> into the space vacated by the recently selected individual, <code>population[j]</code>--doesn\\'t actually require the entire <code>population[]</code> array to be in RAM: you can do it with a dictionary of $k$ pointers instead.  This makes the computation time $O(k \\\\\\\\log(k))$ rather than $O(k)$ but reduces the storage requirements from $O(n)$ to $O(k)$, which can be substantial for small selections from huge populations stored offline.</p>\\n\\n<p>The proof that this algorithm works is inductive.  Obviously it works for $n=1$.  For general $n\\\\\\\\gt 1$, and assuming the <code>random float</code> procedure is a good one, then at the first step (a) each individual is chosen with equal probability and (b) that choice is independent of the next step, which selects $k-1$ individuals from a population of $n-1$.  Because this (inductively) is assumed to be correct, we are done.</p>\\n',\n",
       " \"<p>We had a similar problem when I worked in the medical device industry. We wanted to determine the reliability of our product in my case a pacemaker.  Physicians were encouraged to report failures that are detected when they explant the device.  Most failure are battery depletions but other modes of failure occur.  We used the Kaplan-Meier estimate of time to failure as our performance measure perhaps focussing on its value at 5 years post implant.  Reports are coming in continuously.  So we don't have the problem of picking arbitrary intervals.  This is a different approach to the same problem but it does have valid statistical methods for characterizing the failure time distribution. A parametric approach which would involve say fitting a Weibull distribution to the reporting data would provide you with a function that describes how the failure rate increases or decreases with time given the estimated parameters.</p>\\n\\n<p>But the real underlying problem we had and you might as well is underreporting.  Although the physicians are supposed to report the failures back to the manufacturer or better yet return the explanted device to the manufacturers so that the manufacturer's engineers can diagnose the failure they don't always to it.  Underreporting rates were thought to be as high as 30%.  You can have a very large bias (estimating that your failure rate is lower than it is in reality at any given time since implant).</p>\\n\\n<p>The FDA had a plan for postmarket surveillance that would adjust for this bias based on independent detailed tracking of a random sample of the implants. I published a paper in the DIA Journal that showed that the adjustment depended heavily on some assumptions and overcorrection was possible.  However in principle a statistical methodology for this would be to fit a parametric survival curve and adjust the parameter estimates based on random sampling the devices.</p>\\n\",\n",
       " '<p>I think one reason it is so hard to answer this is that R is so powerful and flexible that a real introduction to R programming goes well beyond what is normally needed in an introduction to statistics.  The books that teach statistics using MiniTab, JMP or SPSS are doing relatively straightforward things with the software that barely scratch the surface of what R is capable of when it comes to data manipulation, simulations, custom-built functions, etc.</p>\\n\\n<p>Having said that, I think that Wilcox\\'s <a href=\"http://rads.stackoverflow.com/amzn/click/1439834563\"><em>Modern Statistics for the Social and Behavioral Sciences: A Practical Introduction</em></a> (2012) is a brilliant new book.  It assumes no statistical knowledge and takes you from scratch right through to a big range of modern robust techniques; and assumes not much more R knowledge than the ability to open it up and load a dataset.    It covers many of the classical techniques too including ANOVA (mentioned in the OP).</p>\\n\\n<p>I would see this book as the equivalent of the books that introduce stats and a stats package like SPSS at the same time.  However, it won\\'t teach you to program in R - only how to do modern statistical analysis with it, with an emphasis on robust techniques that address the known problems with classical analysis that are sidelined by most other approaches to teaching statistics.</p>\\n\\n<p>The three problems with classical methods that this book particularly addresses right from the beginning are sampling from heavy-tailed distributions; skewness; and heteroscedasticity.</p>\\n\\n<p>Wilcox uses R because \"In terms of taking advantage of modern statistical techniques, R clearly dominates.  When analyzing data, it is undoubtedly the most important software development during the last quarter of a century.  And it is free. Although classic methods have fundamental flaws, it is not suggested that they be completely abandoned... Consequently, illustrations are provided on how to apply standard methods with R.  Of particular importance here is that, in addition, illustrations are provided regarding how to apply modern methods using over 900 R functions written for this book.\"</p>\\n\\n<p>This book is so excellent that after we bought a copy for work I purchased my own copy at home.  </p>\\n\\n<p>The chapter headings are: </p>\\n\\n<ol>\\n<li>numerical and graphical summaries of data; </li>\\n<li>probability and related concepts; </li>\\n<li>sampling distributions and confidence intervals; </li>\\n<li>hypothesis testing; </li>\\n<li>regression and correlation; </li>\\n<li>bootstrap methods; </li>\\n<li>comparing two independent groups; </li>\\n<li>comparing two dependent groups; </li>\\n<li>one-way ANOVA; </li>\\n<li>two-way and three-way designs; </li>\\n<li>comparing more than two dependent groups; </li>\\n<li>multiple comparisons; </li>\\n<li>some multivariate methods; </li>\\n<li>robust regression and measures of association; </li>\\n<li>basic methods for analyzing categorical data;</li>\\n</ol>\\n\\n<p><strong>Further edit</strong> - having checked out the David Moore example of what you are looking for, I really think Wilcox\\'s book meets the need.</p>\\n',\n",
       " '<p>I have read about <a href=\"http://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1225&amp;context=usgsnpwrc\">controversies regarding hypothesis testing</a>  with some commentators suggesting that hypothesis testing should not be used. Some commentators suggest that <em>confidence intervals</em> should be used instead. </p>\\n\\n<ul>\\n<li>What is the difference between confidence intervals and hypothesis testing? Explanation with reference and examples would be appreciated.</li>\\n</ul>\\n',\n",
       " '<p>Two thoughts:</p>\\n\\n<ul>\\n<li><p>Prosper and other peer-to-peer lending sites sometimes make their data available.  Not precisely what you are looking for, but it has a similar flavor.</p></li>\\n<li><p>Wharton (U. Pennsylvania) makes tons of data available through <a href=\"https://wrds-web.wharton.upenn.edu/wrds/\" rel=\"nofollow\">WRDS</a>.  It\\'s most definitely not free, but many institutions subscribe to it and yours may as well.  At very least you could look there for the existence of a dataset and try to negotiate directly with the data providers.</p></li>\\n</ul>\\n',\n",
       " '<p>Use the <a href=\"http://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic\" rel=\"nofollow\">Durbin-Watson test</a>, implemented in the <a href=\"http://cran.r-project.org/web/packages/lmtest/\" rel=\"nofollow\">lmtest</a> package.</p>\\n\\n<pre><code>dwtest(prices[,1] ~ prices[,2])\\n</code></pre>\\n',\n",
       " '<p>I am programming a kNN algorithm and would like to know the following:</p>\\n\\n<p>Tie-breaks:</p>\\n\\n<ol>\\n<li>What happens if there is no clear winner in the majority voting? E.g. all k nearest neighbors are from different classes, or for k=4 there are 2 neighbors from class A and 2 neighbors from class B?</li>\\n<li>What happens if it is not possible to determine exactly k nearest neighbors because there are more neighbors which have the same distance? E.g. for the list of distances <code>(x1;2), (x2;3.5), (x3;4.8), (x4;4.8), (x5;4.8), (x6;9.2)</code> it would not be possible to determine the k=3 or k=4 nearest neighbors, because the 3rd to 5th neighbors all have same distance.</li>\\n</ol>\\n\\n<p>Weights:</p>\\n\\n<ol>\\n<li>I read it is good to weight the k-nearest neighbors before selecting the winning class. How does that work? I.e. how are the neighbors weighted and how is then the class determined?</li>\\n</ol>\\n\\n<p>Majority vote alternatives:</p>\\n\\n<ol>\\n<li>Are there other rules/strategies to determine the winning class other than majority vote?</li>\\n</ol>\\n',\n",
       " '<p>These <a href=\"http://www.statslab.cam.ac.uk/~yms/ICL0706.ps\" rel=\"nofollow\">lecture notes</a> on information theory by O. Johnson contain a good introduction to different kinds of entropy.</p>\\n',\n",
       " '<p>I need to compute the estimate the variance component for my data.</p>\\n\\n<p>I have the following model, y = gene, cell line, gene*cell line, DNA extract[cell line]</p>\\n\\n<p>How do I write this for the lme function?  I am confused on how to indicate that \"DNA extract\" is nested within \"cell line\" and also have it marked as a random variable.</p>\\n',\n",
       " \"<p>What is being plotted is the log-odds.  It's log(p/(1-p)).  That's the space of the logistic regression.  You can convert the values using the logistic distribution and the qlogis and plogis functions.</p>\\n\\n<p>I don't know what GAM functions you're using but often times there are options to get the p-values out.</p>\\n\",\n",
       " \"<p>I work with clinical trial data a lot.  A characteristic of such data is the repeating of measurements on subjects at a number of scheduled trial visits.  We use repeated measures ANOVA to look for differences between groups of patients ( often comparing treatment groups with a control group). The regression appraoch doesn't make sense to me either. You have no interest in the slopes of the regression lines.  Your only concern is how the means change over time.</p>\\n\",\n",
       " '<p>I am wondering if there are any packages for python that is capable of performing survival analysis. I have been using the survival package in R but would like to port my work to python. </p>\\n',\n",
       " '<p>I have gone through the techniques and understood the basic ideas. But I want to know which one usually is expected to work better, LDA or Labeled LDA? What are the features of the dataset that help decide amongst the two?</p>\\n',\n",
       " '<p>Suppose you have a coin with probability $p$ to land heads and $(1-p)$ to land tails. Let $x=1$ indicate heads and $x=0$ indicate tails. Define $f$ as follows</p>\\n\\n<p>$$f(x,p)=p^x (1-p)^{1-x}$$</p>\\n\\n<p>$f(x,2/3)$ is probability of x given $p=2/3$, $f(1,p)$ is likelihood of $p$ given $x=1$. Basically likelihood vs. probability tells you which parameter of density is considered to be the variable</p>\\n\\n<p><img src=\"http://yaroslavvb.com/upload/likelihood-vs-probability.png\"></p>\\n',\n",
       " \"<p>Well, you didn't ask about R, but in R you do it using ?qnorm</p>\\n\\n<p>(It's actually the quantile, not the percentile, or so I believe)</p>\\n\\n<pre><code>&gt; qnorm(.5)\\n[1] 0\\n&gt; qnorm(.95)\\n[1] 1.644854\\n</code></pre>\\n\",\n",
       " '<p>This image show a histogram (200 bins) of accumulated distances from a radar distance meter (very noisy).\\nThe peak around 7 meters is an object. At thought this looked kind of like a normal distribution, at least if you ignore values &lt;4m (which for this application is reasonable).</p>\\n\\n<p>I have also achieved a reasonable fit with a <a href=\"http://en.wikipedia.org/wiki/Log-normal_distribution\" rel=\"nofollow\">log-normal distribution</a>.</p>\\n\\n<p>What I am trying to do is to filter out true distances based on the probability distribution.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/tYgBf.png\" alt=\"alt text\">\\n<strong>EDIT:</strong> log-normal distribution:\\n<img src=\"http://i.stack.imgur.com/9UMoY.png\" alt=\"alt text\"></p>\\n\\n<p><strong>Note:</strong> this question can also be found at <a href=\"http://math.stackexchange.com/questions/14162/is-this-a-probability-distribution\">math.stackexchange.com</a></p>\\n',\n",
       " '<p>You are (were) almost there.  Just a few comments - you don\\'t have to make the prior for the <code>beta[,1:2]</code> parameters a joint MV normal; you can make the prior such that <code>beta[i,1]</code> and <code>beta[i,2]</code> are independent, which simplifies things (for example, no prior covariance need be specified.)  Note that doing so doesn\\'t mean they will be independent in the posterior.  </p>\\n\\n<p>Other comments:  Since you have a constant term - <code>alpha</code> - in the regression, the components <code>beta[,1]</code> should have zero mean in the prior.  Also, you don\\'t have a prior for <code>alpha</code> in the code.</p>\\n\\n<p>Here\\'s a model with hierarchical intercept and slope terms; I\\'ve tried to keep to your priors and  notation where possible, given the changes:</p>\\n\\n<pre><code>model {\\n  for(i in 1:n){\\n    mu.y[i] &lt;- alpha + beta0[s[i]] + beta1[s[i]]*(j[i]-jbar)\\n    demVote[i] ~ dnorm(mu.y[i],tau)\\n  }\\n\\n  alpha ~ dnorm(0, 0.001) ## prior on alpha; parameters just made up for illustration\\n  sigma ~ dunif(0,20) ## prior on standard deviation\\n  tau &lt;- pow(sigma,-2) ## convert to precision\\n\\n  ## hierarchical model for each state’s intercept &amp; slope\\n  for (p in 1:120) {\\n     beta0[p] ~ dnorm(0, tau0)\\n     beta1[p] ~ dnorm(mu1, tau1)\\n  }\\n\\n  ## Priors on hierarchical components; parameters just made up for illustration\\n  mu1 ~ dnorm(0, 0.001) \\n  sigma0 ~ dunif(0,20)\\n  sigma1 ~ dunif(0,20)\\n  tau0 &lt;- pow(sigma0,-2)\\n  tau1 &lt;- pow(sigma1,-2)\\n}\\n</code></pre>\\n\\n<p>A very useful resource for hierarchical models, including some \"tricks\" to speed up convergence, is <a href=\"http://www.stat.columbia.edu/~gelman/arm/\">Gelman and Hill</a>.</p>\\n\\n<p>(A little late with the answer, but may be helpful to some future questioner.)</p>\\n',\n",
       " '<p>Recoding your data with numerical values seems ok, provided the assumption of an ordinal scale holds. This is often the case for Likert-type item, but see these related questions:</p>\\n\\n<ul>\\n<li><a href=\"http://stats.stackexchange.com/questions/2401/is-it-appropriate-to-treat-n-point-likert-scale-data-as-n-trials-from-a-binomial/\">Is it appropriate to treat n-point Likert scale data as n trials from a binomial process?</a></li>\\n<li><a href=\"http://stats.stackexchange.com/questions/10/under-what-conditions-should-likert-scales-be-used-as-ordinal-or-interval-data/\">Under what conditions should Likert scales be used as ordinal or interval data?</a></li>\\n</ul>\\n\\n<p>When validating a questionnaire, we often provide usual numerical summaries (mean $\\\\\\\\pm$ sd, range, quartiles) to highlight ceiling/floor effect, that is higher response rate in the extreme range of the scale. Dotplots are also great tool to summarize such data.</p>\\n\\n<p>This is just for visualization/summary purpose. If you want to get into more statistical stuff, you can use proportional odds model or ordinal logistic regression, for ordinal items, and multinomial regression, for discrete ones.</p>\\n',\n",
       " '<p>A slightly simpler formulation is as follows: suppose you believe that your trading strategy has a Sharpe ratio of $\\\\\\\\psi$, \\'annualized\\' to the time units of your mark frequency (monthly in your case, evidently). Then to perform a 1-sided, 1-sample t-test for the null hypothesis that the expected return of your strategy is zero, you should set\\n$$N = \\\\\\\\frac{2.7}{\\\\\\\\psi^2}$$\\nin order to have a power of 0.5 and a type I rate of 0.05 (the \\'magical\\' value). Note this is just a modification of Lehr\\'s rule (as described by <a href=\"http://amzn.com/0470144483\" rel=\"nofollow\">Van Belle</a>). </p>\\n\\n<p>There are a large number of caveats here:</p>\\n\\n<ol>\\n<li>this only holds for reasonably non-skewed distributions of returns.</li>\\n<li>using relative returns (percents) instead of log returns will create a geometric bias.</li>\\n<li>having a small number of samples biases the estimate of Sharpe.</li>\\n</ol>\\n\\n<p>There is probably a similar formula for the two sample t-test to compare mean returns, or a test to compare Sharpe ratios, but I don\\'t know them (yet).</p>\\n',\n",
       " '<p>The constants in this problem do not make much sense unless $X_1$ and $X_2$ have variance $1$ so that $X_1$ and $X_2-\\\\\\\\mu_2$ are standard normal random variables, an assumption that the OP apparently is unwilling to make since this was asked about\\nin the comments, and the OP did not include the assumption in the revised version of the question.</p>\\n\\n<p><strong>Assumption:</strong> $X_1$ and $X_2$ have variance $1$.</p>\\n\\n<p>If $X_1$ is a standard normal random variable, then \\n$P\\\\\\\\{|X_1| \\\\\\\\geq \\\\\\\\Phi^{-1}(1-y/2)\\\\\\\\} = y$.\\nThis result holds for $X_2$ as well if $\\\\\\\\mu_2 = 0$.  Thus, if\\n$X_1$ and $X_2$ both are <em>independent</em> standard normal random variables,\\nthen\\n$$\\\\\\\\begin{align}\\n&amp;\\\\\\\\quad P\\\\\\\\left\\\\\\\\{|X_1| \\\\\\\\geq \\\\\\\\Phi^{-1}(1-\\\\\\\\alpha/2), \\n|X_2| \\\\\\\\geq \\\\\\\\Phi^{-1}(1-\\\\\\\\alpha/2)\\\\\\\\right\\\\\\\\}\\\\\\\\\\\\\\\\\\n&amp;= P\\\\\\\\left\\\\\\\\{|X_1| \\\\\\\\geq \\\\\\\\Phi^{-1}(1-\\\\\\\\alpha/2)\\\\\\\\right\\\\\\\\}\\nP\\\\\\\\left\\\\\\\\{|X_2| \\\\\\\\geq \\\\\\\\Phi^{-1}(1-\\\\\\\\alpha/2)\\\\\\\\right\\\\\\\\}\\\\\\\\\\\\\\\\\\n&amp;= \\\\\\\\alpha^2\\n\\\\\\\\end{align}$$\\nwhile\\n$$\\\\\\\\begin{align}\\n&amp;\\\\\\\\quad P\\\\\\\\left\\\\\\\\{|X_1| \\\\\\\\geq \\\\\\\\Phi^{-1}(1-\\\\\\\\alpha/4), \\n|X_2| \\\\\\\\geq \\\\\\\\Phi^{-1}(1-\\\\\\\\alpha/2)\\\\\\\\right\\\\\\\\}\\\\\\\\\\\\\\\\\\n&amp;= P\\\\\\\\left\\\\\\\\{|X_1| \\\\\\\\geq \\\\\\\\Phi^{-1}(1-\\\\\\\\alpha/4)\\\\\\\\right\\\\\\\\}\\nP\\\\\\\\left\\\\\\\\{|X_2| \\\\\\\\geq \\\\\\\\Phi^{-1}(1-\\\\\\\\alpha/2)\\\\\\\\right\\\\\\\\}\\\\\\\\\\\\\\\\\\n&amp;= (\\\\\\\\alpha/2)\\\\\\\\alpha = \\\\\\\\alpha^2/2\\n\\\\\\\\end{align}$$\\nIn short, for the case $\\\\\\\\mu_2 = 0$, the conjectured result holds\\n(with equality) for the case $\\\\\\\\rho = 0$.  Continuing to look at the\\ncase $\\\\\\\\mu_2 = 0$, if $X_2 = \\\\\\\\pm X_1$ (the case when $\\\\\\\\rho = \\\\\\\\pm 1$),\\nwe have\\n$$\\\\\\\\begin{align}\\n&amp;\\\\\\\\quad P\\\\\\\\left\\\\\\\\{|X_1| \\\\\\\\geq \\\\\\\\Phi^{-1}(1-\\\\\\\\alpha/2), \\n|X_2| \\\\\\\\geq \\\\\\\\Phi^{-1}(1-\\\\\\\\alpha/2)\\\\\\\\right\\\\\\\\}\\\\\\\\\\\\\\\\\\n&amp;= P\\\\\\\\left\\\\\\\\{|X_1| \\\\\\\\geq \\\\\\\\Phi^{-1}(1-\\\\\\\\alpha/2)\\\\\\\\right\\\\\\\\}\\\\\\\\\\\\\\\\\\n&amp;= \\\\\\\\alpha\\n\\\\\\\\end{align}$$\\nwhile\\n$$\\\\\\\\begin{align}\\n&amp;\\\\\\\\quad P\\\\\\\\left\\\\\\\\{|X_1| \\\\\\\\geq \\\\\\\\Phi^{-1}(1-\\\\\\\\alpha/4), \\n|X_2| \\\\\\\\geq \\\\\\\\Phi^{-1}(1-\\\\\\\\alpha/2)\\\\\\\\right\\\\\\\\}\\\\\\\\\\\\\\\\\\n&amp;= P\\\\\\\\left\\\\\\\\{|X_1| \\\\\\\\geq \\\\\\\\Phi^{-1}(1-\\\\\\\\alpha/4)\\\\\\\\right\\\\\\\\}\\\\\\\\\\\\\\\\\\n&amp;= \\\\\\\\alpha/2\\n\\\\\\\\end{align}$$\\nand so once again the conjectured result holds \\n(with equality).  What happens for other values of \\n$\\\\\\\\rho \\\\\\\\in [-1,1]$ is not immediately obvious since\\nthe bivariate cumulative normal distribution function\\nmust be used and the special meanings of $\\\\\\\\Phi^{-1}$\\nare lost. The case $\\\\\\\\mu_2 \\\\\\\\neq 0$ only exacerbates the\\nmessiness of the calculations. Simulation might be \\nthe best option to check whether the conjectured bound\\nis reasonable.</p>\\n',\n",
       " '<p>I\\'ve been researching the <a href=\"http://cran.r-project.org/web/packages/mice/index.html\">mice</a> package, and I haven\\'t yet discovered a way to use the multiple imputations to make a Cox model, then validate that model with the <a href=\"http://cran.r-project.org/web/packages/rms/index.html\">rms</a> package\\'s <code>validate()</code> function.  Here is some sample code of what I have so far, using the data set <code>veteran</code>:</p>\\n\\n<pre><code>library(rms)\\nlibrary(survival)\\nlibrary(mice)\\n\\nremove(veteran)\\ndata(veteran)\\nveteran$trt=factor(veteran$trt,levels=c(1,2))\\nveteran$prior=factor(veteran$prior,levels=c(0,10))\\n\\n#Set random data to NA \\nveteran[sample(137,4),1]=NA\\nveteran[sample(137,4),2]=NA\\nveteran[sample(137,4),7]=NA\\n\\nimpvet=mice(veteran)\\nsurvmod=with(veteran,Surv(time,status))\\n\\n#make a CPH for each imputation\\nfor(i in seq(5)){\\n    assign(paste(\"mod_\",i,sep=\"\"),cph(survmod~trt+celltype+karno+age+prior,\\n        data=complete(impvet,i),x=T,y=T))\\n}\\n\\n#Now there is a CPH model for mod_1, mod_2, mod_3, mod_4, and mod_5.\\n</code></pre>\\n\\n<p>Now, if I were just working with one CPH model, I would do this:</p>\\n\\n<pre><code>validate(mod_1,B=20)\\n</code></pre>\\n\\n<p>The problem I\\'m having is how to take the 5 CPH models (1 for each imputation), and be able to create a pooled model that I can then use with <code>rms</code>.  I know that the <code>mice</code> package has some built-in pooling functions but I don\\'t believe they work with the <code>cph</code> object in <code>rms</code>.  The key here is being able to still use <code>rms</code> after pooling.  I looked into using Harrell\\'s <code>aregImpute()</code> function but I\\'m having some trouble following the examples and documentation;  <code>mice</code> seems simpler to use.</p>\\n',\n",
       " \"<p>It is an interesting twist to find a case where the demographic data will be seen as less reliable as the behavioral data. There isn't much good advice on how to select the calibration variables, other than they should correlate with both the (non-)response process and the variables of interest. The reasoning behind the widespread use of demographic variables for calibration is that age, education, race and gender affect pretty much everything in any social science. You can make a very simple case with your data, however, by modeling the probability of response as a function of all the variables that you think about including -- a propensity model, if you like. If you can demonstrate that donations are more significant in your model than age, nobody would have the grounds to object to your use of the former in calibration.</p>\\n\\n<p>The question of how much calibration is enough has not been addressed much, either. I can think about this conceptually as a trade-off between improving the accuracy (which, for a given response variable $y$ and a set of calibration variables $\\\\\\\\bf x$, is the variance of the residuals $e_i = y_i - {\\\\\\\\bf x}_i' {\\\\\\\\bf b}$) and the increase in the variability of the weights, and hence the design effect $1+{\\\\\\\\rm CV}^2$. As you add predictors that decrease in their strength, the precision gains are diminishing; the CV though will continue increasing, so at some point, arguably, the two curves will meet, giving you the right number of calibration variables. That's just an idea, but may be I should write a paper about it :)</p>\\n\",\n",
       " '<p>Check out the discussion on <a href=\"http://www.analyticbridge.com/forum/topics/vector-time-series-analysis\" rel=\"nofollow\">http://www.analyticbridge.com/forum/topics/vector-time-series-analysis</a>. The whole idea is to seamlessly integrate daily predictions and hourly predictions into one. This is done while detecting and incorporating multiple level shifts, multiple time trends , the window of response around individual holidays while incorporating weather effects, all while isolating unusual values. You could search for a mixed-frequency suite of software as you have both intra and inter day effects.</p>\\n',\n",
       " \"<p>Several points. There really aren't two t-tests (one for equal variances and one for unequal). If the two distributions are normal with unknown means and equal (but unknown) variance, the distribution of the test statistic has a t distribution under the null hypothesis of equal variances. The key point here is that the unknown variance does not figure in the distribution of the test statistic.</p>\\n\\n<p>With 2 normals, unknown means and unknown but different variances, the obvious test statistic has a distribution that depends upon the ratio of the variances. This is the so-called Behrens-Fisher problem. The distribution of the test statistic under the null hypothesis of equal means depends on parameter values that are not known; so an exact rejection region cannot be constructed.</p>\\n\\n<p>Walsh's test is basically a fudge. The test statistic makes intuitive sense; and one adjusts the degrees of freedom so that the result more or less follows a t distribution under the null. Which apparently, it does. Interestingly, R's t.test defaults to Walsh.</p>\\n\\n<p>In any case, what you suggest would be a fudge applied to a fudge, and I am not sure what rationale in the theory of hypothesis testing would justify it. (Nice try, though).</p>\\n\\n<p>The failure of Behrens-Fisher illuminates an important truth: when the variances are wildly different, comparing means probably does not make a huge amount of sense. If they are fairly close, Walsh will work.</p>\\n\\n<p>When two distributions have different variances, you basically need to ask yourself what sort of differences between the underlying processes are of interest to you.</p>\\n\\n<p>Reasonable approaches include:</p>\\n\\n<ul>\\n<li>A non-parametric test.</li>\\n<li>A variance stabilizing transformation</li>\\n<li>Possibly a bootstrap estimate of some kind.</li>\\n</ul>\\n\",\n",
       " '<p>I\\'ve similar problem my solution was naive - create new variables representing each minute of the day if given activite took place then mark that minute by 1 :</p>\\n\\n<pre><code>yawning     -&gt;  yawning  \\n...             ...\\n2:21-2:22       2:21 1\\n3:42-3:45       2:22 1\\n9:20-925        2:23 0\\n14:45-14:32     . \\n.               3:42 1\\n.               3:43 1\\n.               .\\n45:40-45-43     .\\n</code></pre>\\n\\n<p>so we have now new time series, which we could analyse by more standard methods, this worked really good, I\\'ve tested in on simulated data on below logit model, where x is 0-1 variable, z - \"driving\" variable : <code>p(x(t+1)=1|p(x)=1)=exp(x+B1*z)/denominator</code>\\nthe same for y, the closer was B2 to B1 the better dependence between x and y measured by Hamming distance. </p>\\n\\n<p><strong>Methodological problem :</strong> what to do if total time of activity_11 during the day is 10 times higher then that of activity_2 ? Sometimes it doesn\\'t matter, sometimes some weighted distance is needed - in the case when we want to build distance matrix.</p>\\n',\n",
       " \"<p>It is very important that you consider the possibility that the categories of subject have a real difference in reaction times. If that is the case then anything that makes the difference go away will lead to potentially artifactual results. Don't assume that an inconvenient effect is a result of the presence of outliers.</p>\\n\\n<p>Perhaps you could look for a relationship between reaction time and another outcome measure. The form of the relationship may differ between autistic subjects and normal subjects.</p>\\n\",\n",
       " '<p>Rather use <a href=\"http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\" rel=\"nofollow\">Kolmogorov–Smirnov test</a>, which is exactly what you need. R function <code>ks.test</code> implements it.</p>\\n\\n<p>Also check <a href=\"http://stats.stackexchange.com/questions/411/motivation-for-kolmogorov-distance-between-distributions\">this question</a>.</p>\\n',\n",
       " '<p>(Responding to the simulation question)</p>\\n\\n<p>In R:</p>\\n\\n<pre><code>n &lt;- 3000; m &lt;- 300; k &lt;- 20 # Problem parameters\\nnIterations &lt;- 10000         # Number of iterations in the simulation\\nset.seed(17)                 # Make the output reproducible\\n#\\n# All the work is done in the following line.\\n#\\nt &lt;- table(replicate(nIterations, any(tabulate(floor(runif(n, min=1, max=m+1))) &gt;= k)))\\nt[[\"TRUE\"]]/nIterations      # Convert the count to a proportion\\n</code></pre>\\n\\n<p>Expected output:</p>\\n\\n<pre><code>[1] 0.6453\\n</code></pre>\\n\\n<p>To see more deeply into what\\'s going on, look at the detailed distribution in several experiments by executing this command several times:</p>\\n\\n<pre><code>table(tabulate(floor(runif(n, min=1, max=m+1))))\\n</code></pre>\\n\\n<p>Typical output is</p>\\n\\n<pre><code> 2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 \\n 2  5 11 23 23 32 48 37 33 21 19 21  9  6  5  1  2  1 \\n</code></pre>\\n\\n<p>In this experiment, two values in the range 0..299 were observed 19 times (among 3000 independent draws) and one value was observed 21 times.  You will find that most of the time, at least one value occurs 20 or more times.  Because $1/m$ is small and $n$ is large, you should be seeing a Poisson distribution here.  Indeed,</p>\\n\\n<pre><code>1 - ppois(k-1, n/m)^m\\n</code></pre>\\n\\n<p>returns</p>\\n\\n<pre><code>[1] 0.6458719\\n</code></pre>\\n',\n",
       " '<p>Consider a multinomial $2\\\\\\\\times 2$ table $\\\\\\\\begin{pmatrix} x_{11} &amp; x_{12} \\\\\\\\\\\\\\\\ x_{21} &amp; x_{22} \\\\\\\\end{pmatrix}$ with theoretical probabilities: $$\\\\\\\\begin{pmatrix} \\\\\\\\theta_{11} &amp; \\\\\\\\theta_{12} \\\\\\\\\\\\\\\\  \\\\\\\\theta_{21} &amp; \\\\\\\\theta_{22} \\\\\\\\end{pmatrix}.$$ \\nThe conditional distribution of $x_{11}$ given $x_{11}+x_{12}=n$ is binomial with size $n$ and proportion $p=\\\\\\\\dfrac{\\\\\\\\theta_{11}}{\\\\\\\\theta_{11}+\\\\\\\\theta_{12}}$. Therefore we have a straightforward way to perform statistical inference on $p$ by conditioning on $x_{11}+x_{12}$. Is it possible to do an unconditional inference on $p$ ?</p>\\n\\n<p>Parenthesis : I have noted that adopting the Bayesian approach with the Jeffreys (Dirichlet) prior of the unconditional multinomial model is equivalent to adopt the Bayesian approach with the Jeffreys (Beta) prior of the conditional binomial model.</p>\\n',\n",
       " '<p>I would like to clarify how <a href=\"http://en.wikipedia.org/wiki/Granger_causality\">the Granger causality</a> can/should be used in practice, and how to interpret the statistical significance given by the test.</p>\\n\\n<p>Also, I would like to fill this table with things like \"we don\\'t know\" or if we know something, what do we know (It will for sure not be causality, but maybe something else?).</p>\\n\\n<p>$\\\\\\\\begin{matrix}\\\\\\\\n &amp; X granger cause Y significant &amp; X granger cause Y non significant\\\\\\\\\\\\\\\\ \\\\\\\\nY granger cause X significant &amp; ... &amp; ...\\\\\\\\\\\\\\\\ \\\\\\\\nY granger cause X non significant &amp; ... &amp; ...\\\\\\\\n\\\\\\\\end{matrix}\\\\\\\\n$</p>\\n',\n",
       " '<p>I want to interpret an interaction. Usually a conditional regression analysis is done with 1 SD below/above the mean (see Aiken &amp; West, 1993, p. 18). My result is a contrast with $p=.068$ if I take 1.5 SDs $p=.044$. with 2 SDs $p=.033$. </p>\\n\\n<p>It seems that contrasts get stronger by increasing values the number of SDs. Thus, is it appropriate to report contrasts for 2 SDs in a paper? Why does it have to be 1 SDs?</p>\\n',\n",
       " '<p>To me it looks essentially correct. You still have to specify the <code>data.frame</code> the values are taken from in <code>aov</code> like</p>\\n\\n<pre><code>aov(Value~Group+Error(Subject), data)\\n</code></pre>\\n\\n<p>This model assumes that subject were taken at random (within their group). It is justified to drop the variable \"replicate\" from the formula as it bears no information.</p>\\n',\n",
       " '<ol>\\n<li>If this is an SPSS syntax question, the answer is just put the categorical variable, coded appropriately, into the variable list for \"independent variables\" along with the continuous one. </li>\\n<li>On the statistics: Is your categorical variable binary? If so, you need to use a dummy or other valid contrast code. If it is not binary, is your categorical variable ordinal or nominal? If nominal, then again, you must use some contrasting code strategy--in effect modeling the impact of each level of the variable on the outcome or \"dependent\" variable. If the categorical variable is ordinal, then <em>most likely</em> the sensible thing to do is to enter it as-is into the model, just as you would with a continuous predictor (i.e., \"independent\") variable. You would be assuming, in that case, that the increments between levels of the categorical predictor (\"indepdent\") variable; only rarely will this be a mistake, but when it is, you should again use a contrast code &amp; model the impact of each level. This question comes up in this forum quite often -- here is a good <a href=\"http://stats.stackexchange.com/questions/2619/continuous-and-categorical-variable-data-analysis\">analaysis</a></li>\\n<li>How to handle missing data is, in my view, a completely separate matter. My understanding is that pairwise deletion is not viewed as a valid approach for multivariate regression. Listwise is pretty common but can can also bias results &amp; certainly is a shame. Multiple imputation is a thing of beauty.</li>\\n</ol>\\n',\n",
       " '<p>I am looking for a program (in R or SAS or standalone, if free or low cost) that will do power analysis for ordinal logistic regression.</p>\\n',\n",
       " '<p>I have a response variable(y) and 20 independent variables (Xs). I want to select several Xs in the linear regression, but I\\'m not sure how many variables should be selected. To select the best number of variables, I use the sum of the squared residuals (Res) in the 10-fold cross-validation given N selected variables (N=2~20). The process is repeated 1000 times given each N. My idea is that Res should firstly decrease as more variable could explain y better and then it should increase as too many variables should lead to over-fitting. To my surprise, Res decrease continually as N increase(see the Figure). I don\\'t know how to explain it.  Is it mean that all 20 variables contribute to y, or over-fitting happened?</p>\\n\\n<p><strong>P.S.:</strong> there are about 600 data points. The Res is calculated as the sum of the square of the difference between observed y and predicted y in each 10-fold cross-validation.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/mT7v7.png\" alt=\"enter image description here\"></p>\\n',\n",
       " \"<p>Linear regression can accommodate non-straight-line relationships between IVs and the DV through various transformations of variables, addition of polynomial terms and so on.  </p>\\n\\n<p>That is a model like</p>\\n\\n<p>$y = b_0 + b_1x_1^2 + b_2x_1 + b_3x_3^5$</p>\\n\\n<p>is a linear model.  But a model such as</p>\\n\\n<p>$y = b_0 + 2^{b_1x_1}$</p>\\n\\n<p>is not. </p>\\n\\n<p>If the data are <em>really</em> nonlinear, then the choice of model depends partly on what you know about the relationships. If you don't know much, a spline regression may work well. </p>\\n\",\n",
       " '<p>I\\'m reading the example of the book: \"S-PLUS (and R) Manual to Accompany Agresti’s Categorical Data Analysis (2002) 2nd edition \" (page 55) and when I try to reproduce the example I get this:</p>\\n\\n<pre><code>snoring&lt;-c(0,2,4,5) \\n\\nlogit.irls&lt;-glm(cbind(yes=c(24,35,21,30), no=c(1355,603,192,224))~snoring, \\nfamily=binomial(link=logit)) \\n\\nsummary(logit.irls)\\n\\nNull deviance: 65.9045  on 3  degrees of freedom\\nResidual deviance:  2.8089  on 2  degrees of freedom\\n**AIC: 27.061**\\n</code></pre>\\n\\n<p>Ok, I get a Akaike information criterion of 27.061.</p>\\n\\n<p>But when I use the multinorm function from the \"nnet\" library I get this:</p>\\n\\n<pre><code>modLogit = multinom(cbind(c(24,35,21,30),c(1355,603,192,224))~snoring)\\n\\nsummary(modLogit)\\n\\nCoefficients:\\n  (Intercept)    snoring\\n2    3.866248 -0.3973366\\n\\nStd. Errors:\\n  (Intercept)    snoring\\n2   0.1662144 0.05001066\\n\\nResidual Deviance: 837.7316 \\n**AIC: 841.7316** \\n</code></pre>\\n\\n<p>Why the AIC (and the Residual Deviance) are too big?</p>\\n',\n",
       " '<p>I\\'m not 100% clear on the question, but I have a few points to add:</p>\\n\\n<p>I\\'m assuming that the error you are trying to estimate is the prediction error. If so, I agree that 10 fold cross validation would be good (and likely unbiased) approximation of the true prediction error <em>IF</em> your training sets are sufficiently large. Large in this case means that the training sets provide enough information to build a \"good\" SVM (one that, in a sense, captures most of the underlying relationships between between the predictors and response.) Training sets of size 900 are more than likely large enough. In fact, unless the SVM you are fitting is extremely complex, I would recommend using a 5-fold cross validation in order to get a more precise estimate of prediction error (and yes, you can average the error estimates of the 5 folds to obtain an final estimate.)</p>\\n\\n<p>With regards to the question:</p>\\n\\n<p>\"Would events tested using separately trained svms be comparable? i.e. through this technique could I then use my entire dataset instead of setting aside a certain fraction for training, or is this a statistically unwise thing to do?\"</p>\\n\\n<p>I don\\'t understand this question, but since the phrase \"entire dataset\" is in a post about CV, I just want to warn you that estimating prediction error from models fit to all available data is generally a bad idea. For cross validation to make sense, each training set/test set pair should have no points in common. Otherwise, the true error will likely be underestimated.</p>\\n',\n",
       " '<p>This is only an outline of idea. The proportion of variance is defined as</p>\\n\\n<p>$$\\\\\\\\frac{\\\\\\\\lambda_1}{\\\\\\\\lambda_1+...+\\\\\\\\lambda_6},$$</p>\\n\\n<p>where $\\\\\\\\lambda_i$ are the eigenvalues of covariance matrix. Now if we use instead the eigenvalues of correlation matrix then $\\\\\\\\lambda_1+...+\\\\\\\\lambda_6=6$, since the sum of eigenvalues of a matrix is equal to the trace of the matrix, and for correlation matrices the trace is the sum of ones.</p>\\n\\n<p>So if we use the correlation matrices we need to test hypotheses about the difference of two maximal eigenvalues of sample correlation matrices. It is certainly possible to find in the literature the asymptotic distribution of the maximal eigen-value of correlation matrix. So the problem then reduces to some sort of paired or unpaired t-test.</p>\\n',\n",
       " '<p>I believe effects like these are frequently caused by collinearity (see <a href=\"http://stats.stackexchange.com/questions/1149/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-re\">this question</a>). I think the book on multilevel modeling by Gelman and Hill talks about it. The problem is that <code>IV1</code> is correlated with one or more of the other predictors, and when they are all included in the model, their estimation becomes erratic. </p>\\n\\n<p>If the coefficient flipping is due to collinearity, then it\\'s not really interesting to report, because it\\'s not due to the relationship between your predictors to the outcome, but really due to the relationship between predictors.</p>\\n\\n<p>What I\\'ve seen suggested to resolve this problem is residualization. First, you fit a model for <code>IV2 ~ IV1</code>, then take the residuals of that model as <code>rIV2</code>. If all of your variables are correlated, you should really residualize all of them. You may choose do to so like this</p>\\n\\n<pre><code>rIV2 &lt;- resid(IV2 ~ IV1)\\nrIV3 &lt;- resid(IV3 ~ IV1 + rIV2)\\nrIV4 &lt;- resid(IV4 ~ IV1 + rIV2 + rIV3)\\n</code></pre>\\n\\n<p>Now, fit the final model with</p>\\n\\n<pre><code>DV ~ IV1 + rIV2 + rIV3 + rIV4\\n</code></pre>\\n\\n<p>Now, the coefficient for <code>rIV2</code> represents the independent effect of <code>IV2</code> given its correlation with <code>IV1</code>. I\\'ve heard you won\\'t get the same result if you residualized in a different order, and that choosing the residualization order is really a judgment call within your research. </p>\\n',\n",
       " '<p>I\\'m doing an autocorrelation analysis for a spatially distributed collection of observations. To perform my analysis, I am using Moran\\'s I statistic.</p>\\n\\n<p>My questions are: <strong>(1)</strong> What are the implications and benefits of using different weighting functions, i.e. $d^{-1}$, $d^{-2}$, $\\\\\\\\exp(-d)$, and <strong>(2)</strong> Is there any (perhaps informal) answer to which of the possible weighting functions is used most frequently in the geo-statistics literature (and for what purposes)?</p>\\n\\n<p>As for why I care: I am trying to explore whether there is clustering in my data set at different scales of structure, following some of the methodology of <a href=\"http://www.esajournals.org/doi/abs/10.1890/0012-9658%282000%29081%5B0773%3ASDPPIT%5D2.0.CO%3B2\" rel=\"nofollow\">Fauchald 2000</a>. I am plotting Moran\\'s I versus aggregation scale. The interesting thing that the resulting correlation curves show very different qualitative behavior when calculating using $d^{-1}$ and $d^{-2}$ weighting functions ($d^{-1}$ has a discontinuity point, for example). I\\'m having a hard time understanding why this would be true -- does anyone have experience with this who may be able to point me to some references?</p>\\n',\n",
       " \"<p>I've got more than 20 (10 point likert scale) variables with more than 1000 entries each. What I want to do is compare the means of the answers on the questions.</p>\\n\\n<p>A one-way anova seems suitable for this, but you can only categorize by the values of a variabele. I want to categorize by question, the variable itself.</p>\\n\\n<p>Is there any way to do this without having to put all answer beneath each other and having another variable saying which question it is.</p>\\n\\n<p>Just to be clear, I've got:</p>\\n\\n<p>variable: question1 --> 1000 entries ranging from 1 to 10</p>\\n\\n<p>variable: question2 --> 1000 entries ranging from 1 to 10</p>\\n\\n<p>...</p>\\n\\n<p>variable: question20 --> 1000 entries ranging from 1 to 10</p>\\n\\n<p>I want to compare the means of the different questions with one-way anova but I can't choose to factor by question.</p>\\n\",\n",
       " '<p>I would be very thankful for advising me how to test statistical significance of coherence coefficients. I have a statistical software that calculates them for me but it does not test their statistical significance. \\nAs far as I know spectral analysis results may be biased and high coherence may not prove anything. Therefore, I need a test that compares my results to a results I would get for two white noise series (at lest, this is my idea how this test would look like?). </p>\\n\\n<p>My second request is whether there is any statistical package that would do it for me. If not, I would be thankful for formulae to calculate it on my own. </p>\\n\\n<p>And last, but not least, is there any other method that would allow me to search for connection/similarities between two de-trended and cleared with Christiano-Fitzgerald filter series? I would like to prove that long-term US business cycles strongly influence business cycles in Europe (contrary to IMF results) but I do not know what tool to use to receive reliable results. When I used coherence I was told that high coherence can be the result of the method imperfections (spectral analysis) and that I must not even attempt to analyse similarities between UE business cycles and US business cycles using this approach. </p>\\n',\n",
       " '<p>I am taking a class in data mining and I am working on a term project using the <a href=\"http://www.cdc.gov/brfss/\" rel=\"nofollow\">BRFSS</a> dataset. I have a huge dataset with 405 columns and 12,000 rows. There are many columns which are completely empty. I was trying to remove empty columns using SAS, R or Excel but it doesn\\'t work. Could you suggest a method to remove the empty columns or any tutorial that will help me cleaning up the data?  There are a lot of missing cells too. I am using KNIME to train my data and it doesn\\'t work if there are missing values. How can I handle the missing values? </p>\\n',\n",
       " '<p>I would suggest applying a transform which deals with periodicity. i.e. $\\\\\\\\lim_{x \\\\\\\\to 360} f(x) = f(0)$. An easy option is to take the sin and cos, and put them both as covariates in the model. </p>\\n',\n",
       " \"<p>I've been asked to try to do a preliminary 'meta-analysis' of some data, but while comfortable with maths I don't have much experience with stats. I'm hoping to get some feedback with what I have attempted so far, to see if I'm on the right track.</p>\\n\\n<p>Using SPSS for the software.</p>\\n\\n<p><strong>Design</strong></p>\\n\\n<p>One dependant variable 'size' taken from different studies performed over a range of years. Most studies did not go for more than one year.</p>\\n\\n<p>Other factors:</p>\\n\\n<ul>\\n<li>'geography type' - all different types were sampled, some more than others.</li>\\n<li>'geography zone' - all of 4 different zones were sampled</li>\\n<li>'location' - many different locations were sampled, some more than others, some repeated over different years but for different studies, some only once!</li>\\n<li>'technique' - different sampling techniques used.</li>\\n</ul>\\n\\n<p><strong>Questions</strong></p>\\n\\n<ul>\\n<li>Is size dependant on year?</li>\\n<li>Does geography type, geography zone or technique affect size?</li>\\n<li>How should data be combined? Should multiple measurements from the one location in one year be averaged?</li>\\n</ul>\\n\\n<p><strong>Model</strong></p>\\n\\n<p>I think the model should be</p>\\n\\n<ul>\\n<li>Dependant variable: size </li>\\n<li>Fixed-effects: geography type, geography zone</li>\\n<li>Random-effects: location, technique</li>\\n</ul>\\n\\n<p><strong>Tests performed so far</strong></p>\\n\\n<p>Linear Regression:</p>\\n\\n<ol>\\n<li>Linear regression of size vs year showed significant trend. (0.002)</li>\\n<li>Linear regression of size vs year, when multiple measurements from the one location for one year were averaged (one data point per location), showed not-quite-significant trent (0.060)</li>\\n<li>Linear regression of size vs year, when averaged location values were averaged for the entire year (one data point per year), showed no significant trend</li>\\n</ol>\\n\\n<p>Question - which is more appropriate?</p>\\n\\n<p>ANOVA:</p>\\n\\n<ol>\\n<li>Geography type vs size: Levene statistic reported significance of .000, so ANOVA with Games-Howell option checked was performed. Between groups sig of .000</li>\\n<li>Geography zone vs size: Levene statistic reported significance of .007, so ANOVA with Games-Howell option checked was performed. Between groups sig of .001</li>\\n<li>Technique vs size: Levene statistic reported significance of .000, so ANOVA with Games-Howell option checked was performed. Between groups sig of .001</li>\\n</ol>\\n\\n<p>Now I think some of these might be varying together.. How to check?</p>\\n\\n<p>So what next?</p>\\n\\n<p>EDIT: MADE A MISTAKE IN THIS SECTION, have now updated</p>\\n\\n<p>Univariate GLM:</p>\\n\\n<ol>\\n<li>Fixed effects: geography type, geography zone; geo type sig (.014), zone not sig (.885), type * zone not sig (.239)</li>\\n<li>Fixed effects: year, geography type, geography zone; geo type sig (.002), zone not sig (.639), year sig (.000)... type * year also sig (.000)</li>\\n<li>Fixed effects: year, geography type, geography zone; random effects, technique; SPSS could not calculate error term.</li>\\n<li>Fixed effects: year, geography type, geography zone, technique; type, year and technique all significant. type * year also significant.</li>\\n</ol>\\n\\n<p>Is this the correct approach? Is it possible that there is not enough data?</p>\\n\\n<p>When comparing means just looking at zone, there seems to be a difference. With the GLM it seems zone doesn't matter? How is this possible?</p>\\n\",\n",
       " '<p>I have tried to use <a href=\"http://www.alyuda.com/neural-networks-software.htm\" rel=\"nofollow\">Alyuda neurointelligence</a> software to select the best network topology and it seemed to produce some good results, at least for the project I was doing. It uses genetic algorithm to select an optimal network topology</p>\\n',\n",
       " '<p>If I understand correctly the setup, a pseudo-algorithm to generate from this distribution would be</p>\\n\\n<ol>\\n<li>generate $(A,B,C,...)$ from their joint distribution</li>\\n<li>take as your random variable a component of this vector selected with probability $p_1,p_2,\\\\\\\\ldots$</li>\\n</ol>\\n\\n<p>For instance, if the joint of $(A,B,C)$ is normal $\\\\\\\\cal{N}(\\\\\\\\mu,\\\\\\\\Sigma)$,</p>\\n\\n<ol>\\n<li>generate $(A,B,C)\\\\\\\\sim\\\\\\\\cal{N}(\\\\\\\\mu,\\\\\\\\Sigma)$</li>\\n<li>take $X=\\\\\\\\begin{cases}A &amp;\\\\\\\\text{with probability }p_1\\\\\\\\\\\\\\\\B &amp;\\\\\\\\text{with\\n    probability }p_2\\\\\\\\\\\\\\\\C &amp;\\\\\\\\text{with probability }p_3\\\\\\\\end{cases}$</li>\\n</ol>\\n\\n<p>with a corresponding R code for n simulations (assuming mu, Sigma and prob properly defined):</p>\\n\\n<pre><code>ABC=mvrnorm(n,mu=mu,Sigma=Sigma)\\nX=ABC[cbind(1:n,sample(1:3,n,prob=prob,rep=TRUE))]\\n</code></pre>\\n',\n",
       " '<p>\"...The standard SVM takes a set of input data and predicts, for each given input, which of two possible classes the input is a member of, which makes the SVM a non-probabilistic binary linear classifier. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples into one category or the other. An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall on.\"</p>\\n\\n<p>--<a href=\"http://en.wikipedia.org/wiki/Support_vector_machine\" rel=\"nofollow\">Wikipedia</a></p>\\n',\n",
       " '<p>In the case $p(n) = p &lt; 1$, we have some known properties. We can solve the recurrence relation</p>\\n\\n<p>$$ F(n) = p + F(n-1)(1-p); \\\\\\\\; F(0) = p $$</p>\\n\\n<p>has the solution</p>\\n\\n<p>$$ F(n) = P(N \\\\\\\\le n) = 1- (1-p)^{n+1} $$\\nwhich is the <a href=\"http://en.wikipedia.org/wiki/Geometric_distribution\" rel=\"nofollow\">geometric distribution</a>. It is well studied.</p>\\n\\n<p>The more general case of $p(n)$ can probably not be computed in closed form, and thus likely does not have a known distribution. </p>\\n\\n<p>Other cases:</p>\\n\\n<ol>\\n<li>$p(n) = \\\\\\\\frac{p}{n}; \\\\\\\\; p&lt;1; \\\\\\\\;F(0) = p$ has solution\\n$$ F(n) = 1 - \\\\\\\\frac{(1-p)\\\\\\\\Gamma( n + 1 -p)}{\\\\\\\\Gamma( 1-p)\\\\\\\\Gamma(n+1)} $$\\nwhich is not a commonly known distribution.</li>\\n<li>Define $S(n) = 1-F(n)$ (known as the survival function in stats), the recurrence relation  above reduces to the simpler form:\\n$$S(n) =\\\\\\\\left(1 - p(n) \\\\\\\\right) S(n-1)$$</li>\\n<li>From your example, it appears you want a function $p(n)$ that increases in $n$. Your choice $p(n) = kn$ isn\\'t great analytically because of the break at $p&gt;1$. Mathematicians and statisticians prefer <em>smooth</em> things. So I propose\\n$$p(n) = 1 - \\\\\\\\frac{(1-p)}{n+1} \\\\\\\\; p&lt;1$$\\nwhich $p(0) = p$ and converges to 1. Solving the recurrence relation with this $p(n)$, has the nice analytical form:\\n$$ F(n) = 1 - \\\\\\\\frac{ (1-p)^{n+1} }{ n! } $$\\nConsider $S(n) = 1 - F(n) = \\\\\\\\frac{ (1-p)^{n+1} }{ n! }$. A known stat fact is that \\n$$\\\\\\\\sum_{i=0}^{\\\\\\\\infty} S(i) = E[N]$$\\nwhich, if you remember some calculus, looks a lot like the exponential\\'s Taylor series, hence,\\n$$E[N] = (1-p)e^{(1-p) }$$</li>\\n</ol>\\n',\n",
       " '<p>This is a wee late into the debate, but we have a whole chapter in our 2007 book  <em><a href=\"http://rads.stackoverflow.com/amzn/click/1441915753\" rel=\"nofollow\">Introducing Monte Carlo Methods with R</a></em> dealing with this issue. You can also download <a href=\"http://hosho.ees.hokudai.ac.jp/~kubo/Rdoc/library/coda/html/00Index.html\" rel=\"nofollow\">the CODA package from CRAN</a> to this effect.</p>\\n',\n",
       " \"<p>You can't, unless you know the sample size.  A $\\\\\\\\chi^2$ test is a test of whether observed data are plausibly generated from an underlying population with no association between the two variables.  Unless you know how many data were actually observed you can't make any conclusions about whether they are plausibly from that null-hypothesis distribution.</p>\\n\\n<p>If you do know the total sample size, obviously you can just convert your probabilities to counts and perform the test the usual way.</p>\\n\",\n",
       " '<p>Any model selection procedure will affect the standard errors and this is hardly ever accounted for. For example, prediction intervals are computed conditionally on the estimated model and the parameter estimation and model selection are usually ignored.</p>\\n\\n<p>It should be possible to bootstrap the whole procedure in order to estimate the effect of the model selection process. But remember that time series bootstrapping is trickier than normal bootstrapping because you have to preserve the serial correlation. The block bootstrap is one possible approach although it loses some serial correlation due to the block structure.</p>\\n',\n",
       " \"<p>For my thesis I will need to do an experimental user study to compare two user interfaces which both can perform the same tasks. They differ in how they support you in achieving your task. I'm not that familiar with statistics so please correct me if any of the following assumptions I already made are incorrect.</p>\\n\\n<p>I'm pretty sure that I'll need to use a <strong>balanced within-subjects design</strong>. Reading through several papers on user interfaces seems to indicate that long in-field studies are advised. Due to time limitations, and the very experimental design of the interface this isn't an option.</p>\\n\\n<p>The plan is to do both objective and subjective (questionnaire) measurements. Objective measurements could be e.g. time taken to complete a given task.</p>\\n\\n<p>At first I figured I needed a <strong>paired t-test</strong>. However, I will most likely take multiple measurements and not just one. Some reading mentioned a repeated measure ANOVA for such a scenario, but I'm not sure about this at all.</p>\\n\\n<p>Which test should I use? Which power analysis method should I use? I want to determine how many subjects I need (or can do) and what the power of my test will be.</p>\\n\",\n",
       " \"<p>I am new to Matlab and data visualization in general.\\nI am plotting an A vs. B relation based on 100 values of (A,B) using <code>plot(A,B)</code> which gives me a nice 2-D line plot. Now I want to run 1,000 trials of this and see how the A vs. B relation varies across the trials. (A has a random component and B is dependent on A so the trials will yield different results.)</p>\\n\\n<p>What is the best way in matlab to visualize/analyze this? (I presume drawing 1,000 2-D line plots and looking at them isn't an option.) </p>\\n\",\n",
       " '<p>Did you instruct regsubsets to do a forward selection? The default is \"exhaustive\", I believe.</p>\\n\\n<p>In any case, the collinearities will still cause trouble. Any time regsubsets considers a collection of variables that are too collinear (i.e. the design matrix is practically singular), it will fail.</p>\\n\\n<p>\"Best subset\" methods can be unstable with multiple regression, especially when there are a lot of variables. You might want to try a random forest approach.</p>\\n',\n",
       " '<p>I built a SVM-based classifier against a data set, the precision is about 66% and the recall is about 88%. Generally, what are the options to tune the parameter that can increase the precision?</p>\\n',\n",
       " '<h1>Background and Setting</h1>\\n\\n<p>I have data of this format: on each subject the list of exposure to some subtances, some demographics and then a multiple response (whether the subject developed a disease or not and if so then what type of disease). For example:<br>\\nSubject1 was 15 years old male living in city C; he was exposed to subtances A, B and C and the outcome: subject had skin cancer. <br>\\nSubject2 .... outcome: subject had psoriasis...<br>\\nSubject3 .... outcome: subject was healthy...etc<hr></p>\\n\\n<h1>Question</h1>\\n\\n<p>I would like to be able to make predictions from these data, to estimate what disease a new subject  is likely to develop given his demographics and exposure history.\\nI tried to play around with logistic regression, but the response is not binary, it could be a multitude of outcomes/diseases (including cases of absence of disease). How can i proceed to have my predictions - if at all possible? Thanks</p>\\n',\n",
       " \"<p>I am empirically studying Fama &amp; French three factors (market, SMB, HML - independent variables) model on a Viet Nam stock exchange. The dataset is daily stocks' return during almost 4 years. The stocks are grouped in six portfolios - each is dependent variable in each regression. I would like to test the validity of Fama &amp; French three factor model in explaining excess return of portfolios. Below is regression output of one of the portfolios, with a really high t-statistics. Any problem with that?</p>\\n\\n<pre><code>            Coefficients    Standard Error        t Stat    P-value\\nIntercept       0.00191         0.00031          6.27072    0.00000\\nRPt = Rmt - Rf  0.98782         0.01737         56.85940    0.00000\\nSMB             0.87223         0.02796         31.19128    0.00000\\nHML             0.68983         0.02172         31.76355    0.00000\\n</code></pre>\\n\\n<p>Please let me know if it's unclear.</p>\\n\",\n",
       " '<p>I have a dataset that\\'s nominally 16-dimensional.  I have about 100 samples in one case and about 20,000 in another.  Based on various exploratory analyses I\\'ve conducted using PCA and heat maps, I\\'m convinced that the true dimensionality (i.e. the number of dimensions needed to capture most of the \"signal\") is around 4.  I want to create a slide to that effect for a presentation.  The \"conventional wisdom\" about this data, which I\\'m looking to disprove, is that the true dimensionality is one or two.  </p>\\n\\n<p>What\\'s a good, simple visualization for showing the true dimensionality of a dataset?  Preferably it should be understandable to people who have some background in statistics but are not \"real\" statisticians.</p>\\n',\n",
       " '<p>I ran a Canonical Correlation Analysis on about 845 cases with 1000 variables each. (It originally started with 1000 cases and 400 variables but by using a kernel I got a 1000x1000 matrix)</p>\\n\\n<p>As a result I got mainly eigenvalues very close to 1 (e.g. the highest: 0.9999987616532545), then there were a couple with E-8, five with 0 and some negative ones.</p>\\n\\n<p>I find especially the nearly 1000 1\\'s quite strange or is that a result that is possible/makes sense? I didn\\'t quite get the results that I was hoping for, so I was wondering if all those nearly 1\\'s could be the cause and if anyone has an idea what might have caused those?</p>\\n\\n<p>I use EigenDecomposition from Commons Math 3.0 (<a href=\"http://commons.apache.org/math/apidocs/index.html\" rel=\"nofollow\">http://commons.apache.org/math/apidocs/index.html</a>). \\nI also tried some other matrices earlier and then got \"MaxCountExceededException: illegal state: convergence failed\" error. Not sure if that has anything to do with it, but I thought I\\'d add it.</p>\\n',\n",
       " '<p><code>order(p,d,q)</code> means, that you have an ARIMA(p, d, q) model: $\\\\\\\\phi(B)(1-B)^d X_t=\\\\\\\\theta(B)Z_t$, where $B$ is a lag operator and $\\\\\\\\phi(B)=1-\\\\\\\\phi_1B-\\\\\\\\dots-\\\\\\\\phi_pB^p$ also $\\\\\\\\theta(B)=1+\\\\\\\\theta_1B+\\\\\\\\dots+\\\\\\\\theta_qB^q$.</p>\\n\\n<p>The best way to find <code>p, d, q</code> values in R is to use <code>auto.arima</code> function from <code>library(forecast)</code>. For example, <code>auto.arima(x, ic = \"aic\")</code>. For more information look up <code>?auto.arima</code>.</p>\\n',\n",
       " '<p>Lorenz curve is also known under the name of \"<a href=\"http://mrvar.fdv.uni-lj.si/pub/mz/mz3.1/vuk.pdf\" rel=\"nofollow\">lift curve</a>\" when applied to classification/ranking. For a given range of predicted probability values, the lift represents a multiplicative increase in the positive class\\'s rate (due to a given predictive model) over a random guess.</p>\\n\\n<p><a href=\"http://rocr.bioinf.mpi-sb.mpg.de/ROCR.pdf\" rel=\"nofollow\">rocr package</a> can calculate lift values/curves (the manual also has a concise definition of the lift). The Gini index can be calculated from the area under the lift curve (I typically use cumulative lift value at a given predicted probability threshold instead since it is easier to relate to business metrics) </p>\\n',\n",
       " '<p>The stripchart function in the graphics library seems to be what you want if you want to plot the data 1 dimensionally for each group.  It produces a somewhat basic plot but you can customize it</p>\\n\\n<pre><code>business &lt;- runif(50, min = 65, max = 100)\\nlaw &lt;- runif(50, min = 60, max = 95)\\ndf &lt;- data.frame(group = rep(c(\"Business\", \"Law\"), each = 50), value = c(business, law), stringsAsFactors = F)\\n\\nstripchart(value ~ group, data = df, \\n   main = \"Salary Example (dots)\",\\n   pch = 16,\\n   col = c(\"red\", \"green\"))\\n</code></pre>\\n',\n",
       " '<p>Using pie charts to illustrate relative frequencies. More <a href=\"http://psychology.wikia.com/wiki/Pie_chart\">here</a>.</p>\\n',\n",
       " \"<p>I am trying to set up a zero-inflated poisson model in R and JAGS. I am new to JAGS and I need some guidance on how to do that. </p>\\n\\n<p>I've been trying with the following where y[i] is the observed variable </p>\\n\\n<pre><code>model {\\nfor (i in 1:I) {\\n\\n    y.null[i] &lt;- 0\\n    y.pois[i] ~ dpois(mu[i])\\n    pro[i] &lt;- ilogit(theta[i])\\n    x[i] ~ dbern(pro[i])\\n\\n    y[i] &lt;- step(2*x[i]-1)*y.pois[i] + (1-step(2*x[i]-1))*y.null[i]\\n\\n    log(mu[i]) &lt;- bla + bla +bla + ....\\n    theta[i] &lt;- bla + bla + bla + ....\\n}\\n</code></pre>\\n\\n<p>}</p>\\n\\n<p>However this does not work as you cannot use &lt;- on an observed variable. </p>\\n\\n<p>Any ideas how to change/fix this? Is there an other way to set up the zero-inflated poisson model in JAGS? </p>\\n\",\n",
       " '<p>I conducted a paired samples t-test to analyse the effect of an exercise intervention on blood pressure measurements. The test demonstrated a significant p-value. However, when I have calculated 95% confidence intervals for the mean difference between pre and post measures the result suggested there to be no significant difference between pre and post measures, i.e lower level minus and upper level positive. Is this possible? </p>\\n',\n",
       " '<p>I compiled a <a href=\"http://www.amazon.com/Multivariate-statistics-books/lm/R3312L94GKFZD1/\">list of multivariate books</a> when I was preparing multivariate statistics classes (an elective for Ph.D. students, and an intro for senior undergrads).</p>\\n',\n",
       " '<p>So, I\\'m working with GWAS SNP data and want to perform several tests for association between genotype and phenotype. There are two phenotypes (case and control) and 2 or three genotypes. Most of them are Chi-squared tests with different contingency tables, $2 \\\\\\\\times 2$ or $2 \\\\\\\\times 3$, one of them is the Cochran-Armitage trend test (CATT)</p>\\n\\n<p>Once I have constructed the contingency table, I can easily get a $p$-value using the <a href=\"http://commons.apache.org/math/apidocs/org/apache/commons/math/stat/inference/ChiSquareTest.html\" rel=\"nofollow\">Apache commons math library</a> for the Chi-squared tests. No problem.</p>\\n\\n<p>However, the explanation of the <a href=\"http://en.wikipedia.org/wiki/Cochran-Armitage_test_for_trend\" rel=\"nofollow\">CATT on Wikipedia</a> is not sufficient for me to implement it (my statistics knowledge is limited and I\\'m still learning).</p>\\n\\n<p>Like in the example, I suspect a linear trend, so my weights are $t = (0,1,2)$, which make the formula for $T$ to:\\n$$\\nT \\\\\\\\equiv (N_{12}R_2 - N_{22}R_1) + 2(N_{13}R2 - N_{23}R1)\\n$$\\nand the one for the variance\\n$$\\nVar(T) = {{R_1 R_2} \\\\\\\\over N} ( N(C_2+4C_3) - (C_2 - 2C_3)^2)\\n$$</p>\\n\\n<p>I checked how the program <a href=\"http://pngu.mgh.harvard.edu/~purcell/plink/anal.shtml\" rel=\"nofollow\">PLINK</a> does it, since it\\'s already implemented there, but it differs slightly from the above formulas. The C++ source code there would correspond to this:\\n$$\\nT = {(N_{12}R_2 - N_{22}R_1) + 2(N_{13}R2 - N_{23}R1)\\\\\\\\over N}\\n$$\\nand\\n$$\\nVar(T) = {{R_1 R_2} \\\\\\\\over N} {( N(C_2+4C_3) - (C_2 - 2C_3)^2) \\\\\\\\over N^2}\\n$$</p>\\n\\n<p>Then it does calculates a chi-square value like this\\n$$\\n\\\\\\\\chi^2_{T} = {T^2 \\\\\\\\over Var(T)}\\n$$\\nand calculates the $p$-value like for any other chi-squared value with $df = 1$</p>\\n\\n<p>I don\\'t need to understand the theory completely, as long as my program calculates correctly, but understanding it would give me additional confidence.</p>\\n\\n<p>Is this correct or legitimite? Is this how I\\'ll get the $p$-value? </p>\\n',\n",
       " '<p>Ok, so you can just look at the code by typing the name of the function at the R prompt, or use <code>edit(pheatmap)</code> to see it in your default editor.</p>\\n\\n<p>Around line 14 and 23, you\\'ll see that another function is called for computing the distance matrices (for rows and columns), given a distance function (R <code>dist</code>) and a method (compatible with <code>hclust</code> for hierarchical clustering in R). What does this function do? Use <code>getAnywhere(\"cluster_mat\")</code> to print it on screen, and you soon notice that it does nothing more than returning an <code>hclust</code> object, that is your dendrogram computed from the specified distance and linkage options.</p>\\n\\n<p>So, if you already have your distance matrix, change line 14 (rows) or 23 (columns) so that it reads, e.g.</p>\\n\\n<pre><code>tree_row = hclust(my.dist.mat, method=\"complete\")\\n</code></pre>\\n\\n<p>where <code>my.dist.mat</code> is your own distance function, and <code>complete</code> is one of the many methods available in <code>hclust</code> (see <code>help(hclust)</code>). Here, it is important to use <code>fix(pheatmap)</code> and not <code>edit(pheatmap)</code>; otherwise, the edited function will not be callable in the correct environment/namespace.</p>\\n\\n<p>This is a quick and dirty hack that I would not recommend with larger package. It seems to work for me at least, that is I can use a custom distance matrix with complete linkage for the rows.</p>\\n\\n<p>In sum, assuming your distance matrix is stored in a variable named <code>dd</code>,</p>\\n\\n<pre><code>library(pheatmap)\\nfix(pheatmap)\\n# 1. change the function as you see fit\\n# 2. save and go back to R\\n# 3. if your custom distance matrix was simply read as a matrix, make sure\\n#    it is read as a distance matrix\\nmy.dist.map &lt;- dd  # or as.dist(dd)\\n</code></pre>\\n\\n<p>Then, you can call <code>pheatmap</code> as you did but now it will use the results of <code>hclust</code> applied to <code>my.dist.map</code> with <code>complete</code> linkage. Please note that you just have to ensure that <code>cluster_rows=TRUE</code> (which is the default). Now, you may be able to change</p>\\n\\n<ul>\\n<li>the linkage method</li>\\n<li>choose between rows or columns</li>\\n</ul>\\n\\n<p>by editing the package function appropriately.</p>\\n',\n",
       " '<p>Your approach is one possibility.</p>\\n\\n<p>On the other hand, call center forecasting is an active area of research, and there are many people working in this field. One standard way of doing this is adding additional seasonal components to seasonal Exponential Smoothing, so you have one component for yearly and one component for weekly seasonality. You could even go down to hourly data and add another component for intra-daily seasonality, since calls will arrive in different quantities during the morning, the lunch hour, the afternoon and the night. <a href=\"http://www.sciencedirect.com/science/article/pii/S0169207010000464\">This article, along with the following comments, may help you</a>. Or <a href=\"http://www.jstor.org/discover/10.2307/4101650?uid=3737864&amp;uid=2129&amp;uid=2&amp;uid=70&amp;uid=4&amp;sid=21101497892527\">this one by the same author</a>.</p>\\n\\n<p>I recommend that you google for \"call center forecasting\". Electricity demand has similar issues with intra-weekly and intra-daily seasonality, so searching for keywords along this direction will also help.</p>\\n',\n",
       " \"<p>For fixed sets $A$ the definition of the conditional expectation is the following:</p>\\n\\n<p>$$E(X|A)=\\\\\\\\frac{1}{P(A)}EX1_{A},$$</p>\\n\\n<p>with $1_A(w)$ being indicator function of the set. Now your set $A$ is defined as $A=\\\\\\\\{w\\\\\\\\in \\\\\\\\Omega: X(w)&lt;\\\\\\\\bar x\\\\\\\\}$, so $P(A)=F(\\\\\\\\bar x)$. Plugging this into the formula we get:</p>\\n\\n<p>$$E(X|X&lt;\\\\\\\\bar x)=\\\\\\\\frac{1}{F(\\\\\\\\bar x)}\\\\\\\\int_{-\\\\\\\\infty}^{\\\\\\\\bar x}xf(x)dx$$</p>\\n\\n<p>Note that $\\\\\\\\mu$ does not play any part in this formula, unless $\\\\\\\\bar x$ has special meaning.  I've assumed that bar over $x$ is only your choice of typographical differentiation of different quantities.</p>\\n\",\n",
       " '<p><strong>EDIT:</strong>\\n<em>OK, having spent the last couple of hours checking out some pubs on functional data analysis, I am actually starting to feel a little silly having asked the question in the first place! It\\'s becoming obvious (well. Still a tad -ish but getting there.) - Thanks, @PeterFlom for pointing it out! I\\'ll leave the question up for now, should anyone see something worth looking out for as I venture into FDA with my data...?</em></p>\\n\\n<p>Here\\'s what I have (this is for voice analysis):</p>\\n\\n<p>Suppose I have three speakers; each speaker produces say 10 tokens of a particular vowel; I\\'ve measured each token and fitted time-normalised 3rd order polynomial curves to the vowel formants (I\\'m using three formants, which in layman\\'s terms are the frequencies determining the vowel sound, which change over time; the formants are determined by vocal tract shape, so they are interrelated). I have also calculated the average polynomial function to each formant for each speaker, to give me a general idea of what this vowel looks like for each speaker on average, leaving me with a set of three related polynomial functions (one for each vowel formant) for each of the three speakers.</p>\\n\\n<p>Pictures might help clarify! The first figure shows 10 individual tokens (three time-normalised polynomials (because three formants) per token) for three speakers; the second figure shows the average polynomials for each formant for each speaker. (I\\'ve indicated the frequency range of each formant, hope that makes a little clearer what I\\'m talking about!):</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/91AX1.gif\" alt=\"Polynomials for individual tokens for three speakers\">\\n<img src=\"http://i.stack.imgur.com/RhTAf.gif\" alt=\"Average polynomials for three speakers\"></p>\\n\\n<p>(You can see that the degree of variability is quite massive within a single speaker; but you can still guess from the graphs that speakers 1 &amp; 2 are very similar, at least in terms of these particular vowel tokens (our guess is they\\'re the same speaker), and speaker 3 is someone else (this we happen to know).)</p>\\n\\n<p>To start with, I would be happy if I could find a way that would give me a basic distance measure between the average polynomials for each speaker. (Something to say, but in numerical terms, e.g. that speaker 1 and speaker 2\\'s formants look very similar to each other (at least with regards to the first and second formants), but quite different to speaker 3\\'s formants.)</p>\\n\\n<p>Ideally, I want to be able to compare all the speakers\\' tokens, where each token is described by three polynomial functions, and get a probability to tell me how likely it is that the speakers are the same, based on their productions for this vowel (i.e. taking into account within-speaker variability). But if someone could just help me with my first question I just might be able to figure everything else out from there!</p>\\n\\n<p>Thank you so much!</p>\\n',\n",
       " '<p>I\\'m examining correlations in a data set with a large number of variables but small sample sizes. To get a feel for how these quantities behave, I generated some random data and looked at the distribution of correlations:</p>\\n\\n<pre><code>n = 4\\ny = matrix(rnorm(1000 * n), 1000, n)\\nx = matrix(rnorm(1000 * n), 1000, n)\\np = as.numeric(cor(t(x),t(y)))\\nhist(p)\\n</code></pre>\\n\\n<p>To my surprise, the distribution is almost perfectly uniform:\\n<img src=\"http://i.stack.imgur.com/XbDeb.png\" alt=\"histogram\"></p>\\n\\n<p>Does anyone have an explanation for this phenomenon? It makes some sense in that for n=2 we have either p=1 or p=-1, and as n->infinity the distribution becomes normal, so this distribution falls somewhere inbetween. But why uniform? I\\'m stumped.</p>\\n',\n",
       " \"<p>Does it <em>ever</em> make sense? I don't know. It depends on what you are trying to do. But you say you aren't trying to do anything, so there's no good answer.</p>\\n\\n<p>I can't think of a case where the variance of a bimodal distribution makes much sense. But you say (e.g.) bimodal. If the data are non-normal, but not all that non-normal, variance can make sense. My general rule is that, if the mean makes sense, the variance makes sense. If the median makes sense then either the mean absolute deviation or interquartile range makes sense.  For bimodal distributions, neither the mean nor the median makes all that much sense. A density plot is a good method here, or a table of percentiles. </p>\\n\",\n",
       " '<p>I\\'m the author of the <a href=\"http://cran.r-project.org/package=ez\" rel=\"nofollow\">ez package</a> for R, and I\\'m working on an update to include automatic computation of likelihood ratios (LRs) in the output of ANOVAs. The idea is to provide a LR for each effect that is analogous to the test of that effect that the ANOVA achieves. For example, the LR for main effect represents the comparison of a null model to a model that includes the main effect, the LR for an interaction represents the comparison of a model that includes both component main effects versus a model that includes both main effects <em>and</em> their interaction, etc.</p>\\n\\n<p>Now, my understanding of LR computation comes from <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/15732688\" rel=\"nofollow\">Glover &amp; Dixon</a> (<a href=\"http://www.thinkingaboutthinking.org/wp-content/uploads/2010/09/Glover-Dixon-2004-Likelihood-ratios-a-simple-and-flexible-statistic-for-empirical-psychologists.pdf\" rel=\"nofollow\">PDF</a>), which covers basic computations as well as corrections for complexity, and the appendix to <a href=\"http://rads.stackoverflow.com/amzn/click/0521009138\" rel=\"nofollow\">Bortolussi &amp; Dixon</a> (<a href=\"http://www.thinkingaboutthinking.org/wp-content/uploads/2010/09/Dixon-Unknown-Evaluating-Evidence-Appendix-to-Psychonarratology.pdf\" rel=\"nofollow\">appendix PDF</a>), which covers computations involving repeated-measures variables. To test my understanding, I developed <a href=\"https://spreadsheets.google.com/ccc?key=0Ap2N_aeyRMGHdHJxUnVNeEl5VGtvY1RVLVc5UjU4Vmc&amp;hl=en\" rel=\"nofollow\">this spreadsheet</a>, which takes the dfs &amp; SSs from an example ANOVA (generated from a 2*2*3*4 design using fake data) and steps through the computation of the LR for each effect.</p>\\n\\n<p>I would really appreciate it if someone with a little more confidence with such computation could take a look and make sure I did everything correctly. For those that prefer abstract code, <a href=\"http://gist.github.com/590789\" rel=\"nofollow\">here is the R code</a> implementing the update to ezANOVA() (see esp. lines 15-95).</p>\\n',\n",
       " '<p>Multilevel analysis based on aggregate data leads to ecological fallacy.  Here is an interesting paper to read:  <a href=\"http://www.stanford.edu/class/ed260/freedman549.pdf\" rel=\"nofollow\">http://www.stanford.edu/class/ed260/freedman549.pdf</a></p>\\n',\n",
       " '<p>A naive Bayes classifier, as the names suggests, is a simple application of <a href=\"http://en.wikipedia.org/wiki/Bayes%27_theorem\" rel=\"nofollow\">Bayes\\' Theorem</a>. Basically, it calculates the probabilities of quantities of interest (generally unobserved, called parameters or latent classes) based on the observed data. In your case the observed data are: news, football, and tennis. The quantities of interest for which you want to calculate the probabilities are: News and Sports. Seems like you are interested in calculating: $P(\\\\\\\\text{News}|\\\\\\\\text{news}, \\\\\\\\text{football}, \\\\\\\\text{tennis}), P(\\\\\\\\text{News}|\\\\\\\\text{news}, \\\\\\\\text{football}, \\\\\\\\text{tennis})$.</p>\\n\\n<p>Now we will use Bayes theorem to get:</p>\\n\\n<p>$$\\\\\\\\nP(\\\\\\\\text{News}|\\\\\\\\text{news}, \\\\\\\\text{football}, \\\\\\\\text{tennis}) = \\\\\\\\frac{P(\\\\\\\\text{news}, \\\\\\\\text{football}, \\\\\\\\text{tennis}|\\\\\\\\text{News})P(\\\\\\\\text{News})}{P(\\\\\\\\text{news}, \\\\\\\\text{football}, \\\\\\\\text{tennis})}\\\\\\\\n$$\\nThe first term in the numerator is calculated using the fact that given you observe the latent class, that is, News, the observed data, that is news, football, and tennis probabilities are independent (<strong>this may be a questionable assumption, but the answer depends on subject matter</strong>). You can use the law for calculating the probabilties of independent event.<br>\\n$$\\\\\\\\nP(\\\\\\\\text{news}, \\\\\\\\text{football}, \\\\\\\\text{tennis}|\\\\\\\\text{News})=P(\\\\\\\\text{news}|\\\\\\\\text{News})P( \\\\\\\\text{football}|\\\\\\\\text{News})P(\\\\\\\\text{tennis}|\\\\\\\\text{News})\\\\\\\\n$$</p>\\n\\n<p>Proceeding similarly for Sports, we get:</p>\\n\\n<p>$$\\\\\\\\nP(\\\\\\\\text{Sports}|\\\\\\\\text{news}, \\\\\\\\text{football}, \\\\\\\\text{tennis}) = \\\\\\\\frac{P(\\\\\\\\text{news}, \\\\\\\\text{football}, \\\\\\\\text{tennis}|\\\\\\\\text{Sports})P(\\\\\\\\text{Sports})}{P(\\\\\\\\text{news}, \\\\\\\\text{football}, \\\\\\\\text{tennis})} \\\\\\\\n$$\\n$$\\\\\\\\nP(\\\\\\\\text{news}, \\\\\\\\text{football}, \\\\\\\\text{tennis}|\\\\\\\\text{Sports})=P(\\\\\\\\text{news}|\\\\\\\\text{Sports})P( \\\\\\\\text{football}|\\\\\\\\text{Sports})P(\\\\\\\\text{tennis}|\\\\\\\\text{Sports})\\\\\\\\n$$</p>\\n\\n<p>The denominator for both cases can be calculated by using the <a href=\"http://en.wikipedia.org/wiki/Law_of_total_probability\" rel=\"nofollow\">Law of total probability</a>. </p>\\n\\n<p>$$\\\\\\\\nP(\\\\\\\\text{news}, \\\\\\\\text{football}, \\\\\\\\text{tennis}) =P(\\\\\\\\text{news}, \\\\\\\\text{football}, \\\\\\\\text{tennis}|\\\\\\\\text{News})P(\\\\\\\\text{News})+ P(\\\\\\\\text{news}, \\\\\\\\text{football}, \\\\\\\\text{tennis}|\\\\\\\\text{Sports})P(\\\\\\\\text{Sports}) \\\\\\\\n$$ </p>\\n\\n<p>We are now left with only one probability in each case, that is,$P(\\\\\\\\text{News})$ and $P(\\\\\\\\text{Sports})$, respectively. If we know these, every probability until now can be calculated. This can be determined based on prior knowledge, or in your case it might be already provided to you. </p>\\n\\n<p>Plugging in all the probabilities gives you the probabilities of interest.  </p>\\n\\n<p>A high probability value for a specific class implies that the observed document belongs to that class (News or Sports). But how do you decided \"how high is high\", depends, again, on subject matter and a lot of other issues. </p>\\n',\n",
       " \"<p>Suppose I use K-S to figure out if the CDF of $X$ is greater than the CDF of $Y$. I get the statistic $D^+ = \\\\\\\\max_u\\\\\\\\{C_x(u) - C_y(u)\\\\\\\\}$ where $C_x$ is the ECDF for $X$ and similarly for $C_y$.</p>\\n\\n<p>There are various tables which translate this into a p-value. Intuitively, if I'm testing if $C_x &lt; C_y$ it seems like the null hypothesis should be $C_x \\\\\\\\geq C_y$. (If it were $C_x = C_y$ then I don't understand how it would differ from a two-sided test.)</p>\\n\\n<p>Yet for my data set I find low p-values for both the proposition that $C_x &lt; C_y$ and $C_x &gt; C_y$, leading me to believe that the null hypothesis is not the complement of the alternative.</p>\\n\\n<p>Am I completely misunderstanding something?</p>\\n\",\n",
       " '<p>Like this:</p>\\n\\n<pre><code>fit = lm(data ~ .,data = data)\\nse &lt;- sqrt(diag(vcov(fit)))\\n</code></pre>\\n\\n<p>These are the classical asymptotic ones you see in <code>summary</code>.  Please also see the links in my answer to <a href=\"http://stats.stackexchange.com/questions/26650/how-do-i-reference-a-regression-models-coefficients-standard-errors\">this same question</a> about alternative standard error options.</p>\\n',\n",
       " '<p>I think that you might be confusing an extra-sum-of-squares F-test with a likelihood ratio test. Although, both are used to compare two models.</p>\\n\\n<p>A likelihood ratio statistic, denoted by $\\\\\\\\Lambda$, is given by </p>\\n\\n<p>$$\\\\\\\\Lambda = \\\\\\\\frac{L\\\\\\\\text{(reduced model})}{L(\\\\\\\\text{full model})}$$</p>\\n\\n<p>Taking $-2\\\\\\\\log\\\\\\\\Lambda$ produces a statistic that has $\\\\\\\\chi^2_{d.f(\\\\\\\\text{reduced model})-d.f(\\\\\\\\text{full model})}$ distribution. That is to say that taking $-2\\\\\\\\log$ of the $\\\\\\\\Lambda$ gives you a $\\\\\\\\chi^2$ distribution. </p>\\n\\n<p>I have not used SAS so I cannot comment on the output, but I hope that I have been able to answer your question.</p>\\n\\n<p>Note: that $\\\\\\\\Lambda$ is equivalent to your L</p>\\n\\n<hr>\\n\\n<p>Janne: For linear regression you could use either the likelihood ratio test or the extra-sum-squares F-test and you should end up with the same p-value. Despite, this they are not the same thing. </p>\\n\\n<p>As has been mentioned above the likelihood ratio test produces a statistic that has $\\\\\\\\chi^2_{d.f(\\\\\\\\text{reduced model})-d.f(\\\\\\\\text{full model})}$ distribution. Where as an extra-sum-of-squares F-test, given by</p>\\n\\n<p>$$F = \\\\\\\\frac{(SSR_{\\\\\\\\text{reduced model}}-SSR_{\\\\\\\\text{full model}})/d.f_{\\\\\\\\text{reduced model}} - d.f_{\\\\\\\\text{full model}}}{\\\\\\\\hat{\\\\\\\\sigma}^2_\\\\\\\\text{full model}}$$</p>\\n\\n<p>producing a statistic that has $F_{d.f(\\\\\\\\text{reduced model})-d.f(\\\\\\\\text{full model}),d.f(\\\\\\\\text{full model})}$ distribution.\\nWhere SSR is the sum of squared residuals and $\\\\\\\\hat{\\\\\\\\sigma}^2$ is our standard estimate.</p>\\n',\n",
       " '<p>As far as I understand your question, you want to investigate if missing values in your data appear due to some pattern. In this case, you don\\'t need any \"missing value analysis\" -- this is the same problem as checking whether the score is bigger than 0.7 or whatever. Just convert your dataset into two-class factor (missing, not-missing) and look for correlations.</p>\\n',\n",
       " '<p>The experiment has $N$ observations $N_i$ which are Poisson Distributed random variables (i.e. we have histogram with $N$ bins. The width of bins is not fixed so I can adjust it  ) . I have two hypothesis.The null hypothesis predicts $\\\\\\\\lambda_i^{null}$ values of corresponding parameters of Poisson Distribution, alternative  predicts $\\\\\\\\lambda_i^{alt}$. The more data we collect, the better confidence level of excluding of alternative hypothesis we get. But how to calculate this confidence level as a function of size of data we collect? In other words I would like to estimate how many data should I collect to exclude alternative hypothesis at specified confidence level. </p>\\n\\n<p>Firstly, I thought about using chi-squared test and calculate the expected value of chi-square i.e. $\\\\\\\\left&lt;\\\\\\\\chi^2\\\\\\\\right&gt;$. But when I tried different width of bins to obtain the optimal binning, I discovered that the less bins we use the better result we get. So I think it\\'s not the optimal test for this task because we don\\'t fully use information from predictions when we have just a few bins.</p>\\n\\n<p>I would like to know which tests are more appropriate for such problems. \\nI would also be grateful for any kind of advice regarding the literature on this subject.</p>\\n\\n<p>P.S. I\\'ve <a href=\"http://physics.stackexchange.com/questions/23251/what-statistical-test-should-i-use\">asked</a> the same question at physics.stackexchange, but with some physical details. </p>\\n',\n",
       " '<p>Just from reading the link, it seems to me that there is probably some packages that do what you want in the machine learning view on CRAN <a href=\"http://cran.r-project.org/web/views/MachineLearning.html\" rel=\"nofollow\">http://cran.r-project.org/web/views/MachineLearning.html</a></p>\\n\\n<p>Hope this helps.</p>\\n',\n",
       " \"<p>I was wondering how the Bayesians in the CrossValidated community view the problem of <strong>model uncertainty</strong> and how they prefer to deal with it? I will try to pose my question in two parts:</p>\\n\\n<ol>\\n<li><p>How important (in your experience / opinion) is dealing with model uncertainty? I haven't found any papers dealing with this issue in the machine learning community, so I'm just wondering why.</p></li>\\n<li><p>What are the common approaches for handling model uncertainty (bonus points if you provide references)? I've heard of Bayesian model averaging, though I am not familiar with the specific techniques / limitations of this approach. What are some others and why do you prefer one over another?</p></li>\\n</ol>\\n\",\n",
       " '<p>In this case Poisson is good approximation for distribution for number of cases.\\nThere is simple formula to approximate variance of log RR (delta method) .</p>\\n\\n<p>log RR = 10/3 = 1.2, \\nse log RR = sqrt(1/3+1/10) = 0.66, so 95%CI = (-0.09; 2.5)</p>\\n\\n<p>It is not significant difference at 0.05 level using two-sided test.\\nLR based Chi-square test for Poisson model gives p=0.046 and Wald test p=0.067.\\nThis results are similar to Pearson Chi-square test without continuity correction (Chi2 with correction p=0.096).\\nAnother possibility is chisq.test with option simulate.p.value=T, in this case p=0.092 (for 100 000 simulations).</p>\\n\\n<p>In this case test statistics is rather discrete, so Fisher test can be conservative.\\nThere is some evidence that difference can be significant. Before final conclusion data collecting process should be taken into account.</p>\\n',\n",
       " '<p>Python has a wide range of ML libraries (check out mloss.org as well). However, I always have the feeling that it\\'s more of use for ml researchers than for ml practitioners.</p>\\n\\n<p><a href=\"http://scipy.org\">Numpy/SciPy</a> and <a href=\"http://matplotlib.sf.net\">matplotlib</a> are excellent tools for scientific work with Python. If you are not afraid to hack in most of the math formulas yourself, you will not be disappointed. Also, it is very easy to use the GPU with <a href=\"http://code.google.com/p/cudamat/\">cudamat</a> or <a href=\"http://www.cs.toronto.edu/~tijmen/gnumpy.html\">gnumpy</a> - experiments that took days before are now completed in hours or even minutes.</p>\\n\\n<p>The latest kid on the block is probably <a href=\"http://www.google.de/search?sourceid=chrome&amp;ie=UTF-8&amp;q=theano%20python\">Theano</a>. It is a symbolic language for mathematical expressions that comes with opmitimzations, GPU implementations and the über-feature automatic differentiation which is nothing short of awesome for gradient based methods.</p>\\n\\n<p>Also, as far as I know the NLTK mentioned by JMS is basically the number one open source natural language library out there.</p>\\n\\n<p>Python is the right tool for machine learning.</p>\\n',\n",
       " '<p>I am classifying different texts and I wondering about some features that are highly correlated. I have 49 features. Some features are absolute counters (integers) but most features are relative counters(float between 0-1).\\nI am running F-score (univariate) and I am getting the following three features with the highest scores: 1-fourth root of number of word forms, 2- number of word forms and 3-number of sentences. \\nI am running a feature ranking based on extremely randomized trees (scikit-learn ensemble forests) and I am getting exactly the same three features as the highest ranking features. The ranking based on randomized trees is using bootstrapping and GINI.\\nIn the F-scores results I can understand that highly correlated features may have the highest ranking because it is univariate based (measure only one feature at the time).\\nIn the random tree ranking I was expecting that only one of features related to the \"length of the text\" should have a high rank and the others should have a lower and the correlation problem should be solved. But the results are not according to my expectations. I must be doing something wrong! Could it be realted to the fact that all three features are integer counters (values in 1000-range) and the other features are relative counters(0-1). But as I undertand ranking based on random trees should be able to handle large discrepancies between the features. \\nMy question is how should I handle this issue. Should I discard some features? How can I find out the best feature that characterizes the text length??? Any help here is appreciated! </p>\\n',\n",
       " '<p>I think you want either A) a survival analysis with time-varying covariates. The dependent variable is then \"time to nesting\" and the covariate is \"amount of rain\" or B) a survival analysis where the dependent variable is \"rainfall to nesting\". Which one would depend on whether time also is of interest (I\\'m guessing it is, but it\\'s your field). Cox proportional hazards would probably be a good choice of survival models. </p>\\n\\n<p>I would start, though, with some graphs. If you don\\'t have a great many birds, you could graph each one\\'s behavior. If you do have many (more than, say, about 30) then the standard plots from survival analysis would be good. </p>\\n\\n<p>What software are you using? </p>\\n',\n",
       " '<p>Am I looking for a better behaved distribution for the independent variable in question, or to reduce the effect of outliers, or something else?</p>\\n',\n",
       " '<p>It\\'s hard to go past R for graphics. You could do what you want in 3 lines. For example, assuming the csv file has four columns:</p>\\n\\n<pre><code>x &lt;- read.csv(\"file.csv\")\\nmatplot(x[,1],x[,2:4],type=\"l\",col=1:3)\\nlegend(\"topleft\",legend=c(\"A\",\"B\",\"C\"),lty=1,col=1:3)\\n</code></pre>\\n',\n",
       " '<p>I loathe these sort of cut-off-based rules. I think it depends on design and what your <em>a priori</em> hypotheses and expectations were. If you expecting the outcome to vary with time then I\\'d say you should keep time in, as you would for any other \\'blocking\\' factor. On the other hand, if you were replicating the same experiments at different times and had no reason to think the outcome would vary with time but wished to check this was the case, then having done so and found little or no evidence for it varying with time, i\\'d say it\\'s quite entirely reasonable to then drop time. </p>\\n\\n<p>I\\'ve never heard of Underwood before. It may be a standard text for \\'Experiments in Ecology\\' (the book\\'s title), but there\\'s no obvious reason that experiments in ecology should be treated any differently from any other experiments in this respect, so to view it as \"<em>the</em> standard text on this issue\" seems unjustified.</p>\\n',\n",
       " '<p>I have problem on generating correlation matrices using Wishart distribution. I read some articles about Wishart distribution, and it turns out that Wishart distribution is commonly used to generate covariance matrices. Is it possible to generate correlation matrices using Wishart distribution? Any information about useful sources about it?</p>\\n',\n",
       " '<p>With Latin hypercube samples, you have to decide on the number of samples, so that you break your range into either 10 or 20 bins to begin with. Otherwise, you will likely miss some parts of your space. My understanding is that the only quasi-Monte Carlo method that allows to take the next sample (or next $N$ samples) easily without trying to figure out their dependence on the previously collected samples is <a href=\"http://en.wikipedia.org/wiki/Halton_sequence\" rel=\"nofollow\">Halton sequence</a>. See the encompassing treatment in <a href=\"http://rads.stackoverflow.com/amzn/click/0898712955\" rel=\"nofollow\">Niederreiter (1992)</a>.</p>\\n\\n<p>In fact, as far as I recall from computational physics literature (most likely, it was in <a href=\"http://dx.doi.org/10.1006/jcph.1995.1209\" rel=\"nofollow\">Morokoff and Caflisch (1995)</a>), for the sequence of length up to about $6^d$ where $d$ is the dimension of your space, quasi Monte Carlo sequences do not show appreciable gains over the standard pseudo-random number generators. So you may not have to bother with LHC and agonize over the choice between 10 and 20 samples -- you can just start with any random number generator you have at hand, and keep adding new ones if you are not satisfied with the achieved precision.</p>\\n',\n",
       " '<p>Is your question materially focussed on the hospital element? Or are you really interested in the age group element, and it just so happens that the hospitals are providing a sampling framework? </p>\\n\\n<p>[EDIT: after writing this answer, I just saw the comment by whuber about \"Grouping hospitals is tantamount to an assumption of homogeneity that ought to be checked.\" which is what I\\'m suggesting you address here.]</p>\\n\\n<p>If the latter is the case, then you might consider taking an approach that is based around having clustered data, wherein the results from individual clusters (hospitals) are then combined to come up with an \"overall\" pattern of results. </p>\\n\\n<p>Before I waffle on about clustering, given the huge number of hospitals you have these following methods may not majorly influence the results obtained compared to ignoring hospital (pooling results across all hospitals by age group, and then analysing as though all from a single sample.) This approach would in part depend on the relative sizes of the different hospitals (e.g. if there is a hospital serving a massive population that has quite different practices to other hospitals, then this hospital will dominate the results obtained).</p>\\n\\n<p>Conceptually you could think of this as being a meta-analysis of the age -> treatment relationship, where each hospital is treated as a unique \"study\".</p>\\n\\n<p>In practice, one could:</p>\\n\\n<ol>\\n<li><p>run some kind of multilevel model for this, </p></li>\\n<li><p>use tools designed for analysing clustered survey data (and specifying hospital as a cluster unit, with the regression model then looking at the age -> treatment decision relationship,) </p></li>\\n<li><p>calculate summary statistics within each cluster (hospital) -- for example, odds ratios between age groups -- and then use the mean and standard deviation of these odds ratios across all 800 hospitals to construct a confidence interval for the odds ratios. One could similarly calculate proportions per age group per hospital, then use similar methods to get some descriptive statistics around the proportions.</p></li>\\n</ol>\\n\\n<p>The following link (from Martin Bland) talks about randomised controlled trials, but most of the principles still apply to observational data:</p>\\n\\n<p><a href=\"http://www-users.york.ac.uk/~mb55/talks/clusml.htm\" rel=\"nofollow\">http://www-users.york.ac.uk/~mb55/talks/clusml.htm</a></p>\\n\\n<p>Two outstanding points: You\\'ll probably still run into problems with zero-counts or sparse cells in each hospital/age-group combination if your actual data look like the example (you will have odds ratios comparing groups on a per-hospital basis that are zero or infinity). There is also a second issue around treating the given/contraindicated/refused as independent outcomes -- I might have to pass on addressing this part for the moment.</p>\\n',\n",
       " \"<p>Firstly, I don't think it makes sense to say $x_{max}=f^{-1}(y_{max})$ here, that's like implying that it's a one-to-one function although $x_{max}$ is explained by other unobserved variables.</p>\\n\\n<p>Secondly, it really depends on the context for which one to treat as an independent or dependent variable. From my experience, unless theory strongly suggests one way; either way is ok. From your comments on Oct 7, it seems like $x$ is the dependent while $y$ is the independent.</p>\\n\\n<p>If possible, look at the residuals and see if you can squeeze anything out of it. There could be another variable that you forgot; or it may help to transform your variables.</p>\\n\",\n",
       " '<p>There are three types of statisticians;</p>\\n\\n<ol>\\n<li>those that (prefer to) work with real data,</li>\\n<li>those that (prefer to) work with simulated data,</li>\\n<li>those that (prefer to) work with the symbol $X$.</li>\\n</ol>\\n\\n<p>math stat types would be (3). Typically, type \\n(1) statisticians have some prefix attached to \\nmake clear the <em>source</em> of the data they work with \\n(biostatistics, econometrics, psychometrics,....) \\nbecause these fields have implicit shared \\nassumptions about the data they use and some commonly accepted ordering\\nof the plausibility of these assumptions.  </p>\\n',\n",
       " \"<p>I'm heavily into role-playing systems, scripted a set of utilities in ruby for pen-and-paper games and I sort of understood statistics when I took it, but I could never for the life of me figure out the following:</p>\\n\\n<p>Given a varying-length series of independent 50/50 probabilities, i.e. 2, 3, 5 or 7 coin tosses, would the total number of successes (heads) have the same probability distribution as a fair roll of a die with number of sides equivalent to the number of coins + 1 (given that coins can produce all failures -- a zero)?</p>\\n\\n<p>This is a comparison, say, between the d20 system used by DnD and the multiple success-check dice used by Abberant.  So in other words, would rolling a 20-sided die and flipping 20 coins and counting the heads result in the same probability distribution?</p>\\n\\n<p>I would be interested in a link that explains and visually represents the difference between distributions of independent events and a single event.</p>\\n\\n<p>EDIT:  I was able to satisfy my basic curiosity with the above script, modified.  No hard mathematical answer, but I'm not well-versed in this area anyway.  Figured it was worth learning a bit though.</p>\\n\",\n",
       " '<p>What you are looking for is called \"model ensembling\". A simple introductory tutorial with R code can be found here:\\n<a href=\"http://viksalgorithms.blogspot.jp/2012/01/intro-to-ensemble-learning-in-r.html\">http://viksalgorithms.blogspot.jp/2012/01/intro-to-ensemble-learning-in-r.html</a></p>\\n',\n",
       " \"<p>What's the distribution of a symmetric distribution? Someone told me that a random variable $X$ came from a symmetric distribution if and only if $X$ and $-X$ has the same distribution. But I think this definition is partly true. Because I can present a counterexample $X\\\\\\\\sim N(\\\\\\\\mu,\\\\\\\\sigma^{2})$ and $\\\\\\\\mu\\\\\\\\neq0$. \\nObviously, it has a symmetric distribution, but $X$ and $-X$ have different distribution!\\nAm I right? Do you guys ever think about this question? What's the exact definition of symmetric distribution?</p>\\n\",\n",
       " '<p>I had asked a general question about conditional inference trees via party a while <a href=\"http://stats.stackexchange.com/questions/12140/conditional-inference-trees-vs-traditional-decision-trees\">back</a> and gotten a great reply.</p>\\n\\n<p>I am revisiting this procedure and trying to make sense of the linear statistic that is being used (Hothorn et al., Unbiased Recursive Partitioning: A Conditional Inference Framework, Research Report Series, 2004, <a href=\"http://epub.wu.ac.at/676/1/document.pdf\" rel=\"nofollow\">page 4, equation (1)</a>).</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/g5uWU.jpg\" alt=\"enter image description here\"></p>\\n\\n<p>I am not at all clear how this statistic is calculated. Can anyone help?</p>\\n\\n<p>Here is what it seems to me that if</p>\\n\\n<ul>\\n<li>$g(\\\\\\\\cdot)$ and $h(\\\\\\\\cdot)$ are the identity functions</li>\\n<li>and there is a predictor $x$ and a response $y$, both numeric, the statistic is simply a scalar. This is not what is shown, so I am wrong :)</li>\\n</ul>\\n\\n<p>Example data:</p>\\n\\n<pre><code>x&lt;-c(1,3,4,67,32,23,3,12,4)\\ny&lt;-c(43,23,45,22,12,465,6,54,3)\\nw&lt;-rep(1,9)\\n\\nT&lt;-0\\nfor (i in 1:length(y))\\n{\\n\\n  T&lt;-T+(w[i]*x[i]*y[i])  \\n\\n}\\nT #13523\\n</code></pre>\\n\\n<p>I <em>THINK</em> the result needs to be a vector of length 81. </p>\\n',\n",
       " '<p>I am using the estimated county-level poverty measure from the Small Area Income and Poverty Estimates <a href=\"http://www.census.gov/did/www/saipe/\" rel=\"nofollow\">(SAIPE)</a> as the dependent variable in a regression analysis. This value is itself the result of a model and comes complete with upper and lower 90% confidence interval bounds. To be clear, each of my 3000+ observations has an estimated value and its own confidence interval based on the sample size for that county (targeted at 2.5% of population)</p>\\n\\n<p>I am wondering about the best way to incorporate the uncertainty in my dependent variable into my regression model.</p>\\n\\n<p>One way I have imagined is doing a random draw from a distribution using the estimated value and confidence interval for each observation. I would then re-run my regression using this simulated value for the dependent variable in my regression analysis and compare model outcomes to the model using the estimated value. By simulating new values and comparing many times I would gain an understanding of how sensitive my findings are to the estimates on the dependent variable.</p>\\n\\n<p>A key component of this is pulling a random number based on the estimated value and the upper and lower confidence intervals. The data is left and right censored at 0 and 100 and the estimated value is not centered within the confidence interval that is: abs(estimate-cl) != abs(estimate-cu)</p>\\n\\n<p>I was intrigued by the discussion here: <a href=\"http://stats.stackexchange.com/questions/12742/sampling-random-numbers-from-a-distribution-with-asymmetric-confidence-intervals\">Sampling random numbers from a distribution with asymmetric confidence intervals generated by a bootstrapped estimate</a></p>\\n\\n<p>but a modified version of the code just generates the estimated value\\nHere is an example using the first 6 records from the 2008 SAIPE</p>\\n\\n<pre><code>sample.size&lt;-c(1258.850,4405.300,745.900,539.725,1444.850,273.02)\\nPOV08L90&lt;-c(8.77,8.24,18.08,13.90,10.55,22.45)\\nPOV08H90&lt;-c(12.54,11.18,24.82,20.87,15.27,33.83)\\nPOV08&lt;-c(10.7,9.9,24.5,18.5,13.1,33.6)\\ntest.data&lt;-data.frame(sample.size,POV08L90,POV08H90,POV08)\\n\\ngammaGenerate&lt;-function(dat){\\n  for(i in 1:length(dat$sample.size)){\\n    n&lt;-dat[i,\"sample.size\"]\\n    cl&lt;-dat[i,\"POV08L90\"]\\n    cu&lt;-dat[i,\"POV08H90\"]\\n    barx&lt;-dat[i,\"POV08\"]\\n    talpha = qt(p=0.95,df=n-1)\\n    s = (cu - cl)*sqrt(n)/(2*talpha)\\n    kappa = 6*s*s*n*( cl - barx + talpha*s/sqrt(n) )\\n    gamma.shape = 4/(kappa*kappa)\\n    gamma.scale = s/sqrt(gamma.shape)\\n    gamma.shift = barx - gamma.shape*gamma.scale\\n    print(c(barx,(rgamma(n = 5, shape = gamma.shape) + gamma.shift)))    \\n  }\\n}\\ngammaGenerate(test.data)\\n</code></pre>\\n\\n<p>Any help you can offer--either directing me to a better method of dealing with the uncertainty in my dependent variable, or an explanation for why my rgamma always lands at 0 would be very welcome.</p>\\n',\n",
       " \"<p>I have 2 groups, each containing 20 individuals. For each individual I have a dissimilarity matrix of a number of judged items. I'd like to evaluate whether there is an overall difference between the groups in the semantic networks underlying their judgements. As a first step, I computed 2D MDS spaces for each group separately using the <code>smacofIndDiff()</code> function from the <code>smacof</code> package in R, which I understand permits one to characterize the overall space for a group of individuals who may vary slightly from one to another. This yields two 2D spaces (really, two 2xK matricies, where K is the number of items evaluated for dissimilarity), but I'm not sure how to proceed with regards to evaluating the degree to which the resulting spaces differ and whether this difference might reasonably be expected by chance.</p>\\n\\n<p>Any suggestions? Note that I'm not wed to the 2D MDS idea, so feel free to suggest alternative ways to represent the judgement data as a semantic space.</p>\\n\",\n",
       " \"<p>I have a simple random sample (SRS) $S$ of size $n$ drawn from a population $D_1$ containing $N_1$ members.  A new set $D_2$ with $N_2$ members is added to the population.  I would like to create a new set $S&#39;$ that is a SRS from $D_1 \\\\\\\\cup D_2$. </p>\\n\\n<p>One strategy that won't work is to draw a SRS of size $nN_2/N_1$ (or any other size) from $D_2$ and add is to $S$. (The result would be a stratified random sample from $D1 \\\\\\\\cup D2$, but not a SRS from it.) </p>\\n\\n<p>A strategy that will work is to first make one trial on a random variable with a hypergeometric distribution (population size $N_1+N_2$, number of draws $n&#39;$, and number of success states in the population $N_1$).  The resulting value $n&#39;_{1}$ is the number of items in $S&#39;$ that should be taken from $D_1$.  If $n&#39;_{1} &lt; n$, we discard a SRS of size $n - n&#39;_{1}$ from $S$ to form the initial $S&#39;$.  If $n&#39;_{1} &gt; n$, we select an additional SRS of size $n&#39;_{1} - n$ from $D_1 \\\\\\\\setminus S$, and put these items, plus all items from $S$ in $S&#39;$.  We also draw an SRS of size $n&#39;_{2} = n&#39; - n&#39;_{1}$ from $D_2$ and add it to $S&#39;$.   </p>\\n\\n<p>I'm interested in choosing $n&#39;$ to minimize $E[|S&#39; \\\\\\\\setminus S|]$, the expected size of $S&#39; \\\\\\\\setminus S$.  In other words, the expected value of the number of items from $D_2$ plus the number of new items, if any, from $D_1$.  </p>\\n\\n<p>I have three questions: </p>\\n\\n<p>Q1. If we have the additional constraint that $n&#39; \\\\\\\\ge j$ for some $j$, is there a simple argument that the optimal $n&#39;$ is $j$?  Seems like it has to be, but I'd rather not work out the expected value formula and slog through the derivatives.</p>\\n\\n<p>Q2. Does anyone know of a better algorithm for the above problem? </p>\\n\\n<p>Q3. The problem I'd <em>really</em> like to solve is more complex. Each item in $D_1$ and $D_2$ is either an A or a B.  $S$ has at least $k$ A's, and I want to minimize $E[|S&#39; \\\\\\\\setminus S|]$ subject to the constraint that $S&#39;$ has at least $k$ A's.  The above algorithm no longer works, because we can't pick $n&#39;$ in advance. (We don't know how rich $D_2$ is in A's, and we might get unlucky anyway.)  Is there a simple algorithm for this case?  I've perused </p>\\n\\n<p>Thompson, S. K. and Seber, G. A. G.  <em>Adaptive Sampling</em>. Wiley, New York, 1996 </p>\\n\\n<p>but can't find exactly this problem treated.  But my gut tells me that both the above problems have been faced and solved many times before. </p>\\n\",\n",
       " '<p>Here are some online ressources I found interesting without going into detail (and I\\'m not a specialist of this topic):</p>\\n\\n<ul>\\n<li><a href=\"http://www.cse.buffalo.edu/faculty/mbeal/papers/hdp.pdf\">Hierarchical Dirichlet Processes</a>, by Teh et al. (2005)</li>\\n<li><a href=\"http://www.cs.cmu.edu/~kbe/dp_tutorial.pdf\">Dirichlet Processes A gentle tutorial</a>, by El-Arini (2008)</li>\\n<li><a href=\"http://www.mit.edu/~9.520/spring10/Classes/class21_dirichlet_2010.pdf\">Bayesian Nonparametrics</a>, by Rosasco (2010)</li>\\n<li><a href=\"http://learning.eng.cam.ac.uk/zoubin/talks/uai05tutorial-b.pdf\">Non-parametric Bayesian Methods</a>, by Ghahramani (2005)</li>\\n</ul>\\n\\n<p>The definitive reference seems to be</p>\\n\\n<blockquote>\\n  <p>N. Hjort, C. Holmes, P. Müller, and S.\\n  Walker, editors. <em><a href=\"http://www.cambridge.org/gb/knowledge/isbn/item2707877/?site_locale=en_GB\">Bayesian\\n  Nonparametrics</a></em>. Number 28 in\\n  Cambridge Series in Statistical and\\n  Probabilistic Mathematics. Cambridge\\n  University Press, 2010.</p>\\n</blockquote>\\n\\n<p>About R, there seems to be some other packages worth to explore if the <a href=\"http://cran.r-project.org/web/packages/DPpackage/index.html\">DPpackage</a> does not suit your needs, e.g. <a href=\"http://cran.r-project.org/web/packages/dpmixsim/\">dpmixsim</a>, <a href=\"http://www.bioconductor.org/help/bioc-views/release/bioc/html/BHC.html\">BHC</a>, or <a href=\"http://www.stat.washington.edu/hoff/Code/MBSC/\">mbsc</a> found on <a href=\"http://www.rseek.org\">Rseek.org</a>.</p>\\n',\n",
       " '<p>Following up to Steve\\'s reply, there is a much faster way in data.table :</p>\\n\\n<pre><code>&gt; # Preamble\\n&gt; dx &lt;- data.frame(\\n+     ID = sort(sample(1:7000, 400000, TRUE))\\n+     , AGE = sample(18:65, 400000, TRUE)\\n+     , FEM = sample(0:1, 400000, TRUE)\\n+ )\\n&gt; dxt &lt;- data.table(dx, key=\\'ID\\')\\n\\n&gt; # fast self join\\n&gt; system.time(ans2&lt;-dxt[J(unique(ID)),mult=\"first\"])\\n user  system elapsed \\n0.048   0.016   0.064\\n\\n&gt; # slower using .SD\\n&gt; system.time(ans1&lt;-dxt[, .SD[1], by=ID])\\n  user  system elapsed \\n14.209   0.012  14.281 \\n\\n&gt; mapply(identical,ans1,ans2)  # ans1 is keyed but ans2 isn\\'t, otherwise identical\\n  ID  AGE  FEM \\nTRUE TRUE TRUE \\n</code></pre>\\n\\n<p>If you merely need the first row of each group, it\\'s much faster to join to that row directly. Why create the .SD object each time, only to use the first row of it?</p>\\n\\n<p>Compare the 0.064 of data.table to \"Matt Parker\\'s alternative to Chase\\'s solution\" (which seemed to be the fastest so far) :</p>\\n\\n<pre><code>&gt; system.time(ans3&lt;-dxt[c(TRUE, dxt$ID[-1] != dxt$ID[-length(dxt$ID)]), ])\\n user  system elapsed \\n0.284   0.028   0.310 \\n&gt; identical(ans1,ans3)\\n[1] TRUE \\n</code></pre>\\n\\n<p>So ~5 times faster, but it\\'s a tiny table at under 1 million rows. As size increases, so does the difference.</p>\\n',\n",
       " \"<p>I have two multivariate linear regression models (multiple outcomes, i.e., the responses are a matrix), and I'm measuring their performance using $R^2$ in cross-validation, over these individual responses, and across all responses. I'd also like to quantify the difference in $R^2$ using a p-value, for example t-test (after Fisher transform of $R^2$ for better normality etc).</p>\\n\\n<p>Now, these responses are highly correlated with each other. Therefore, averaging the $R^2$ over the responses into a single estimate worries me as they are not independent, and a significance test that ignores this might be badly biased. Should I instead get a separate p-value for each outcome and then use some multiple-testing correction?</p>\\n\",\n",
       " \"<p>Disclaimer: I am a biologist, so please don't hesitate to correct me if I am making unwise assumptions.</p>\\n\\n<p>I am looking at the distribution of a unique 6mer in a series of 2100mer (I have 415 independent 2100mer). More precisely, I am looking into how many times that 6mer is present in each 2100mer. As I deal with DNA sequence (A, T, G or C), each 6mer is expected to be found in random DNA every 4096 bp or 0.51 times in each 2100mer.\\nClearly a non-normal distribution (qqplot).</p>\\n\\n<p>Thus, I believe I am looking at a power law distribution or what should be a power law.</p>\\n\\n<p>Now, I want to compare that observed distribution to a simulated one and see if they are the same.</p>\\n\\n<p>Here are the raw counts:  </p>\\n\\n<pre><code>RawObserved = c(87, 112, 111, 60, 20, 18, 7)  \\nRawExpected = c(44.6, 28.7, 19.0, 7.7, 2.0, 1.5, 0.5)  \\n</code></pre>\\n\\n<p>Here is my data rendered as proportion: </p>\\n\\n<pre><code>NormObserved = c(0.21, 0.27, 0.27, 0.14, 0.05, 0.04, 0.02) \\nNormExpected = c(0.11, 0.07, 0.05, 0.02, 0.00, 0.00, 0.00)  \\n</code></pre>\\n\\n<p>I clearly cannot use a t-test but I was surprised to see that a chi-square test returned a non-significant result. To me those distributions look very different, qualitatively and I would like to statistically test that.</p>\\n\\n<p>I hope my explanation is sufficient and makes sense.\\nThanks!</p>\\n\",\n",
       " '<p>The answer is <em>exactly 0</em> in theory and <em>approximately 0</em> in practice.</p>\\n\\n<p>Let $X$ be a continuous random variable. Then $Y=X_i-X_j$ is also continuous.</p>\\n\\n<p>If $P(Y=0)=0$ then the probability of two observations of $X$ being equal is $0$, since $$P(X_i=X_j)=P(X_i-X_j=0)=P(Y=0)=0.$$ If $P(Y=0)&gt;0$ then the probability of doublets is greater than $0$.</p>\\n\\n<p>To see that $P(Y=x)&gt;0$ is an impossibility for any $x$, note that $Y$ being continuous means that $F(x)=P(Y\\\\\\\\leq x)$ is <a href=\"http://en.wikipedia.org/wiki/Continuous_function\">continuous</a> in $x$. Thus, since $P(a&lt;Y\\\\\\\\leq b)=F(b)-F(a)$,</p>\\n\\n<p>$$P(Y=x)=\\\\\\\\lim_{\\\\\\\\epsilon\\\\\\\\rightarrow 0} P(x-\\\\\\\\epsilon&lt;Y\\\\\\\\leq x+\\\\\\\\epsilon)=\\\\\\\\lim_{\\\\\\\\epsilon\\\\\\\\rightarrow 0}\\\\\\\\Big( F(x+\\\\\\\\epsilon)-F(x-\\\\\\\\epsilon)\\\\\\\\Big)=0.$$</p>\\n\\n<p>Thus $P(X_i=X_j)=P(Y=0)=0.$</p>\\n\\n<p><strong>This works in the same way as does length.</strong> The length of a single point is $0$, but the length of an interval containing an uncountably infinite number of points is more than $0$. Similarly, the probability of $Y=x$ is $0$ but the probability that $Y\\\\\\\\in (x-\\\\\\\\epsilon,x+\\\\\\\\epsilon)$ is greater than $0$.</p>\\n\\n<p><strong>Real data, on the other hand, is never continuous</strong>. Even measurements with great precision have a finite number of decimals attached to them. This means that there actually is a small probability of getting doublets.</p>\\n\\n<p>Let $X_{obs}$ be the observed value of $X$, rounded to four decimal places. Then, as an example,\\n$$P(X_{obs}=2.5934)=P(|X-2.5934|&lt;0.00005)&gt;0.$$\\nThe probability of getting the same observation again is therefore the probability that $X$ falls into a small interval surrounding it, as this will yield the same $X_{obs}$ again.</p>\\n\\n<p>Despite there being no continuous data, continuous distributions are very useful as <em>approximations</em>, since working with integrals often is much much easier than working with complicated sums (which is what we would get if we always tried to use highly granular discrete distributions).</p>\\n\\n<p><strong>Edit:</strong> thanks to <a href=\"http://stats.stackexchange.com/users/10525/procrastinator\">Procrastinator</a>, <a href=\"http://stats.stackexchange.com/users/2592/didier\">Didier</a> and <a href=\"http://stats.stackexchange.com/users/8402/stephane-laurent\">Stéphane</a> for helping to improve this answer. :)</p>\\n',\n",
       " '<p>I\\'ve come to decide that the best approach to the analysis of signal detection data (frankly, any data with dichotomous stimuli &amp; responses) collected in multiple participants is to use a generalized mixed model, treating participant as a random effect and predicting response as a function of truth and whatever other explanatory variables are in the experiment. Effects that involve the variable specifying the truth reflect effects on in discriminability, while effects not involving that variable reflect effects on response bias.</p>\\n\\n<p>For example, say you present a list of items for a participant to remember, then later present them with a second list containing some items they were asked to remember as well as some new items, asking the participant to label each as \"old\" or \"new\". You use words of different concreteness, and want to determine whether word concreteness affects discrimination ability. Thus, you would have data as:</p>\\n\\n<pre><code>participantID    word    concreteness    truth    response\\n1                brick   10              old      old\\n1                happy   2               old      new\\n1                river   8               new      new\\n1                peace   1               old      old\\n...\\n</code></pre>\\n\\n<p>You could fit the model (first converting \"response\" to 0/1) using the <code>lmer</code> function from the <a href=\"http://cran.r-project.org/web/packages/lme4/index.html\" rel=\"nofollow\">lme4 package</a> in R:</p>\\n\\n<pre><code>my_data$response_num = as.numeric(factor(my_data$response))-1\\nmy_mix = lmer(\\n    formula = response_num ~ (1|participant) + truth*concreteness\\n    , family = binomial\\n    , data = my_data\\n)\\nprint(my_mix)\\n</code></pre>\\n\\n<p>Or, if you want likelihood ratios (<a href=\"http://www.distans.hkr.se/joto/psych/p.pdf\" rel=\"nofollow\">and you should!</a>), you can use <code>ezMixed</code> function from the <a href=\"http://cran.r-project.org/web/packages/ez/index.html\" rel=\"nofollow\">ez package</a> in R:</p>\\n\\n<pre><code>my_mix = ezMixed(\\n    data = my_data\\n    , dv = .(response_num)\\n    , random = .(participant)\\n    , fixed = .(truth, concreteness)\\n    , family = binomial\\n)\\nprint(my_mix$summary)\\n</code></pre>\\n\\n<p>In both approaches (<code>ezMixed</code> is simply a wrapper around <code>lmer</code>, but with additional computation of likelihood ratios), the intercept reflects any overall bias in labelling words as new/old. The main effect of truth reflects the discriminability of new/old words. The main effect of concreteness reflects any effect of concreteness on response bias. Finally, the truth:concreteness interaction reflects any effect of concreteness on discriminability of new/old words.</p>\\n\\n<p>A couple points about this case specifically. Since this example deals with lexical stimuli, it may be reasonable to model words as a random effect as well (add <code>+ (1|word)</code> to the <code>lmer</code> formula, or add <code>word</code> to the list of random effects in the call to <code>ezMixed</code>). Additionally, the model above fits a linear function to the effects involving concreteness. If you want to account for non-linearity, you might employ generalized additive mixed models (implemented in the <a href=\"http://cran.r-project.org/web/packages/gamm4/index.html\" rel=\"nofollow\">gamm4</a> package). Unfortunately <code>ezMixed</code> currently only handles non-linearity by permitting polynomials up to a user-specified degree, which I feel is less useful than GAMM. Adding GAMM to <code>ezMixed</code> is on my <a href=\"https://github.com/mike-lawrence/ez/issues/10\" rel=\"nofollow\">to do list</a>...</p>\\n',\n",
       " '<p>Check out homals <a href=\"http://cran.r-project.org/web/packages/homals/vignettes/homals.pdf\" rel=\"nofollow\">vignette</a>. </p>\\n',\n",
       " '<p>You should read <a href=\"http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html\" rel=\"nofollow\">Spreadsheet Adiction</a> and the links from that page before trusting any results from Excel.</p>\\n\\n<p>From your question it appears that you don\\'t have a firm grasp on what confidence intervals and prediction intervals are.  You should really consult a good intro stats book, and/or take a class or meet with a consultant to get these concepts down.  But here is a short explanation:</p>\\n\\n<p>The condifence interval is a statement about where we believe the true population parameter (the mean above) to be based on the sample data.  So not knowing the population mean does not mean that you cannot do a confidence interval.  If your sample is large and you are willing to assume that the population is not overly skewed or would produce outliers, then the Central Limit Theorem says that a confidence interval on the mean based on the assumption of a normal population will be a good approximation even if the population is not normal.  So you can use normal based theory without knowing if the population is normal as long as you are willing to make the above assumptions.</p>\\n\\n<p>The prediction interval is a statement about where we expect future individual data points to be.  This prediction will depend much more on the shape of the distribution.</p>\\n\\n<p>The big difference in concept is whether you are talking about the mean of all future data, or individual data points (I could not tell which you are interested in from the question).</p>\\n\\n<p>The norminv function in Excel does not fit a normal distribution, but gives the x-value for a given area under the curve (probability) for a normal with the specified mean and standard deviation.  That function could be used as part of the computations to get either of the intervals, but that assumes that you know the population standard deviation, if you are using the sample standard deviation then it is more appropriate to use the t distribution rather than the normal.  Also note that the prediction interval takes into account the uncertainty in you estimate of the mean and standard deviation in addition to the randomness of the individual data points, so norminv probably is not what you want.</p>\\n',\n",
       " \"<p>Yes, your examples are correct.</p>\\n\\n<p>The Oxford English Dictionary defines <em>pool</em> as:</p>\\n\\n<blockquote>\\n  <p>pool, v.</p>\\n  \\n  <p>(puːl) </p>\\n  \\n  <p>1.1 trans. To throw into a common stock or fund to be distributed according to agreement; to combine (capital or interests) for the common benefit; spec. of competing railway companies, etc.: To share or divide (traffic or receipts). </p>\\n</blockquote>\\n\\n<p>Another example would be:</p>\\n\\n<p>you measure blood levels of substance X in males and females. You don't see statistical differences between the two groups so you <em>pool the data together</em>, ignoring the sex of the experimental subject.</p>\\n\\n<p>Whether it is statistically correct to do so depends very much on the specific case.</p>\\n\",\n",
       " '<p>An alternative approach is to look at the quantiles of the the sample: order the observations so $x_1 \\\\\\\\le x_2 \\\\\\\\le x_3 \\\\\\\\le \\\\\\\\cdots \\\\\\\\le x_n$ and (for reasonably large samples from a continuous distribution) there is a 95% probability that the interval $$\\\\\\\\left[x_{\\\\\\\\frac{n}{2} - 0.98 \\\\\\\\sqrt{n}}, x_{\\\\\\\\frac{n}{2} + 0.98 \\\\\\\\sqrt{n}}\\\\\\\\right]$$ contains the population median.</p>\\n\\n<p>Clearly ${\\\\\\\\frac{n}{2} \\\\\\\\pm 0.98 \\\\\\\\sqrt{n}}$ may not be an integer: you can round outwards to be conservative or use one of the <a href=\"https://en.wikipedia.org/wiki/Quantiles#Estimating_the_quantiles_of_a_population\" rel=\"nofollow\">many possibilities for interpolating quantiles</a>: you are now looking for $Q_p$ where $p =  {\\\\\\\\frac{1}{2} \\\\\\\\pm 0.98 /\\\\\\\\sqrt{n}}$.</p>\\n\\n<p>Again for <em>relative</em>, you can divide by the median. </p>\\n',\n",
       " '<p>Preliminary comment: I think if you build trees with <code>rpart</code> or <code>mvpart</code> you want to have the nonparametric nature that comes along with them. Hence you would want to use something nonparametric to test the differences in mean or some location parameter. </p>\\n\\n<p>What you could try is to treat the partitioning as if it was given and proceed treating the segments as groups. Then the problem reduces to a $k$-group comparison of mean or other location parameter, with the test of your choice (e.g. a one-factor ANOVA or a Kruskal-Wallis procedure). This test, and I can\\'t stress this enough, will be conditional on the overall tree partition.  </p>\\n\\n<p>Perhaps a bit more elegant then the crude \"use the partitions as factor levels\" approach would be if you already build your tree based on conditional inference. The function <code>party::ctree</code> allows to do that. It uses a nonparametric linear test statistic to decide whether to split, hence locally the difference between the two daughter nodes will be significant (which then traverses down the tree independently of the other branch, so there is no guarantee that globally all differences are significant). </p>\\n',\n",
       " '<p>For a quick and easy graph, you should give <a href=\"http://graphcalc.com\" rel=\"nofollow\">GraphCalc</a> a try.</p>\\n',\n",
       " '<p>I have a homework assignment that is giving me a hard time on the statistics. Lets say you have 3 stocks, all with n expected return (mean) $\\\\\\\\mu = 8\\\\\\\\%$, a risk (standard deviation) $\\\\\\\\sigma = 16\\\\\\\\%$ and a correlation coefficient $\\\\\\\\rho = 0.3$  between every tow stocks. The homework questions is: <code>If you build a portfolio from three stocks with an equal part form each stock. What is the expected return, and the risk?</code>. The expected return is easy, its the average of 8%,8%,8% which is 8%. But how to I calculate the risk (standard deviation) of all three? There are plenty of examples how to do it with 2 random variables, its: $\\\\\\\\sigma_1^2*w_1^2+\\\\\\\\sigma_2^2*w2^2-\\\\\\\\sigma_1*\\\\\\\\sigma_2*w_1*w_2*\\\\\\\\rho_{1,2}$ (Where $w_1, w_2$ are the relative weights of the rabdom variable). But what is it for three? My guess is$\\n\\\\\\\\sigma_1^2*w_1^2+\\\\\\\\sigma_2^2*w_2^2-\\\\\\\\sigma_1*\\\\\\\\sigma_2*w_1*w_2*\\\\\\\\rho_{1,2} + \\\\\\\\\\\\\\\\\\n\\\\\\\\sigma_2^2*w_2^2+\\\\\\\\sigma_3^2*w_3^2-\\\\\\\\sigma_2*\\\\\\\\sigma_3*w_2*w_3*\\\\\\\\rho_{2,3} + \\\\\\\\\\\\\\\\\\n\\\\\\\\sigma_1^2*w_1^2+\\\\\\\\sigma_3^2*w_3^2-\\\\\\\\sigma_1*\\\\\\\\sigma_3*w_1*w_3*\\\\\\\\rho_{1,3} + \\\\\\\\\\\\\\\\ \\\\\\\\sigma_1^2*w_1^2+\\\\\\\\sigma_2^2*w_2^2+\\\\\\\\sigma_3^2*w_3^2 + \\\\\\\\sigma_1*\\\\\\\\sigma_2*\\\\\\\\sigma_3*w_1*w_2*w_3*\\\\\\\\rho_{1,2,3}$</p>\\n\\n<p>In my question where $\\\\\\\\sigma_1=\\\\\\\\sigma_2=\\\\\\\\sigma_3=16\\\\\\\\%$ and $w_1=w_2=w_3=8\\\\\\\\%$ and $\\\\\\\\rho_{1,2}=\\\\\\\\rho_{2,3}=\\\\\\\\rho_{1,3}=0.3$ I get a simplified formula: $(\\\\\\\\sigma^2*w^2*2-\\\\\\\\sigma^2*w^2*\\\\\\\\rho_{1,2}) * 3 + \\\\\\\\sigma^3*w^3*\\\\\\\\rho_{1,2,3}$</p>\\n\\n<p>I took this idea from the <a href=\"http://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle\" rel=\"nofollow\">Inclusion–exclusion principle</a>.</p>\\n\\n<p>Now my questions: Is that even true for random variables? If so, how do I know $\\\\\\\\rho_{1,2,3}$? (Please leave an explanation\\\\\\\\reference on $\\\\\\\\rho_{1,2,3}$ and not just a formula.)</p>\\n\\n<p>Vocabulary (in case it was not clear how the financial terms fit into statistics):</p>\\n\\n<ul>\\n<li>random variable = stock</li>\\n<li>mean = $\\\\\\\\mu$ = expected return of a stock</li>\\n<li>standard deviation = $\\\\\\\\sigma$ = risk of a stock\\\\\\\\portfolio</li>\\n</ul>\\n',\n",
       " '<p><strong>Use a moving window.</strong></p>\\n\\n<p>For example, consider these synthetic data (created in <code>R</code>):</p>\\n\\n\\n\\n<pre><code>time &lt;- 1:(365*24)/24\\namplitude &lt;- time * ((1+length(time))/24-time) / length(time)\\nset.seed(17)\\nx &lt;- (x0 &lt;- amplitude * cos(2*pi * (time + 6 * (time/365)^2))) + rnorm(length(time), sd=5)\\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/zW6mv.png\" alt=\"Time series\"></p>\\n\\n<p>The dark points are the underlying noiseless signal; the gray line is the signal with added Gaussian noise (subsampled at 7 hour intervals to make it clearer).  The signal\\'s amplitude obviously changes, but a little less obviously, the frequency increases with time, too.</p>\\n\\n<p>Running a window over this series, computing the periodogram for each position, and extracting the value at the dominant frequency found <em>in the first window</em>, produces a series of powers corresponding to the centers of the windows.  If there is no change in power over time, a plot of this series should be almost flat. Deviations can be interpreted as temporal changes in power.  It is helpful to smooth this plot; a lowess smooth (with a narrow window of 1/4 the total width) is used here.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/P9sJO.png\" alt=\"Results\"></p>\\n\\n<p>As a basis for comparison, the same procedure was applied to the underlying signal; it is plotted in gray. The smoothed windowed power function closely approximates the windowed power function for the underlying signal (and is very close at the largest powers).</p>\\n\\n<p>(The powers shown here are computed using natural logarithms, not as decibels.)</p>\\n\\n<p>One could make a plot for any desired frequency, in addition to this \"dominant\" or expected frequency.</p>\\n\\n<h3>Appendix</h3>\\n\\n\\n\\n<p>Here is the <code>R</code> code that produced the figures.</p>\\n\\n<pre><code>#\\n# Return the square of the periodogram (normalized by the length of x)\\n# evaluated at index n.\\n#\\npower &lt;- function(x, n) {\\n  x.hat &lt;- spec.pgram(x, plot=FALSE)\\n  (x.hat$spec[n] / length(x))^2       #$(TeX bug workaround)\\n}\\n#\\n# Window `power(*,n)` across array `x` using a weighted symmetric \\n# window extending to `width` on either side (and therefore of length\\n# 2*width+1), subsampled every `skip` elements.\\n#\\npower.window &lt;- function(x, n, width=1, weight=1, skip=1) {\\n  i &lt;- seq.int(from=width+1, to=length(x)-width, by=skip)\\n  sapply(i, function(i) power(x[(i-width):(i+width)] * weight, n))\\n}\\n#\\n# Simulate and plot an interesting time series.\\n#\\ntime &lt;- 1:(365*24)/24\\namplitude &lt;- time * ((1+length(time))/24-time) / length(time)\\nset.seed(17)\\nx0 &lt;- amplitude * cos(2*pi * (time + 6 * (time/365)^2))\\nx &lt;- x0 + rnorm(length(time), sd=5)\\ni &lt;- seq.int(from=1, to=length(time), by=7)\\nplot(time[i], x[i], type=\"l\", xlab=\"Days\", ylab=\"x\", main=\"Data\", col=\"Gray\")\\npoints(time, x0, pch=19, cex=0.2)\\n#\\n# Find the frequency where power is maximized within the initial window.\\n#\\nwidth &lt;- 30*24\\nx.hat &lt;- spec.pgram(x[1:(2*width+1)], plot=FALSE)\\ni.max &lt;- which.max(x.hat$spec) #$ TeX bug workaround\\n#\\n# Compute and plot the power moving window for the underlying \\n# series (`x0`) and its noisy version (`x`).  For plotting, subsample\\n# the series.\\n#\\nskip &lt;- 7\\nx.power &lt;- power.window(x, n=i.max, width=width, weight=1, skip=skip)\\nx0.power &lt;- power.window(x0, n=i.max, width=width, weight=1, skip=skip)\\nx.power.smooth &lt;- lowess(x.power, f=1/4)\\ni &lt;- seq(from=(width+1)/24, by=skip/24, length.out=length(x.power))\\nmain &lt;- sprintf(\"Moving power at 1/%4.1f hours, %d hour window\", \\n                1/x.hat$freq[i.max], 2*width+1) #$ TeX bug\\nplot(i, log(x.power), type=\"l\", lwd=2, xlab=\"Time (days)\", main=main)\\nlines(i, log(x.power.smooth$y), lty=2, lwd=3, col=\"Blue\")\\nlines(i, log(x0.power), lwd=2, col=\"Gray\")\\nlegend(30, -4, \"Underlying\", bty=\"n\", box.col=\"White\", box.lwd=0, lwd=2, col=\"Gray\")\\nlegend(30, -5, \"Windowed\", bty=\"n\", box.col=\"White\", box.lwd=0, lwd=2)\\nlegend(30, -6, \"Smooth\", bty=\"n\", box.col=\"White\", box.lwd=0, lwd=3, col=\"Blue\", lty=2)\\n</code></pre>\\n',\n",
       " '<p>Supposing one has a time series from which one can take various measurements such as period, maximum, minimum, average etc. and then use these to create a model sine wave with the same attributes, are there any statistical approaches one can use that could quantify how closely the actual data fit the assumed model? The number of data points in the series would range between 10 and 50 points. </p>\\n\\n<p>A very simplistic first thought of mine was to ascribe a value to the directional movement of the sine wave, i.e. +1 +1 +1 +1 -1 -1 -1 -1 -1 -1 -1 -1 +1 +1 +1 +1, do the same to the actual data, and then somehow quantify the degree of similarity of directional movement. </p>\\n\\n<p>Edit: Having given more thought to what I really want to do with my data, and in light of responses to my original question, what I need is a decision making algorithm to choose between competing assumptions: namely that my data is basically linear (or trending) with noise that could possibly have cyclic elements; my data is basically cyclic with no directional trend to speak of; the data is essentially just noise; or it is transitioning between any of these states.</p>\\n\\n<p>My thoughts now are to maybe combine some form of Bayesian analysis and Euclidean/LMS metric. The steps in this approach would be</p>\\n\\n<p>Create the assumed sine wave from data measurements</p>\\n\\n<p>Fit a LMS straight line to the data</p>\\n\\n<p>Derive an Euclidean or LMS metric for departures from the original data for each of the above</p>\\n\\n<p>Create a Bayesian prior for each based on this metric i.e. 60 % of the combined departures attach to one, 40 % to the other, hence favour the 40 %</p>\\n\\n<p>slide a window one data point along the data and repeat the above to obtain new % metrics for this slightly changed data set - this is the new evidence - do the Bayesian analysis to create a posterior and change the probabilities that favour each assumption</p>\\n\\n<p>repeat along the whole data set (3000+ data points) with this sliding window (window length 10-50 data points). The hope/intent is to identify the predominant/favoured assumption at any point in the data set and how this changes with time </p>\\n\\n<p>Any comments on this potential methodology would be welcome, particularly on how I could actually implement the Bayesian analysis part.</p>\\n',\n",
       " '<p><strong>You can use a generalized linear mixed model</strong> (binomial family), with person as a random effect and sibling as a fixed effect.</p>\\n\\n<p>In <code>R</code> this would be conducted as </p>\\n\\n<pre><code>data &lt;- data.frame(person=as.factor(c(1,1,2,2)), \\n                   sibling=as.factor(c(0,1,0,1)), \\n                   attempts=c(300,35,125,40), \\n                   hits=c(15,5,10,8))\\ndata$failures &lt;- data$attempts - data$hits\\nrequire(lme4)\\nfit &lt;- lmer(cbind(hits, failures) ~ sibling + (1 | person), family=binomial(), data=data)\\nsummary(fit)\\n</code></pre>\\n\\n<p>The output gives a (two-sided) p-value of $0.00101$ in this example:</p>\\n\\n<pre><code>Fixed effects:\\n            Estimate Std. Error z value Pr(&gt;|z|)    \\n(Intercept)  -2.7726     0.2062 -13.449  &lt; 2e-16 ***\\nsibling1      1.2104     0.3682   3.288  0.00101 ** \\n</code></pre>\\n\\n<p>The positive estimate indicates the siblings do better.  This differs from the result of combining a bunch of chi-squared tests, one for each person:</p>\\n\\n<pre><code>chisq &lt;- by(data, data$person, function(x) chisq.test(cbind(x$failures, x$hits)), \\n                simplify=FALSE) #$\\nstat &lt;- -2 * sum(unlist(lapply(chisq, function(x) log(x$p.value)))) #$\\npchisq(stat, df=2, lower.tail=FALSE)\\n</code></pre>\\n\\n<p>The separate p-values are $6.9$% and $6.8$%, respectively, which when combined as shown yield a p-value of $0.47$% rather than the GLMM p-value of $0.10$%.</p>\\n\\n<p>One danger of using the chi-squared tests occurs when the effects for different people are in opposite directions: combining their (two-sided) p-values would be erroneous.  There is no such problem with the GLMM.</p>\\n\\n<p>There is more power and flexibility in using the GLMM compared to conducting the chi-squared tests.  Moreover, the GLMM will handle multiple siblings per person and multiple experiments per person without any change; it is difficult to see how to adapt chi-squared contingency table analysis to those generalizations.</p>\\n',\n",
       " \"<p>I'd suggest that you have a look at a graph of the distribution of scores in each of your groups- either a histogram or a density plot, where your score (in this case the hormone level) is on the x-axis and the count or proportion of people with that score is on the y-axis. I'm not sure what software you're using, but the histogram at least should be very easy to produce in most statistics software. Look for asymmetry in the way the scores are spread around the mean, i.e. skew.</p>\\n\\n<p>The unpaired t-test assumes that both sets of scores are normally distributed with the same variance.\\nThe Welch t-test assumes that both sets of scores are normally distributed, but does not assume equal variance.\\nMann-Whitney doesn't assume either of those things, and is therefore a good choice when your data isn't normally distributed.</p>\\n\\n<p>By looking at graphs of the distributions of scores, you'll probably get some sense of <em>why</em> the tests are giving different results for your data, because of their different assumptions. </p>\\n\",\n",
       " '<p>It\\'s not exactly clear where these effect estimates in your output are coming from. I\\'m unfamiliar with SPSS, so bear with me.</p>\\n\\n<p>I will assume that SPSS has treated all the 0 levels of your factors as \"referent\" levels. So the intercept represents the mean effect among people having group=0, stimulus=0, and empathy=0. I\\'m also assuming that empathy (or empathy score) is treated as \"continuous\".</p>\\n\\n<ol>\\n<li><p>I think the first model is testing for the global interaction between empathy and stimulus. Which is why it\\'s a 2-df numerator F-test. The full model has the additional parameters for levels where [stimulus==-1]*empathy and [stimulus==1]*empathy, hence a difference of parameters of tow.</p></li>\\n<li><p>I think the second model is a pairwise test for the individual parameters I mentioned above. In the full model, we have three possible effects for empathy stratified by stimulus whereas in the reduced model there is only one. We examine the differences of empathy across each strata empirically, but there are only two t-tests available to us to describe the pairwise disparity in such effects relative to the null model.</p></li>\\n<li><p>I do not understand your output. It\\'s unclear to me whether there were insufficient samples to estimate effects in these groups, or whether SPSS has determined some tests as redundant as before (but is unable to report them).</p></li>\\n</ol>\\n\\n<p>At any rate, you want to focus on global tests, as was the case in model 1, because the scientific question when testing for interaction is stated as follows, \"Is there any stratum in the sample for which the estimated effect of our predictor of interest is significantly different than the stratified effect estimate?\" This leads us to the proper interpretation for the p-value of the F-test reported above. You can follow this up by reporting the most significantly different estimated difference in effects based on the pairwise comparisons, but this is unreliable and can be confusing.</p>\\n\\n<p>I should mention again that, if you\\'re interested in the test of 3-way interaction, then the F-test should have 1 numerator degree of freedom. This is because you should have all 2-way interactions included in the null model. Or perhaps you\\'re interested in whether there are any 2-way interactions instead (giving a 5 numerator degrees of freedom F test). It begs the question whether you\\'re asking for the test you really want here.</p>\\n',\n",
       " \"<p>It's the same for t-tests.  There's no need for the data-set <em>as a whole</em> to be normally distributed (&amp; if there <em>is</em> a difference in means between the groups it won't be), only the residuals. Of course, for a t-test, saying the residuals are normally distributed is the same as saying each group is normally distributed.</p>\\n\\n<p>The distribution of the independent variables (in the case of t-tests it's the group label) is always irrelevant in ANOVA, regression, &amp; the like: you're interested in the distribution of the response variable conditional on given values of the independent variables.</p>\\n\",\n",
       " '<p>I would personally favor cross-validated score evaluation because:</p>\\n\\n<ul>\\n<li><p>it is easily interpretable by the analyst provided that the underlying score function (accuracy, f1-score, RMSE...) is interpretable too,</p></li>\\n<li><p>it gives an idea of the uncertainty by looking at the stdev of the score values across CV folds,</p></li>\\n<li><p>it gives a way to decompose the error into bias (error measured on train folds) and variance (difference of errors measured on train and  test folds).</p></li>\\n</ul>\\n\\n<blockquote>\\n  <p>Model size is not a factor with the computing power </p>\\n</blockquote>\\n\\n<p>This is not always true: deep learning machine learning models for instance have a model size that is often limited by the hardware (typically the amount of RAM on the GPU card).</p>\\n',\n",
       " '<p>Your class sample sizes do not seem so unbalanced since you have 30% of observations in your minority class. Logistic regression should be well performing in your case. Depending on the number of predictors that enter your model, you may consider some kind of penalization for parameters estimation, like ridge (L2) or lasso (L1). For an overview of problems with very unbalanced class, see Cramer (1999), The Statistician, 48: 85-94 (<a href=\"http://www.tinbergen.nl/discussionpapers/98085.pdf\">PDF</a>).</p>\\n\\n<p>I am not familiar with credit scoring techniques, but I found some papers that suggest that you could use SVM with weighted classes, e.g. <a href=\"http://www.springerlink.com/content/uwx80880224n25w3/\">Support Vector Machines for Credit Scoring: Extension to Non Standard Cases</a>. As an alternative, you can look at <a href=\"http://mpra.ub.uni-muenchen.de/8156/1/MPRA_paper_8156.pdf\">boosting</a> methods with CART, or Random Forests (in the latter case, it is possible to adapt the sampling strategy so that each class is represented when constructing the classification trees). The paper by Novak and LaDue discuss the pros and cons of <a href=\"http://ageconsearch.umn.edu/bitstream/15129/1/31010109.pdf\">GLM vs Recursive partitioning</a>. I also found this article, <a href=\"http://fic.wharton.upenn.edu/fic/handpaper.pdf\">Scorecard construction with unbalanced class sizes</a> by Hand and Vinciotti.</p>\\n',\n",
       " '<p>Not by itself. In fact, this is done all the time. If the variable in the column is a dichotomy, then 0,1 makes it pretty straightforward to interpret results. If the variable can take 3 or more values, there are various coding schemes including dummy coding and effect coding.</p>\\n',\n",
       " '<p>The lifetime of a particular brand of batteries is known to have a gamma distribution. Tests on a large sample of these batteries show a mean lifetime of 480 hours and a standard deviation of 96 hours.</p>\\n\\n<p>Some questions I require help on</p>\\n\\n<ol>\\n<li><p>Show that the moment estimators for the gamma distribution parameters are alpha(hat)=25, and lambda(hat)=5/96. Hence write down and simplify the density function for the lifetime (in hours), using these values.</p></li>\\n<li><p>What is the exact distribution of the sum of the lifetimes of 20 of these batteries? Find the exact distribution of the sample mean?</p></li>\\n<li><p>Using a normal approximation, find the probability that the sample mean lifetime from these 20 batteries will be less than 460 hours.</p></li>\\n<li><p>Based on the parameters of the exact distribution in (b), does the normal approximation in (c) seem reasonable?</p></li>\\n</ol>\\n\\n<p>I have been able to do #1 ok, but are getting stuck on the rest, if someone could help that would be great.</p>\\n',\n",
       " '<p>If $\\\\\\\\sigma$ is the standard deviation then it is positive (or at least non-negative). </p>\\n\\n<p>If $\\\\\\\\sigma$ is a constant then $$-E\\\\\\\\left[ \\\\\\\\frac 2 \\\\\\\\sigma \\\\\\\\right] = -\\\\\\\\frac 2 \\\\\\\\sigma$$</p>\\n\\n<p>In the expression for the <a href=\"http://en.wikipedia.org/wiki/Fisher_information\" rel=\"nofollow\">Fisher Information</a> you can often find</p>\\n\\n<p>$$\\\\\\\\n\\\\\\\\mathcal{I}(\\\\\\\\theta) = - \\\\\\\\operatorname{E} \\\\\\\\left[\\\\\\\\left. \\\\\\\\frac{\\\\\\\\partial^2}{\\\\\\\\partial\\\\\\\\theta^2} \\\\\\\\log f(X;\\\\\\\\theta)\\\\\\\\right|\\\\\\\\theta \\\\\\\\right]\\\\\\\\,\\\\\\\\n$$</p>\\n\\n<p>the sign is present because the expectation is negative.  There is another expression $$\\\\\\\\mathcal{I}(\\\\\\\\theta)=\\\\\\\\operatorname{E} \\\\\\\\left[\\\\\\\\left. \\\\\\\\left(\\\\\\\\frac{\\\\\\\\partial}{\\\\\\\\partial\\\\\\\\theta} \\\\\\\\log f(X;\\\\\\\\theta)\\\\\\\\right)^2\\\\\\\\right|\\\\\\\\theta \\\\\\\\right]$$ withought the negative sign.</p>\\n',\n",
       " '<p>$ \\\\\\\\frac{| \\\\\\\\tilde{A} \\\\\\\\bigcup \\\\\\\\tilde{B} |}{| \\\\\\\\tilde{A} | + |  \\\\\\\\tilde{B} |}$ is not an unbiased estimate for  $\\\\\\\\frac{| A \\\\\\\\bigcup B |}{| A |+|B |}$. Similarly $ \\\\\\\\frac{| \\\\\\\\tilde{A} \\\\\\\\bigcap \\\\\\\\tilde{B} |}{| \\\\\\\\tilde{A} | + |  \\\\\\\\tilde{B} |}$ is not an unbiased estimate for  $\\\\\\\\frac{| A \\\\\\\\bigcap B |}{| A |+|B |}$.  I think for the intersection you should be using multiplication not addition in the denominator.</p>\\n\\n<p>To take a simple example, suppose $A=B=A \\\\\\\\cap B$ and $n_A=n_B =100$ and you take a sample size $m_\\\\\\\\tilde{A}=1$ from $A$ and a sample size $m_\\\\\\\\tilde{B}=1$ from $B$.  Then $E\\\\\\\\left[\\\\\\\\frac{| \\\\\\\\tilde{A} \\\\\\\\bigcup \\\\\\\\tilde{B} |}{| \\\\\\\\tilde{A} | + |  \\\\\\\\tilde{B} |}\\\\\\\\right] = 0.995$  and $E\\\\\\\\left[\\\\\\\\frac{| \\\\\\\\tilde{A} \\\\\\\\bigcap \\\\\\\\tilde{B} |}{| \\\\\\\\tilde{A} | + |  \\\\\\\\tilde{B} |}\\\\\\\\right] = 0.005$ but $\\\\\\\\frac{| A \\\\\\\\bigcup B |}{| A |+|B |} =\\\\\\\\frac{| A \\\\\\\\bigcap B |}{| A |+|B |}= 0.5$.  By contrast $E\\\\\\\\left[\\\\\\\\frac{| \\\\\\\\tilde{A} \\\\\\\\bigcap \\\\\\\\tilde{B} |}{| \\\\\\\\tilde{A} | \\\\\\\\times |  \\\\\\\\tilde{B} |}\\\\\\\\right] = \\\\\\\\frac{| A \\\\\\\\bigcap B |}{| A |\\\\\\\\times |B |}= 0.01.$</p>\\n\\n<p>The probability an individual element of $A$ is in the sample  $\\\\\\\\tilde{A}$ is $\\\\\\\\frac{m_\\\\\\\\tilde{A}}{n_A}$ and similarly with an individual element of $B$;  the probability that an individual element of $A \\\\\\\\cap B$ is in both samples is therefore the product of those probabilities, and the expected number of elements appearing in both samples is $E\\\\\\\\left[| \\\\\\\\tilde{A} \\\\\\\\cap \\\\\\\\tilde{B} |\\\\\\\\right] = |A\\\\\\\\cap B|\\\\\\\\frac{m_\\\\\\\\tilde{A} m_\\\\\\\\tilde{B}}{n_A n_B}$.  So $$\\\\\\\\frac{| \\\\\\\\tilde{A} \\\\\\\\cap \\\\\\\\tilde{B} |}{| \\\\\\\\tilde{A} | \\\\\\\\times |  \\\\\\\\tilde{B} |}$$ is an unbiased estimator of $$\\\\\\\\frac{| A \\\\\\\\cap B |}{| A | \\\\\\\\times |B |}$$ and I think this is what you want.  The equivalent statement for the union is more complicated to state but easily calculated in practice.</p>\\n',\n",
       " '<p>I rather like <a href=\"http://en.wikipedia.org/wiki/Violin_plot\" rel=\"nofollow\">violin plots</a> myself, as this gives an idea of the shape of the distribution.  However if the large range of values is the issue, then maybe it would be best to plot the log of the data rather than the raw values, that would then make choosing the box sizes for histograms etc.  As the display is for laymen, don\\'t mention logs and mark the axis 10, 100, 1000, 10000, 100000, 1000000 etc.</p>\\n',\n",
       " '<p>Given that you have an ARIMA filter, you can restate this as a pure AR by simply multiplying out the ARI and then dividing by the MA. This would be a pure auto-regressive formulation which is called the PI WEIGHTS ( see any book on time series analysis ) . The right-hand side constant might take you some to time to figure it out but I am sure you will. Use this pure right-hand side equation in conjunction with your residuals and you will get a realization of a process that will have \"the same acf\" as the series that was used to identify the ARIMA Model. Of course what you will have will be a realization of a sample ACF which of course would depend on the input \"error/noise vector\".</p>\\n',\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " 'Cluster analysis with skewed distibutions',\n",
       " 'Statistics and econometrics distance learning',\n",
       " 'Mean absolute error vs sum absolute error',\n",
       " 'What does \"20/ln(2)\" mean in logistic regression?',\n",
       " 'Prediction errors in orthogonal regression',\n",
       " 'Statistically back-calculating: Markov Chain?',\n",
       " 'How should I analyze a data set with 1 IV and 2DVs (all scale variables)?',\n",
       " 'General rules for choosing machine-learning algorithm?',\n",
       " 'User Defined Substitution Cost Matrix',\n",
       " 'What is meant by a \"random variable\"?',\n",
       " 'Random effect nested under fixed effect model in R',\n",
       " 'Benefits on kernel trick and one related question',\n",
       " 'References for methods for calculating the confidence interval for Theil-Sen Estimator',\n",
       " 'ISOMAP for classification',\n",
       " 'How much do answers A,B,C link with the answer on question D?',\n",
       " 'Countering false positives in GWAS due to linkage disequilibrium',\n",
       " 'Code to do Monte Carlo using SPSS?',\n",
       " 'Weighted kernel density plot in R',\n",
       " 'How do calculate Present Value in Google Spreadsheet?',\n",
       " 'Lorenz curve and Gini coefficient for measuring classifier performance',\n",
       " 'How to test a directional hypothesis using ANOVA?',\n",
       " 'Spatial statistics models -- CAR vs SAR',\n",
       " \"Bayes' Theorem - Probability Pants problem\",\n",
       " 'Sample size to achieve given confidence level',\n",
       " 'How do I figure out what kind of distribution represents this data on ping response times?',\n",
       " 'Using Beta to interpret interaction in general linear model',\n",
       " 'How to incorporate sample groups?',\n",
       " 'How to estimate storage needs using the PERT distribution for filesizes?  How to aggregate them without falling into the flaw of extremes?',\n",
       " \"What's the difference between $p(x \\\\\\\\mid \\\\\\\\theta)$ and $p(x; \\\\\\\\theta)$?\",\n",
       " 'How can I even out a random distribution while minimising how far each data point is moved?',\n",
       " 'How dummy variable can be analyzed in panel data using fixed effects in Stata?',\n",
       " 'Eliciting priors ... with money!',\n",
       " 'Determine whether a n-th finite moment of X exists',\n",
       " \"Data mining techniques in Obama's campaign\",\n",
       " 'Classification of cluster-correlated data',\n",
       " '$ARIMA(p,d,q)+X_t$, Simulation over Forecasting period',\n",
       " 'Learning the parameter of linear model',\n",
       " 'Calculating the percentage change of 2 values',\n",
       " 'How can I calculate proportion of variance of an aggregate time-series accounted for by the variance of a sub-series?',\n",
       " 'Statistical significance with sample size of 5 or less',\n",
       " 'The usage of Kolomogorov-Smirnov statistic in predictive modelling',\n",
       " \"fisher's test vs pairwise fisher's test\",\n",
       " 'Are rejection regions always open/closed?',\n",
       " 'How to assess regression performance when using PCA components as response variables?',\n",
       " 'What test do I use for language variance data?',\n",
       " 'Critical values for Anderson-Darling test',\n",
       " 'Which ANOVA is appropriate?',\n",
       " 'What is the relationship between propensity scores and sufficient statistics?',\n",
       " 'Drawing a realization from a random variable',\n",
       " 'A best measure for speaker recognition',\n",
       " 'Fisher classification method for normally distributed classes',\n",
       " 'Is principal components analysis valid if the distribution(s) are Zipf like? What would be similar to PCA but suited to non gaussian data?',\n",
       " 'Effect of uncertainty in a parameter of a probability model on decisions',\n",
       " 'How to do a pretty scatter plot in R?',\n",
       " 'Posterior expression for Gibbs sampling',\n",
       " 'Can I use a variable which has a non-linear relationship to the dependent variable in logistic regression?',\n",
       " 'Custom error function for randomForest R package ',\n",
       " 'Centroid matching problem',\n",
       " 'Gradient descent oscillating a lot. Have I chosen my step direction incorrectly?',\n",
       " 'Probability of selecting each item at least once when sampling with replacement',\n",
       " 'If an ANOVA indicates no main effect and no interaction, should the lack of interaction be stated?',\n",
       " 'Applying principal component analysis on variables of different dimensions?',\n",
       " 'Getting rid of file download warning in SAS',\n",
       " 'Building a betting tip system based on previous matches',\n",
       " 'How to calculate confidence intervals for pooled odd ratios in meta-analysis?',\n",
       " 'Forecasting & social influence data/experiment - Seeking research strategies',\n",
       " 'Box-Cox like transformation for independent variables?',\n",
       " 'Useful statistical functions for business -- for use by a newbie',\n",
       " 'Use of survival analysis for trigger mining',\n",
       " 'Can I split a series of observations of a variable over time into two groups instead of working with time series?',\n",
       " 'How to denote element-wise difference of two matrices',\n",
       " 'Can deep belief networks be applied to sparse feature vectors/classification problems?',\n",
       " 'Estimating out-of sample forecast for an ARIMA model',\n",
       " 'Which model to use with repeated measures data that contains multiple binary dependent variables',\n",
       " 'Make R report error on using non-existent column name in a data frame',\n",
       " 'Distribution of sample correlation',\n",
       " 'Is 2-sigma limit applicable as a measure of variance to a distribution?',\n",
       " 'How to find and describe regularities in a distribution of interarrival times of a recurring event?',\n",
       " 'How to analyse repeated measure ANOVA with three or more conditions presented in randomised order?',\n",
       " 'Is there any test for a null hypothesis of non-normality?',\n",
       " 'Variance and Asymptotic normality of sample variance of normal distribution',\n",
       " 'Using control variates & antithetic method with Monte Carlo',\n",
       " 'Empirical change of DAG for Bayesian Network',\n",
       " 'Bayesian network fundementals',\n",
       " 'Detecting Clusters of \"similar\" source codes',\n",
       " 'Best practices when treating range data as continuous',\n",
       " \"logistic regression always yielding increasing f'n when should sometimes be decreasing (using R)\",\n",
       " 'Endogenous instruments in IV 2SLS regression in Stata',\n",
       " 'How to get the actual regression value from regression analysis in R statistical package',\n",
       " 'How exactly does Chi-square feature selection work?',\n",
       " 'Is it valid to select a model based upon AUC?',\n",
       " 'In a census-like situation, does it make sense to use Bayes Rule?',\n",
       " 'Is it ok to bin residuals before examining them?',\n",
       " 'How to interpret ADF-test optimal lag model reduction?',\n",
       " 'BY and WITH command in mixed model SPSS',\n",
       " 'Bayesian Updating Without Conjugate Prior',\n",
       " 'How to convert from VAR (vector-auto-regression) on differences to original series. How to simulate orignal series from VAR on differences',\n",
       " 'Testing for confounding',\n",
       " \"What's a good tool to create sankey diagrams?\",\n",
       " 'Deriving ordered statistics minimum cdf',\n",
       " 'Inference with one-sample Likert-type data',\n",
       " 'Comparing two sets of pixels to determine whether they belong to the same object',\n",
       " 'Nonlinear estimates of regression coefficients?',\n",
       " 'How to compare accuracy measures in method comparison studies?',\n",
       " 'Statistical hypothesis testing - how to demonstrate that power $>$ size',\n",
       " 'Categorial features in linear machine learning algorithms',\n",
       " 'How to choose a data subset in RapidMiner?',\n",
       " \"What do statisticians do that can't be automated?\",\n",
       " '1-sample vs 2-sample test question',\n",
       " 'Bayesian statistics example',\n",
       " 'Does it make sense to apply a chi-squared test on a contingency table when the whole population has been surveyed?',\n",
       " \"'Fixing' PearsonFitML to fit to a Pearson V distribution\",\n",
       " 'Difference between different methods of outlier determination?',\n",
       " 'Omega H asymptotic',\n",
       " 'Determining the influential features for an outcome',\n",
       " 'Which statistical test to test if counts from different populations are actually different? t-test versus chi-squared',\n",
       " 'Perl advantages in data management/statistical analysis?',\n",
       " 'How to test for serial correlation of a time series itself (not residuals)?',\n",
       " 'Drawing balls with two features',\n",
       " 'What is the standard error for the distribution of the difference in proportions (for hypothesis testing)?',\n",
       " 'Caret re-sampling methods',\n",
       " 'Data Augmentation Examples',\n",
       " 'any alternative to kaggle for data mining & analysis competitions and training?',\n",
       " 'When does a random test fail?',\n",
       " 'How to test whether two means of categorisation generate significantly different results',\n",
       " 'Is high multicollinearity always an issue in OLS?',\n",
       " 'Detect statistical anomalies',\n",
       " 'Coverage probabilities of the basic bootstrap confidence Interval',\n",
       " 'Differences between heavy tail and fat tail distributions',\n",
       " 'Can I use a likelihood ratio test when the error distributions differ?',\n",
       " 'Is it possible to fit a multivariate regression model where the independent variable is latent?',\n",
       " 'Constructing a Bayesian Classifier in R',\n",
       " 'Rescaling for desired standard deviation',\n",
       " 'How to calculate the expected number of distinct items when drawing pairs?',\n",
       " 'Means of groups A and B differ significantly. Can I build a Classifier that will classify values into A or B?',\n",
       " 'Logit with ordinal independent variables',\n",
       " 'Regarding the R packages that share the similar functionalities of NLTK toolkit',\n",
       " 'Unbalanced design effect',\n",
       " 'Collapsing (combining) two variables into one for analysis',\n",
       " 'Papers on the risks of using regular subsets',\n",
       " 'Bayesian additive regression trees (BART) for classification analysis of gene expression data',\n",
       " 'What is the number of free parameters for a directed acyclic graph?',\n",
       " 'Software quality control references',\n",
       " 'extractSeqFromW function',\n",
       " 'Examples of spatial generalized linear models',\n",
       " 'Missing values in GLM',\n",
       " 'Frequentism and priors',\n",
       " 'Is this a CUSUM limitation?',\n",
       " 'Trying to Calculate Basic Lunar Probabilities',\n",
       " 'Is it valid to aggregate a time series to make it look more meaningful?',\n",
       " 'Data collection and storage for time series analysis',\n",
       " 'Should I use the mean-squared-prediction-error from LOOCV for prediction intervals?',\n",
       " 'Basic easy rules for statistics',\n",
       " 'VGAM checkwz errors',\n",
       " 'Bootstrapping with a small number of observations',\n",
       " 'What is the Unscented Kalman Filter?',\n",
       " 'R: update a graph dynamically',\n",
       " 'Amoeba Interview Question',\n",
       " \"How to calculate weighted Hedges' g effect size in meta-analysis when some effect sizes share a control group?\",\n",
       " 'Papers on Bayesian factor analysis?',\n",
       " 'Problem calculating joint and marginal distribution of two uniform distributions',\n",
       " 'Incremental learning methods in R',\n",
       " 'Hypothesis test for odds ratios',\n",
       " 'Time series analysis with neural networks',\n",
       " 'Why do demographers give rates per 100,000 people?',\n",
       " 'Meaning of p-values in regression',\n",
       " 'Difference between theoretical model and fit',\n",
       " 'Individual-level parameter estimates from ordered choice regressions',\n",
       " 'How do I calculate and compare incidence of infection between age-groups',\n",
       " 'How to use cross validation to estimate autocorrelation?',\n",
       " 'Measuring correlation or dependence between two data sets',\n",
       " 'Meaning of output terms in gbm package?',\n",
       " 'Calculating mean age from grouped census data',\n",
       " 'How to resample in R without repeating permutations?',\n",
       " 'Testing for moderation with continuous vs. categorical moderators',\n",
       " 'Probability of throwing same sequence of 13 heads or tails',\n",
       " 'Convergence in probability and $L_2$ for normal random variables',\n",
       " 'Markov Process about only depending on previous state',\n",
       " 'Linear regression with repeated measures in R',\n",
       " 'Taleb and the Black Swan',\n",
       " 'Problem with R code for spectral clustering',\n",
       " \"Sense of correlation coefficient matrix of particle filter's parameters\",\n",
       " 'Kalman filter update returns an invalid covariance matrix?',\n",
       " 'estimating beta parameter',\n",
       " 'Egger’s linear regression method intercept in meta analysis',\n",
       " 'How to interpret results from R anova in quantile regression?',\n",
       " 'Connection between bootstrap (both parametric and non-parametric) and maximum likelihood',\n",
       " 'Arima model giving high forecast values',\n",
       " 'What is the number of subjects needed to be representative for a small, finite population?',\n",
       " 'Logistic vs linear regression',\n",
       " 'Generate random correlated data between a  binary and a continuous variable',\n",
       " 'What is the equivalent confidence interval for a directional hypotheses?',\n",
       " 'Getting started with time series in R',\n",
       " \"What's the correct distribution for page reading time?\",\n",
       " 'Strategies for Recovering Missing Data',\n",
       " 'Estimating adjusted rate ratios for categorical independent variable measured only as population percents',\n",
       " 'Can lmer() use splines as random effects?',\n",
       " 'What is the null hypothesis in the Mann-Whitney test?',\n",
       " 'Is it possible to determine the relationship between X and Y, with only X and Z, where Z = X + Y?',\n",
       " 'Split-split-plot design and lme',\n",
       " 'What is the probability that a normal distribution with infinite variance has a value greater than its mean?',\n",
       " 'First quick glance at a dataset',\n",
       " 'Is it okay to convert PCA scores to absolute value?',\n",
       " 'How to calculate the interaction standard error of a linear regression model in R?',\n",
       " 'How can I work around \"lumpiness\" in simulated maximum likelihood estimation?',\n",
       " 'Enrichment analysis by gene duplication level',\n",
       " 'How to test equality of variances with circular data',\n",
       " 'Managing error with GPS routes (theoretical framework?)',\n",
       " 'Multiple dependent variables in factorial design',\n",
       " 'Coordinate descent for the lasso or elastic net',\n",
       " \"Nonparametric Pearsons's r-value and p-value\",\n",
       " 'Multi class LDA vs 2 class LDA',\n",
       " 'Association between 2 categorical data (using SPSS)',\n",
       " 'Definition of \"degree of interaction\" in the MARS model',\n",
       " 'High quality publishing house for books in the field of statistics',\n",
       " 'What algorithm is used in linear regression?',\n",
       " 'How do you put values over a simple bar chart in Excel?',\n",
       " 'Feature selection and latent variables',\n",
       " 'How to find out if an online poker-site is fair?',\n",
       " 'Relation between two ordinal variables',\n",
       " 'Variance explained for binomial proportions',\n",
       " \"What's the difference between Normalization and Standardization?\",\n",
       " 'Which model should I use for Cox proportional hazards with paired data?',\n",
       " 'Testing for significance of difference between two populations',\n",
       " 'Is the symmetry of u function for  the robust M estimation mandatory?',\n",
       " 'Can I trust a regression if variables are autocorrelated?',\n",
       " 'Calculate initial propabilities of Hidden Markov Chain',\n",
       " 'Filter out information in Kalman Filter',\n",
       " 'PCA, ICA and Laplacian eigenmaps',\n",
       " 'Is it valid to analyze signal detection data without employing metrics derived from signal detection theory?',\n",
       " 'Random assignment: why bother?',\n",
       " 'Evaluating clusters of first-order Markov chains',\n",
       " 'libSVM for unbalanced data',\n",
       " 'How to compute the number of games that must be played for the better team to win 70% of the time?',\n",
       " 'How do I do group wise clustering in R?',\n",
       " 'Calculating prediction intervals when using cross validation',\n",
       " 'Can we use cross-validation to measure how well a distribution fits sample data?',\n",
       " 'Is a strong background in maths a total requisite for ML?',\n",
       " 'Predicting a maximum value with little data',\n",
       " 'Does the GARCH approach model prices or returns?',\n",
       " 'Comparable salary in a different city calculations',\n",
       " 'What software (paid or free) exists for learning large datasets?',\n",
       " 'Comprehensive overview of loss functions?',\n",
       " 'Two studies, opposite results: Am I allowed to compute an overall mean using random-effect model?',\n",
       " 'Benchmark dataset for decision tree algorithm',\n",
       " 'Using time series analysis to analyze/predict violent behavior',\n",
       " 'Sampling extremely large and diverse dataset?',\n",
       " 'Natural language processing application in algorithmic trading and sports betting',\n",
       " 'One-way repeated measures ANOVA and missing data',\n",
       " 'How to calculate the standard deviation on a sample set with irregular time periods',\n",
       " \"What's the best formula to fit the distribution of website user number over a day\",\n",
       " 'Best method for transforming low discrepancy sequence into normal distribution?',\n",
       " 'References for use of symplectic geometry in statistics?',\n",
       " 'When to use proportional odds model?',\n",
       " '\"Strength\" of hypothesis tests',\n",
       " 'When shall I include an intercept in linear regression?',\n",
       " 'Identifying the communicating classes and stating whether they are period, aperiodic, recurrent or transient',\n",
       " 'What will be the correct answer, if we modify the \"Best statistics question ever\"?',\n",
       " 'What is the probability of a team winning many golf tournaments when conducted with handicaps (i.e., has sandbagging occurred)?',\n",
       " 'Does a stationary process implies a normal distribution of the data?',\n",
       " 'Interpretation and validation of a Cox proportional hazards regression model using R in plain English',\n",
       " 'Iterated expectations theory',\n",
       " '\"Studentized\" bootstrap confidence interval for variance of OLS error terms',\n",
       " 'Unique subsequences of the human genome',\n",
       " 'Identifying sequential patterns',\n",
       " 'What is one-step ahead static forecast?',\n",
       " 'Interpretation of an integrable time series of an order zero',\n",
       " 'How to summarize data by group in R?',\n",
       " 'Assumptions of factor analysis',\n",
       " 'Which non-parametric test can I use to identify significant interactions of independent variables?',\n",
       " 'Is there an algorithmic way to check if a dataset oscillates?',\n",
       " 'How can I create nice graphs automatically?',\n",
       " 'How to do weighted pair hierarchical clustering in R?',\n",
       " 'Inaccurate R prediction',\n",
       " 'Jonckheere-Terpstra interpretation',\n",
       " 'What is the best method for checking convergence in MCMC?',\n",
       " 'Structural shifts',\n",
       " 'What are the options in proportional hazard regression model when Schoenfeld residuals are not good?',\n",
       " 'What is the confidence interval calculated in a spectral density periodogram in R?',\n",
       " 'The relationship between the number of support vectors and the number of features',\n",
       " 'Understanding summations in COV formula for time series',\n",
       " \"Time series 'clustering' in R\",\n",
       " 'How do I deal with datasets that have many values out of range / over threshold?',\n",
       " 'What resources/methods exist for testing/validation or evaluation of Statistical Methods',\n",
       " 'Learning from unordered tuples?',\n",
       " 'Deep learning vs. Decision trees and boosting methods',\n",
       " 'How to calculate probabilities for a 4 by 2 table?',\n",
       " 'Akaike Information Criterion and composite variables',\n",
       " 'I want to build a crime index and political instability index based in news stories',\n",
       " 'Assuming $u\\\\\\\\sim N(0,\\\\\\\\sigma^2)$ when y is highly skewed',\n",
       " 'Algorithm to evaluate whether you should buy now or wait',\n",
       " 'What are good values to assume for correlation and sphericity when estimating sample size for a within-subjects design',\n",
       " 'How to interpret factor scores saved using the regression method in SPSS?',\n",
       " 'Why squared residuals instead of absolute residuals in OLS estimation?',\n",
       " 'Summing values of state transitions accumulated in an absorbing Markov chain',\n",
       " 'Maximum likelihood of function of the mean on a restricted parameter space',\n",
       " 'Defining the \"uniformity\" of a dataset',\n",
       " 'Regression when the response is a proportion that can be 0 or 1',\n",
       " 'Naive Bayes fails with a perfect predictor',\n",
       " 'Textbook deriving Metropolis-Hastings and Gibbs Sampling',\n",
       " 'Using the hypergeometric distribution for skipping events in transcriptome sequencing',\n",
       " 'Whether to report untransformed data when performing ANOVA on transformed data?',\n",
       " 'Is it possible to specify a lmer model without any fixed effects?',\n",
       " 'Bootstrapped parameter and fit estimates with non-normality for structural equation models',\n",
       " 'What is the correct way to test for significant differences between coefficients?',\n",
       " 'Computational considerations of multinomial probit versus binomial probit',\n",
       " 'Significance of multinomial data',\n",
       " 'Can p values be used to show impact of treatment',\n",
       " 'Statistical test for whether a process is a red noise',\n",
       " 'Using Fractional Polynomials for Logistic Regression Modelling in R',\n",
       " 'Extracting numbers from excel table items for diagram visualization',\n",
       " 'Mean of all natural numbers?',\n",
       " 'Mean and Median properties',\n",
       " 'How to interpret this output in Dimension Reduction?',\n",
       " 'Study replication from a Bayesian point of view',\n",
       " 'How can I avoid artificially inflating percentages?',\n",
       " 'Multiple comparisons with ANOVA including one between and one within-subject effect',\n",
       " 'Frequentist statistics references for someone well versed in modern probability theory',\n",
       " 'Why would I use ANOVA instead of a Rank-Sum test?',\n",
       " 'Multiple paired samples t-tests?',\n",
       " 'Relative predictive power of predictors used in time series models like kalman filter',\n",
       " 'Problem with singular covariance matrices when doing Gaussian process regression',\n",
       " 'How to distribute a prize among a group of people given their scores?',\n",
       " 'Inter-rater statistic for skewed rankings',\n",
       " 'Is a partial F-Test on a model reduced by only one variable valid?',\n",
       " 'Calculating the transfer entropy in R',\n",
       " 'Custom power analysis in R',\n",
       " 'Sparse representations for denoising problems',\n",
       " 'Slope Derivation for the variance of a least square problem via Matrix notation',\n",
       " 'Asymptotic relative efficiency of median vs mean for Student t distribution',\n",
       " 'Statistics based on fractal mathematics',\n",
       " 'Calculate error for monthly exchange rates',\n",
       " 'Can you recommend an online survey platform for 5k+ participants?',\n",
       " 'Generating random variables satisfying constraints',\n",
       " 'Regression line envelope from Census ACS data',\n",
       " \"How to calculate the number of trials for a 'meaningful' result\",\n",
       " 'Confidence interval for success probability in negative binomial experiment',\n",
       " 'Bayesian models and exchangeability',\n",
       " 'What are essential rules for designing and producing plots?',\n",
       " 'Random walks in multinomial case ',\n",
       " 'Growing hierarchical self-organizing maps',\n",
       " 'How to normalize data to let each feature lie between [-1,1]?',\n",
       " 'Binary classification of DNA sequence motifs',\n",
       " 'Dynamic decision networks',\n",
       " 'Is there any identification assumption for IV?',\n",
       " 'How to test for normality in a 2x2 ANOVA?',\n",
       " 'Is it \"rows by columns\" or \"columns by rows\"?',\n",
       " 'Posterior distribution for multinomial parameter',\n",
       " \"What can you do with 'crazy' data?\",\n",
       " 'Deriving ${\\\\\\\\rm var}(\\\\\\\\overline{X})$ from expected value definition',\n",
       " 'On the use of the periodogram of a time series',\n",
       " 'Mixed model specification with nlme in R',\n",
       " 'How to model categorical (discrete-valued) time series?',\n",
       " 'Rank transformed 2-way ANOVA',\n",
       " 'How do you handle the situation where the residual variance is very high compared to the other variance parameter estimates?',\n",
       " 'A problem in probability theory',\n",
       " 'How do extreme values scale with sample size?',\n",
       " 'Visualize before and after score',\n",
       " 'What are some techniques for sampling two correlated random variables?',\n",
       " 'Why does continuous Bayesian analysis seem to give this contradictory result?',\n",
       " 'Grouped income data in Stata or SAS',\n",
       " 'Appropriate probability threshold for Jarque-Bera test',\n",
       " 'Steady state probabilities for a continuous-time Markov chain',\n",
       " 'Two fair coins, why increasing trials in prop.test not reduce false positive?',\n",
       " 'Fitting model for two normal distributions in PyMC',\n",
       " 'How to determine calibration accuracy/uncertainty of a linear regression?',\n",
       " 'Linear contrast OLS - Simultaneous',\n",
       " 'How can I calculate margin of error in a NPS (Net Promoter Score) result?',\n",
       " 'Random Variable as a function of mean probability',\n",
       " 'Geometric intuition for why an outer product of two vectors makes a correlation matrix?',\n",
       " 'how Neural Network works on image recognition',\n",
       " 'Difference between likelihood principle and repeated sampling principle',\n",
       " 'Unknown p-value calculation',\n",
       " 'Inner loop overfitting in nested cross-validation',\n",
       " 'Bound on variance of sum of variables',\n",
       " 'How to estimate an upperbound for logistic regression by only 5 to 7 data points?',\n",
       " 'Why is the tick marker for zero after the bar in this qplot bar chart? ',\n",
       " 'Is there a difference between an index score and a composite score?',\n",
       " 'Interpreting main effect coefficient in different models',\n",
       " 'How to sample uniformly from an intersection of simplices?',\n",
       " 'Maximum Likelihood Estimation question - minimum log likelihood',\n",
       " 'Decision theory - reject option',\n",
       " 'Confidence Intervals for Holdout R^2?',\n",
       " 'Trouble using convert() in RTAQ package',\n",
       " 'How to equate questionnaires with different response scales',\n",
       " 'How to calculate overall error from a series of dependent experiments?',\n",
       " 'How can you summarize growth in the \"production\" rate per person?',\n",
       " '(Non-linear) Transformation of confidence interval for multinomial parameters',\n",
       " 'Given a 10D MCMC chain, how can I determine its posterior mode(s) in R?',\n",
       " 'Dirichlet processes for supervised learning?',\n",
       " 'What should I do while designing stratified cluster randomized sampling survey if cluster size is large/smaller than I expect?',\n",
       " 'How to model a bounded lognormal dependent variable with many zeros',\n",
       " 'General Non-linear Regularized Models',\n",
       " 'Given a set of sub-graphs, how to infer the underlying graph?',\n",
       " '\"Central limit theorem\" for weighted sum of correlated random variables',\n",
       " 'Is SVM-RFE a \"filter\" or a \"wrapper\" feature selection algorithm?',\n",
       " 'Latent Variables',\n",
       " 'Normalize random numbers picked up into lognormal distribution',\n",
       " \"Why don't I get the p-value doing an ANOVA test?\",\n",
       " 'Sample session for analysis of conjoint data in R',\n",
       " 'What is the correct way of estimating the proportion of individuals in a population from a count of their individual parts?',\n",
       " 'Derivation of M-step for pLSA',\n",
       " 'What is the distribution of the sum of non i.i.d. gaussian variates?',\n",
       " 'Online algorithm for mean absolute deviation and large data set',\n",
       " 'Correlation with binary response and Likert scale response',\n",
       " 'R: Defining a new continuous distribution function to use with Kolmogorov-Smirnov Test',\n",
       " 'What can go wrong if I include two categorical variables and intercept in linear regression?',\n",
       " 'Help on SVM for road image processing',\n",
       " \"What is a good use of the 'comment' function in R?\",\n",
       " 'Monomial distribution of $X^a \\\\\\\\cdot Y^b$ ',\n",
       " 'Intra- and inter-rater reliability on the same data',\n",
       " 'How to estimate the deposit mix of a bank using interest rate as the independent variable?',\n",
       " 'How to perform linear regression with categorical factors?',\n",
       " 'Variance gamma process, simulation and plot differ from ideal',\n",
       " 'Difference between the Stopping Criteria',\n",
       " 'How best to analyze length of stay data in a hospital-based RCT?',\n",
       " 'Problems with libRblas.so on ubuntu with rpy2',\n",
       " 'Combine MultiClassClassifier and MetaCost in WEKA and make cost matrices for each binary classifier',\n",
       " 'How to form a meaningful statistical indicator to reflect user interaction with a website',\n",
       " 'Is it possible to do data analysis in Open Office Calc?',\n",
       " 'Theory on discriminant analysis in small sample size conditions',\n",
       " 'How to calculate average time in service',\n",
       " 'Updating \"support\" or \"range\" in transformation variable',\n",
       " 'Adding a quadratic term: should I use the squared original (and not the squared standardized?)',\n",
       " 'Distance threshold for clustering',\n",
       " 'The effectiveness of coordinate ascent',\n",
       " 'Why is pruning not needed for random forest trees?',\n",
       " 'How to get standard error of a function (delta method vs. simulation)?',\n",
       " 'Estimating the effect of one time series on another in the context of personal health',\n",
       " 'How to classify country names given possible alternate spellings or abbreviations?',\n",
       " 'In a neural network with N covariates, are >N hidden units useless?',\n",
       " 'Time series with multiple subjects and multiple variables',\n",
       " 'What does it mean for a study to be over-powered?',\n",
       " 'Most relevant algorithms for Collaborative Filtering to test against',\n",
       " 'Sample size for multiple regression: How much more data do I need?',\n",
       " 'How do I use vector auto regression using the statsmodels library in python?',\n",
       " 'How to report  Games-Howell post hoc tests following ANOVA?',\n",
       " 'A random censored regression problem?',\n",
       " 'Difference between t-test and ANOVA in linear regression',\n",
       " 'Probability of independent events within a specified window',\n",
       " 'Robust multivariate Gaussian fit in R',\n",
       " 'Variance on the sum of predicted values from a mixed effect model on a timeseries',\n",
       " 'Data visualization from hierarchical clustering',\n",
       " 'Confidence intervals for mean, when variance is unknown',\n",
       " 'Is it appropriate to identify and remove outliers because they cause problems?',\n",
       " 'Subset data by month in R',\n",
       " 'Central limit theorem versus law of large numbers',\n",
       " 'Alpha Expansion algorithm over graph with vertices having different sets of labels',\n",
       " 'Normalization of series',\n",
       " 'What problem do shrinkage methods solve?',\n",
       " 'Is there a style guide for statistical graphs intended for presentations?',\n",
       " 'Multivariate ordered logit or probit',\n",
       " 'Evaluating probability mass function for a discrete random variable',\n",
       " 'Hypothesis testing with very high z value',\n",
       " 'Significance of different mean values for two processes',\n",
       " 'Probability of a relation on the uniform distribution of points over 2D space',\n",
       " 'Overall standard deviation',\n",
       " 'Calculation of Relative Risk Confidence Interval',\n",
       " 'Cloud computing platforms for machine learning',\n",
       " 'Statistics regarding different data sets having a common response variable',\n",
       " 'Expectation of $\\\\\\\\left(X-M\\\\\\\\right)^T\\\\\\\\left(X-M\\\\\\\\right)\\\\\\\\left(X-M\\\\\\\\right)^T\\\\\\\\left(X-M\\\\\\\\right)$',\n",
       " 'How to use weights in function lm in R?',\n",
       " 'Combining two time-series by averaging the data points',\n",
       " 'Machine Learning and training time: is it really relevant?',\n",
       " 'Predicting response propensity in a rolling data collection',\n",
       " 'If I take n-standard deviations of my data what would be my confidence level with the estimate?',\n",
       " 'Tobit with difference-in-differences specification',\n",
       " 'Should I be concerned if the cells of values obtained from bootstrapping are correlated?',\n",
       " 'Probability for finding a double-as-likely event',\n",
       " 'Bootstrapping standard normal data does not produce a mean of zero and standard deviation of 1',\n",
       " 'Matching covariates between more than 2 groups',\n",
       " 'Removing factors from a 3-way ANOVA table',\n",
       " 'Matlab image blending',\n",
       " 'Examples of when confidence interval and credible interval coincide',\n",
       " 'Merging the results of a statistical test in different conditions and calculating p-value',\n",
       " 'inverse logistic regression with binary covariates',\n",
       " 'Form of $\\\\\\\\chi^2$ test',\n",
       " 'Estimate the \"meaningful\" predictors for a value in a CART model (rpart)',\n",
       " 'Parametric Surface Reconstruction from Contours with Quick Rescaling',\n",
       " 'Interpreting plot of residuals vs. fitted values from Poisson regression',\n",
       " 'How to calculate  95% confidence intervals for  partial eta squared and eta squared?',\n",
       " 'How to calculate the rowMeans with some single rows in data?',\n",
       " 'Can someone explain Gibbs sampling in very simple words?',\n",
       " 'Which form of data analysis would be most appropriate, T-Test or Chi-Squared?',\n",
       " 'Sampling in discrete distribution',\n",
       " 'How to generate two correlated categorical variables?',\n",
       " 'Model averaging in prediction -- \"Wisdom of the Crowd\"',\n",
       " 'Origin of strange formula for equilibrium standard deviation',\n",
       " 'What is the proper ratio of mean squares for a two-level nested ANOVA?',\n",
       " 'Searching for geometric network dataset',\n",
       " 'Differences between Baum-Welch and Viterbi training',\n",
       " 'Normalization across columns in linear regression',\n",
       " 'How to shape a distribution?',\n",
       " 'Is it possible for  $R^2$ of a regression on two variables be higher than the sum of $R^2$ for two regressions on the individual variables?',\n",
       " 'Project management for remote collaboration in prediction',\n",
       " 'How to draw random samples from a negative binomial distribution in R?',\n",
       " 'How can you relate the coefficients of a multivariate regression of ${\\\\\\\\bf Y} \\\\\\\\sim {\\\\\\\\bf X}$ to the coefficients of ${\\\\\\\\bf X} \\\\\\\\sim {\\\\\\\\bf Y}$?',\n",
       " 'In a hidden Markov model, how do all observations and one state give you all states?',\n",
       " 'What should I do to compare different sets of data?',\n",
       " 'Standard deviation of standard deviation',\n",
       " 'Interpreting logistic regression output in R',\n",
       " 'Must there be \"an effect to be mediated\" in mediational analysis (i.e., must IVs & DVs be correlated)?',\n",
       " 'What is the significance of logistic regression coefficients?',\n",
       " 'Bagging with oversampling for rare event predictive models',\n",
       " 'Covariates in regression models',\n",
       " 'What is the distribution of maximum of a pair of iid draws, where the minimum is an order statistic of other minima?',\n",
       " 'Likelihood of (bivariate?) Bernoulli distribution',\n",
       " 'How to test hypothesis for group differences?',\n",
       " 'Pearson correlation of data sets with possibly zero standard deviation?',\n",
       " 'Can the Johnson-Neyman moderation interaction technique be used with a categorical moderator?',\n",
       " 'What is the difference between hazard and crude ratio?',\n",
       " 'Dynamic estimation of required sample size in R',\n",
       " 'Probability of several simulataneous events in Markov chains',\n",
       " 'Is it at all defensible to stratify a data set by the size of the residual and do a two-sample comparison?',\n",
       " 'Confusion related to scaling factors in HMM',\n",
       " 'Long-tailed distribution of time events',\n",
       " 'Strange behavior of predictive model gain curve',\n",
       " 'Should you use a regression line predicting drop from new car sale price from car age to set the price of a used car?',\n",
       " 'Fill colors in Gnuplot',\n",
       " 'How many samples should be taken to capture the mean and variance of the original set with given error?',\n",
       " 'Persistence in time series',\n",
       " 'Kriging on log transformed rainfall data',\n",
       " 'How to setup xreg argument in auto.arima() in R?',\n",
       " 'What do I do if my predicted values are out of the dependent variable range?',\n",
       " 'How to increase longer term reproducibility of research (particularly using R and Sweave)',\n",
       " 'Generate three correlated uniformly random variables',\n",
       " 'How to derive the probability mass function for Zero-inflated Poisson Distribution?',\n",
       " 'How do I estimate the parameters of a log-normal distribution from the sample mean and sample variance',\n",
       " 'How princomp() works?',\n",
       " 'Will logistic regression fit this situation?',\n",
       " 'Mean regression factor scores and attributes cross tabulation yields. Why are all expected signs reversed?',\n",
       " 'General question on oversampling',\n",
       " 'How to interpret inconsistent Beta values in different steps of hierarchical regression analysis?',\n",
       " 'What is the difference between Exploratory Factor Analysis and Principal Components Analysis (PCA)?',\n",
       " 'How can I test $H_0:\\\\\\\\sigma^2_1=\\\\\\\\sigma^2_2$?',\n",
       " 'Significance test for non-normal population?',\n",
       " 'Inferences about non-overlapping lines',\n",
       " 'Is goodness of fit needed for regression models when interpreted causally?',\n",
       " 'Alternative name for \"incidence rate ratio\"',\n",
       " 'How do you predict the value of new instance, when the training data were normalized?',\n",
       " 'Why autocovariances could fully characterise a time series?',\n",
       " 'How to determine if predictor in piece-wise linear regression is significant?',\n",
       " 'Visualizing standard deviation on a Cartesian plot',\n",
       " 'Finding k-representatives from a 2 dimensional data set',\n",
       " 'Repeated measures ANOVA or Friedman test?',\n",
       " 'How to report permutation ANOVA results in a scientific publication?',\n",
       " 'MSE of filtered noisy signal - Derivation',\n",
       " 'How to deal with a variable that ranges from 0 to 1 and the distribution has two spikes at these values with normal-like distribution in the middle.',\n",
       " 'Bayesians: slaves of the likelihood function?',\n",
       " 'What does weighted cumulative frequency distribution mean?',\n",
       " 'What is the relationship between a chi squared test and test of equal proportions?',\n",
       " 'Is AdaBoost less or more prone to overfitting?',\n",
       " 'How to check variance with GARCH (1,1) model?',\n",
       " 'Alternative to SAS Enterprise Guide',\n",
       " 'How to calculate regularization parameter in ridge regression given degrees of freedom and input matrix?',\n",
       " 'Suggestions on how to merge multiple datasets with an imperfect ID across databases?',\n",
       " 'How to compute the Kullback-Leibler divergence when the PMF contains 0s?',\n",
       " 'Testing the importance of an item among a finite set of items',\n",
       " 'Show that Y/Z does not have finite expectation',\n",
       " \"What's the difference between principal components analysis and multidimensional scaling?\",\n",
       " 'Compressing an integer series',\n",
       " 'Simultaneous confidence intervals for multinomial proportions: pooling replicates or not?',\n",
       " 'Moving average of irregular time series data using R',\n",
       " 'Are there any good popular science book about statistics or machine learning?',\n",
       " 'PCA eigenvectors with dimensionality reduction',\n",
       " 'Boosting for regression systems',\n",
       " 'What is a \"Unit Information Prior\"?',\n",
       " 'How to get scored combination of features',\n",
       " 'Small Beta weight equals small structural coefficient',\n",
       " 'Accurate estimates of the variance of maximum likelihood estimators',\n",
       " 'Is there any way to fit an exponential model with a discrete predictor in R?',\n",
       " 'What statistical model is used to calculate the test results for GWO?',\n",
       " 'Proof for a binomial equation',\n",
       " 'Probability of getting between',\n",
       " 'Approximate vs. Strict Factor model specification in R',\n",
       " 'Order statistics of absolute value of bivariate normal distribution',\n",
       " 'Adding training examples to Bayesian classifier reduces accuracy',\n",
       " 'Why does adding statistically significant interaction reduce true positives?',\n",
       " 'Pattern finding approaches',\n",
       " 'Is there an unpaired version of the sign test?',\n",
       " 'Interpreting odds ratios with log-transformed continuous variables in a logistic regression',\n",
       " 'Calculating necessary sample size using bootstrap',\n",
       " 'Product of Independent Gaussian Variables',\n",
       " 'Difference in  using normalized gradient and gradient',\n",
       " 'Modeling a Poisson process with 10000 events per minute',\n",
       " 'Limits of integration for computing a marginal distribution',\n",
       " 'Generating random samples from a custom distribution',\n",
       " 'Should I re-scale the PCA score before classification?',\n",
       " 'How to compute the optimal ridge regression model',\n",
       " 'Plotting decay in clustering?',\n",
       " 'Multiple regression and OLS. How to choose the best \"non-linear\" specification?',\n",
       " 'An R function for performing searches',\n",
       " 'Identifiying performance of individual feature components in accuracy of support vector methods',\n",
       " 'What is the effect of dichotomising variables?',\n",
       " 'Sampling from a fixed population',\n",
       " 'When to use robust standard errors in Poisson regression?',\n",
       " 'Logistic regression for questionnaires on 7 points Likert scale',\n",
       " 'How do I control for a confounding variable with this distribution?',\n",
       " 'Why does the OLS estimator simplify as follows for the single regressor case?',\n",
       " 'How do you identify the variables that separate several groups?',\n",
       " 'k-means quick cluster analysis error in SPSS',\n",
       " 'Can LSA be used for document similarity?',\n",
       " 'How should I transform non-negative data including zeros?',\n",
       " 'What is the meaning of rank in the context of change-detection?',\n",
       " 'How to compare results obtained by non-linear regression',\n",
       " 'Efficient fitting of noncentral chi-squared distribution to data?',\n",
       " 'How to digest statistical context?',\n",
       " \"How to interpret the T-depiction of multiple comparisons used in R's multcompView package?\",\n",
       " 'Kernel logistic regression',\n",
       " 'R language-statistics-significance testing',\n",
       " 'Shapiro-Francia test error',\n",
       " 'Looking for a good followup to the Stanford AI course',\n",
       " 'What are latest statistical challenges regarding missing data?',\n",
       " 'Advanced Excel graphs',\n",
       " \"How to estimate a critical value of Spearman's correlation for n=100?\",\n",
       " 'Imputation of missing response variables',\n",
       " 'How to call intensity domain elements?',\n",
       " 'Hypothesis testing based on 5 samples',\n",
       " 'Do Bayesians ever argue there are cases in which their approach generalizes/overlaps with the frequentist approach?',\n",
       " 'What is the correct way to calculate the explained variance of each EOF as calculated from a gappy data set?',\n",
       " 'Unconditional inference in the multinomial model',\n",
       " 'Expected value of modified geometric distribution',\n",
       " 'Difference between logistic regression and neural networks',\n",
       " 'Interpreting 2D correspondence analysis plots',\n",
       " 'Is it possible to calculate Pearson/Spearman correlation of more than two judges in MATLAB?',\n",
       " 'Individuals standard deviations and/or standard errors for groups after implementing ANOVA?',\n",
       " \"Are ecologists the only ones who didn't know that the arcsine is asinine?\",\n",
       " 'Statistical model of a website',\n",
       " 'Java code for thin SVD',\n",
       " 'Does it ever make sense to treat categorical data as continuous?',\n",
       " 'What methods can be used to determine the Order of Integration of a time series?',\n",
       " 'Measuring error in the lag when cross-correlating data',\n",
       " 'Interpretation of the Granger causality test',\n",
       " 'Strange pattern of residuals',\n",
       " 'How to use LOF for outlier detection as I have training and test dataset?',\n",
       " 'How to produce a vector based on two probability distributions?',\n",
       " 'Symmetric measure of variable contribution to a regression',\n",
       " 'How to decode NOAA TREND files?',\n",
       " 'Merging spatial and temporal clusters',\n",
       " 'Computation of composite score or summed score from Likert scale',\n",
       " 'Validating principal component analysis',\n",
       " 'Unable to figure out right transformation',\n",
       " 'Given a kernel, how to find mapping phi?',\n",
       " 'Classification accuracy is better with noise added?',\n",
       " 'Excel pivot table with multiple total columns',\n",
       " 'What bandwidth should I use for my kernel density estimation?',\n",
       " 'Control Chart Structure - X axis',\n",
       " 'Problem with ets from R forecast package',\n",
       " 'What is a lift chart?',\n",
       " 'Comparing player percentages effectiveness with different sample sizes',\n",
       " 'How to interpret relatively good -2LL value & Nagelkerke $R ^2$ without any significant IVs',\n",
       " 'Find the minimum of an expensive-to-sample noisy paraboloid',\n",
       " 'Do hidden Markov models contain Markov chains?',\n",
       " 'Correlation analysis and correcting p-values for multiple testing II',\n",
       " 'AIC for semi-parametric model',\n",
       " 'Formula for comparing the youth of different groups?',\n",
       " 'Interpolating the empirical cumulative function',\n",
       " 'Aggregating results from Arima runs R',\n",
       " 'How to intuitively understand formula for estimate of pooled variance when testing  differences between group  means?',\n",
       " 'Regression vs classification within decision trees in OpenCV',\n",
       " 'Quantile-Quantile Plot with Unknown Distribution?',\n",
       " 'Statistical significance when A/B test has multiple values',\n",
       " 'How to assess predictive power of set of categorical predictors of a binary outcome? Calculate probabilities or logistic regression?',\n",
       " 'calculate the rate of change',\n",
       " 'How do I transform the parameters of the F distribution?',\n",
       " 'R package for marketing',\n",
       " 'What is the difference between GARCH and ARMA',\n",
       " 'How do I know what the slope of the line is by being provided Theta(1) (aka ø(1)) ? (Cost function and linear regression)',\n",
       " 'Controlling p-value in SPSS',\n",
       " 'Correlation of non-uniform time-series data?',\n",
       " 'Is a multigroup confirmatory factor analysis appropriate for comparing the measurement model across two groups?',\n",
       " 'p-value for nonlinear equation',\n",
       " 'R: compute correlation by group',\n",
       " 'Imputing missing values in time series using SAS',\n",
       " 'Viterbi realignment',\n",
       " 'Finding anomalies in industrial part failures',\n",
       " '2x2x2 plus control group: nested ANOVA?',\n",
       " 'Double exponential smoothing in multivariate multilevel panel regression',\n",
       " 'Predicting total number of bugs based on number of bugs revealed by each tester',\n",
       " 'Temporal analysis of variation in random effects',\n",
       " 'Generating a quality score',\n",
       " 'Linear mixed model, negative information criteria values and Hessian matrix not positive definite',\n",
       " 'How to avoid multicolinearity in SVM input data?',\n",
       " 'Property of KL-divergence',\n",
       " 'Distribution of product of sums of a set',\n",
       " 'Do we need to apply the same transformation of predictors on a test dataset?',\n",
       " 'Least angle regression keeps the correlations monotonically decreasing and tied?',\n",
       " 'Why is the Poisson distribution chosen to model arrival processes in Queueing theory problems?',\n",
       " 'Significance testing of slopes with replicates',\n",
       " 'Random forest implementation for sparse data',\n",
       " 'A measure of the \"flatness\" of log likelihood at the MLE',\n",
       " 'What are some standard practices for creating synthetic data sets?',\n",
       " \"Friedman's test for binary data - possible or not?\",\n",
       " 'How to quantify statistical insignificance?',\n",
       " 'How does gentle boosting differ from AdaBoost?',\n",
       " 'Simple linear regression',\n",
       " 'Subsample of a random sample: random sample?',\n",
       " 'Which test to use for a set of data',\n",
       " 'Achieving statistical significance in an A/B test',\n",
       " 'Issues with the Huynh-Feldt values using either Anova or ezANOVA in R?',\n",
       " 'Which of the parameters in LibSVM is the slack variable?',\n",
       " 'Square root of inverse gamma distribution?',\n",
       " 'Checking If two experiments match',\n",
       " 'Confidence interval for gam model',\n",
       " 'How to change data between wide and long formats in R?',\n",
       " 'How to calculate the confidence interval of the mean of means?',\n",
       " 'How to test if a distribution is exponential in Stata?',\n",
       " 'Standard error of a ratio',\n",
       " 'Why do LR and Wald p-values differ by orders of magnitude?',\n",
       " 'Likelihood ratio test overestimates significance. More appropriate test?',\n",
       " 'Exploratory factor analysis among items with different measurements',\n",
       " \"What is the difference between effectiveness and efficacy in determining the benefit of therapy 'A' on condition 'B'?\",\n",
       " 'Good Internet resource for preparation for studying mathematical statistics',\n",
       " 'What is the difference between 2x2 factorial design experiment and a 2-way ANOVA?',\n",
       " 'fitting an exponential function using least squares vs. generalized linear model vs. nonlinear least squares',\n",
       " 'Density of multivariate normal given linear condition',\n",
       " 'How to test homoscedasticity when the errors are DEPENDENT?',\n",
       " 'How to create a dataset with conditional probability?',\n",
       " 'Distribution of the difference of two independent uniform variables, truncated at 0',\n",
       " 'Interaction Terms and Logit Models',\n",
       " 'Find the probability of having k red balls after d days',\n",
       " 'Is it necessary to detrend and decycle time-series data when using machine learning methods?',\n",
       " 'Three-level nested logit in R',\n",
       " 'How do I do multiscale HMM classification?',\n",
       " 'Using [R] to connect to a Web Service',\n",
       " 'Naive Bayes classification for \"That\\'s what she said\" problem',\n",
       " 'GLM on unbalanced design',\n",
       " 'What is a market basket problem?',\n",
       " 'Are those two models equals?',\n",
       " 'Difference between panel data & mixed model',\n",
       " \"Risk assesment and non-statistician's perception of percentages\",\n",
       " 'Cross validation using experimental design methods?',\n",
       " 'Interpreting significance of predictor vs significance of predictor coeffs in multinomial logistic regression',\n",
       " 'Is it Multiple Regression that I should chose?',\n",
       " 'What would be the most potent free CAS for mathematical statistics?',\n",
       " \"How to do a 'beer and diapers' correlation analysis\",\n",
       " 'Nelder-Mead simplex (fminsearch) and crossvalidation (cvpartition) with a nested function approach - Valid?',\n",
       " 'Derivative of $H$ with respect to $W$ when performing generalized linear squares',\n",
       " 'significan\\u200bt lags in causality test?',\n",
       " 'Forecasting a complex time series by splitting into subseries',\n",
       " 'Most interesting statistical paradoxes',\n",
       " 'When to use an offset in a Poisson regression?',\n",
       " 'Why applying model selection using AIC gives me non-significant p-values for the variables',\n",
       " 'Mixed (type III) model ANOVA in R and GraphPad Prism',\n",
       " 'Asymmetrical selective sampling for linear classification',\n",
       " 'What are the breakthroughs in Statistics of the past 15 years?',\n",
       " 'Applicability of chi-square test if many cells have frequencies less than 5',\n",
       " \"How do I interpret this McNemar's agreement plot generated by SAS?\",\n",
       " 'Why does including latitude and longitude in a GAM account for spatial autocorrelation?',\n",
       " 'Confidence interval for the biggest difference between means',\n",
       " 'How to simulate data that satisfy specific constraints such as having specific mean and standard deviation?',\n",
       " 'How to draw a Hasse diagram of a large number of pairwise comparisons?',\n",
       " 'Consequences from comparison of two models',\n",
       " 'How to deal with an error such as \"Coefficients: 14 not defined because of singularities\" in R?',\n",
       " 'Half-normal distributed DV in generalized linear model',\n",
       " 'Summarization of correlated but noisy measurement data',\n",
       " 'Calculating standard error for funnel plots',\n",
       " 'Factor analysis and regression',\n",
       " 'Decision tree for censored data',\n",
       " 'Interpretation problems with hypothesis testing ',\n",
       " 'Presenting marginal effects of logit with fixed effects',\n",
       " 'What is perplexity?',\n",
       " 'How to add multiple question to a variable in AMOS?',\n",
       " 'What is the correlation coefficient between two zero random variables?',\n",
       " 'Standard error of binomial variables derived from continuous variables',\n",
       " 'How do I model time to an event with online data?',\n",
       " 'Possible non-determinism in transcan from rms package',\n",
       " \"How to calculate standard errors in OLS without inverting the X'X matrix?\",\n",
       " 'How to present and compare mixed models vs. regular linear regression models?',\n",
       " 'Invertibility of AR(p) model',\n",
       " 'multiple continuous independent variables, single dependent var: which analysis?',\n",
       " 'Comparing rates of events between an exposed and unexposed group',\n",
       " 't-test that means are the same?',\n",
       " 'Multiple regression with categorical variables',\n",
       " 'simple sampling method for a Kernel Density Estimator',\n",
       " 'Predicting dichotomous outcome of temporal data set with covariates',\n",
       " 'Converting an SPSS file to a SAS file?',\n",
       " 'Modification of \"corrected repeated k-fold cv test” when also averaging Random Forest results across multiple sampling seeds?',\n",
       " 'What is the distribution of $\\\\\\\\chi^n_k$?',\n",
       " 'How can it be proved that if number of samples are less than $d+1$, then the sample set is linearly separable?',\n",
       " 'Recommended procedure for factor analysis on dichotomous data with R',\n",
       " 'Regarding the size of training data for building classifier',\n",
       " 'Assessing difference observations of two samples',\n",
       " 'Which distribution is likely to be the source of the largest value?',\n",
       " 'Principal component regression and including variables not in the PCA',\n",
       " 'Whether to use robust linear regression or bootstrapping when there is heteroscedasticity?',\n",
       " 'Can GARCH(1,1) be applied to homoskedastic time series when comparing against heteroskedastic time series?',\n",
       " 'Use a trendline formula to get values for any given X with Excel',\n",
       " 'How to make Marsaglia polar method return values [0, 1)?',\n",
       " 'Is whitening always good?',\n",
       " 'How do I calculate probability',\n",
       " 'Clustering: Should I use the Jensen-Shannon Divergence or its square?',\n",
       " \"matlab gmdistribution.fit 'Regularize' - what regularization method?\",\n",
       " 'Accounting for categorical variable in only a few observations',\n",
       " 'Ideas for \"lab notebook\" software?',\n",
       " 'Pseudo R squared formula for GLMs',\n",
       " 'How to exploit periodicity to reduce noise of a signal?',\n",
       " 'Online fitting for normal distributions',\n",
       " 'Odd error with caret function rfe',\n",
       " \"How to perform Student's t-test having only sample size, sample average and population average are known?\",\n",
       " 'Constrained Maximum Lilkihood Estimation',\n",
       " 'Using Singular Value Decomposition to Compute Variance Covariance Matrix from linear regression model',\n",
       " 'Deriving covariance from \"embedded\" random variable',\n",
       " 'How to calculate turbulence and similarity index with TraMineR?',\n",
       " 'Testing whether two regression coefficients are significantly different',\n",
       " 'Quartiles in Excel ',\n",
       " \"what's the advantages of bayesian version of linear regression, logistic regression etc\",\n",
       " 'How to calculate perplexity of a holdout with Latent Dirichlet Allocation?',\n",
       " 'Whether to exclude a case from two analyses when the case is only an outlier in one?',\n",
       " 'Basic Inter-rater reliability and type of data',\n",
       " 'Mathematical Statistics Videos',\n",
       " 'Finding windows of high-correlation across coordinates in R',\n",
       " 'Dealing with a categorical variable that can take multiple levels simultaneously',\n",
       " 'How can I improve a simple regression here?',\n",
       " 'How to generate uniformly distributed points on the surface of the 3-d unit sphere?',\n",
       " 'Why is it claimed that a sample is often more accurate than a census?',\n",
       " 'Conceptual understanding of root mean squared error and mean bias deviation',\n",
       " 'Bias of autocorrelation function in finite sample for ARMA processes',\n",
       " 'What are some applications of Chinese restaurant processes?',\n",
       " 'A problem using Bayesian modeling to impute a variable measured with error',\n",
       " 'Similarities and differences between regression and estimation',\n",
       " 'More info needed on second order regular variation in extreme value theory',\n",
       " 'What does truncated distribution mean?',\n",
       " 'Human body organs growth graph or data',\n",
       " 'What variation can I expect between lab results if the coefficient of variation is 9%?',\n",
       " 'Dealing with correlated regressors',\n",
       " 'Is there a data decay method that decays data based on how much new data it receives?',\n",
       " 'Pandas / Statsmodel / Scikits-learn',\n",
       " 'Hierarchical multinomial logit with R/JAGS',\n",
       " 'Spearman or Kendall correlation?',\n",
       " 'Why are the following probabilities equal to each other?',\n",
       " 'What is the best tool for customer segmentation?',\n",
       " 'Exponential decay function',\n",
       " 'Significant differences and related variables',\n",
       " 'Computing p-value using bootstrap with R',\n",
       " 'Can an svmlight model be converted to work with libsvm?',\n",
       " 'Removing duplicated rows from R data frame',\n",
       " 'Being totally stuck at my master thesis: cant figure out this 3 equation SEM',\n",
       " 'Best practices for measuring and avoiding overfitting?',\n",
       " 'Weakly informative prior distributions for scale parameters',\n",
       " 'What free tool can I use to do simple Monte Carlo simulations on OS X?',\n",
       " 'Estimating 2D mean from uneven samples',\n",
       " 'How to perform a basic forecasting model from pooled cross-sectional timeseries data in SPSS?',\n",
       " 'Aliases in fractional factorial designs',\n",
       " 'How to graph quantitative and qualitative improvement in human health related measures?',\n",
       " 'How to statistically and graphically compare distributions for nine groups where group sample sizes are unequal?',\n",
       " 'How to characterize symmetric discrete distribution?',\n",
       " 'How to log transform Z-scores?',\n",
       " 'How to create a bivariate distribution from copula and marginals?',\n",
       " 'What is the probability that a multivariate Normal RV lies within a sphere of radius R?',\n",
       " 'What to do when a NN learns a static representation for a large chunk of your dataset?',\n",
       " \"What's the difference between correlation and simple linear regression?\",\n",
       " 'Estimating event probability from historical time series with clear seasonality',\n",
       " 'How can I tell which location is the best?',\n",
       " 'Really a knifes edge?',\n",
       " 'Designing a test for a psychic who says he can influence dice rolls',\n",
       " 'Reporting $\\\\\\\\chi^2$ test results in APA format',\n",
       " 'How to limit my input data for Jaccard item-item similarity calculation?',\n",
       " 'Toeplitz variance matrix with nlme',\n",
       " 'Clustering time series with wavelets in R',\n",
       " 'Linear behaviour of nonlinear SVM in higher dimensional space',\n",
       " \"Market share based on comparison of competitors' average sales\",\n",
       " \"Help using maxFratio() in R (Hartley's test)\",\n",
       " 'Size of the data set for measuring variation',\n",
       " 'How can I demonstrate non-linearity without categorising a predictor?',\n",
       " 'Books on non-parametric theory with correlated data',\n",
       " 'How does one fit conditional Poisson regression in SAS?',\n",
       " 'How to calculate the boundary value for a random variable which is sum of variables with gamma and uniform distributions?',\n",
       " 'Natural interpretation for LDA hyperparameters',\n",
       " 'What test should be used to compare several regression lines?',\n",
       " 'Nonextensive statistics',\n",
       " 'learn R (Statistics)',\n",
       " 'randomForest vs. cforest; Can I get partial dependence plots and percent variance explained in package party?',\n",
       " 'Estimating specific variance for items in factor analysis - how to achieve the theoretical maximum?',\n",
       " 'How to change column names in data frame in R?',\n",
       " 'Probit model marginal effects',\n",
       " 'Comparison between MaxEnt, ML, Bayes and other kind of statistical inference methods',\n",
       " 'What is a good way to measure the \"linearity\" of a dataset?',\n",
       " 'Symmetrical regression',\n",
       " 'Problem printing greyscale or B&W ggplot2 images',\n",
       " 'Dependent vs. independent samples',\n",
       " 'Inference in linear model with conditional heteroskedasticity',\n",
       " 'Subsetting a data-frame in R based on dates',\n",
       " 'A question about parameters of Gamma distribution in Bayesian econometrics',\n",
       " 'Why does the Cauchy distribution have no mean?',\n",
       " 'Regression in projective space?',\n",
       " 'Find outlier in time domain dataset',\n",
       " 'Probability distribution of $\\\\\\\\sigma$',\n",
       " 'Is it allowed to use a 5% trimmed mean for analyzing data from a creativity task (quantity of ideas)?',\n",
       " 'Is it correct to compute LR stat after maximising likelihood with bounds?',\n",
       " 'Determining eigenvalues and eigenvectors in R',\n",
       " 'Justification of aggregation in multi-level model - ICC in SPSS',\n",
       " 'Accuracy of a telescope, measurement without systematic error',\n",
       " 'What is the statistical justification of interpolation?',\n",
       " 'How to store checks of gradient algorithm in a matrix using R?',\n",
       " 'What programming language do you recommend to prototype a machine learning problem?',\n",
       " 'Market segmentation based on a time of consumption',\n",
       " 'Calculating the probability of the Nth sample in the birthday problem w/o knowing the previous samples',\n",
       " 'Comparing two non-linear biological models: differences in y vs. differences in slope',\n",
       " 'Training a decision tree against unbalanced data',\n",
       " 'questionnaire stacked bar plot with R',\n",
       " 'Assessing quality of similarity measure',\n",
       " 'Fast method for finding best metaparameters of SVM (that is faster than grid search)',\n",
       " 'Deep belief network performs worse than a simple MLP',\n",
       " 'Looking for some intuition regarding the MCD estimator',\n",
       " 'TF-IDF cutoff percentage for tweets',\n",
       " 'Detecting changes in distribution of multiple variables',\n",
       " 'Nested repeated measures analysis',\n",
       " 'Estimating hyperparameter in basis functions (Gaussian and sigmoid) for linear regression',\n",
       " 'Not sure how to work out SSG and SSE in ANOVA table',\n",
       " 'Seasonal data forecasting issues',\n",
       " 'Is there a software package designed to automatically check the assumptions of various statistical tests? ',\n",
       " 'Random generation of scores similar to those of a classification model',\n",
       " 'Is it possible to analytically integrate $x$ multiplied by the lognormal probability density function?',\n",
       " '\"Interestingness\" function for StackExchange questions',\n",
       " 'Weighting datapoints by uncertainty - asymmetric CIs',\n",
       " 'Random number generation for gaussian, cauchy and levy',\n",
       " 'How to determine voter preference from a \"small sample\" poll using Bayesian statistics?',\n",
       " 'Selection of priors for a BYM spatial regression model',\n",
       " 'Comparing two linear regression models',\n",
       " 'Ordinary kriging stationary case',\n",
       " 'If you perform an ARMA on the volatility and add the squared returns as external variable, do you obtain a GARCH?',\n",
       " 'Interpreting F and p statistics from R grangertest results',\n",
       " 'Calculating significance of difference between two binary values on one dataset',\n",
       " 'Inspect generators and defining relations of a fractional factorial design',\n",
       " 'What is a reasonable sample size for correlation analysis for both overall and sub-group analyses?',\n",
       " 'How to understand a note for the distribution of the sample for confidence interval?',\n",
       " 'Normalizing when doing count statistics of patient numbers on different arms of a clinical trial',\n",
       " 'how to group time series data into stationary & non-stationary?',\n",
       " 'What is the expectation of exponential of the product of two random variables?',\n",
       " 'In caret what is the real difference between cv and repeatedcv?',\n",
       " 'AR(1) selection using sample ACF-PACF',\n",
       " 'Colinearity and scaling when using k-means',\n",
       " 'Accuracy of each path in a decision tree in Matlab',\n",
       " 'Errors-in-variables regression: is it valid to pool data from three sites?',\n",
       " 'Do low-discrepancy sequences work in discrete spaces?',\n",
       " 'Standard deviation of the sum of regression coefficients',\n",
       " 'Which R package to use to conduct a latent class growth analysis (LCGA) / growth mixture model (GMM)?',\n",
       " 'Is it appropriate to use the term \"bits\" to discuss a log-base-2 likelihood ratio?',\n",
       " 'Statistics/Probability Videos for Beginners',\n",
       " 'Quantifying QQ plot',\n",
       " 'Is there always a maximizer for any MLE problem?',\n",
       " 'Do quasi random number generators sample only uniform distribution?',\n",
       " 'What is an example of a chi-squared distribution?',\n",
       " 'Problem with mixture discriminant analysis in R returning NA for predictions',\n",
       " 'Distribution of arrival times to server for an M/M/1 queue (what the server experiences)',\n",
       " \"'Forward Stagewise' option in LARS algorithm\",\n",
       " 'Inequality convolution distribution function',\n",
       " 'Any advice on technical analysis of financial data?',\n",
       " 'Support vector machines and regression',\n",
       " 'What is the correct sample size when calculating CI for a subset of results?',\n",
       " 'Checking a panel unit root test in R done manually',\n",
       " 'Can we get confidence intervals for entries in stationary vector for an absorbing, time-independent Markov chain?',\n",
       " 'Сonfidence interval of histogram probability density function estimator',\n",
       " 'When do I need a model?',\n",
       " 'Bias from increased information in FLAME clustering',\n",
       " 'How to do an instrumental variables regression with an instrumented interaction term in Stata?',\n",
       " 'Is such transformed beta a known distribution?',\n",
       " 'Covariance and variance relation',\n",
       " 'Read decimal values in SAS',\n",
       " 'Probability of collision (two bivariate normal distributions)',\n",
       " 'References about the theory of linear regression or regression in general',\n",
       " 'Normalizing rating in a group of people [finding effectiveness]',\n",
       " 'Methods for evaluating partial autocorrelation for identification of ARIMA models',\n",
       " 'Examples of text mining with R (tm package)',\n",
       " 'Error while using dlmModReg',\n",
       " \"Abusing Linear Models under Multicollinearity: Simulation for 'realistic' movement of predictors\",\n",
       " 'Fitting a special mixed model in R - alternatives to optim()',\n",
       " 'Why is the Jeffreys prior useful?',\n",
       " 'How to plot a fan (Polar) Dendrogram in R?',\n",
       " 'Odds ratios multiple comparisons',\n",
       " 'Is there an R package which implements weighted maximum likelihood for Rasch models?',\n",
       " 'Specification and interpretation of interaction terms using glm()',\n",
       " 'How is the bayesian framework better in interpretation when we usually use uninformative or subjective priors?',\n",
       " 'White noise for level, log and log differences data sets',\n",
       " 'Comparing relative frequencies between two groups',\n",
       " 'Calculating linear regression from limited information',\n",
       " 'What is the probability of rejection in rejection sampling?',\n",
       " 'Stambaugh bias definition',\n",
       " 'How to test for differences between two group means when the data is not normally distributed?',\n",
       " 'R package for symbolic data analysis',\n",
       " 'Expected number of runs of length n in coin toss',\n",
       " 'Difference between Hausdorff and earth mover (EMD) distance',\n",
       " 'Polychoric PCA and component loadings in Stata',\n",
       " 'Model for multivariate ordered categorical data with a time-varying continuous covariate',\n",
       " 'Might be an unbalanced within subjects repeated measures?',\n",
       " 'PCA before random forest classification',\n",
       " 'Fixed-sum question',\n",
       " 'How do I capture regularity of a time series in a normalized way?',\n",
       " 'How to estimate the mode using non-parametric methods of a 4-variate random vector drawn from a continuous multivariate distribution?',\n",
       " 'How to deal with gaps/NaNs in time series data when using Matlab for autocorrelation and neural networks?',\n",
       " 'Bonferroni for outlier detection?',\n",
       " 'Examples of hidden Markov models problems?',\n",
       " 'p-value adjustment via permutation for multiple correlations',\n",
       " 'Factor Analysis using Varimax model',\n",
       " 'Method to reliably determine abnormal statistical values',\n",
       " 'How to combine the responses of two sensors?',\n",
       " 'Average of a tail of a normal distribution',\n",
       " 'How to test for equality of means for proportions? And two-sided or one-sided?',\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " '<multiple-comparisons><power>',\n",
       " '<machine-learning><logistic><nonlinear-regression>',\n",
       " '<spss><k-means>',\n",
       " '<algorithms><cart>',\n",
       " '<normal-distribution><distance-functions><information-theory>',\n",
       " '<repeated-measures><ancova>',\n",
       " '<regression><panel-data>',\n",
       " '<modeling><generalized-linear-model>',\n",
       " '<multiple-comparisons><bonferroni>',\n",
       " '<nonparametric><sample-size>',\n",
       " '<regression><machine-learning><categorical-data><normalization>',\n",
       " '<standard-deviation><online>',\n",
       " '<meta-analysis><clinical-trials>',\n",
       " '<self-study><power><z-test>',\n",
       " '<time-series><statistical-significance><aggregation>',\n",
       " '<multiple-regression><ordinal><ordered-variables>',\n",
       " '<r><regression><multivariate-analysis>',\n",
       " '<population><mode>',\n",
       " '<correlation><probit>',\n",
       " '<monte-carlo><information-theory><importance-sampling>',\n",
       " '<regression><lasso><java>',\n",
       " '<hypothesis-testing><permutation>',\n",
       " '<r><hypothesis-testing><clustering>',\n",
       " '<poisson><multinomial><negative-binomial><conditioning>',\n",
       " '<p-value><interpretation><presentation>',\n",
       " '<forecasting><longitudinal>',\n",
       " '<regression><anova><spss>',\n",
       " '<distributions><variance><binomial><proportion>',\n",
       " '<bayesian><multiple-comparisons>',\n",
       " '<anova><t-test><multiple-comparisons><within-subjects>',\n",
       " '<interpretation><regression-coefficients><causal-inference><statistical-control><difference-in-difference>',\n",
       " '<r><hypothesis-testing><logistic><random-forest>',\n",
       " '<normal-distribution><data-transformation><t-test><assumptions>',\n",
       " '<dataset><nonlinear-regression>',\n",
       " '<scales><composite>',\n",
       " '<regression><logistic><r-squared>',\n",
       " '<hypothesis-testing><correlation><multivariate-analysis>',\n",
       " '<regression><learning><pls>',\n",
       " '<r><regression><data-visualization><interaction><continuous-data>',\n",
       " '<regression><lasso><regularization><ridge-regression>',\n",
       " '<self-study><confidence-interval>',\n",
       " '<data-visualization><multivariate-analysis><spatial><big-list>',\n",
       " '<probability><stata><mathematical-statistics>',\n",
       " '<aic><likelihood-ratio><lme>',\n",
       " '<r><splus>',\n",
       " '<r><modeling><sas><p-value>',\n",
       " '<references><theory><learning><bounds>',\n",
       " '<regression><p-value><logistic>',\n",
       " '<pca><spatial>',\n",
       " '<bayesian><predictive-models><prediction-interval>',\n",
       " '<multiple-regression><r-squared>',\n",
       " '<time-series><forecasting><online>',\n",
       " '<reliability><composite>',\n",
       " '<time-series><data-visualization>',\n",
       " '<forecasting><spatial><econometrics><missing-data><measurement-error>',\n",
       " '<probability><self-study><poisson><zero-inflation>',\n",
       " '<humor>',\n",
       " '<r><hypothesis-testing><contingency-tables>',\n",
       " '<r><hypothesis-testing><statistical-significance><repeated-measures>',\n",
       " '<probability><hypothesis-testing><anova><self-study><p-value>',\n",
       " '<multiple-regression><model-selection><interaction>',\n",
       " '<machine-learning><bayesian>',\n",
       " '<regression><feature-selection>',\n",
       " '<categorical-data><goodness-of-fit><genetics><bioinformatics><power-law>',\n",
       " '<stochastic-processes><markov-process>',\n",
       " '<mixed-model><spss><repeated-measures><predictive-models>',\n",
       " '<dataset><error><curve-fitting><c++>',\n",
       " '<anova><proportion>',\n",
       " '<survival><cox-model>',\n",
       " '<regression><bayesian>',\n",
       " '<machine-learning><predictive-models><cross-validation><svm>',\n",
       " '<terminology><likelihood-ratio><information>',\n",
       " '<r><data-visualization><computational-statistics><pie-chart>',\n",
       " '<r><time-series><forecasting>',\n",
       " '<machine-learning><hidden-markov-model><kalman-filter><pattern-recognition><graphical-model>',\n",
       " '<mean><sample>',\n",
       " '<experiment-design><spatial><ecology><gis>',\n",
       " '<r><regression><poisson><count-data>',\n",
       " '<hypothesis-testing><finite-population>',\n",
       " '<confidence-interval><statistical-significance>',\n",
       " '<distributions><queueing><interarrival-time>',\n",
       " '<regression><variance><deming-regression>',\n",
       " '<correlation><spatial><uncertainty><variogram>',\n",
       " '<algorithms>',\n",
       " '<r><p-value>',\n",
       " '<binomial><beta-binomial><beta>',\n",
       " '<r><garch>',\n",
       " '<mixed-model><matlab><variance-covariance><mixed-effect>',\n",
       " '<missing-data>',\n",
       " '<logistic><lasso><ridge-regression><convergence>',\n",
       " '<hypothesis-testing><p-value>',\n",
       " '<regression><self-study><standard-error><heteroscedasticity>',\n",
       " '<multiple-regression><multinomial>',\n",
       " '<binary><nominal>',\n",
       " '<r><rank-correlation><arma>',\n",
       " '<algorithms><mcmc><networks><graph-theory>',\n",
       " '<bayesian><mcmc><jags>',\n",
       " '<survey><multicollinearity>',\n",
       " '<p-value><spearman><t-distribution>',\n",
       " '<sampling><pdf><kde>',\n",
       " '<r><regression><regression-coefficients>',\n",
       " '<r><cart><rpart>',\n",
       " '<regression><cross-validation><ridge-regression>',\n",
       " '<r><repeated-measures><simulation>',\n",
       " '<mcmc><monte-carlo><random-forest><cart>',\n",
       " '<categorical-data><generalized-linear-model><interaction>',\n",
       " '<r><machine-learning><hypothesis-testing>',\n",
       " '<r><effect-size>',\n",
       " '<regression><machine-learning><multivariate-analysis><large-data>',\n",
       " '<poisson><stochastic-processes><markov-process>',\n",
       " '<bayesian><naive-bayes>',\n",
       " '<estimation><variance><reliability><kappa>',\n",
       " '<autocorrelation><panel-data><heteroscedasticity>',\n",
       " '<standard-deviation><outliers><mad><z-test>',\n",
       " '<regression><hypothesis-testing><multiple-regression><econometrics>',\n",
       " '<regression><online><gaussian-process>',\n",
       " '<standard-error><descriptive-statistics>',\n",
       " '<regression><sample><r-squared><population>',\n",
       " '<correlation><multiple-comparisons><permutation>',\n",
       " '<r><signal-processing><fourier-transform><wavelet>',\n",
       " '<bayesian><jags><posterior>',\n",
       " '<distributions><probability><self-study><normal-distribution>',\n",
       " '<regression><probability><distributions><binomial><references>',\n",
       " '<confidence-interval><discrete-data>',\n",
       " '<probability><bayesian><self-study><conditional-probability>',\n",
       " '<svm><libsvm>',\n",
       " '<epidemiology><propensity-scores>',\n",
       " '<time-series><hypothesis-testing><functional-data-analysis>',\n",
       " '<time-series><estimation><stationarity>',\n",
       " '<r><machine-learning><caret>',\n",
       " '<regression><mixed-model><linear-model><residuals><diagnostic>',\n",
       " '<sampling><multivariate-analysis>',\n",
       " '<matlab><python><mixture>',\n",
       " '<machine-learning><neural-networks><covariance><normalization>',\n",
       " '<dataset><data-mining><software><pmml>',\n",
       " '<maximum-likelihood>',\n",
       " '<r><time-series><frequency>',\n",
       " '<sampling><chi-squared>',\n",
       " '<classification><svm><references><boosting>',\n",
       " '<r><teaching><power>',\n",
       " '<bayesian><model-selection><prior>',\n",
       " '<data-mining><compression>',\n",
       " '<self-study><stochastic-processes>',\n",
       " '<r><matlab><ridge-regression>',\n",
       " '<r><regression><data-transformation><multivariable>',\n",
       " '<logistic><neural-networks><small-sample>',\n",
       " '<regression><autocorrelation><nonlinear-regression><longitudinal><exponential>',\n",
       " '<r><time-series><data-mining><outliers>',\n",
       " '<normal-distribution><mixed-model>',\n",
       " '<dataset><survival><bias>',\n",
       " '<splines><statistical-learning>',\n",
       " '<genetics><permutation><multiple-comparisons>',\n",
       " '<mixed-model><repeated-measures><missing-data><zero-inflation>',\n",
       " '<matrix><information-theory><entropy>',\n",
       " '<svm><text-mining><posterior><multi-class>',\n",
       " '<normal-distribution><random-generation>',\n",
       " '<text-mining><distance-functions>',\n",
       " '<bounds>',\n",
       " '<machine-learning><k-nearest-neighbour>',\n",
       " '<dataset><spss><binomial>',\n",
       " '<books><error>',\n",
       " '<r><anova><mixed-model><gls>',\n",
       " '<hypothesis-testing><logistic><r-squared><odds-ratio><bic>',\n",
       " '<estimation><matrix-inverse>',\n",
       " '<time-series><mean><basic-concepts>',\n",
       " '<maximum-likelihood><logistic><expected-value>',\n",
       " '<regression><correlation><self-study><covariance>',\n",
       " '<bayesian><prior><rasch>',\n",
       " '<statistical-significance><modeling><predictive-models><data-transformation><feature-selection>',\n",
       " '<count-data><likert><composite>',\n",
       " '<standard-deviation><least-squares>',\n",
       " '<machine-learning><classification><data-mining>',\n",
       " '<hypothesis-testing><multiple-comparisons><p-value>',\n",
       " '<standard-deviation><intuition><coefficient-of-variation>',\n",
       " '<r><regression><multivariate-analysis><multiple-regression>',\n",
       " '<algorithms><normalization><recommender-system>',\n",
       " '<machine-learning><correlation>',\n",
       " '<logistic><econometrics><panel-data><probit>',\n",
       " '<r><cart>',\n",
       " '<mixed-model><interaction><post-hoc><lme>',\n",
       " '<median><definition>',\n",
       " '<maximum-likelihood><discrete-data><paired-comparisons>',\n",
       " '<correlation><anova><multivariate-analysis><rule-of-thumb><manova>',\n",
       " '<time-series><outliers><local-statistics>',\n",
       " '<causal-inference><linear-model><graphical-model>',\n",
       " '<r><pca><categorical-data>',\n",
       " '<time-series><forecasting><arma>',\n",
       " '<r><genetics><bioinformatics><microarray><biostatistics>',\n",
       " '<stata><r-squared><autoregressive>',\n",
       " '<probability><distributions><order-statistics><joint-distribution>',\n",
       " '<bayesian><causal-inference><bugs>',\n",
       " '<r><time-series><logistic>',\n",
       " '<cross-validation><text-mining><weka>',\n",
       " '<mixed-model><count-data>',\n",
       " '<distributions><statistical-significance><small-sample>',\n",
       " '<correlation><dataset><simulation>',\n",
       " '<bayesian><bugs><prediction><winbugs>',\n",
       " '<regression><hypothesis-testing><econometrics>',\n",
       " '<regression><correlation><prediction-limit>',\n",
       " '<correlation><random-forest>',\n",
       " '<r><anova><linear-model><ancova>',\n",
       " '<self-study><simulation><monte-carlo><expected-value>',\n",
       " '<r><data-visualization><multivariate-analysis>',\n",
       " '<regression><econometrics><tobit-regression>',\n",
       " '<estimation><fitting><kolmogorov-smirnov>',\n",
       " '<distributions><normal-distribution><poisson>',\n",
       " '<r><regression><logistic><poisson><negative-binomial>',\n",
       " '<logistic><power-analysis><ordinal>',\n",
       " '<time-series><correlation><summary-statistics>',\n",
       " '<probability><data-transformation><maximum-likelihood><simulation><kolmogorov-smirnov>',\n",
       " '<time-series><data-mining><signal-processing><trend>',\n",
       " '<regression><model-selection><multivariable>',\n",
       " '<r><machine-learning><data-mining><large-data><big-data>',\n",
       " '<algorithms><optimization><neural-networks>',\n",
       " '<time-series><autocorrelation>',\n",
       " '<correlation><multiple-comparisons><spss>',\n",
       " '<r><repeated-measures><power-analysis>',\n",
       " '<meta-analysis><funnel-plot><publication-bias>',\n",
       " '<logistic><terminology>',\n",
       " '<svm><matlab>',\n",
       " '<time-series><forecasting><smoothing><kde><kernel>',\n",
       " '<distributions><probability><copula>',\n",
       " '<repeated-measures><standard-deviation><mean><standardization><data-cleaning>',\n",
       " '<anova><sums-of-squares>',\n",
       " '<probability><hypothesis-testing><statistical-significance><t-test><p-value>',\n",
       " '<data-visualization><dataset><pca><data-mining>',\n",
       " '<confidence-interval><sampling><experiment-design><sample-size>',\n",
       " '<clinical-trials><randomization>',\n",
       " '<goodness-of-fit><ranking><rating>',\n",
       " '<aic><nonlinear-regression>',\n",
       " '<r><data-mining><outliers><similarities>',\n",
       " '<normal-distribution><optimization><queueing><decision-theory>',\n",
       " '<statistical-significance><sampling><survey><polling><media>',\n",
       " '<correlation><anova><repeated-measures><interaction>',\n",
       " '<sampling><small-sample>',\n",
       " '<predictive-models><neural-networks>',\n",
       " '<multiple-regression><f-test>',\n",
       " '<r><correlation><data-visualization><scatterplot>',\n",
       " '<regression><logistic><factor-analysis><multicollinearity>',\n",
       " '<information-theory>',\n",
       " '<reliability><latent-variable><path-model>',\n",
       " '<normal-distribution><mixture><histogram><unsupervised-learning>',\n",
       " '<random-variable><sample>',\n",
       " '<r><spss><sas><learning>',\n",
       " '<hypothesis-testing><modeling><large-data><outliers><best-practices>',\n",
       " '<bayesian><kde>',\n",
       " '<r><time-series><regression><arima>',\n",
       " '<multivariate-analysis><outliers>',\n",
       " '<r><mathematical-statistics><integral>',\n",
       " '<time-series><variance><interpretation>',\n",
       " '<r><multinomial><glmnet>',\n",
       " '<time-series><machine-learning><estimation><forecasting><predictive-models>',\n",
       " '<r><odds-ratio><fishersexact>',\n",
       " '<multivariate-analysis><normality><robust>',\n",
       " '<probability><random-variable><conditional-probability>',\n",
       " '<self-study><data-transformation><linear-model>',\n",
       " '<dataset><autocorrelation>',\n",
       " '<books><experiment-design><clinical-trials>',\n",
       " '<machine-learning><text-mining><feature-selection>',\n",
       " '<time-series><correlation><cointegration><stationarity>',\n",
       " '<correlation><clinical-trials>',\n",
       " '<confounding>',\n",
       " '<r><regression><correlation><p-value>',\n",
       " '<distributions><sampling><matlab>',\n",
       " '<bayesian><predictive-models><cross-validation><sas><roc>',\n",
       " '<sampling><dataset>',\n",
       " '<time-series><regression><forecasting><updating>',\n",
       " '<r><machine-learning><matlab><software><python>',\n",
       " '<time-series><data-visualization><variance><normalization>',\n",
       " '<distance-functions>',\n",
       " '<survival><epidemiology><inference><longitudinal>',\n",
       " '<r><distributions><estimation><beta>',\n",
       " '<r><t-test><post-hoc><latex>',\n",
       " '<correlation><random-generation>',\n",
       " '<modeling><sampling><conditional-probability><gibbs>',\n",
       " '<error><logistic>',\n",
       " '<time-series><spss><autocorrelation><panel-data><cross-section>',\n",
       " '<regression><kernel><smoothing><loess>',\n",
       " '<r><model-selection><generalized-linear-model>',\n",
       " '<categorical-data><scales><ordinal><psychometrics><interval>',\n",
       " '<logistic><stata><multilevel-analysis><data-imputation><gllamm>',\n",
       " '<python><curve-fitting><nonparametric-bayes>',\n",
       " '<data-visualization><sas>',\n",
       " '<multiple-comparisons><permutation>',\n",
       " '<correlation><variance-covariance><intuition><matrix-inverse><partial-correlation>',\n",
       " '<r><contingency-tables><correspondence-analysis>',\n",
       " '<r><probability><modeling><predictive-models>',\n",
       " '<excel><summary-statistics><curve-fitting>',\n",
       " '<machine-learning><data-mining><algorithms>',\n",
       " '<logistic><poisson><epidemiology><odds-ratio><relative-risk>',\n",
       " '<hypothesis-testing><monte-carlo><random-generation><randomness>',\n",
       " '<data-transformation><scales><normality><normalization>',\n",
       " '<distributions><estimation><beta>',\n",
       " '<kalman-filter><linear-algebra>',\n",
       " '<r><distributions><bootstrap><zero-inflation>',\n",
       " '<likert><reliability><psychometrics><validity>',\n",
       " '<hypothesis-testing><p-value><kolmogorov-smirnov>',\n",
       " '<probability><algorithms><stochastic-processes><expected-value>',\n",
       " '<r><regression><mixed-model><bugs><jags>',\n",
       " '<r><dataset><basic-concepts><quantiles><sql>',\n",
       " '<svm><neural-networks><theory>',\n",
       " '<time-series><logistic><classification><forecasting>',\n",
       " '<time-series><error>',\n",
       " '<probability><estimation><nonparametric>',\n",
       " '<statistical-significance><standard-error><measurement-error>',\n",
       " '<interpretation><prior><topic-models><hyperparameter>',\n",
       " '<logistic><nonlinear-regression>',\n",
       " '<r><machine-learning><classification><neural-networks><feature-selection>',\n",
       " '<regression><multivariate-analysis><model-selection><multiple-regression>',\n",
       " '<r><bayesian><networks><bayes-network>',\n",
       " '<logistic><classification><model-selection><model-comparison>',\n",
       " '<variance><proportion><hypothesis-testing>',\n",
       " '<distributions><clustering><sampling><markov-process>',\n",
       " '<time-series><machine-learning><svm>',\n",
       " '<methodology>',\n",
       " '<machine-learning><svm><python><kernel>',\n",
       " '<regression><stata><linear-model><mse>',\n",
       " '<r><standard-deviation><biostatistics><error-propagation>',\n",
       " '<theory>',\n",
       " '<ab-test>',\n",
       " '<categorical-data><continuous-data>',\n",
       " '<statistical-significance><chi-squared>',\n",
       " '<clustering><spss><continuous-data><distance>',\n",
       " '<sampling><randomness>',\n",
       " '<model-selection><multiple-comparisons><categorical-data><dummy-variables>',\n",
       " '<factor-analysis><reliability><psychometrics><latent-variable><validity>',\n",
       " '<r><regression><machine-learning><lasso><regularization>',\n",
       " '<poisson-process>',\n",
       " '<curve-fitting><mixture><pdf><method-of-moments>',\n",
       " '<r><model-selection><generalized-linear-model><count-data><negative-binomial>',\n",
       " '<random-generation><independence>',\n",
       " '<machine-learning><pca><lsa>',\n",
       " '<clustering><k-means><multidimensional-scaling><metric>',\n",
       " '<estimation><efficiency>',\n",
       " '<normal-distribution><variance><bivariate>',\n",
       " '<data-visualization><multivariate-analysis><excel>',\n",
       " '<interaction><ancova>',\n",
       " '<matlab><feature-selection><information-theory><mutual-information>',\n",
       " '<multiple-comparisons><contingency-tables><fisher><stratification>',\n",
       " '<r><regression><modeling>',\n",
       " '<regression><time-series><outliers>',\n",
       " '<predictive-models><optimization>',\n",
       " '<r><multiple-comparisons><binomial><post-hoc>',\n",
       " '<regression><spss><categorical-data><least-squares><statistical-control>',\n",
       " '<correlation><adjustment><geostatistics>',\n",
       " '<standard-deviation><variance><tables>',\n",
       " '<likelihood-function><t-distribution>',\n",
       " '<correlation><measurement>',\n",
       " '<stochastic-processes><point-process>',\n",
       " '<r><multilevel-analysis>',\n",
       " '<r><time-series><machine-learning><neural-networks><nnet>',\n",
       " '<correlation><feature-selection>',\n",
       " '<r><python><numpy><scipy>',\n",
       " '<machine-learning><lsa>',\n",
       " '<multilevel-analysis><lmer>',\n",
       " '<bayesian><winbugs><error-message>',\n",
       " '<pca><normal-distribution><zipf>',\n",
       " '<epidemiology><contingency-tables><p-value>',\n",
       " '<predictive-models><standard-error><error><prediction>',\n",
       " '<r><estimation><modeling><categorical-data><panel-data>',\n",
       " '<hotelling>',\n",
       " '<image-processing><functional-data-analysis><nominal>',\n",
       " '<randomness>',\n",
       " '<spss><missing-data><data-imputation>',\n",
       " '<regression><spss><forecasting>',\n",
       " '<modeling><predictive-models><inference>',\n",
       " '<self-study><interpretation>',\n",
       " '<spss><missing-data><likert><ordinal>',\n",
       " '<matlab><mathematica>',\n",
       " '<normality>',\n",
       " '<normal-distribution><curve-fitting><javascript>',\n",
       " '<statistical-significance><coefficient-of-variation>',\n",
       " '<spss><predictive-models><arima>',\n",
       " '<hypothesis-testing><multiple-comparisons><proportion>',\n",
       " '<wishart><inverse-gamma>',\n",
       " '<r><regression-coefficients><negative-binomial><standardization>',\n",
       " '<regression><variance><bias>',\n",
       " '<generalized-linear-model><residuals>',\n",
       " '<anova><power>',\n",
       " '<ranking><weighted-sampling><frequency>',\n",
       " '<big-list><markov-process><books><hidden-markov-model>',\n",
       " '<distributions><estimation><normal-distribution>',\n",
       " '<multivariate-analysis><terminology><distance-functions>',\n",
       " '<lasso><ridge-regression><loss-functions>',\n",
       " '<bayesian><self-study><normal-distribution><inverse-gamma>',\n",
       " '<data-transformation><large-data>',\n",
       " '<r><confidence-interval><multivariate-analysis>',\n",
       " '<clustering><multivariate-analysis><pca><dataset><svd>',\n",
       " '<repeated-measures><sas><multiple-comparisons>',\n",
       " '<regression><self-study><standard-error>',\n",
       " '<t-test><sample-size><references><assumptions><power>',\n",
       " '<mixed-model><multilevel-analysis><meta-analysis>',\n",
       " '<r><anova><confidence-interval><repeated-measures>',\n",
       " '<correlation><dataset>',\n",
       " '<self-study><normal-distribution><unbiased-estimator>',\n",
       " '<r><distributions><estimation><joint-distribution><copula>',\n",
       " '<time-series><predictive-models><autocorrelation>',\n",
       " '<random-generation><randomness>',\n",
       " '<r><time-series><estimation><arima>',\n",
       " '<variance><multivariate-analysis><pca>',\n",
       " '<r><machine-learning><svm><naive-bayes>',\n",
       " '<unsupervised-learning><clustering>',\n",
       " '<r><inter-rater><kappa>',\n",
       " '<scales><pdf><probability>',\n",
       " '<machine-learning><statistical-significance><cross-validation>',\n",
       " '<multilevel-analysis><information><fisher-information>',\n",
       " '<classification><clustering><factor-analysis><psychometrics>',\n",
       " '<outliers><data-cleaning>',\n",
       " '<normal-distribution><pdf><tables>',\n",
       " '<econometrics><theory><careers>',\n",
       " '<spss><categorical-data><pca><factor-analysis><binary-data>',\n",
       " '<anova><experiment-design><t-test>',\n",
       " '<data-visualization><excel>',\n",
       " '<time-series><monte-carlo><simulation><non-independent><prediction-limit>',\n",
       " '<regression><confidence-interval><predictive-models><prediction-interval>',\n",
       " '<categorical-data><nonlinear-regression><multicollinearity>',\n",
       " '<r><regression><directional-statistics>',\n",
       " '<r><clustering><model-selection>',\n",
       " '<p-value><granger-causality>',\n",
       " '<distributions><probability><normal-distribution>',\n",
       " '<poisson><generalized-linear-model><count-data><residuals>',\n",
       " '<survival><assumptions><hazard><logrank>',\n",
       " '<logistic><odds-ratio>',\n",
       " '<time-series><books><stochastic-processes><references>',\n",
       " '<random-variable><probability-inequalities><summations>',\n",
       " '<regression><time-series><change-point>',\n",
       " '<probability><binomial><negative-binomial>',\n",
       " '<bayesian><spatial>',\n",
       " '<normal-distribution><matlab><covariance>',\n",
       " '<distributions><hypothesis-testing><random-generation>',\n",
       " '<probability><self-study><inference><pdf><dice>',\n",
       " '<correlation><multivariate-analysis>',\n",
       " '<probability><logistic><logit>',\n",
       " '<spss><pca><factor-analysis><rotation>',\n",
       " '<time-series><garch><volatility-forecasting><finance>',\n",
       " '<hypothesis-testing><generalized-linear-model><outliers><prediction>',\n",
       " '<mcmc><optimization><monte-carlo>',\n",
       " '<classification><pattern-recognition>',\n",
       " '<confidence-interval><autocorrelation><average>',\n",
       " '<basic-concepts><proof><sums-of-squares>',\n",
       " '<r><multivariate-analysis><manova><multiple-regression><multivariate-regression>',\n",
       " '<books><mathematical-statistics>',\n",
       " '<stata><panel-data><fixed-effects-model><hausman>',\n",
       " '<statistical-significance><spss><poisson><p-value><kolmogorov-smirnov>',\n",
       " '<time-series><autocorrelation><outliers>',\n",
       " '<correlation><variance><garch>',\n",
       " '<correlation><chi-squared><independence>',\n",
       " '<regression><time-series><poisson><count-data>',\n",
       " '<clustering><nonparametric><online><sequential-analysis><particle-filter>',\n",
       " '<regression><poisson><binary-data>',\n",
       " '<feature-selection><measurement>',\n",
       " '<variance><online>',\n",
       " '<meta-regression>',\n",
       " '<data-visualization><binary-data>',\n",
       " '<dimensionality-reduction><discriminant-analysis>',\n",
       " '<r><cart><hypothesis-testing><package><gbm>',\n",
       " '<machine-learning><neural-networks><best-practices>',\n",
       " '<time-series><estimation><maximum-likelihood><standard-error><arma>',\n",
       " '<time-series><matrix-decomposition><svd>',\n",
       " '<time-series><econometrics><trend>',\n",
       " '<r><epidemiology><data-imputation><censoring>',\n",
       " '<model-selection><multiple-regression><feature-selection>',\n",
       " '<regression><probability><stata><treatment-effect>',\n",
       " '<r><t-test><mean><group-differences>',\n",
       " '<multivariate-analysis><simulation>',\n",
       " '<variance><nonparametric><heteroscedasticity><kruskal-wallis><glmm>',\n",
       " '<pca><econometrics>',\n",
       " '<self-study><poisson><average>',\n",
       " '<logistic><ordinal><probit>',\n",
       " '<confidence-interval><interpretation>',\n",
       " '<time-series><correlation><pearson>',\n",
       " '<data-visualization><genetics>',\n",
       " '<logistic><experiment-design>',\n",
       " '<statistical-significance><normal-distribution><normality><assumptions>',\n",
       " '<r><regression><lmer>',\n",
       " '<r><multiple-comparisons><binary-data>',\n",
       " '<sampling><variance><mean><error>',\n",
       " '<regression><interpretation><linear-model><random-effects-model><fixed-effects-model>',\n",
       " '<regression><residuals><residual-analysis>',\n",
       " '<r><splines><multivariate-regression>',\n",
       " '<machine-learning><predictive-models><model-selection><categorical-data>',\n",
       " '<repeated-measures><multivariate-analysis><multiple-regression><experiment-design><multilevel-analysis>',\n",
       " '<data-transformation><lognormal>',\n",
       " '<bayesian><confidence-interval><binomial>',\n",
       " '<spss><sem><mediation>',\n",
       " '<hypothesis-testing><statistical-significance>',\n",
       " '<r><regression><interpretation><empirical><interpolation>',\n",
       " '<r><optimization><matrix>',\n",
       " '<machine-learning><computational-statistics><recommender-system>',\n",
       " '<regression><variance><interpretation>',\n",
       " '<sampling><mcmc><graphical-model>',\n",
       " '<clustering><online>',\n",
       " '<machine-learning><naive-bayes>',\n",
       " '<time-series><probability>',\n",
       " '<sampling><data-mining><big-data>',\n",
       " '<cross-validation><cart>',\n",
       " '<variance><spatial><pattern-recognition>',\n",
       " '<confidence-interval><multilevel-analysis>',\n",
       " '<distributions><hypothesis-testing><kolmogorov-smirnov>',\n",
       " '<multiple-comparisons><kruskal-wallis>',\n",
       " '<categorical-data><mean>',\n",
       " '<regression><matrix>',\n",
       " '<r><survey>',\n",
       " '<ridge-regression>',\n",
       " '<meta-analysis><sample-size><standard-error><funnel-plot><publication-bias>',\n",
       " '<probability><normal-distribution><mathematical-statistics>',\n",
       " '<survey><calibration><multiple-imputation>',\n",
       " '<r><regression><robust>',\n",
       " '<random-forest><reproducible-research>',\n",
       " '<mathematical-statistics><decision-theory>',\n",
       " '<self-study><inference><order-statistics>',\n",
       " '<exponential-smoothing>',\n",
       " '<r><distributions><spss><kolmogorov-smirnov>',\n",
       " '<r><factor-analysis><small-sample>',\n",
       " '<chi-squared><nonparametric><contingency-tables><association-measure>',\n",
       " '<categorical-data><missing-data>',\n",
       " '<bayesian><binomial><multilevel-analysis>',\n",
       " '<r><time-series><independence><fourier-transform><model-based-clustering>',\n",
       " '<bootstrap><resampling><average><notation>',\n",
       " '<machine-learning><weka>',\n",
       " '<time-series><epidemiology><monitoring>',\n",
       " '<regression><aic>',\n",
       " '<pca><data-transformation><fourier-transform>',\n",
       " '<mixed-model><reliability><intraclass-correlation><repeatability>',\n",
       " '<data-transformation><covariance><random-variable>',\n",
       " '<hypothesis-testing><statistical-significance><power><equivalence>',\n",
       " '<units>',\n",
       " '<machine-learning><clustering><classification><discriminant-analysis>',\n",
       " '<estimation><binomial>',\n",
       " '<multivariate-analysis><data-mining><outliers>',\n",
       " '<matrix-decomposition>',\n",
       " '<mixed-model><stata><repeatability>',\n",
       " '<time-series><genetic-algorithms>',\n",
       " '<regression><anova><interaction>',\n",
       " '<regression><finance><cointegration>',\n",
       " '<smoothing><kde><kernel>',\n",
       " '<normal-distribution><self-study>',\n",
       " '<correlation><normal-distribution><lognormal>',\n",
       " '<r><mixed-model><bootstrap>',\n",
       " '<binomial><random-effects-model><glmm><glmer>',\n",
       " '<probability><hypothesis-testing>',\n",
       " '<clustering><ward>',\n",
       " '<sas><sample-size><power>',\n",
       " '<stata><multinomial><logit>',\n",
       " '<r><logistic><odds-ratio>',\n",
       " '<hypothesis-testing><normality><philosophical>',\n",
       " '<standard-deviation><summary-statistics>',\n",
       " '<survey><reliability><validity>',\n",
       " '<statistical-significance><p-value>',\n",
       " '<bayesian><logistic><jags><bugs><rjags>',\n",
       " '<time-series><probability><logistic><epidemiology><non-independent>',\n",
       " '<r><correlation>',\n",
       " '<spss><meta-analysis><funnel-plot><publication-bias>',\n",
       " '<correlation><power>',\n",
       " '<r><normal-distribution><data-transformation><interpolation>',\n",
       " '<classification><fuzzy>',\n",
       " '<mixed-model><sas><covariance><genetics><variance-covariance>',\n",
       " '<r><repeated-measures><longitudinal><nested>',\n",
       " '<markov-process><likelihood-function><probit><autoregressive>',\n",
       " '<model-selection>',\n",
       " '<python><histogram><binning>',\n",
       " '<r><mean><anova>',\n",
       " '<density-function><maximum>',\n",
       " '<hypothesis-testing><sequential-analysis>',\n",
       " '<r><spatial><ecology>',\n",
       " '<spss><pca><similarities>',\n",
       " '<anova><mixed-model><spss><residuals>',\n",
       " '<poisson><generalized-linear-model><negative-binomial><overdispersion>',\n",
       " '<r><time-series><autocorrelation>',\n",
       " '<clustering><distance-functions><distance><similarities><euclidean>',\n",
       " '<regression><logistic><autocorrelation><predictor>',\n",
       " '<statistical-significance><group-differences>',\n",
       " '<regression><bayesian><classification><prior>',\n",
       " '<r><distributions><probability><modeling>',\n",
       " '<correlation><multivariate-analysis><similarities>',\n",
       " '<em-algorithm><lsa>',\n",
       " '<hypothesis-testing><f-test><f-distribution>',\n",
       " '<logistic><optimization>',\n",
       " '<time-series><computational-statistics><functional-data-analysis>',\n",
       " '<inference><cross-correlation><covariance>',\n",
       " '<regression><modeling><generalized-linear-model><nonlinear-regression>',\n",
       " '<correlation><self-study><mathematical-statistics>',\n",
       " '<anova><assumptions><residuals>',\n",
       " '<r><confidence-interval><mixed-model>',\n",
       " '<dlm><state-space-models>',\n",
       " '<regression><random-generation><r-squared>',\n",
       " '<time-series><self-study><books><stata>',\n",
       " '<mcmc><convergence><proof><kde>',\n",
       " '<machine-learning><logistic>',\n",
       " '<r><survival><wilcoxon>',\n",
       " '<t-test><p-value><wilcoxon>',\n",
       " '<mean><average>',\n",
       " '<r><multivariate-analysis><cart>',\n",
       " '<distributions><python><p-value>',\n",
       " '<r><generalized-linear-model><summary-statistics>',\n",
       " '<bayesian><particle-filter><kalman-filter>',\n",
       " '<terminology><biostatistics>',\n",
       " '<hypothesis-testing><p-value><interpretation>',\n",
       " '<classification><white-noise>',\n",
       " '<regression><lasso>',\n",
       " '<distributions><mixed-model><sas><monte-carlo><weighted-sampling>',\n",
       " '<monte-carlo><particle-filter>',\n",
       " '<correlation><meta-analysis>',\n",
       " '<mcmc><signal-processing>',\n",
       " '<multiple-regression><beta-regression>',\n",
       " '<panel-data>',\n",
       " '<econometrics><cointegration>',\n",
       " '<clustering><dataset>',\n",
       " '<classification><data-mining><boosting>',\n",
       " '<normal-distribution><quantiles><aggregation>',\n",
       " '<r><regression><random-generation>',\n",
       " '<real-time>',\n",
       " '<r><bootstrap><nonlinear-regression>',\n",
       " '<regression><residuals><heteroscedasticity><robust><rlm>',\n",
       " '<bayesian><jags><psychology><hierarchical-bayesian>',\n",
       " '<multivariate-analysis><ancova><covariance>',\n",
       " '<distributions><classification><discriminant-analysis>',\n",
       " '<algorithms><random-generation><density-function>',\n",
       " '<data-visualization><logistic><fixed-effects-model><logit>',\n",
       " '<time-series><probability><forecasting>',\n",
       " '<regression><cross-validation><ordinal><rms>',\n",
       " '<distributions><regression><data-transformation><logarithm>',\n",
       " '<hypothesis-testing><self-study><standard-deviation>',\n",
       " '<time-series><correlation><error><error-propagation>',\n",
       " '<time-series><hidden-markov-model><convergence><expectation-maximization>',\n",
       " '<distributions><maximum><normal-distribution>',\n",
       " '<self-study><categorical-data><binomial><poisson><uniform>',\n",
       " '<regression><spss><negative-binomial>',\n",
       " '<nonparametric-bayes>',\n",
       " '<kullback-leibler><gamma-distribution><exponential-family>',\n",
       " '<regression><splines>',\n",
       " '<regression><bayesian><logistic><multilevel-analysis>',\n",
       " '<classification><matlab><pattern-recognition>',\n",
       " '<fitting><model>',\n",
       " '<estimation><nonparametric><kernel>',\n",
       " '<stata><multilevel-analysis>',\n",
       " '<bayesian><references><teaching>',\n",
       " '<classification><pca><group-differences>',\n",
       " '<spss><generalized-linear-model><ancova>',\n",
       " '<r><mixed-model><repeated-measures><lmer>',\n",
       " '<regression><interaction><contrasts>',\n",
       " '<stepwise-regression><multiple-imputation>',\n",
       " '<regression><cart><extreme-value>',\n",
       " '<kalman-filter><state-space-models><linear-dynamical-system>',\n",
       " '<multivariate-analysis><variance-covariance><matrix-inverse><hotelling>',\n",
       " '<bayesian><prior><conjugate-prior>',\n",
       " '<distributions><curve-fitting>',\n",
       " '<r><statistical-significance><boxplot>',\n",
       " '<hypothesis-testing><multiple-comparisons><experiment-design><sequential-analysis>',\n",
       " '<logistic><experiment-design><ordinal>',\n",
       " '<time-series><survival>',\n",
       " '<r><data-transformation><generalized-linear-model><gamma-distribution>',\n",
       " '<probability><bayesian><inference>',\n",
       " '<r><mixed-model><stata><lmer>',\n",
       " '<r><mean><statistical-significance><spss>',\n",
       " '<r><logistic><factor-analysis>',\n",
       " '<time-series><normalization>',\n",
       " '<r><cross-validation><survival><cox-model>',\n",
       " '<r><regression><modeling><negative-binomial><standardization>',\n",
       " '<self-study><sampling><survey>',\n",
       " '<normal-distribution><multivariate-analysis><distance>',\n",
       " '<binomial><standard-deviation>',\n",
       " '<chi-squared><p-value>',\n",
       " '<machine-learning><classification><svm><kernel>',\n",
       " '<bias><cox-model>',\n",
       " '<probability><bayesian><conditional-probability>',\n",
       " '<regression><probability><variance><stochastic-processes>',\n",
       " '<logistic><multiple-regression><stata>',\n",
       " '<regression><multiple-comparisons>',\n",
       " '<modeling><categorical-data><chi-squared>',\n",
       " '<text-mining><networks>',\n",
       " '<time-series><proportion><smoothing><signal-processing>',\n",
       " '<reliability><distance><metric>',\n",
       " '<reliability><inter-rater><measurement><intraclass-correlation>',\n",
       " '<r><psychometrics><sem><binary-data><measurement-error>',\n",
       " '<machine-learning><data-visualization><python><graphical-model>',\n",
       " '<logistic><data-transformation>',\n",
       " '<distributions><optimization><weibull>',\n",
       " '<r><classification><ordinal><roc>',\n",
       " '<logistic><survival><multicollinearity>',\n",
       " '<regression><repeated-measures><reliability><measurement-error>',\n",
       " '<r><data-visualization><genetics>',\n",
       " '<estimation><census>',\n",
       " '<logistic><maximum-likelihood><multinomial>',\n",
       " '<time-series><correlation><clustering><matrix><spatio-temporal>',\n",
       " '<r><regression><multiple-regression>',\n",
       " '<machine-learning><classification><neural-networks><binary>',\n",
       " '<estimation><variance><unbiased-estimator><weighted-mean>',\n",
       " '<machine-learning><multivariate-analysis><finance>',\n",
       " '<modeling><predictive-models><propensity-scores>',\n",
       " '<regression><least-squares><heteroscedasticity>',\n",
       " '<confidence-interval><multiple-comparisons><paired-data><paired-comparisons>',\n",
       " '<regression><confounding><media><partial-correlation>',\n",
       " '<uniform><integral>',\n",
       " '<correlation><repeated-measures><terminology><covariance><independence>',\n",
       " '<discriminant-analysis><suppressor>',\n",
       " '<distributions><statistical-significance><population>',\n",
       " '<chi-squared><goodness-of-fit><residuals>',\n",
       " '<regression><panel-data><multicollinearity>',\n",
       " '<causal-inference><mediation>',\n",
       " '<spss><likert><summary-statistics>',\n",
       " '<count-data>',\n",
       " '<statistical-significance><sample-size><bias>',\n",
       " '<sampling><survey><dataset><resampling>',\n",
       " '<r><contingency-tables>',\n",
       " '<regression><references><error>',\n",
       " '<time-series><simulation><markov-process>',\n",
       " '<precision-recall><recommender-system>',\n",
       " '<machine-learning><roc>',\n",
       " '<t-test><normalization>',\n",
       " '<average>',\n",
       " '<hypothesis-testing><statistical-significance><self-study><inference>',\n",
       " '<data-visualization><books><descriptive-statistics><eda>',\n",
       " '<data-visualization><dendrogram>',\n",
       " '<time-series><statistical-significance><independence>',\n",
       " '<r><sample-size><small-sample><sample>',\n",
       " '<r><ranking><quantiles>',\n",
       " '<hypothesis-testing><manova>',\n",
       " '<regression><bayesian><multiple-regression><multilevel-analysis>',\n",
       " '<probability><stochastic-processes>',\n",
       " '<anova><repeated-measures><multiple-comparisons><survival><experiment-design>',\n",
       " '<probability><estimation><conditional-probability><continuous-data>',\n",
       " '<data-visualization><spearman><ranks>',\n",
       " '<regression><anova><nonlinear-regression><ancova>',\n",
       " '<repeated-measures><t-test><manova>',\n",
       " '<regression><ordinal><probit>',\n",
       " '<stochastic-processes><expected-value>',\n",
       " '<multilevel-analysis><aggregation>',\n",
       " '<r><survival><large-data>',\n",
       " '<r><logistic><modeling>',\n",
       " '<normal-distribution><multivariate-analysis>',\n",
       " '<r><machine-learning><neural-networks><r-squared>',\n",
       " '<r><logistic><categorical-data>',\n",
       " '<machine-learning><multilevel-analysis><linear-model><identifiability>',\n",
       " '<r><probit>',\n",
       " '<regression><ancova><robust><assumptions>',\n",
       " '<regression><confidence-interval><mathematica>',\n",
       " '<cross-validation><bootstrap><multivariate-regression><jackknife>',\n",
       " '<r><sensitivity-analysis>',\n",
       " '<regression><t-test><ordinal><likert>',\n",
       " '<cross-validation><glmnet>',\n",
       " '<sas><neural-networks>',\n",
       " '<variance><standard-deviation><standard-error><average>',\n",
       " '<confidence-interval><normal-distribution><forecasting>',\n",
       " '<confidence-interval><spss>',\n",
       " '<r><regression><time-series><hypothesis-testing><statistical-significance>',\n",
       " '<regression><categorical-data><interpretation><multiple-regression>',\n",
       " '<pca><discrete-data><correspondence-analysis>',\n",
       " '<estimation><density>',\n",
       " '<group-differences>',\n",
       " '<pca><psychometrics><scales><reliability>',\n",
       " '<bootstrap><small-sample>',\n",
       " '<regression><time-series><spss><categorical-data>',\n",
       " '<probability><conditional-probability><paradox>',\n",
       " '<time-series><forecasting><simulation><arima><ecology>',\n",
       " '<regression><generalized-linear-model><count-data>',\n",
       " '<r><self-study><logistic>',\n",
       " '<r><generalized-linear-model><spatial><gam><mgcv>',\n",
       " '<odds>',\n",
       " '<basic-concepts><descriptive-statistics>',\n",
       " '<r><group-differences><matching>',\n",
       " '<dataset><model-selection>',\n",
       " '<r><matrix>',\n",
       " '<distributions><bayesian><prior>',\n",
       " '<bayesian><likelihood-function>',\n",
       " '<generalized-linear-model><multiple-regression><logistic><diagnostic>',\n",
       " '<probability><bayesian><frequentist>',\n",
       " '<machine-learning><graph-theory><kernel-trick>',\n",
       " '<confidence-interval><variance><mean>',\n",
       " '<r><code>',\n",
       " '<time-series><extreme-value>',\n",
       " '<repeated-measures><multiple-regression><interaction>',\n",
       " '<r><aggregation>',\n",
       " '<random-generation>',\n",
       " '<regression><trend>',\n",
       " '<r><distributions><mathematical-statistics><simulation>',\n",
       " '<spatial><determinant><maximum-likelihood>',\n",
       " '<hypothesis-testing><mathematical-statistics><power-analysis>',\n",
       " '<r><distributions><kolmogorov-smirnov><uniform>',\n",
       " '<r><regression><anova>',\n",
       " '<sample-size><effect-size>',\n",
       " '<logistic><logit><link-function>',\n",
       " '<multiple-regression><multivariate-regression>',\n",
       " '<python><hidden-markov-model>',\n",
       " '<clustering><large-data>',\n",
       " '<distributions><statistical-significance><mathematical-statistics>',\n",
       " '<r><regression><residuals>',\n",
       " '<correlation><repeated-measures><group-differences>',\n",
       " '<normality><kolmogorov-smirnov>',\n",
       " '<r><regression><count-data><beta-binomial><overdispersion>',\n",
       " '<normality><hypothesis-testing><small-sample>',\n",
       " '<r><contingency-tables><frequency>',\n",
       " '<r><time-series><forecasting><exponential><arima>',\n",
       " '<online><model><binary>',\n",
       " '<cross-correlation>',\n",
       " '<matlab><discriminant-analysis>',\n",
       " '<classification><predictive-models><cross-validation><out-of-sample>',\n",
       " '<r><regression><multicollinearity>',\n",
       " '<regression><heteroscedasticity>',\n",
       " '<excel><quantiles>',\n",
       " '<machine-learning><hidden-markov-model>',\n",
       " '<psychometrics><scales><reliability><likert>',\n",
       " '<hypothesis-testing><measurement-error><method-comparison>',\n",
       " '<probability><correlation>',\n",
       " '<logistic><assumptions>',\n",
       " '<regression><multiple-regression><optimization><least-squares><multicollinearity>',\n",
       " '<mcmc><likelihood-function><posterior>',\n",
       " '<information-theory><mutual-information>',\n",
       " '<logistic><residuals>',\n",
       " '<repeated-measures><experiment-design>',\n",
       " '<distributions><machine-learning><distance-functions><information-retrieval>',\n",
       " '<self-study><variance><sample>',\n",
       " '<generalized-linear-model><interaction>',\n",
       " '<r><optimization><library><package>',\n",
       " '<regression><mixed-model><model-selection><multiple-comparisons><interaction>',\n",
       " '<data-mining><outliers>',\n",
       " '<machine-learning><statistical-significance><feature-selection><networks>',\n",
       " '<inference><philosophical><fiducial>',\n",
       " '<estimation><normal-distribution><sampling><variance>',\n",
       " '<pca><feature-selection><independence>',\n",
       " '<machine-learning><svm><kernel><normalization>',\n",
       " '<beta-binomial>',\n",
       " '<sample-size><estimation><least-squares><maximum-likelihood>',\n",
       " '<time-series><change-point><structural-change>',\n",
       " '<matlab><python><stochastic-processes><markov-process>',\n",
       " '<probability><fourier-transform><method-of-moments>',\n",
       " '<r><regression><logistic><data-visualization><categorical-data>',\n",
       " '<r><chi-squared><contingency-tables>',\n",
       " '<r><regression><logistic><multiple-regression><multivariate-regression>',\n",
       " '<distributions><bootstrap><hypothesis-testing><moments>',\n",
       " '<software><teaching><learning>',\n",
       " '<regression><time-series><lags>',\n",
       " '<regression><sas><standard-error>',\n",
       " '<confidence-interval><estimation><mixed-model><psychometrics><random-effects-model>',\n",
       " '<bayesian><discrete-data>',\n",
       " '<r><cross-validation><cart><rpart>',\n",
       " '<r><hypothesis-testing><autocorrelation><spatial><software>',\n",
       " '<r><machine-learning><nonparametric><feature-selection>',\n",
       " '<r><hypothesis-testing><mixed-model><lmer><contrasts>',\n",
       " '<data-imputation>',\n",
       " '<hypothesis-testing><statistical-significance><group-differences>',\n",
       " '<confidence-interval><standard-deviation><excel>',\n",
       " '<statistical-significance><permutation>',\n",
       " '<r><data-visualization><directional-statistics>',\n",
       " '<scipy><interpolation><regression><curve-fitting><least-squares>',\n",
       " '<psychometrics><reliability><inter-rater><latent-variable><irt>',\n",
       " '<r><k-nearest-neighbour>',\n",
       " '<r><multilevel-analysis><random-effects-model><weibull>',\n",
       " '<r><time-series><panel-data><data-imputation>',\n",
       " '<r><python><r-squared>',\n",
       " '<python><kernel><numpy>',\n",
       " '<r><distributions><aggregation>',\n",
       " '<meta-analysis><variability>',\n",
       " '<r><data-visualization><k-nearest-neighbour>',\n",
       " '<multiple-regression><t-test>',\n",
       " '<statistical-significance><standard-deviation><mean>',\n",
       " '<machine-learning><classification><data-mining><logistic><stratification>',\n",
       " '<data-visualization><clustering><markov-process>',\n",
       " '<r><matlab><markov-process>',\n",
       " '<regression><clinical-trials>',\n",
       " '<covariance><distance-functions><metric>',\n",
       " '<multivariate-analysis><pdf><distance><bivariate><wishart>',\n",
       " '<predictive-models><feature-selection><feature-construction>',\n",
       " '<normal-distribution><uniform><eigenvalues>',\n",
       " '<r><regression><multiple-comparisons><inference><ancova>',\n",
       " '<aic><likelihood-ratio>',\n",
       " '<data-visualization><confidence-interval><stata><matlab>',\n",
       " '<classification><data-mining><neural-networks>',\n",
       " '<r><confidence-interval><chi-squared>',\n",
       " '<time-series><forecasting><algorithms><weighted-mean>',\n",
       " '<probability><estimation><random-variable><pdf><cdf>',\n",
       " '<r><poisson><lmer>',\n",
       " '<r><clustering><weka><unsupervised-learning>',\n",
       " '<probability><bayesian><classification><feature-selection><scoring>',\n",
       " '<r><generalized-linear-model><overdispersion><quasi-likelihood>',\n",
       " '<central-limit-theorem><asymptotics>',\n",
       " '<r><probability><simulation><stochastic-processes><gaussian-process>',\n",
       " '<distributions><probability><stochastic-processes>',\n",
       " '<r><modeling><nonlinear-regression><nls>',\n",
       " '<statistical-significance><p-value><f-test>',\n",
       " '<time-series><statistical-significance><spss>',\n",
       " '<regression><logistic><multinomial>',\n",
       " '<r><classification><naive-bayes>',\n",
       " '<regression><multiple-regression><regression-coefficients>',\n",
       " '<spss><multiple-regression><survival>',\n",
       " '<dataset><spatial><networks><spatio-temporal>',\n",
       " '<regression><econometrics><causal-inference><instrumental-variables>',\n",
       " '<distributions><hypothesis-testing><standard-deviation><normal-distribution>',\n",
       " '<multiple-comparisons><survival>',\n",
       " '<r><time-series><survival><predictive-models>',\n",
       " '<time-series><epidemiology>',\n",
       " '<mixed-model><poisson><biostatistics>',\n",
       " '<estimation><maximum-likelihood><optimization><mixture><anderson-darling>',\n",
       " '<genetics><biostatistics>',\n",
       " '<self-study><covariance><conditioning>',\n",
       " '<probability><dice>',\n",
       " '<measurement-error><unbiased-estimator>',\n",
       " '<books><clinical-trials><psychology>',\n",
       " '<normal-distribution><repeated-measures><wilcoxon>',\n",
       " '<bayesian><estimation><experiment-design>',\n",
       " '<chi-squared><sources>',\n",
       " '<regression><generalized-linear-model><poisson><frequency><offset>',\n",
       " '<t-test><feature-selection><p-value><least-squares>',\n",
       " '<logistic><p-value>',\n",
       " '<machine-learning><classification><data-mining><precision-recall>',\n",
       " '<estimation><moments><matrix-inverse><method-of-moments><two-step-estimation>',\n",
       " '<regression><assumptions><diagnostic>',\n",
       " '<biostatistics>',\n",
       " '<correlation><pearson><notation><spearman-rho>',\n",
       " '<hypothesis-testing><students-t>',\n",
       " '<normal-distribution><distance-functions>',\n",
       " '<r><machine-learning><data-mining><feature-selection>',\n",
       " '<regression><machine-learning><ridge-regression>',\n",
       " '<bayesian><model-selection><bic><parameterization>',\n",
       " '<mixed-model><signal-processing>',\n",
       " '<correlation><interpretation><partial-correlation>',\n",
       " '<regression><cross-validation><model-selection><feature-selection><elastic-net>',\n",
       " '<anova><spss><effect-size>',\n",
       " '<self-study><expected-value><gamma-distribution>',\n",
       " '<r><distributions><modeling><data-mining><basic-concepts>',\n",
       " '<mutual-information><information-theory><probability>',\n",
       " '<pca><missing-data>',\n",
       " '<distributions><estimation><data-mining><partitioning>',\n",
       " '<r><time-series><statistical-significance><multivariate-analysis><stochastic-processes>',\n",
       " '<pca><factor-analysis><survey><psychometrics>',\n",
       " '<regression><machine-learning><information-retrieval>',\n",
       " '<regression><data-mining>',\n",
       " '<binomial><beta>',\n",
       " '<r><nonparametric><kernel><conditional-probability>',\n",
       " '<terminology><normalization>',\n",
       " '<point-process><cross-correlation>',\n",
       " '<r><time-series><estimation>',\n",
       " '<variance><generalized-linear-model><large-data><bias>',\n",
       " '<r><data-visualization><clustering><categorical-data><basic-concepts>',\n",
       " '<sampling><mcmc>',\n",
       " '<probability><dataset><natural-language>',\n",
       " '<random-effects-model><fixed-effects-model><degrees-of-freedom>',\n",
       " '<distributions><hypothesis-testing><nonparametric><wilcoxon><mann-whitney-u-test>',\n",
       " '<maximum-likelihood><markov-process><likelihood-ratio>',\n",
       " '<r><time-series><autocorrelation><arima><kalman-filter>',\n",
       " '<time-series><machine-learning><pca><data-transformation><multivariate-analysis>',\n",
       " '<modeling><aic><cross-validation><bic><model-selection>',\n",
       " '<goodness-of-fit><aic><model-comparison><bic><irt>',\n",
       " '<k-means><lsa><nlp>',\n",
       " '<r><pca><spss>',\n",
       " '<outliers><spatial>',\n",
       " '<estimation><nonparametric><convergence>',\n",
       " '<r><regression><censoring><threshold>',\n",
       " '<meta-analysis><longitudinal>',\n",
       " '<permutation><skewness><kurtosis>',\n",
       " '<time-series><hypothesis-testing><data-mining><markov-process><censoring>',\n",
       " '<r><probability><self-study><binomial>',\n",
       " '<regression><self-study><mean>',\n",
       " '<cross-validation><matching>',\n",
       " '<hypothesis-testing><p-value><genetics><hypergeometric>',\n",
       " '<variance><cart>',\n",
       " '<books><multivariate-analysis>',\n",
       " '<multivariate-analysis><algorithms>',\n",
       " '<regression><stata><panel-data><fixed-effects-model>',\n",
       " '<r><variance><econometrics><resampling><gini>',\n",
       " '<interpretation><ancova>',\n",
       " '<r><gbm>',\n",
       " '<regression><quantile-regression>',\n",
       " '<r><bootstrap><fixed-effects-model>',\n",
       " '<machine-learning><feature-selection><unbalanced-classes>',\n",
       " '<clustering><distance>',\n",
       " '<classification><data-mining><feature-selection><text-mining>',\n",
       " '<multilevel-analysis><gee>',\n",
       " '<time-series><forecasting><assumptions>',\n",
       " '<regression><curve-fitting><nonlinear-regression>',\n",
       " '<nonparametric><kolmogorov-smirnov>',\n",
       " '<generalized-linear-model><optimization>',\n",
       " '<self-study><asymptotics><proof><gls>',\n",
       " '<curve-fitting><networks><lognormal><power-law>',\n",
       " '<maximum-likelihood><optimization><covariance>',\n",
       " '<r><anova><contrasts>',\n",
       " '<survival><epidemiology><life-expectancy>',\n",
       " '<pca><scores>',\n",
       " '<odds-ratio><propensity-scores>',\n",
       " '<time-series><hypothesis-testing><simulation><computing>',\n",
       " '<modeling><exponential>',\n",
       " '<moments>',\n",
       " '<summary-statistics>',\n",
       " '<hypothesis-testing><sample-size><dataset><large-data>',\n",
       " '<classification><logistic><multilabel>',\n",
       " '<r><stepwise-regression>',\n",
       " '<machine-learning><svm><text-mining><kernel><libsvm>',\n",
       " '<distributions><normal-distribution><sampling><expected-value><skewness>',\n",
       " '<probability><correlation><conditional-probability><random-generation>',\n",
       " '<mixed-model><generalized-linear-model><poisson><random-effects-model>',\n",
       " '<regression><multiple-regression><excel>',\n",
       " '<distributions><hypothesis-testing><statistical-significance><standard-error><multinomial>',\n",
       " '<r><proportion><gee>',\n",
       " '<regression><model-selection><cross-validation>',\n",
       " '<r><bootstrap><algorithms>',\n",
       " '<r><hypothesis-testing><normal-distribution>',\n",
       " '<logistic><epidemiology><odds-ratio><case-control-study><observational-study>',\n",
       " '<hypothesis-testing><clinical-trials>',\n",
       " '<self-study><mixed-model><multilevel-analysis><random-effects-model>',\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 13.0,\n",
       " 17.0,\n",
       " 14.0,\n",
       " 19.0,\n",
       " 16.0,\n",
       " 20.0,\n",
       " 21.0,\n",
       " 23.0,\n",
       " 24.0,\n",
       " 25.0,\n",
       " 26.0,\n",
       " 27.0,\n",
       " 28.0,\n",
       " 29.0,\n",
       " 30.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 35.0,\n",
       " 41.0,\n",
       " 36.0,\n",
       " 44.0,\n",
       " 40.0,\n",
       " 38.0,\n",
       " 46.0,\n",
       " 42.0,\n",
       " 45.0,\n",
       " 43.0,\n",
       " 49.0,\n",
       " 47.0,\n",
       " 48.0,\n",
       " 52.0,\n",
       " 55.0,\n",
       " 50.0,\n",
       " 57.0,\n",
       " 56.0,\n",
       " 53.0,\n",
       " 59.0,\n",
       " 54.0,\n",
       " 62.0,\n",
       " 61.0,\n",
       " 64.0,\n",
       " 65.0,\n",
       " 58.0,\n",
       " 67.0,\n",
       " 68.0,\n",
       " 70.0,\n",
       " 71.0,\n",
       " 72.0,\n",
       " 74.0,\n",
       " 76.0,\n",
       " 90.0,\n",
       " 93.0,\n",
       " 94.0,\n",
       " 22.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 32.0,\n",
       " 33.0,\n",
       " 34.0,\n",
       " 37.0,\n",
       " 39.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 51.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 31.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 15.0,\n",
       " 18.0,\n",
       " 5.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 32.0,\n",
       " 49.0,\n",
       " 55.0,\n",
       " 59.0,\n",
       " 61.0,\n",
       " 64.0,\n",
       " 65.0,\n",
       " 72.0,\n",
       " 78.0,\n",
       " 80.0,\n",
       " 85.0,\n",
       " 89.0,\n",
       " 91.0,\n",
       " 94.0,\n",
       " 104.0,\n",
       " 110.0,\n",
       " 111.0,\n",
       " 131.0,\n",
       " 135.0,\n",
       " 147.0,\n",
       " 149.0,\n",
       " 151.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 174.0,\n",
       " 179.0,\n",
       " 191.0,\n",
       " 198.0,\n",
       " 200.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 207.0,\n",
       " nan,\n",
       " 209.0,\n",
       " 217.0,\n",
       " 221.0,\n",
       " 229.0,\n",
       " 232.0,\n",
       " 262.0,\n",
       " nan,\n",
       " nan,\n",
       " 265.0,\n",
       " nan,\n",
       " nan,\n",
       " 268.0,\n",
       " nan,\n",
       " nan,\n",
       " 271.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 279.0,\n",
       " nan,\n",
       " nan,\n",
       " 282.0,\n",
       " 293.0,\n",
       " 26111.0,\n",
       " 306.0,\n",
       " 307.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 329.0,\n",
       " 338.0,\n",
       " nan,\n",
       " 353.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 376.0,\n",
       " nan,\n",
       " 378.0,\n",
       " 379.0,\n",
       " nan,\n",
       " 381.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 385.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 416.0,\n",
       " 428.0,\n",
       " 439.0,\n",
       " 445.0,\n",
       " 457.0,\n",
       " 479.0,\n",
       " 482.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 506.0,\n",
       " 518.0,\n",
       " nan,\n",
       " nan,\n",
       " 522.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 532.0,\n",
       " 538.0,\n",
       " 553.0,\n",
       " 559.0,\n",
       " 560.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 566.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 585.0,\n",
       " nan,\n",
       " nan,\n",
       " 588.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 599.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 606.0,\n",
       " 611.0,\n",
       " 625.0,\n",
       " nan,\n",
       " nan,\n",
       " 628.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 634.0,\n",
       " 635.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 644.0,\n",
       " nan,\n",
       " 646.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 677.0,\n",
       " 680.0,\n",
       " 682.0,\n",
       " 711.0,\n",
       " 720.0,\n",
       " 721.0,\n",
       " 741.0,\n",
       " 758.0,\n",
       " 759.0,\n",
       " 762.0,\n",
       " 767.0,\n",
       " 772.0,\n",
       " 777.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 796.0,\n",
       " 822.0,\n",
       " 824.0,\n",
       " 827.0,\n",
       " 829.0,\n",
       " 839.0,\n",
       " 844.0,\n",
       " 849.0,\n",
       " 851.0,\n",
       " 858.0,\n",
       " 862.0,\n",
       " 867.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 875.0,\n",
       " 876.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 904.0,\n",
       " 905.0,\n",
       " 916.0,\n",
       " 917.0,\n",
       " 918.0,\n",
       " 926.0,\n",
       " 932.0,\n",
       " 937.0,\n",
       " 940.0,\n",
       " 941.0,\n",
       " nan,\n",
       " nan,\n",
       " 956.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 962.0,\n",
       " 963.0,\n",
       " 971.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 978.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1000.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1011.0,\n",
       " 1014.0,\n",
       " 1020.0,\n",
       " 1026.0,\n",
       " 1029.0,\n",
       " 1041.0,\n",
       " 1048.0,\n",
       " 1056.0,\n",
       " 1058.0,\n",
       " 1061.0,\n",
       " 1065.0,\n",
       " 1068.0,\n",
       " 1086.0,\n",
       " 1097.0,\n",
       " 1098.0,\n",
       " 1106.0,\n",
       " 1108.0,\n",
       " 1113.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1170.0,\n",
       " 1171.0,\n",
       " 1187.0,\n",
       " 1189.0,\n",
       " 26290.0,\n",
       " 1201.0,\n",
       " 1210.0,\n",
       " nan,\n",
       " 1212.0,\n",
       " 1213.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1220.0,\n",
       " 1251.0,\n",
       " 1263.0,\n",
       " 1273.0,\n",
       " 1276.0,\n",
       " 1277.0,\n",
       " 1282.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1297.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1306.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1322.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1326.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1335.0,\n",
       " nan,\n",
       " 1349.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1353.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1364.0,\n",
       " 1365.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 78427.0,\n",
       " 1372.0,\n",
       " 1384.0,\n",
       " nan,\n",
       " nan,\n",
       " 1390.0,\n",
       " nan,\n",
       " 1392.0,\n",
       " 1393.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1397.0,\n",
       " 1401.0,\n",
       " 1402.0,\n",
       " 1404.0,\n",
       " 1407.0,\n",
       " 1414.0,\n",
       " 1416.0,\n",
       " 1425.0,\n",
       " 1428.0,\n",
       " 1431.0,\n",
       " 1435.0,\n",
       " 1446.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1456.0,\n",
       " 1457.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1473.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1479.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1489.0,\n",
       " 1503.0,\n",
       " 1504.0,\n",
       " 1518.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1532.0,\n",
       " 1537.0,\n",
       " 1539.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1543.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1558.0,\n",
       " 1560.0,\n",
       " 1567.0,\n",
       " 1573.0,\n",
       " 1579.0,\n",
       " 1582.0,\n",
       " 1589.0,\n",
       " nan,\n",
       " 1594.0,\n",
       " nan,\n",
       " 1596.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1607.0,\n",
       " 1608.0,\n",
       " 1609.0,\n",
       " 1616.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1632.0,\n",
       " 1633.0,\n",
       " nan,\n",
       " 1635.0,\n",
       " 1636.0,\n",
       " nan,\n",
       " 1639.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1648.0,\n",
       " 1650.0,\n",
       " 1665.0,\n",
       " 1693.0,\n",
       " 26388.0,\n",
       " 1706.0,\n",
       " 1717.0,\n",
       " 1721.0,\n",
       " 1730.0,\n",
       " 1732.0,\n",
       " 1733.0,\n",
       " 1738.0,\n",
       " 1741.0,\n",
       " 1751.0,\n",
       " 1754.0,\n",
       " nan,\n",
       " 1762.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1775.0,\n",
       " 1785.0,\n",
       " 1791.0,\n",
       " 1792.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1806.0,\n",
       " nan,\n",
       " nan,\n",
       " 1816.0,\n",
       " 1823.0,\n",
       " 1827.0,\n",
       " 1830.0,\n",
       " 1843.0,\n",
       " 1845.0,\n",
       " nan,\n",
       " 1849.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1855.0,\n",
       " nan,\n",
       " nan,\n",
       " 1861.0,\n",
       " 1869.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1879.0,\n",
       " nan,\n",
       " 1887.0,\n",
       " 1901.0,\n",
       " 1903.0,\n",
       " 1917.0,\n",
       " 1918.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1930.0,\n",
       " nan,\n",
       " 1945.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1953.0,\n",
       " 1959.0,\n",
       " 1974.0,\n",
       " 1976.0,\n",
       " 1981.0,\n",
       " 1989.0,\n",
       " 1994.0,\n",
       " 2019.0,\n",
       " 2026.0,\n",
       " 2036.0,\n",
       " 2040.0,\n",
       " 2043.0,\n",
       " 2053.0,\n",
       " 2057.0,\n",
       " 2074.0,\n",
       " 2080.0,\n",
       " 2081.0,\n",
       " 2084.0,\n",
       " 2087.0,\n",
       " 2094.0,\n",
       " 2097.0,\n",
       " 2100.0,\n",
       " 2103.0,\n",
       " 2106.0,\n",
       " 2115.0,\n",
       " 2123.0,\n",
       " 2128.0,\n",
       " 2133.0,\n",
       " 2138.0,\n",
       " 2139.0,\n",
       " 2141.0,\n",
       " 2153.0,\n",
       " 2157.0,\n",
       " 2168.0,\n",
       " 2172.0,\n",
       " 2177.0,\n",
       " 2180.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2202.0,\n",
       " 2206.0,\n",
       " 2218.0,\n",
       " 2221.0,\n",
       " 2222.0,\n",
       " 2232.0,\n",
       " 2233.0,\n",
       " 2235.0,\n",
       " nan,\n",
       " nan,\n",
       " 2246.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2251.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2263.0,\n",
       " 2257.0,\n",
       " 2265.0,\n",
       " 2279.0,\n",
       " 2283.0,\n",
       " 2285.0,\n",
       " 2287.0,\n",
       " 2288.0,\n",
       " 2297.0,\n",
       " 2303.0,\n",
       " 2305.0,\n",
       " 2307.0,\n",
       " 2310.0,\n",
       " 2319.0,\n",
       " 2332.0,\n",
       " 2338.0,\n",
       " 2340.0,\n",
       " 2342.0,\n",
       " 2346.0,\n",
       " 2359.0,\n",
       " 2360.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2366.0,\n",
       " nan,\n",
       " nan,\n",
       " 2375.0,\n",
       " 2387.0,\n",
       " 2389.0,\n",
       " 2392.0,\n",
       " 2394.0,\n",
       " 2406.0,\n",
       " 2408.0,\n",
       " 2409.0,\n",
       " 2411.0,\n",
       " 2415.0,\n",
       " 2421.0,\n",
       " 2424.0,\n",
       " 2426.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2442.0,\n",
       " 2445.0,\n",
       " 2440.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2456.0,\n",
       " 2463.0,\n",
       " 2472.0,\n",
       " 2473.0,\n",
       " 2478.0,\n",
       " 2482.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2495.0,\n",
       " nan,\n",
       " 2497.0,\n",
       " 2498.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2506.0,\n",
       " 2509.0,\n",
       " 2511.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2519.0,\n",
       " nan,\n",
       " 2521.0,\n",
       " nan,\n",
       " nan,\n",
       " 2524.0,\n",
       " 2530.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2538.0,\n",
       " 2539.0,\n",
       " nan,\n",
       " nan,\n",
       " 2542.0,\n",
       " nan,\n",
       " 2550.0,\n",
       " 2558.0,\n",
       " 2570.0,\n",
       " 2571.0,\n",
       " nan,\n",
       " 2593.0,\n",
       " nan,\n",
       " 2595.0,\n",
       " 2596.0,\n",
       " nan,\n",
       " nan,\n",
       " 2599.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2605.0,\n",
       " 2606.0,\n",
       " 2607.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2618.0,\n",
       " 2621.0,\n",
       " 2622.0,\n",
       " 2627.0,\n",
       " 2629.0,\n",
       " 2633.0,\n",
       " 2647.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2651.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2664.0,\n",
       " 2665.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2674.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2680.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2701.0,\n",
       " 2707.0,\n",
       " 2721.0,\n",
       " 2735.0,\n",
       " 2738.0,\n",
       " 2740.0,\n",
       " 2741.0,\n",
       " 2747.0,\n",
       " 2750.0,\n",
       " 2766.0,\n",
       " 2773.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2781.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2778.0,\n",
       " 2789.0,\n",
       " 2790.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2798.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2822.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2834.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2847.0,\n",
       " nan,\n",
       " nan,\n",
       " 2850.0,\n",
       " nan,\n",
       " nan,\n",
       " 2853.0,\n",
       " nan,\n",
       " nan,\n",
       " 2872.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2887.0,\n",
       " 2888.0,\n",
       " 2890.0,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 14.0,\n",
       " 17.0,\n",
       " 15.0,\n",
       " 19.0,\n",
       " 20.0,\n",
       " 21.0,\n",
       " 22.0,\n",
       " 23.0,\n",
       " 24.0,\n",
       " 25.0,\n",
       " 26.0,\n",
       " 27.0,\n",
       " 28.0,\n",
       " 29.0,\n",
       " 30.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 40.0,\n",
       " 35.0,\n",
       " 43.0,\n",
       " 44.0,\n",
       " 38.0,\n",
       " 45.0,\n",
       " 47.0,\n",
       " 48.0,\n",
       " 49.0,\n",
       " 50.0,\n",
       " 51.0,\n",
       " 52.0,\n",
       " 53.0,\n",
       " 54.0,\n",
       " 55.0,\n",
       " 56.0,\n",
       " 57.0,\n",
       " 58.0,\n",
       " 59.0,\n",
       " 60.0,\n",
       " 61.0,\n",
       " 62.0,\n",
       " 63.0,\n",
       " 64.0,\n",
       " 65.0,\n",
       " 66.0,\n",
       " 67.0,\n",
       " 68.0,\n",
       " 69.0,\n",
       " 70.0,\n",
       " 71.0,\n",
       " 72.0,\n",
       " 73.0,\n",
       " 74.0,\n",
       " 75.0,\n",
       " 76.0,\n",
       " 77.0,\n",
       " 78.0,\n",
       " 79.0,\n",
       " 80.0,\n",
       " 81.0,\n",
       " 82.0,\n",
       " 83.0,\n",
       " 84.0,\n",
       " 85.0,\n",
       " 86.0,\n",
       " 87.0,\n",
       " 88.0,\n",
       " 89.0,\n",
       " 90.0,\n",
       " 91.0,\n",
       " 92.0,\n",
       " 93.0,\n",
       " 94.0,\n",
       " 95.0,\n",
       " 96.0,\n",
       " 97.0,\n",
       " 98.0,\n",
       " 99.0,\n",
       " 100.0,\n",
       " 101.0,\n",
       " 102.0,\n",
       " 103.0,\n",
       " 104.0,\n",
       " 105.0,\n",
       " 106.0,\n",
       " 107.0,\n",
       " 108.0,\n",
       " 109.0,\n",
       " 110.0,\n",
       " 111.0,\n",
       " 112.0,\n",
       " 113.0,\n",
       " 114.0,\n",
       " 115.0,\n",
       " 116.0,\n",
       " 117.0,\n",
       " 118.0,\n",
       " 119.0,\n",
       " 120.0,\n",
       " 121.0,\n",
       " 122.0,\n",
       " 123.0,\n",
       " 124.0,\n",
       " 125.0,\n",
       " 126.0,\n",
       " 127.0,\n",
       " 128.0,\n",
       " 129.0,\n",
       " 130.0,\n",
       " 131.0,\n",
       " 132.0,\n",
       " 133.0,\n",
       " 134.0,\n",
       " 135.0,\n",
       " 136.0,\n",
       " 137.0,\n",
       " 138.0,\n",
       " 139.0,\n",
       " 140.0,\n",
       " 141.0,\n",
       " 142.0,\n",
       " 143.0,\n",
       " 144.0,\n",
       " 145.0,\n",
       " 146.0,\n",
       " 147.0,\n",
       " 148.0,\n",
       " 149.0,\n",
       " 150.0,\n",
       " 151.0,\n",
       " 152.0,\n",
       " 153.0,\n",
       " 154.0,\n",
       " 155.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 161.0,\n",
       " 158.0,\n",
       " 167.0,\n",
       " 169.0,\n",
       " 166.0,\n",
       " 170.0,\n",
       " 171.0,\n",
       " 172.0,\n",
       " 174.0,\n",
       " 175.0,\n",
       " 176.0,\n",
       " 177.0,\n",
       " 178.0,\n",
       " 179.0,\n",
       " 180.0,\n",
       " 181.0,\n",
       " 182.0,\n",
       " 183.0,\n",
       " 184.0,\n",
       " 185.0,\n",
       " 186.0,\n",
       " 187.0,\n",
       " 188.0,\n",
       " 189.0,\n",
       " 190.0,\n",
       " 191.0,\n",
       " 37.0,\n",
       " 193.0,\n",
       " 194.0,\n",
       " 195.0,\n",
       " 196.0,\n",
       " 39.0,\n",
       " 198.0,\n",
       " 199.0,\n",
       " 200.0,\n",
       " 201.0,\n",
       " 202.0,\n",
       " 203.0,\n",
       " 204.0,\n",
       " 205.0,\n",
       " 206.0,\n",
       " 41.0,\n",
       " 208.0,\n",
       " 207.0,\n",
       " 210.0,\n",
       " 209.0,\n",
       " 212.0,\n",
       " 213.0,\n",
       " 42.0,\n",
       " 215.0,\n",
       " 216.0,\n",
       " 217.0,\n",
       " 218.0,\n",
       " 219.0,\n",
       " 220.0,\n",
       " 221.0,\n",
       " 222.0,\n",
       " 223.0,\n",
       " 224.0,\n",
       " 225.0,\n",
       " 226.0,\n",
       " 227.0,\n",
       " 228.0,\n",
       " 229.0,\n",
       " 230.0,\n",
       " 231.0,\n",
       " 232.0,\n",
       " 233.0,\n",
       " 234.0,\n",
       " 235.0,\n",
       " 236.0,\n",
       " 237.0,\n",
       " 238.0,\n",
       " 239.0,\n",
       " 240.0,\n",
       " 241.0,\n",
       " 242.0,\n",
       " 243.0,\n",
       " 244.0,\n",
       " 245.0,\n",
       " 246.0,\n",
       " 247.0,\n",
       " 248.0,\n",
       " 249.0,\n",
       " 250.0,\n",
       " 251.0,\n",
       " 252.0,\n",
       " 253.0,\n",
       " 254.0,\n",
       " 255.0,\n",
       " 256.0,\n",
       " 257.0,\n",
       " 258.0,\n",
       " 259.0,\n",
       " 260.0,\n",
       " 261.0,\n",
       " 262.0,\n",
       " 263.0,\n",
       " 264.0,\n",
       " 265.0,\n",
       " 266.0,\n",
       " 267.0,\n",
       " 268.0,\n",
       " 269.0,\n",
       " 270.0,\n",
       " 271.0,\n",
       " 272.0,\n",
       " 273.0,\n",
       " 274.0,\n",
       " 275.0,\n",
       " 276.0,\n",
       " 277.0,\n",
       " 278.0,\n",
       " 279.0,\n",
       " 280.0,\n",
       " 281.0,\n",
       " 282.0,\n",
       " 283.0,\n",
       " 284.0,\n",
       " 285.0,\n",
       " 286.0,\n",
       " 287.0,\n",
       " 288.0,\n",
       " 289.0,\n",
       " 290.0,\n",
       " 291.0,\n",
       " 292.0,\n",
       " 293.0,\n",
       " 294.0,\n",
       " 295.0,\n",
       " 296.0,\n",
       " 297.0,\n",
       " 298.0,\n",
       " 299.0,\n",
       " 300.0,\n",
       " 301.0,\n",
       " 302.0,\n",
       " 303.0,\n",
       " 304.0,\n",
       " 305.0,\n",
       " 306.0,\n",
       " 307.0,\n",
       " 308.0,\n",
       " 309.0,\n",
       " 310.0,\n",
       " 311.0,\n",
       " 312.0,\n",
       " 313.0,\n",
       " 314.0,\n",
       " 315.0,\n",
       " 316.0,\n",
       " 317.0,\n",
       " 318.0,\n",
       " 319.0,\n",
       " 320.0,\n",
       " 321.0,\n",
       " 322.0,\n",
       " 323.0,\n",
       " 324.0,\n",
       " 325.0,\n",
       " 326.0,\n",
       " 327.0,\n",
       " 328.0,\n",
       " 329.0,\n",
       " 330.0,\n",
       " 331.0,\n",
       " 332.0,\n",
       " 333.0,\n",
       " 334.0,\n",
       " 335.0,\n",
       " 336.0,\n",
       " 337.0,\n",
       " 338.0,\n",
       " 339.0,\n",
       " 340.0,\n",
       " 341.0,\n",
       " 342.0,\n",
       " 343.0,\n",
       " 344.0,\n",
       " 345.0,\n",
       " 346.0,\n",
       " 347.0,\n",
       " 348.0,\n",
       " 349.0,\n",
       " 350.0,\n",
       " 351.0,\n",
       " 352.0,\n",
       " 353.0,\n",
       " 354.0,\n",
       " 355.0,\n",
       " 356.0,\n",
       " 357.0,\n",
       " 358.0,\n",
       " 359.0,\n",
       " 360.0,\n",
       " 361.0,\n",
       " 362.0,\n",
       " 363.0,\n",
       " 364.0,\n",
       " 365.0,\n",
       " 366.0,\n",
       " 367.0,\n",
       " 368.0,\n",
       " 369.0,\n",
       " 370.0,\n",
       " 371.0,\n",
       " 372.0,\n",
       " 373.0,\n",
       " 374.0,\n",
       " 375.0,\n",
       " 376.0,\n",
       " 377.0,\n",
       " 378.0,\n",
       " 379.0,\n",
       " 380.0,\n",
       " 381.0,\n",
       " 382.0,\n",
       " 383.0,\n",
       " 384.0,\n",
       " 385.0,\n",
       " 386.0,\n",
       " 387.0,\n",
       " 388.0,\n",
       " 389.0,\n",
       " 390.0,\n",
       " 391.0,\n",
       " 392.0,\n",
       " 393.0,\n",
       " 394.0,\n",
       " 395.0,\n",
       " 396.0,\n",
       " 397.0,\n",
       " 398.0,\n",
       " 399.0,\n",
       " 400.0,\n",
       " 401.0,\n",
       " 402.0,\n",
       " 403.0,\n",
       " 404.0,\n",
       " 405.0,\n",
       " 406.0,\n",
       " 407.0,\n",
       " 408.0,\n",
       " 409.0,\n",
       " 410.0,\n",
       " 411.0,\n",
       " 412.0,\n",
       " 413.0,\n",
       " 414.0,\n",
       " 415.0,\n",
       " 416.0,\n",
       " 417.0,\n",
       " 418.0,\n",
       " 419.0,\n",
       " 420.0,\n",
       " 421.0,\n",
       " 422.0,\n",
       " 423.0,\n",
       " 424.0,\n",
       " 425.0,\n",
       " 426.0,\n",
       " 427.0,\n",
       " 428.0,\n",
       " 429.0,\n",
       " 430.0,\n",
       " 431.0,\n",
       " 432.0,\n",
       " 433.0,\n",
       " 434.0,\n",
       " 435.0,\n",
       " 436.0,\n",
       " 437.0,\n",
       " 438.0,\n",
       " 439.0,\n",
       " 440.0,\n",
       " 441.0,\n",
       " 442.0,\n",
       " 443.0,\n",
       " 444.0,\n",
       " 445.0,\n",
       " 446.0,\n",
       " 447.0,\n",
       " 448.0,\n",
       " 449.0,\n",
       " 450.0,\n",
       " 451.0,\n",
       " 452.0,\n",
       " 453.0,\n",
       " 454.0,\n",
       " 455.0,\n",
       " 456.0,\n",
       " 457.0,\n",
       " 458.0,\n",
       " 459.0,\n",
       " 460.0,\n",
       " 461.0,\n",
       " 462.0,\n",
       " 463.0,\n",
       " 464.0,\n",
       " 465.0,\n",
       " 466.0,\n",
       " 467.0,\n",
       " 468.0,\n",
       " 469.0,\n",
       " 470.0,\n",
       " 471.0,\n",
       " 472.0,\n",
       " 473.0,\n",
       " 474.0,\n",
       " 475.0,\n",
       " 476.0,\n",
       " 477.0,\n",
       " 478.0,\n",
       " 479.0,\n",
       " 480.0,\n",
       " 481.0,\n",
       " 482.0,\n",
       " 483.0,\n",
       " 484.0,\n",
       " 485.0,\n",
       " 486.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 490.0,\n",
       " 491.0,\n",
       " 492.0,\n",
       " 493.0,\n",
       " nan,\n",
       " nan,\n",
       " 496.0,\n",
       " 497.0,\n",
       " 498.0,\n",
       " 494.0,\n",
       " 500.0,\n",
       " 501.0,\n",
       " 502.0,\n",
       " 503.0,\n",
       " 504.0,\n",
       " 499.0,\n",
       " 506.0,\n",
       " 507.0,\n",
       " 508.0,\n",
       " 505.0,\n",
       " 510.0,\n",
       " 511.0,\n",
       " 512.0,\n",
       " 513.0,\n",
       " 514.0,\n",
       " 515.0,\n",
       " 516.0,\n",
       " 517.0,\n",
       " 518.0,\n",
       " 519.0,\n",
       " 520.0,\n",
       " 521.0,\n",
       " 522.0,\n",
       " 523.0,\n",
       " 524.0,\n",
       " 525.0,\n",
       " 526.0,\n",
       " 527.0,\n",
       " 528.0,\n",
       " 529.0,\n",
       " 530.0,\n",
       " 531.0,\n",
       " 532.0,\n",
       " 533.0,\n",
       " 534.0,\n",
       " 535.0,\n",
       " 536.0,\n",
       " 537.0,\n",
       " 538.0,\n",
       " 539.0,\n",
       " 540.0,\n",
       " 541.0,\n",
       " 542.0,\n",
       " 543.0,\n",
       " 544.0,\n",
       " 545.0,\n",
       " 546.0,\n",
       " 547.0,\n",
       " 548.0,\n",
       " 549.0,\n",
       " 550.0,\n",
       " 551.0,\n",
       " 552.0,\n",
       " 553.0,\n",
       " 554.0,\n",
       " 555.0,\n",
       " 556.0,\n",
       " 557.0,\n",
       " 558.0,\n",
       " 559.0,\n",
       " 560.0,\n",
       " 561.0,\n",
       " 562.0,\n",
       " 563.0,\n",
       " 564.0,\n",
       " 565.0,\n",
       " 566.0,\n",
       " 567.0,\n",
       " 568.0,\n",
       " 569.0,\n",
       " 570.0,\n",
       " 571.0,\n",
       " 572.0,\n",
       " 573.0,\n",
       " 574.0,\n",
       " 575.0,\n",
       " 576.0,\n",
       " 577.0,\n",
       " 578.0,\n",
       " 579.0,\n",
       " 580.0,\n",
       " 581.0,\n",
       " 582.0,\n",
       " 583.0,\n",
       " 584.0,\n",
       " 585.0,\n",
       " 586.0,\n",
       " 587.0,\n",
       " 588.0,\n",
       " 589.0,\n",
       " 590.0,\n",
       " 591.0,\n",
       " 592.0,\n",
       " 593.0,\n",
       " 594.0,\n",
       " 595.0,\n",
       " 596.0,\n",
       " 597.0,\n",
       " 598.0,\n",
       " 599.0,\n",
       " 600.0,\n",
       " 601.0,\n",
       " 602.0,\n",
       " 603.0,\n",
       " 604.0,\n",
       " 605.0,\n",
       " 606.0,\n",
       " 607.0,\n",
       " 608.0,\n",
       " 609.0,\n",
       " 610.0,\n",
       " 611.0,\n",
       " 612.0,\n",
       " 613.0,\n",
       " 614.0,\n",
       " 615.0,\n",
       " 616.0,\n",
       " 617.0,\n",
       " 618.0,\n",
       " 619.0,\n",
       " 620.0,\n",
       " 621.0,\n",
       " 622.0,\n",
       " 623.0,\n",
       " 624.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 629.0,\n",
       " 630.0,\n",
       " nan,\n",
       " 632.0,\n",
       " 633.0,\n",
       " 634.0,\n",
       " 635.0,\n",
       " 636.0,\n",
       " 637.0,\n",
       " 638.0,\n",
       " 639.0,\n",
       " 631.0,\n",
       " 640.0,\n",
       " 642.0,\n",
       " 641.0,\n",
       " 644.0,\n",
       " 645.0,\n",
       " 646.0,\n",
       " 647.0,\n",
       " 648.0,\n",
       " 649.0,\n",
       " 650.0,\n",
       " 651.0,\n",
       " 652.0,\n",
       " 653.0,\n",
       " 654.0,\n",
       " 655.0,\n",
       " 656.0,\n",
       " 657.0,\n",
       " 658.0,\n",
       " 659.0,\n",
       " 660.0,\n",
       " 661.0,\n",
       " 662.0,\n",
       " 663.0,\n",
       " 664.0,\n",
       " 665.0,\n",
       " 666.0,\n",
       " 667.0,\n",
       " 668.0,\n",
       " 669.0,\n",
       " 670.0,\n",
       " 671.0,\n",
       " 672.0,\n",
       " 673.0,\n",
       " 674.0,\n",
       " 675.0,\n",
       " 676.0,\n",
       " 677.0,\n",
       " 678.0,\n",
       " 679.0,\n",
       " 680.0,\n",
       " 681.0,\n",
       " 682.0,\n",
       " 684.0,\n",
       " 685.0,\n",
       " 686.0,\n",
       " 687.0,\n",
       " 688.0,\n",
       " 689.0,\n",
       " 690.0,\n",
       " 691.0,\n",
       " 692.0,\n",
       " 693.0,\n",
       " 694.0,\n",
       " 695.0,\n",
       " 696.0,\n",
       " 697.0,\n",
       " 698.0,\n",
       " 699.0,\n",
       " 700.0,\n",
       " 701.0,\n",
       " 702.0,\n",
       " 703.0,\n",
       " 704.0,\n",
       " 705.0,\n",
       " 706.0,\n",
       " 707.0,\n",
       " 708.0,\n",
       " 709.0,\n",
       " 710.0,\n",
       " 711.0,\n",
       " 712.0,\n",
       " 713.0,\n",
       " 714.0,\n",
       " 715.0,\n",
       " 716.0,\n",
       " 717.0,\n",
       " 718.0,\n",
       " 719.0,\n",
       " 720.0,\n",
       " 721.0,\n",
       " 722.0,\n",
       " 723.0,\n",
       " 724.0,\n",
       " 725.0,\n",
       " 726.0,\n",
       " 727.0,\n",
       " 728.0,\n",
       " 729.0,\n",
       " 730.0,\n",
       " 731.0,\n",
       " 732.0,\n",
       " 733.0,\n",
       " 734.0,\n",
       " 735.0,\n",
       " 736.0,\n",
       " 737.0,\n",
       " 738.0,\n",
       " 739.0,\n",
       " 740.0,\n",
       " 741.0,\n",
       " 742.0,\n",
       " 743.0,\n",
       " 744.0,\n",
       " 745.0,\n",
       " 746.0,\n",
       " 747.0,\n",
       " 748.0,\n",
       " 749.0,\n",
       " 750.0,\n",
       " 751.0,\n",
       " 752.0,\n",
       " 753.0,\n",
       " 754.0,\n",
       " 755.0,\n",
       " 756.0,\n",
       " 757.0,\n",
       " 758.0,\n",
       " 759.0,\n",
       " 760.0,\n",
       " 761.0,\n",
       " 762.0,\n",
       " 763.0,\n",
       " 764.0,\n",
       " 765.0,\n",
       " 766.0,\n",
       " 767.0,\n",
       " 768.0,\n",
       " 769.0,\n",
       " 770.0,\n",
       " 771.0,\n",
       " 772.0,\n",
       " 773.0,\n",
       " 774.0,\n",
       " 775.0,\n",
       " 776.0,\n",
       " 777.0,\n",
       " 778.0,\n",
       " 779.0,\n",
       " 780.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 156.0,\n",
       " 157.0,\n",
       " 786.0,\n",
       " 788.0,\n",
       " 795.0,\n",
       " 796.0,\n",
       " 797.0,\n",
       " 790.0,\n",
       " 799.0,\n",
       " 159.0,\n",
       " 801.0,\n",
       " 802.0,\n",
       " 800.0,\n",
       " 804.0,\n",
       " 805.0,\n",
       " 160.0,\n",
       " 807.0,\n",
       " 806.0,\n",
       " 803.0,\n",
       " 808.0,\n",
       " 811.0,\n",
       " 32.0,\n",
       " 813.0,\n",
       " 814.0,\n",
       " 815.0,\n",
       " 162.0,\n",
       " 816.0,\n",
       " 818.0,\n",
       " 817.0,\n",
       " 820.0,\n",
       " 819.0,\n",
       " 163.0,\n",
       " 823.0,\n",
       " 824.0,\n",
       " 821.0,\n",
       " 164.0,\n",
       " 827.0,\n",
       " 825.0,\n",
       " 829.0,\n",
       " 830.0,\n",
       " 831.0,\n",
       " 165.0,\n",
       " 833.0,\n",
       " 834.0,\n",
       " 832.0,\n",
       " 836.0,\n",
       " 33.0,\n",
       " 838.0,\n",
       " 835.0,\n",
       " 840.0,\n",
       " 841.0,\n",
       " 842.0,\n",
       " 843.0,\n",
       " 844.0,\n",
       " 845.0,\n",
       " 168.0,\n",
       " 847.0,\n",
       " 846.0,\n",
       " 849.0,\n",
       " 850.0,\n",
       " 851.0,\n",
       " 852.0,\n",
       " 853.0,\n",
       " 854.0,\n",
       " 855.0,\n",
       " 856.0,\n",
       " 857.0,\n",
       " 858.0,\n",
       " 859.0,\n",
       " 860.0,\n",
       " 861.0,\n",
       " 34.0,\n",
       " 862.0,\n",
       " 864.0,\n",
       " 865.0,\n",
       " 866.0,\n",
       " 867.0,\n",
       " 868.0,\n",
       " 869.0,\n",
       " 870.0,\n",
       " 173.0,\n",
       " 872.0,\n",
       " 873.0,\n",
       " 874.0,\n",
       " 871.0,\n",
       " 876.0,\n",
       " 877.0,\n",
       " 875.0,\n",
       " 879.0,\n",
       " 880.0,\n",
       " 881.0,\n",
       " 878.0,\n",
       " 883.0,\n",
       " 884.0,\n",
       " 885.0,\n",
       " 886.0,\n",
       " 887.0,\n",
       " 888.0,\n",
       " 889.0,\n",
       " 890.0,\n",
       " 891.0,\n",
       " 892.0,\n",
       " 893.0,\n",
       " 894.0,\n",
       " 895.0,\n",
       " 896.0,\n",
       " 897.0,\n",
       " 898.0,\n",
       " 899.0,\n",
       " 900.0,\n",
       " 901.0,\n",
       " 902.0,\n",
       " 903.0,\n",
       " 904.0,\n",
       " 905.0,\n",
       " 906.0,\n",
       " 907.0,\n",
       " 908.0,\n",
       " 909.0,\n",
       " 910.0,\n",
       " 36.0,\n",
       " 912.0,\n",
       " 913.0,\n",
       " 914.0,\n",
       " 915.0,\n",
       " 916.0,\n",
       " 911.0,\n",
       " 918.0,\n",
       " 919.0,\n",
       " 920.0,\n",
       " 921.0,\n",
       " 922.0,\n",
       " 923.0,\n",
       " 924.0,\n",
       " 925.0,\n",
       " 926.0,\n",
       " 927.0,\n",
       " 928.0,\n",
       " 929.0,\n",
       " 930.0,\n",
       " 931.0,\n",
       " 932.0,\n",
       " 933.0,\n",
       " 934.0,\n",
       " 935.0,\n",
       " 936.0,\n",
       " 937.0,\n",
       " 938.0,\n",
       " 939.0,\n",
       " 940.0,\n",
       " 941.0,\n",
       " 942.0,\n",
       " 943.0,\n",
       " 944.0,\n",
       " 945.0,\n",
       " 946.0,\n",
       " 947.0,\n",
       " 948.0,\n",
       " 949.0,\n",
       " 950.0,\n",
       " 951.0,\n",
       " 952.0,\n",
       " 953.0,\n",
       " 954.0,\n",
       " 955.0,\n",
       " 956.0,\n",
       " 957.0,\n",
       " 958.0,\n",
       " 959.0,\n",
       " 960.0,\n",
       " 961.0,\n",
       " 962.0,\n",
       " 963.0,\n",
       " 964.0,\n",
       " 965.0,\n",
       " 966.0,\n",
       " 967.0,\n",
       " 968.0,\n",
       " 969.0,\n",
       " 970.0,\n",
       " 192.0,\n",
       " 972.0,\n",
       " 973.0,\n",
       " 974.0,\n",
       " 971.0,\n",
       " 976.0,\n",
       " nan,\n",
       " 978.0,\n",
       " 979.0,\n",
       " 980.0,\n",
       " 981.0,\n",
       " 982.0,\n",
       " 983.0,\n",
       " 984.0,\n",
       " 985.0,\n",
       " 986.0,\n",
       " 987.0,\n",
       " 988.0,\n",
       " 989.0,\n",
       " 990.0,\n",
       " 991.0,\n",
       " 197.0,\n",
       " 992.0,\n",
       " 994.0,\n",
       " 993.0,\n",
       " 995.0,\n",
       " 997.0,\n",
       " 998.0,\n",
       " 999.0,\n",
       " 1000.0,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 15.0,\n",
       " 16.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 3.0,\n",
       " 19.0,\n",
       " 4.0,\n",
       " 23.0,\n",
       " 24.0,\n",
       " 21.0,\n",
       " 26.0,\n",
       " 5.0,\n",
       " 27.0,\n",
       " 28.0,\n",
       " 25.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 7.0,\n",
       " 8.0,\n",
       " 42.0,\n",
       " 35.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 56.0,\n",
       " 2.0,\n",
       " 11.0,\n",
       " 58.0,\n",
       " 14.0,\n",
       " 18.0,\n",
       " 136.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 3.0,\n",
       " 10.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 15.0,\n",
       " 21.0,\n",
       " 22.0,\n",
       " 24.0,\n",
       " 5.0,\n",
       " 27.0,\n",
       " 20.0,\n",
       " 26.0,\n",
       " 30.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 37.0,\n",
       " 7.0,\n",
       " 8.0,\n",
       " 36.0,\n",
       " 45.0,\n",
       " 9.0,\n",
       " 47.0,\n",
       " 44.0,\n",
       " 42.0,\n",
       " 46.0,\n",
       " 51.0,\n",
       " 43.0,\n",
       " 54.0,\n",
       " 55.0,\n",
       " 56.0,\n",
       " 57.0,\n",
       " 60.0,\n",
       " 12.0,\n",
       " 61.0,\n",
       " 63.0,\n",
       " 65.0,\n",
       " 66.0,\n",
       " 67.0,\n",
       " 13.0,\n",
       " 69.0,\n",
       " 68.0,\n",
       " 71.0,\n",
       " 74.0,\n",
       " 75.0,\n",
       " 78.0,\n",
       " 79.0,\n",
       " 80.0,\n",
       " 16.0,\n",
       " 82.0,\n",
       " 17.0,\n",
       " 88.0,\n",
       " 89.0,\n",
       " 18.0,\n",
       " 19.0,\n",
       " 108.0,\n",
       " 111.0,\n",
       " 113.0,\n",
       " 23.0,\n",
       " 25.0,\n",
       " 127.0,\n",
       " 135.0,\n",
       " 137.0,\n",
       " 28.0,\n",
       " 29.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 32.0,\n",
       " 33.0,\n",
       " 34.0,\n",
       " 35.0,\n",
       " 190.0,\n",
       " 39.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 233.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 5.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 8.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 22.0,\n",
       " 23.0,\n",
       " 24.0,\n",
       " 25.0,\n",
       " 26.0,\n",
       " 29.0,\n",
       " 30.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 34.0,\n",
       " 36.0,\n",
       " 39.0,\n",
       " 46.0,\n",
       " 52.0,\n",
       " 54.0,\n",
       " 56.0,\n",
       " 58.0,\n",
       " 59.0,\n",
       " 61.0,\n",
       " 62.0,\n",
       " 68.0,\n",
       " 69.0,\n",
       " 71.0,\n",
       " 72.0,\n",
       " 74.0,\n",
       " 81.0,\n",
       " 82.0,\n",
       " 87.0,\n",
       " 88.0,\n",
       " 89.0,\n",
       " 92.0,\n",
       " 96.0,\n",
       " 98.0,\n",
       " 101.0,\n",
       " 103.0,\n",
       " 107.0,\n",
       " 108.0,\n",
       " 110.0,\n",
       " 114.0,\n",
       " 115.0,\n",
       " 118.0,\n",
       " 123.0,\n",
       " 127.0,\n",
       " 138.0,\n",
       " 139.0,\n",
       " 142.0,\n",
       " 144.0,\n",
       " 154.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 159.0,\n",
       " 162.0,\n",
       " 168.0,\n",
       " 169.0,\n",
       " 171.0,\n",
       " 172.0,\n",
       " 174.0,\n",
       " 175.0,\n",
       " 179.0,\n",
       " 183.0,\n",
       " 190.0,\n",
       " 196.0,\n",
       " 198.0,\n",
       " 199.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 215.0,\n",
       " 217.0,\n",
       " 218.0,\n",
       " 219.0,\n",
       " 220.0,\n",
       " 221.0,\n",
       " 223.0,\n",
       " 226.0,\n",
       " 227.0,\n",
       " 229.0,\n",
       " 230.0,\n",
       " 240.0,\n",
       " 247.0,\n",
       " 251.0,\n",
       " 253.0,\n",
       " 255.0,\n",
       " nan,\n",
       " 264.0,\n",
       " nan,\n",
       " 266.0,\n",
       " 267.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 276.0,\n",
       " nan,\n",
       " nan,\n",
       " 279.0,\n",
       " nan,\n",
       " nan,\n",
       " 282.0,\n",
       " 287.0,\n",
       " 291.0,\n",
       " 300.0,\n",
       " 303.0,\n",
       " 307.0,\n",
       " 315.0,\n",
       " 319.0,\n",
       " 333.0,\n",
       " 334.0,\n",
       " 339.0,\n",
       " 352.0,\n",
       " 364.0,\n",
       " 368.0,\n",
       " nan,\n",
       " 375.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 380.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 386.0,\n",
       " 401.0,\n",
       " 414.0,\n",
       " 428.0,\n",
       " 438.0,\n",
       " 439.0,\n",
       " 442.0,\n",
       " 446.0,\n",
       " 447.0,\n",
       " 449.0,\n",
       " 461.0,\n",
       " 478.0,\n",
       " 485.0,\n",
       " 486.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 493.0,\n",
       " nan,\n",
       " 495.0,\n",
       " nan,\n",
       " 506.0,\n",
       " 509.0,\n",
       " 511.0,\n",
       " 516.0,\n",
       " 520.0,\n",
       " 521.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 527.0,\n",
       " 528.0,\n",
       " 529.0,\n",
       " 530.0,\n",
       " 539.0,\n",
       " 556.0,\n",
       " 557.0,\n",
       " 559.0,\n",
       " nan,\n",
       " 561.0,\n",
       " 562.0,\n",
       " nan,\n",
       " nan,\n",
       " 565.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 573.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 582.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 588.0,\n",
       " 597.0,\n",
       " 601.0,\n",
       " 603.0,\n",
       " 609.0,\n",
       " 615.0,\n",
       " 621.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 632.0,\n",
       " nan,\n",
       " 634.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 644.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 656.0,\n",
       " 660.0,\n",
       " 665.0,\n",
       " 666.0,\n",
       " 686.0,\n",
       " 696.0,\n",
       " 702.0,\n",
       " 704.0,\n",
       " 740.0,\n",
       " 741.0,\n",
       " 749.0,\n",
       " 760.0,\n",
       " 770.0,\n",
       " 775.0,\n",
       " 776.0,\n",
       " 779.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 782.0,\n",
       " 791.0,\n",
       " 795.0,\n",
       " 800.0,\n",
       " 805.0,\n",
       " 812.0,\n",
       " 825.0,\n",
       " 827.0,\n",
       " 830.0,\n",
       " 840.0,\n",
       " 845.0,\n",
       " 847.0,\n",
       " 858.0,\n",
       " nan,\n",
       " 870.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 877.0,\n",
       " nan,\n",
       " 881.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 887.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 892.0,\n",
       " 900.0,\n",
       " 913.0,\n",
       " 914.0,\n",
       " 919.0,\n",
       " 930.0,\n",
       " 937.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 957.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 961.0,\n",
       " nan,\n",
       " nan,\n",
       " 966.0,\n",
       " 968.0,\n",
       " 972.0,\n",
       " 976.0,\n",
       " 977.0,\n",
       " 979.0,\n",
       " 986.0,\n",
       " 988.0,\n",
       " 990.0,\n",
       " 994.0,\n",
       " 995.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1005.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1019.0,\n",
       " 1021.0,\n",
       " 1036.0,\n",
       " 1043.0,\n",
       " 1048.0,\n",
       " 1050.0,\n",
       " 1056.0,\n",
       " 1065.0,\n",
       " 1075.0,\n",
       " 1077.0,\n",
       " 1080.0,\n",
       " 1084.0,\n",
       " 1098.0,\n",
       " 1102.0,\n",
       " 1106.0,\n",
       " 1107.0,\n",
       " 1108.0,\n",
       " 1112.0,\n",
       " 1114.0,\n",
       " 1117.0,\n",
       " 1118.0,\n",
       " 1119.0,\n",
       " 1121.0,\n",
       " 1122.0,\n",
       " 1124.0,\n",
       " 1138.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1144.0,\n",
       " 1146.0,\n",
       " 1150.0,\n",
       " 1154.0,\n",
       " 1185.0,\n",
       " 1191.0,\n",
       " 1195.0,\n",
       " 1205.0,\n",
       " 1216.0,\n",
       " 1219.0,\n",
       " 1220.0,\n",
       " 1227.0,\n",
       " 1247.0,\n",
       " 1260.0,\n",
       " 1261.0,\n",
       " 1265.0,\n",
       " 1267.0,\n",
       " 1270.0,\n",
       " 1291.0,\n",
       " 1307.0,\n",
       " 1314.0,\n",
       " 1316.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1320.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1324.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1329.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1342.0,\n",
       " 1347.0,\n",
       " 1351.0,\n",
       " 1352.0,\n",
       " 1355.0,\n",
       " 1356.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1371.0,\n",
       " 1376.0,\n",
       " 1378.0,\n",
       " 1381.0,\n",
       " nan,\n",
       " 1389.0,\n",
       " 1390.0,\n",
       " nan,\n",
       " 1392.0,\n",
       " 1393.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1397.0,\n",
       " 1411.0,\n",
       " 1434.0,\n",
       " 1441.0,\n",
       " 1443.0,\n",
       " 1445.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1451.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1456.0,\n",
       " 1474.0,\n",
       " 1475.0,\n",
       " 1496.0,\n",
       " 1497.0,\n",
       " 1499.0,\n",
       " 1512.0,\n",
       " 1513.0,\n",
       " 1514.0,\n",
       " 1515.0,\n",
       " 1516.0,\n",
       " 1520.0,\n",
       " 1531.0,\n",
       " 1537.0,\n",
       " 1538.0,\n",
       " 1540.0,\n",
       " 1542.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1549.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1564.0,\n",
       " 1566.0,\n",
       " 1569.0,\n",
       " 1573.0,\n",
       " 1574.0,\n",
       " 1583.0,\n",
       " 1585.0,\n",
       " 1586.0,\n",
       " nan,\n",
       " nan,\n",
       " 1595.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1614.0,\n",
       " 1618.0,\n",
       " 1636.0,\n",
       " 1637.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1643.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1657.0,\n",
       " 1670.0,\n",
       " 1679.0,\n",
       " 1683.0,\n",
       " 1689.0,\n",
       " 1699.0,\n",
       " 1706.0,\n",
       " 1709.0,\n",
       " 1716.0,\n",
       " 1720.0,\n",
       " 1725.0,\n",
       " 1735.0,\n",
       " 1739.0,\n",
       " 1750.0,\n",
       " 1760.0,\n",
       " 1762.0,\n",
       " 1763.0,\n",
       " 1764.0,\n",
       " 1765.0,\n",
       " 1766.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1784.0,\n",
       " 1785.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1815.0,\n",
       " 1818.0,\n",
       " 1829.0,\n",
       " 1833.0,\n",
       " 1834.0,\n",
       " 1835.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1874.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1893.0,\n",
       " 1894.0,\n",
       " 1909.0,\n",
       " 1913.0,\n",
       " 1915.0,\n",
       " 1934.0,\n",
       " 1938.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1950.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1963.0,\n",
       " 1965.0,\n",
       " 1970.0,\n",
       " 1977.0,\n",
       " 1979.0,\n",
       " 1991.0,\n",
       " 1994.0,\n",
       " 2019.0,\n",
       " 2020.0,\n",
       " 2023.0,\n",
       " 2028.0,\n",
       " 2036.0,\n",
       " 2039.0,\n",
       " 2040.0,\n",
       " 2043.0,\n",
       " 2044.0,\n",
       " 2046.0,\n",
       " 2063.0,\n",
       " 2067.0,\n",
       " 2074.0,\n",
       " 2078.0,\n",
       " 2079.0,\n",
       " 2101.0,\n",
       " 2102.0,\n",
       " 2108.0,\n",
       " 2111.0,\n",
       " 2116.0,\n",
       " 2126.0,\n",
       " 2129.0,\n",
       " 2132.0,\n",
       " 2144.0,\n",
       " 2148.0,\n",
       " 2150.0,\n",
       " 2164.0,\n",
       " 2166.0,\n",
       " 2168.0,\n",
       " 2170.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2195.0,\n",
       " 2228.0,\n",
       " 2238.0,\n",
       " 2251.0,\n",
       " 2252.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2260.0,\n",
       " 2261.0,\n",
       " nan,\n",
       " 2271.0,\n",
       " 2310.0,\n",
       " 2316.0,\n",
       " 2322.0,\n",
       " 2325.0,\n",
       " 2328.0,\n",
       " 2342.0,\n",
       " 2343.0,\n",
       " 2344.0,\n",
       " 2361.0,\n",
       " 2376.0,\n",
       " 2379.0,\n",
       " 2387.0,\n",
       " 2392.0,\n",
       " 2403.0,\n",
       " 2405.0,\n",
       " 2423.0,\n",
       " 2425.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2451.0,\n",
       " 2456.0,\n",
       " 2485.0,\n",
       " 2510.0,\n",
       " 2513.0,\n",
       " nan,\n",
       " 2516.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2526.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2546.0,\n",
       " 2564.0,\n",
       " 2566.0,\n",
       " 2592.0,\n",
       " 2595.0,\n",
       " 2599.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2616.0,\n",
       " 2617.0,\n",
       " 2634.0,\n",
       " 2635.0,\n",
       " 2645.0,\n",
       " 2658.0,\n",
       " 2660.0,\n",
       " 2665.0,\n",
       " 2669.0,\n",
       " nan,\n",
       " 2676.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2704.0,\n",
       " 2714.0,\n",
       " 2719.0,\n",
       " 2725.0,\n",
       " 2728.0,\n",
       " 2750.0,\n",
       " 2753.0,\n",
       " 2770.0,\n",
       " 2772.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2789.0,\n",
       " 2795.0,\n",
       " 2798.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2806.0,\n",
       " 2807.0,\n",
       " 2808.0,\n",
       " nan,\n",
       " nan,\n",
       " 2816.0,\n",
       " 2817.0,\n",
       " 2820.0,\n",
       " 2824.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2830.0,\n",
       " nan,\n",
       " nan,\n",
       " 2833.0,\n",
       " nan,\n",
       " 2842.0,\n",
       " 2848.0,\n",
       " 2849.0,\n",
       " 2860.0,\n",
       " 2875.0,\n",
       " 2885.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2898.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2902.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2910.0,\n",
       " 2912.0,\n",
       " 2914.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2921.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2942.0,\n",
       " 2952.0,\n",
       " 2956.0,\n",
       " 2958.0,\n",
       " 2965.0,\n",
       " 2970.0,\n",
       " 2975.0,\n",
       " 2981.0,\n",
       " 2989.0,\n",
       " 2993.0,\n",
       " 2999.0,\n",
       " 3000.0,\n",
       " 3014.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3036.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3040.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3052.0,\n",
       " 3058.0,\n",
       " 3077.0,\n",
       " 3094.0,\n",
       " 3097.0,\n",
       " 3098.0,\n",
       " 3108.0,\n",
       " 3111.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3136.0,\n",
       " nan,\n",
       " 3143.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3178.0,\n",
       " 3184.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3198.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3248.0,\n",
       " 3262.0,\n",
       " 3265.0,\n",
       " 3270.0,\n",
       " 3272.0,\n",
       " 3277.0,\n",
       " 3280.0,\n",
       " 3293.0,\n",
       " 3301.0,\n",
       " 3305.0,\n",
       " 3306.0,\n",
       " 3309.0,\n",
       " 3310.0,\n",
       " 3330.0,\n",
       " 3331.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3342.0,\n",
       " nan,\n",
       " nan,\n",
       " 3345.0,\n",
       " 3369.0,\n",
       " 3376.0,\n",
       " 3382.0,\n",
       " 3388.0,\n",
       " 3396.0,\n",
       " 3401.0,\n",
       " 3405.0,\n",
       " 3406.0,\n",
       " 3411.0,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 7.0,\n",
       " 3.0,\n",
       " 17.0,\n",
       " 2.0,\n",
       " 11.0,\n",
       " 4.0,\n",
       " 22.0,\n",
       " 23.0,\n",
       " 21.0,\n",
       " 25.0,\n",
       " 26.0,\n",
       " 30.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 33.0,\n",
       " 40.0,\n",
       " 36.0,\n",
       " 44.0,\n",
       " 8.0,\n",
       " 39.0,\n",
       " 47.0,\n",
       " 50.0,\n",
       " 51.0,\n",
       " 10.0,\n",
       " 53.0,\n",
       " 54.0,\n",
       " 58.0,\n",
       " 62.0,\n",
       " 73.0,\n",
       " 75.0,\n",
       " 93.0,\n",
       " 95.0,\n",
       " 97.0,\n",
       " 100.0,\n",
       " 103.0,\n",
       " 109.0,\n",
       " 113.0,\n",
       " 114.0,\n",
       " 118.0,\n",
       " 124.0,\n",
       " 125.0,\n",
       " 128.0,\n",
       " 130.0,\n",
       " 134.0,\n",
       " 138.0,\n",
       " 145.0,\n",
       " 146.0,\n",
       " 152.0,\n",
       " 155.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 165.0,\n",
       " 166.0,\n",
       " 161.0,\n",
       " 168.0,\n",
       " 170.0,\n",
       " 173.0,\n",
       " 175.0,\n",
       " 35.0,\n",
       " 181.0,\n",
       " 183.0,\n",
       " 192.0,\n",
       " 194.0,\n",
       " 195.0,\n",
       " 196.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 203.0,\n",
       " nan,\n",
       " 205.0,\n",
       " 206.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 212.0,\n",
       " 213.0,\n",
       " 216.0,\n",
       " 220.0,\n",
       " 222.0,\n",
       " 223.0,\n",
       " 224.0,\n",
       " 225.0,\n",
       " 242.0,\n",
       " 249.0,\n",
       " 256.0,\n",
       " 257.0,\n",
       " 258.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 267.0,\n",
       " nan,\n",
       " 269.0,\n",
       " 270.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 276.0,\n",
       " 277.0,\n",
       " 278.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 283.0,\n",
       " 287.0,\n",
       " 288.0,\n",
       " 290.0,\n",
       " 298.0,\n",
       " 305.0,\n",
       " 321.0,\n",
       " 322.0,\n",
       " 328.0,\n",
       " 341.0,\n",
       " 346.0,\n",
       " 354.0,\n",
       " 359.0,\n",
       " 362.0,\n",
       " 363.0,\n",
       " 369.0,\n",
       " 372.0,\n",
       " 373.0,\n",
       " 374.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 395.0,\n",
       " 396.0,\n",
       " 409.0,\n",
       " 411.0,\n",
       " 414.0,\n",
       " 418.0,\n",
       " 421.0,\n",
       " 423.0,\n",
       " 452.0,\n",
       " 459.0,\n",
       " 480.0,\n",
       " 481.0,\n",
       " 485.0,\n",
       " 486.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 490.0,\n",
       " nan,\n",
       " 492.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 498.0,\n",
       " 499.0,\n",
       " 507.0,\n",
       " 517.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 524.0,\n",
       " nan,\n",
       " 526.0,\n",
       " 527.0,\n",
       " nan,\n",
       " nan,\n",
       " 534.0,\n",
       " 539.0,\n",
       " 548.0,\n",
       " 555.0,\n",
       " 558.0,\n",
       " nan,\n",
       " nan,\n",
       " 562.0,\n",
       " 563.0,\n",
       " 564.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 570.0,\n",
       " 573.0,\n",
       " 575.0,\n",
       " 577.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 581.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 590.0,\n",
       " 594.0,\n",
       " 596.0,\n",
       " 602.0,\n",
       " 604.0,\n",
       " 608.0,\n",
       " 612.0,\n",
       " 614.0,\n",
       " 622.0,\n",
       " 624.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 631.0,\n",
       " nan,\n",
       " 633.0,\n",
       " nan,\n",
       " 638.0,\n",
       " 641.0,\n",
       " 643.0,\n",
       " 645.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 652.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 660.0,\n",
       " 665.0,\n",
       " 672.0,\n",
       " 685.0,\n",
       " 692.0,\n",
       " 712.0,\n",
       " 715.0,\n",
       " 723.0,\n",
       " 725.0,\n",
       " 726.0,\n",
       " 743.0,\n",
       " 764.0,\n",
       " 769.0,\n",
       " 770.0,\n",
       " 775.0,\n",
       " 779.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 156.0,\n",
       " 790.0,\n",
       " 795.0,\n",
       " 798.0,\n",
       " 805.0,\n",
       " 806.0,\n",
       " 812.0,\n",
       " 820.0,\n",
       " 825.0,\n",
       " 834.0,\n",
       " 837.0,\n",
       " 841.0,\n",
       " 846.0,\n",
       " 847.0,\n",
       " 852.0,\n",
       " 856.0,\n",
       " 859.0,\n",
       " 866.0,\n",
       " 868.0,\n",
       " nan,\n",
       " 870.0,\n",
       " 871.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 880.0,\n",
       " 881.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 886.0,\n",
       " 887.0,\n",
       " nan,\n",
       " nan,\n",
       " 890.0,\n",
       " nan,\n",
       " nan,\n",
       " 897.0,\n",
       " 898.0,\n",
       " 899.0,\n",
       " 913.0,\n",
       " 920.0,\n",
       " 924.0,\n",
       " 927.0,\n",
       " 928.0,\n",
       " 929.0,\n",
       " 939.0,\n",
       " 942.0,\n",
       " 944.0,\n",
       " 946.0,\n",
       " 949.0,\n",
       " 951.0,\n",
       " 952.0,\n",
       " nan,\n",
       " 955.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 961.0,\n",
       " nan,\n",
       " nan,\n",
       " 964.0,\n",
       " 973.0,\n",
       " 977.0,\n",
       " 980.0,\n",
       " 1001.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1012.0,\n",
       " 1015.0,\n",
       " 1016.0,\n",
       " 1023.0,\n",
       " 1028.0,\n",
       " 1040.0,\n",
       " 1045.0,\n",
       " 1047.0,\n",
       " 1052.0,\n",
       " 1053.0,\n",
       " 1054.0,\n",
       " 1060.0,\n",
       " 1062.0,\n",
       " 1063.0,\n",
       " 1066.0,\n",
       " 1081.0,\n",
       " 1082.0,\n",
       " 1084.0,\n",
       " 1093.0,\n",
       " 1095.0,\n",
       " 1099.0,\n",
       " 1112.0,\n",
       " 1115.0,\n",
       " 1123.0,\n",
       " 1126.0,\n",
       " 1130.0,\n",
       " 1133.0,\n",
       " nan,\n",
       " nan,\n",
       " 1142.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1149.0,\n",
       " 1160.0,\n",
       " 1164.0,\n",
       " 1169.0,\n",
       " 1173.0,\n",
       " 1174.0,\n",
       " 1184.0,\n",
       " 1194.0,\n",
       " 1195.0,\n",
       " 1202.0,\n",
       " 1205.0,\n",
       " 1207.0,\n",
       " 1215.0,\n",
       " 1223.0,\n",
       " 1224.0,\n",
       " 1228.0,\n",
       " 1241.0,\n",
       " 1249.0,\n",
       " 1252.0,\n",
       " 1253.0,\n",
       " 1256.0,\n",
       " 1257.0,\n",
       " 1261.0,\n",
       " 1266.0,\n",
       " 1268.0,\n",
       " 1270.0,\n",
       " 1272.0,\n",
       " 1274.0,\n",
       " 1278.0,\n",
       " 1286.0,\n",
       " 1289.0,\n",
       " 1292.0,\n",
       " 1293.0,\n",
       " 1296.0,\n",
       " 1308.0,\n",
       " 1315.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1321.0,\n",
       " nan,\n",
       " nan,\n",
       " 1324.0,\n",
       " nan,\n",
       " 1337.0,\n",
       " 1350.0,\n",
       " 1352.0,\n",
       " 1355.0,\n",
       " 1357.0,\n",
       " 1358.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1369.0,\n",
       " nan,\n",
       " 1376.0,\n",
       " 1378.0,\n",
       " 1380.0,\n",
       " 1383.0,\n",
       " 1385.0,\n",
       " 1386.0,\n",
       " nan,\n",
       " 1389.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1395.0,\n",
       " nan,\n",
       " nan,\n",
       " 1399.0,\n",
       " 1405.0,\n",
       " 1412.0,\n",
       " 1413.0,\n",
       " 1424.0,\n",
       " 1427.0,\n",
       " 1430.0,\n",
       " 1432.0,\n",
       " 1437.0,\n",
       " 1444.0,\n",
       " 1447.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1455.0,\n",
       " nan,\n",
       " 1458.0,\n",
       " 1459.0,\n",
       " 1460.0,\n",
       " 1462.0,\n",
       " 1469.0,\n",
       " 1471.0,\n",
       " 1475.0,\n",
       " 1478.0,\n",
       " 1485.0,\n",
       " 1487.0,\n",
       " 1493.0,\n",
       " 1495.0,\n",
       " 1501.0,\n",
       " 1507.0,\n",
       " 1517.0,\n",
       " 1519.0,\n",
       " 1520.0,\n",
       " 1521.0,\n",
       " 1525.0,\n",
       " 1529.0,\n",
       " 1531.0,\n",
       " 1534.0,\n",
       " 1536.0,\n",
       " 1538.0,\n",
       " 1540.0,\n",
       " 1542.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1555.0,\n",
       " 1556.0,\n",
       " 1557.0,\n",
       " 1561.0,\n",
       " 1562.0,\n",
       " 1564.0,\n",
       " 1571.0,\n",
       " 1576.0,\n",
       " 1580.0,\n",
       " 1583.0,\n",
       " 1590.0,\n",
       " nan,\n",
       " nan,\n",
       " 1595.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1601.0,\n",
       " nan,\n",
       " 1604.0,\n",
       " 1610.0,\n",
       " 1611.0,\n",
       " 1621.0,\n",
       " 1622.0,\n",
       " 1624.0,\n",
       " 1637.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1645.0,\n",
       " 1646.0,\n",
       " nan,\n",
       " 1649.0,\n",
       " 1651.0,\n",
       " 1660.0,\n",
       " 1661.0,\n",
       " 1667.0,\n",
       " 1668.0,\n",
       " 1674.0,\n",
       " 1676.0,\n",
       " 1681.0,\n",
       " 1687.0,\n",
       " 1699.0,\n",
       " 1708.0,\n",
       " 1709.0,\n",
       " 1713.0,\n",
       " 1719.0,\n",
       " 1729.0,\n",
       " 1735.0,\n",
       " 1736.0,\n",
       " 1737.0,\n",
       " 1753.0,\n",
       " 1757.0,\n",
       " 1761.0,\n",
       " 1764.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1773.0,\n",
       " 1776.0,\n",
       " 1780.0,\n",
       " 1781.0,\n",
       " 1787.0,\n",
       " 1790.0,\n",
       " 1797.0,\n",
       " 1799.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1805.0,\n",
       " nan,\n",
       " 1807.0,\n",
       " nan,\n",
       " 1812.0,\n",
       " 1813.0,\n",
       " 1815.0,\n",
       " 1818.0,\n",
       " 1822.0,\n",
       " 1826.0,\n",
       " 1829.0,\n",
       " 1838.0,\n",
       " 1841.0,\n",
       " 1844.0,\n",
       " 1848.0,\n",
       " nan,\n",
       " 1850.0,\n",
       " nan,\n",
       " nan,\n",
       " 1853.0,\n",
       " nan,\n",
       " nan,\n",
       " 1856.0,\n",
       " nan,\n",
       " 1860.0,\n",
       " 1862.0,\n",
       " 1863.0,\n",
       " 1865.0,\n",
       " 1866.0,\n",
       " 1870.0,\n",
       " nan,\n",
       " nan,\n",
       " 1873.0,\n",
       " 1874.0,\n",
       " 1875.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1881.0,\n",
       " 1883.0,\n",
       " 1895.0,\n",
       " 1904.0,\n",
       " 1906.0,\n",
       " 1908.0,\n",
       " 1912.0,\n",
       " 1914.0,\n",
       " 1915.0,\n",
       " 1923.0,\n",
       " 1927.0,\n",
       " 1935.0,\n",
       " 1942.0,\n",
       " 1944.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1955.0,\n",
       " 1961.0,\n",
       " 1963.0,\n",
       " 1964.0,\n",
       " 1966.0,\n",
       " 1970.0,\n",
       " 1972.0,\n",
       " 1980.0,\n",
       " 1995.0,\n",
       " 1998.0,\n",
       " 2002.0,\n",
       " 2007.0,\n",
       " 2008.0,\n",
       " 2010.0,\n",
       " 2022.0,\n",
       " 2024.0,\n",
       " 2032.0,\n",
       " 2035.0,\n",
       " 2037.0,\n",
       " 2038.0,\n",
       " 2059.0,\n",
       " 2061.0,\n",
       " 2066.0,\n",
       " 2067.0,\n",
       " 2072.0,\n",
       " 2077.0,\n",
       " 2085.0,\n",
       " 2086.0,\n",
       " 2091.0,\n",
       " 2092.0,\n",
       " 2093.0,\n",
       " 2099.0,\n",
       " 2104.0,\n",
       " 2108.0,\n",
       " 2111.0,\n",
       " 2116.0,\n",
       " 2119.0,\n",
       " 2121.0,\n",
       " 2125.0,\n",
       " 2130.0,\n",
       " 2131.0,\n",
       " 2134.0,\n",
       " 2140.0,\n",
       " 2142.0,\n",
       " 2149.0,\n",
       " 2151.0,\n",
       " 2154.0,\n",
       " 2167.0,\n",
       " 2169.0,\n",
       " 2170.0,\n",
       " 2171.0,\n",
       " 2179.0,\n",
       " 2181.0,\n",
       " 2182.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2190.0,\n",
       " nan,\n",
       " nan,\n",
       " 2197.0,\n",
       " 2198.0,\n",
       " 2213.0,\n",
       " 2214.0,\n",
       " 2219.0,\n",
       " 2220.0,\n",
       " 2223.0,\n",
       " 2230.0,\n",
       " 2234.0,\n",
       " 2237.0,\n",
       " 2244.0,\n",
       " 2245.0,\n",
       " 2248.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2256.0,\n",
       " nan,\n",
       " 2258.0,\n",
       " 2259.0,\n",
       " nan,\n",
       " nan,\n",
       " 2262.0,\n",
       " 2264.0,\n",
       " 2269.0,\n",
       " 2272.0,\n",
       " 2275.0,\n",
       " 2282.0,\n",
       " 2290.0,\n",
       " 2291.0,\n",
       " 2294.0,\n",
       " 2296.0,\n",
       " 2298.0,\n",
       " 2299.0,\n",
       " 2306.0,\n",
       " 2315.0,\n",
       " 2323.0,\n",
       " 2326.0,\n",
       " 2328.0,\n",
       " 2335.0,\n",
       " 2337.0,\n",
       " 2343.0,\n",
       " 2344.0,\n",
       " 2348.0,\n",
       " 2350.0,\n",
       " 2352.0,\n",
       " 2356.0,\n",
       " 2358.0,\n",
       " 2370.0,\n",
       " 2374.0,\n",
       " 2377.0,\n",
       " 2379.0,\n",
       " 2384.0,\n",
       " 2385.0,\n",
       " 2390.0,\n",
       " 2391.0,\n",
       " 2397.0,\n",
       " 2401.0,\n",
       " 2405.0,\n",
       " 2410.0,\n",
       " 2419.0,\n",
       " 2420.0,\n",
       " 2423.0,\n",
       " 2427.0,\n",
       " 2430.0,\n",
       " 2432.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2439.0,\n",
       " 2446.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2455.0,\n",
       " 2457.0,\n",
       " 2466.0,\n",
       " 2467.0,\n",
       " 2469.0,\n",
       " 2476.0,\n",
       " 2481.0,\n",
       " 2483.0,\n",
       " 2492.0,\n",
       " 2493.0,\n",
       " 2499.0,\n",
       " 2504.0,\n",
       " 2510.0,\n",
       " 2513.0,\n",
       " nan,\n",
       " 2516.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2537.0,\n",
       " 2541.0,\n",
       " 2547.0,\n",
       " 2555.0,\n",
       " 2559.0,\n",
       " 2563.0,\n",
       " 2572.0,\n",
       " 2573.0,\n",
       " 2576.0,\n",
       " 2579.0,\n",
       " 2580.0,\n",
       " 2585.0,\n",
       " 2587.0,\n",
       " 2591.0,\n",
       " 2592.0,\n",
       " 2597.0,\n",
       " 2598.0,\n",
       " nan,\n",
       " 2602.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2611.0,\n",
       " 2613.0,\n",
       " 2615.0,\n",
       " 2617.0,\n",
       " 2619.0,\n",
       " 2623.0,\n",
       " 2628.0,\n",
       " 2631.0,\n",
       " 2635.0,\n",
       " 2639.0,\n",
       " 2641.0,\n",
       " 2648.0,\n",
       " 2650.0,\n",
       " 2661.0,\n",
       " 2667.0,\n",
       " 2670.0,\n",
       " 2675.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2679.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2684.0,\n",
       " 2686.0,\n",
       " 2688.0,\n",
       " 2689.0,\n",
       " 2691.0,\n",
       " 2711.0,\n",
       " 2715.0,\n",
       " 2717.0,\n",
       " 2726.0,\n",
       " 2728.0,\n",
       " 2730.0,\n",
       " 2739.0,\n",
       " 2742.0,\n",
       " 2743.0,\n",
       " 2746.0,\n",
       " 2748.0,\n",
       " 2764.0,\n",
       " 2768.0,\n",
       " 2770.0,\n",
       " 2772.0,\n",
       " nan,\n",
       " nan,\n",
       " 2777.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2787.0,\n",
       " 2788.0,\n",
       " 2794.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2806.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2819.0,\n",
       " 2823.0,\n",
       " 2824.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2828.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2844.0,\n",
       " 2846.0,\n",
       " 2849.0,\n",
       " 2852.0,\n",
       " 2854.0,\n",
       " 2860.0,\n",
       " 2863.0,\n",
       " 2886.0,\n",
       " 2891.0,\n",
       " 2892.0,\n",
       " 2893.0,\n",
       " 2894.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2904.0,\n",
       " nan,\n",
       " 2906.0,\n",
       " 2909.0,\n",
       " 2910.0,\n",
       " 2914.0,\n",
       " 2915.0,\n",
       " nan,\n",
       " 2917.0,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " '2012-05-05 12:56:40',\n",
       " '2012-07-16 02:55:37',\n",
       " '2011-02-02 20:50:36',\n",
       " '2012-01-28 09:37:15',\n",
       " '2011-06-10 06:46:57',\n",
       " '2011-01-19 14:34:42',\n",
       " '2010-11-19 11:20:52',\n",
       " '2012-03-06 19:14:25',\n",
       " '2010-11-02 12:15:04',\n",
       " '2012-04-30 16:19:06',\n",
       " '2012-01-25 20:31:50',\n",
       " '2012-01-24 14:56:01',\n",
       " '2012-05-03 16:41:33',\n",
       " '2010-09-18 21:59:55',\n",
       " '2012-10-20 15:11:10',\n",
       " '2011-02-13 11:51:25',\n",
       " '2012-03-02 19:24:55',\n",
       " '2014-06-18 04:35:37',\n",
       " '2012-06-27 18:26:54',\n",
       " '2012-09-20 09:48:56',\n",
       " '2012-05-22 21:40:33',\n",
       " '2010-10-14 07:24:54',\n",
       " '2010-12-03 03:44:23',\n",
       " '2011-10-16 14:23:18',\n",
       " '2010-08-04 07:39:52',\n",
       " '2014-09-04 20:28:37',\n",
       " '2012-07-05 17:47:50',\n",
       " '2012-08-24 17:24:18',\n",
       " '2012-08-16 23:31:35',\n",
       " '2010-09-20 07:39:17',\n",
       " '2013-01-18 08:18:21',\n",
       " '2014-02-25 05:57:19',\n",
       " '2011-02-01 14:57:09',\n",
       " '2012-02-14 17:15:44',\n",
       " '2014-08-13 13:15:04',\n",
       " '2011-10-05 21:18:11',\n",
       " '2010-12-04 15:54:10',\n",
       " '2012-06-15 16:08:28',\n",
       " '2011-04-28 21:31:47',\n",
       " '2010-11-23 20:58:18',\n",
       " '2013-01-31 03:28:00',\n",
       " '2011-05-22 15:40:30',\n",
       " '2012-10-01 12:05:15',\n",
       " '2012-08-02 12:22:24',\n",
       " '2012-01-12 17:11:33',\n",
       " '2012-07-15 02:25:52',\n",
       " '2012-11-04 03:35:07',\n",
       " '2011-11-09 02:42:10',\n",
       " '2011-11-16 20:58:47',\n",
       " '2011-11-02 21:33:17',\n",
       " '2012-05-11 14:30:13',\n",
       " '2012-06-23 17:45:53',\n",
       " '2012-10-01 09:11:34',\n",
       " '2012-07-24 09:37:04',\n",
       " '2011-02-12 05:51:37',\n",
       " '2013-12-02 22:47:49',\n",
       " '2011-09-02 07:04:59',\n",
       " '2012-01-19 22:46:10',\n",
       " '2012-10-31 22:08:35',\n",
       " '2013-01-15 14:46:04',\n",
       " '2012-02-04 21:14:00',\n",
       " '2012-08-15 01:09:27',\n",
       " '2011-08-30 10:50:46',\n",
       " '2011-05-09 06:43:41',\n",
       " '2013-08-20 00:02:54',\n",
       " '2011-10-24 22:14:27',\n",
       " '2010-09-17 11:04:45',\n",
       " '2012-09-22 14:26:35',\n",
       " '2012-06-29 06:16:53',\n",
       " '2014-05-10 08:55:51',\n",
       " '2011-09-06 16:02:43',\n",
       " '2010-09-24 04:24:59',\n",
       " '2013-01-16 17:09:32',\n",
       " '2013-01-17 12:09:47',\n",
       " '2012-10-09 00:08:53',\n",
       " '2012-04-06 17:01:18',\n",
       " '2010-11-18 10:53:37',\n",
       " '2010-09-23 10:54:12',\n",
       " '2011-03-31 22:03:58',\n",
       " '2012-10-20 17:50:11',\n",
       " '2011-10-17 12:45:09',\n",
       " '2010-09-15 00:21:43',\n",
       " '2012-07-04 19:38:50',\n",
       " '2010-11-29 01:14:51',\n",
       " '2013-01-17 05:38:42',\n",
       " '2012-09-25 21:03:43',\n",
       " '2012-05-23 20:44:14',\n",
       " '2012-05-11 19:15:33',\n",
       " '2011-07-29 18:54:11',\n",
       " '2011-05-31 12:11:46',\n",
       " '2011-04-02 12:45:38',\n",
       " '2012-07-29 22:12:33',\n",
       " '2012-02-08 16:15:29',\n",
       " '2013-01-13 01:08:52',\n",
       " '2012-11-22 13:49:55',\n",
       " '2012-06-01 17:05:32',\n",
       " '2012-02-14 17:28:11',\n",
       " '2012-04-23 10:31:04',\n",
       " '2012-07-30 08:46:15',\n",
       " '2011-08-08 17:35:19',\n",
       " '2011-10-04 12:29:01',\n",
       " '2012-11-07 21:47:43',\n",
       " '2012-10-03 01:23:31',\n",
       " '2013-07-22 04:02:59',\n",
       " '2011-11-17 15:30:18',\n",
       " '2012-09-21 22:34:23',\n",
       " '2011-05-06 13:47:26',\n",
       " '2012-02-09 02:14:22',\n",
       " '2012-07-10 01:01:21',\n",
       " '2012-08-25 08:28:34',\n",
       " '2012-05-30 18:49:18',\n",
       " '2013-03-20 17:55:18',\n",
       " '2012-01-31 20:32:40',\n",
       " '2011-11-28 08:13:06',\n",
       " '2011-09-12 19:09:20',\n",
       " '2012-07-06 13:49:14',\n",
       " '2012-08-26 11:19:20',\n",
       " '2012-05-16 12:21:17',\n",
       " '2011-02-02 19:06:48',\n",
       " '2012-12-31 09:27:25',\n",
       " '2013-01-08 19:10:47',\n",
       " '2011-03-24 20:54:54',\n",
       " '2012-08-30 23:39:04',\n",
       " '2010-10-07 08:45:27',\n",
       " '2011-12-16 09:51:25',\n",
       " '2012-07-06 00:36:30',\n",
       " '2011-01-07 23:43:23',\n",
       " '2012-11-14 16:16:40',\n",
       " '2010-12-12 14:00:57',\n",
       " '2014-01-25 18:15:13',\n",
       " '2012-04-16 17:55:40',\n",
       " '2012-08-07 15:33:06',\n",
       " '2012-12-06 20:37:02',\n",
       " '2011-08-25 16:02:20',\n",
       " '2012-01-07 10:25:35',\n",
       " '2012-04-06 13:38:42',\n",
       " '2014-07-04 17:45:07',\n",
       " '2012-05-10 21:12:52',\n",
       " '2013-09-03 11:58:44',\n",
       " '2012-02-15 01:33:28',\n",
       " '2011-09-16 08:23:27',\n",
       " '2012-10-29 13:14:42',\n",
       " '2011-09-03 09:42:05',\n",
       " '2014-03-11 16:39:59',\n",
       " '2012-12-02 19:45:27',\n",
       " '2013-06-07 09:25:31',\n",
       " '2012-01-27 13:03:42',\n",
       " '2011-10-14 19:55:56',\n",
       " '2012-08-02 06:49:54',\n",
       " '2013-04-09 21:28:42',\n",
       " '2013-01-09 14:43:20',\n",
       " '2011-07-18 07:33:13',\n",
       " '2012-07-29 20:47:03',\n",
       " '2012-08-12 01:30:43',\n",
       " '2012-09-30 15:36:16',\n",
       " '2012-04-28 03:04:18',\n",
       " '2012-08-04 17:02:44',\n",
       " '2011-03-30 18:36:01',\n",
       " '2012-10-22 13:28:48',\n",
       " '2012-11-28 20:54:38',\n",
       " '2011-06-01 03:44:48',\n",
       " '2012-08-11 08:06:58',\n",
       " '2012-12-11 14:47:31',\n",
       " '2012-02-15 17:26:42',\n",
       " '2012-02-20 18:07:57',\n",
       " '2013-05-25 16:57:33',\n",
       " '2011-08-23 07:23:11',\n",
       " '2012-12-24 19:18:33',\n",
       " '2012-06-14 07:38:52',\n",
       " '2012-06-26 06:38:42',\n",
       " '2014-03-09 17:21:20',\n",
       " '2012-05-30 21:52:50',\n",
       " '2012-07-13 07:12:05',\n",
       " '2013-03-18 20:02:56',\n",
       " '2012-12-24 15:29:45',\n",
       " '2010-09-17 13:20:58',\n",
       " '2012-08-07 18:50:41',\n",
       " '2012-05-14 21:20:38',\n",
       " '2010-09-07 22:25:29',\n",
       " '2011-06-22 13:54:33',\n",
       " '2013-01-05 15:12:33',\n",
       " '2010-11-01 18:29:13',\n",
       " '2012-08-14 13:35:58',\n",
       " '2013-03-20 05:53:51',\n",
       " '2012-12-06 12:03:11',\n",
       " '2011-06-06 18:39:55',\n",
       " '2011-12-21 17:37:45',\n",
       " '2012-03-22 03:53:41',\n",
       " '2012-07-05 18:45:48',\n",
       " '2012-12-13 16:20:30',\n",
       " '2010-12-23 20:53:49',\n",
       " '2012-12-04 20:37:41',\n",
       " '2011-11-21 17:28:18',\n",
       " '2010-12-14 13:20:06',\n",
       " '2011-11-15 11:31:26',\n",
       " '2012-12-19 00:29:46',\n",
       " '2012-11-06 18:12:52',\n",
       " '2010-09-30 21:25:14',\n",
       " '2012-01-13 13:46:33',\n",
       " '2012-06-13 10:26:13',\n",
       " '2010-09-19 22:06:26',\n",
       " '2012-02-06 11:24:53',\n",
       " '2012-07-31 11:27:12',\n",
       " '2010-12-26 16:39:54',\n",
       " '2012-04-02 12:39:51',\n",
       " '2011-04-22 02:56:08',\n",
       " '2011-09-27 04:23:57',\n",
       " '2010-11-27 15:10:15',\n",
       " '2011-10-25 20:17:09',\n",
       " '2013-01-10 12:43:10',\n",
       " '2011-04-04 20:29:03',\n",
       " '2011-04-05 15:50:11',\n",
       " '2011-02-13 23:28:51',\n",
       " '2013-06-27 13:15:47',\n",
       " '2010-09-01 00:08:44',\n",
       " '2012-05-10 06:21:58',\n",
       " '2012-05-26 22:50:03',\n",
       " '2011-01-19 09:33:06',\n",
       " '2011-02-06 14:48:56',\n",
       " '2011-04-01 06:02:59',\n",
       " '2012-10-25 02:16:17',\n",
       " '2012-03-12 22:10:22',\n",
       " '2012-12-21 04:59:34',\n",
       " '2011-05-02 07:07:58',\n",
       " '2011-01-23 09:26:32',\n",
       " '2011-06-23 10:30:41',\n",
       " '2012-05-19 18:14:34',\n",
       " '2012-07-15 21:43:37',\n",
       " '2012-05-15 01:54:02',\n",
       " '2012-11-09 00:52:11',\n",
       " '2012-10-19 14:56:28',\n",
       " '2010-08-30 15:02:00',\n",
       " '2013-08-06 10:41:15',\n",
       " '2012-08-27 09:32:58',\n",
       " '2011-09-05 12:56:57',\n",
       " '2012-09-13 10:24:54',\n",
       " '2010-10-15 14:42:37',\n",
       " '2012-01-11 18:45:10',\n",
       " '2012-10-29 22:56:42',\n",
       " '2012-12-11 09:40:17',\n",
       " '2011-08-28 18:29:43',\n",
       " '2012-10-18 03:53:28',\n",
       " '2012-02-29 08:05:58',\n",
       " '2012-08-12 17:27:45',\n",
       " '2012-05-27 10:33:23',\n",
       " '2010-09-10 15:37:00',\n",
       " '2013-09-02 09:49:30',\n",
       " '2013-08-10 21:25:44',\n",
       " '2011-12-07 15:17:54',\n",
       " '2012-05-30 22:10:25',\n",
       " '2010-11-20 23:51:23',\n",
       " '2012-08-19 14:16:48',\n",
       " '2013-03-31 01:01:08',\n",
       " '2011-02-22 00:06:57',\n",
       " '2012-04-05 16:49:29',\n",
       " '2012-06-12 07:02:35',\n",
       " '2011-10-30 16:46:45',\n",
       " '2012-10-31 09:44:49',\n",
       " '2012-07-02 19:35:23',\n",
       " '2014-02-18 13:56:02',\n",
       " '2011-12-05 21:13:34',\n",
       " '2013-04-19 00:22:25',\n",
       " '2010-09-17 20:23:23',\n",
       " '2011-12-05 08:43:20',\n",
       " '2010-12-19 06:48:42',\n",
       " '2012-11-27 07:54:45',\n",
       " '2012-09-06 15:40:46',\n",
       " '2011-03-09 10:28:37',\n",
       " '2012-07-21 21:14:14',\n",
       " '2012-04-02 18:03:54',\n",
       " '2010-10-10 11:33:27',\n",
       " '2012-07-05 18:27:51',\n",
       " '2012-05-31 15:43:47',\n",
       " '2011-10-03 18:39:00',\n",
       " '2013-07-09 00:13:45',\n",
       " '2013-07-17 02:30:07',\n",
       " '2012-09-30 12:34:40',\n",
       " '2012-06-14 05:27:22',\n",
       " '2012-08-21 02:03:07',\n",
       " '2012-11-05 14:09:06',\n",
       " '2013-12-24 03:32:08',\n",
       " '2013-09-12 10:05:16',\n",
       " '2012-03-23 11:38:24',\n",
       " '2012-10-03 14:45:17',\n",
       " '2011-06-17 14:09:25',\n",
       " '2012-05-13 16:29:33',\n",
       " '2012-11-05 15:51:05',\n",
       " '2012-04-30 16:34:23',\n",
       " '2011-09-01 16:37:01',\n",
       " '2012-10-11 02:39:33',\n",
       " '2012-03-28 23:07:00',\n",
       " '2012-02-19 18:58:20',\n",
       " '2012-04-28 17:27:09',\n",
       " '2012-01-06 20:37:03',\n",
       " '2012-04-18 14:21:21',\n",
       " '2012-06-19 19:57:09',\n",
       " '2011-11-26 22:08:22',\n",
       " '2012-08-04 09:56:30',\n",
       " '2013-01-22 18:52:18',\n",
       " '2012-12-17 16:27:39',\n",
       " '2012-11-13 11:04:41',\n",
       " '2012-10-02 17:03:08',\n",
       " '2012-11-02 09:32:52',\n",
       " '2010-07-27 17:15:57',\n",
       " '2010-10-20 11:56:39',\n",
       " '2012-02-16 00:33:39',\n",
       " '2011-07-19 08:23:38',\n",
       " '2012-10-17 21:14:24',\n",
       " '2012-12-14 09:27:33',\n",
       " '2012-10-17 20:34:49',\n",
       " '2011-05-24 15:46:47',\n",
       " '2010-12-06 10:26:02',\n",
       " '2012-08-21 11:16:14',\n",
       " '2011-04-11 17:57:45',\n",
       " '2013-08-31 20:38:23',\n",
       " '2013-09-03 15:28:49',\n",
       " '2012-06-06 15:34:46',\n",
       " '2011-09-14 13:00:05',\n",
       " '2011-10-04 00:49:26',\n",
       " '2012-01-29 18:00:01',\n",
       " '2012-05-08 21:19:50',\n",
       " '2011-01-29 12:04:57',\n",
       " '2013-01-21 03:15:22',\n",
       " '2012-03-16 23:56:21',\n",
       " '2012-12-20 09:08:53',\n",
       " '2013-06-23 09:47:00',\n",
       " '2011-06-27 20:17:37',\n",
       " '2012-12-17 17:14:52',\n",
       " '2013-06-25 00:19:43',\n",
       " '2012-02-04 09:38:57',\n",
       " '2011-12-02 20:28:39',\n",
       " '2011-03-13 20:33:09',\n",
       " '2012-02-26 22:50:26',\n",
       " '2011-11-18 20:21:35',\n",
       " '2011-08-25 07:47:46',\n",
       " '2011-08-20 05:06:25',\n",
       " '2012-05-10 12:20:32',\n",
       " '2011-11-27 23:54:34',\n",
       " '2012-02-12 02:40:24',\n",
       " '2012-02-23 15:54:59',\n",
       " '2012-04-23 09:12:15',\n",
       " '2012-06-26 14:31:23',\n",
       " '2012-11-05 22:06:00',\n",
       " '2012-03-23 11:36:48',\n",
       " '2012-02-20 18:26:27',\n",
       " '2012-02-22 02:22:34',\n",
       " '2013-09-07 16:18:51',\n",
       " '2013-01-20 06:47:52',\n",
       " '2011-03-10 05:05:51',\n",
       " '2013-12-01 03:18:23',\n",
       " '2012-04-12 14:22:59',\n",
       " '2012-08-11 22:02:37',\n",
       " '2012-11-05 14:59:52',\n",
       " '2011-05-07 09:50:17',\n",
       " '2012-07-16 20:12:16',\n",
       " '2010-09-20 10:34:39',\n",
       " '2012-03-08 17:14:23',\n",
       " '2011-01-15 07:57:33',\n",
       " '2012-05-30 14:12:59',\n",
       " '2010-11-08 09:23:42',\n",
       " '2011-11-17 09:20:58',\n",
       " '2011-07-31 21:56:20',\n",
       " '2011-04-16 00:10:23',\n",
       " '2010-09-15 05:54:47',\n",
       " '2013-01-10 23:15:27',\n",
       " '2012-09-10 20:58:45',\n",
       " '2012-01-01 23:22:38',\n",
       " '2012-06-12 06:47:35',\n",
       " '2012-09-01 15:11:20',\n",
       " '2011-01-30 06:32:25',\n",
       " '2013-04-27 14:09:42',\n",
       " '2010-07-28 17:34:31',\n",
       " '2011-03-16 08:44:56',\n",
       " '2012-01-23 22:21:27',\n",
       " '2010-10-14 22:40:19',\n",
       " '2012-03-01 11:35:51',\n",
       " '2011-01-19 10:52:57',\n",
       " '2012-11-02 08:25:05',\n",
       " '2013-01-16 13:38:00',\n",
       " '2012-06-30 23:55:12',\n",
       " '2013-01-04 10:52:17',\n",
       " '2012-05-03 16:07:12',\n",
       " '2012-11-29 01:59:45',\n",
       " '2011-03-15 19:05:03',\n",
       " '2012-08-07 15:27:53',\n",
       " '2012-02-14 14:10:12',\n",
       " '2012-07-19 08:47:46',\n",
       " '2013-02-04 22:00:34',\n",
       " '2012-10-27 10:57:39',\n",
       " '2011-12-19 21:08:22',\n",
       " '2013-05-24 13:02:17',\n",
       " '2011-07-17 07:54:47',\n",
       " '2012-04-15 09:24:33',\n",
       " '2011-11-13 15:55:23',\n",
       " '2012-08-30 23:36:08',\n",
       " '2012-03-12 12:05:48',\n",
       " '2012-11-29 12:50:37',\n",
       " '2011-08-13 16:48:56',\n",
       " '2013-12-23 12:07:09',\n",
       " '2012-11-25 12:07:22',\n",
       " '2013-01-18 17:47:57',\n",
       " '2011-02-13 12:34:08',\n",
       " '2011-01-13 16:42:04',\n",
       " '2010-09-29 15:23:59',\n",
       " '2011-03-11 18:45:41',\n",
       " '2012-02-23 09:11:37',\n",
       " '2011-09-24 19:05:19',\n",
       " '2011-01-19 21:44:00',\n",
       " '2011-10-19 19:49:14',\n",
       " '2011-01-31 04:16:40',\n",
       " '2011-05-20 09:04:49',\n",
       " '2013-07-21 23:15:07',\n",
       " '2010-10-09 20:11:09',\n",
       " '2012-10-20 09:23:29',\n",
       " '2012-07-18 19:07:28',\n",
       " '2013-10-25 22:30:15',\n",
       " '2013-07-29 07:01:54',\n",
       " '2011-07-29 11:16:03',\n",
       " '2012-06-13 01:58:28',\n",
       " '2012-07-06 12:43:27',\n",
       " '2014-07-05 04:04:48',\n",
       " '2012-07-30 23:22:28',\n",
       " '2011-12-07 02:22:03',\n",
       " '2012-04-24 22:52:32',\n",
       " '2012-04-10 13:47:03',\n",
       " '2010-07-26 14:06:09',\n",
       " '2012-12-18 19:33:05',\n",
       " '2011-11-10 03:06:42',\n",
       " '2010-09-24 03:06:00',\n",
       " '2012-08-15 20:09:08',\n",
       " '2012-07-05 17:06:45',\n",
       " '2012-07-23 08:19:02',\n",
       " '2012-08-26 07:47:45',\n",
       " '2013-01-21 21:51:50',\n",
       " '2010-12-11 23:40:23',\n",
       " '2012-03-12 23:57:21',\n",
       " '2011-07-12 14:14:45',\n",
       " '2012-04-10 08:20:12',\n",
       " '2011-05-27 14:02:45',\n",
       " '2012-09-20 16:16:30',\n",
       " '2012-12-09 09:35:39',\n",
       " '2011-12-17 02:19:43',\n",
       " '2011-12-04 07:53:55',\n",
       " '2012-05-10 03:32:36',\n",
       " '2012-07-03 20:06:44',\n",
       " '2013-07-28 18:06:25',\n",
       " '2012-05-02 14:19:06',\n",
       " '2011-02-22 23:27:11',\n",
       " '2012-11-02 08:08:32',\n",
       " '2011-07-19 21:13:26',\n",
       " '2012-07-28 19:43:47',\n",
       " '2012-06-12 18:41:44',\n",
       " '2012-10-28 17:34:07',\n",
       " '2012-03-30 09:54:55',\n",
       " '2012-04-12 08:25:43',\n",
       " '2012-07-12 01:58:53',\n",
       " '2012-09-01 19:48:49',\n",
       " '2012-07-08 08:40:04',\n",
       " '2013-01-28 11:12:53',\n",
       " '2010-09-18 08:56:35',\n",
       " '2011-02-22 09:24:27',\n",
       " '2011-06-21 20:33:30',\n",
       " '2011-05-26 02:51:52',\n",
       " '2011-06-17 11:38:35',\n",
       " '2010-10-20 21:12:08',\n",
       " '2013-08-31 16:30:45',\n",
       " '2011-02-25 19:51:48',\n",
       " '2013-09-14 21:01:34',\n",
       " '2011-01-02 11:22:32',\n",
       " '2011-06-08 14:35:23',\n",
       " '2011-09-25 12:06:55',\n",
       " '2012-10-22 00:00:43',\n",
       " '2013-01-09 14:10:14',\n",
       " '2010-10-15 17:25:32',\n",
       " '2011-08-23 09:34:16',\n",
       " '2012-10-22 18:51:15',\n",
       " '2011-06-23 13:00:46',\n",
       " '2012-07-09 17:02:05',\n",
       " '2011-09-16 09:46:33',\n",
       " '2012-03-21 22:48:57',\n",
       " '2012-08-23 16:46:01',\n",
       " '2014-05-10 14:50:02',\n",
       " '2010-08-04 13:43:26',\n",
       " '2012-02-13 12:55:54',\n",
       " '2011-02-26 21:08:01',\n",
       " '2011-12-14 05:24:40',\n",
       " '2012-04-02 20:41:16',\n",
       " '2013-04-14 03:04:40',\n",
       " '2012-11-28 17:42:26',\n",
       " '2012-06-25 01:59:45',\n",
       " '2012-10-14 13:47:43',\n",
       " '2012-10-30 14:46:23',\n",
       " '2010-11-17 17:07:15',\n",
       " '2012-12-01 16:36:25',\n",
       " '2012-11-18 08:16:03',\n",
       " '2012-06-12 14:33:30',\n",
       " '2011-10-03 08:37:54',\n",
       " '2010-09-09 00:10:56',\n",
       " '2013-01-15 11:57:48',\n",
       " '2012-08-25 13:24:53',\n",
       " '2012-10-09 16:11:04',\n",
       " '2012-05-20 05:06:02',\n",
       " '2011-06-07 12:01:15',\n",
       " '2013-05-17 01:53:49',\n",
       " '2012-10-16 13:52:49',\n",
       " '2012-05-11 21:59:51',\n",
       " '2013-01-31 20:09:13',\n",
       " '2011-10-18 03:45:35',\n",
       " '2012-05-30 02:05:15',\n",
       " '2012-11-11 21:44:02',\n",
       " '2012-01-20 18:42:06',\n",
       " '2012-12-26 09:04:48',\n",
       " '2011-08-17 18:33:08',\n",
       " '2012-12-02 14:21:59',\n",
       " '2012-08-16 10:13:34',\n",
       " '2011-07-29 01:50:38',\n",
       " '2012-10-21 04:22:00',\n",
       " '2011-12-14 15:45:27',\n",
       " '2011-02-24 13:15:07',\n",
       " '2011-08-02 19:08:32',\n",
       " '2010-11-17 16:30:37',\n",
       " '2011-03-21 11:28:01',\n",
       " '2011-06-15 12:35:29',\n",
       " '2013-09-16 16:48:19',\n",
       " '2012-07-18 00:25:53',\n",
       " '2012-10-10 10:32:41',\n",
       " '2012-05-27 08:49:48',\n",
       " '2013-06-07 04:34:33',\n",
       " '2012-11-23 11:06:17',\n",
       " '2012-06-14 15:00:48',\n",
       " '2012-05-21 04:39:00',\n",
       " '2011-05-24 13:36:02',\n",
       " '2012-07-07 19:13:31',\n",
       " '2011-10-03 23:35:33',\n",
       " '2011-07-26 02:49:42',\n",
       " '2012-03-01 11:50:08',\n",
       " '2011-01-31 18:21:57',\n",
       " '2012-11-20 12:39:47',\n",
       " '2010-12-09 18:25:58',\n",
       " '2012-08-06 13:39:15',\n",
       " '2011-06-08 15:35:27',\n",
       " '2012-07-24 06:44:54',\n",
       " '2013-01-14 09:38:26',\n",
       " '2012-12-19 09:59:31',\n",
       " '2012-03-28 21:38:00',\n",
       " '2012-11-29 00:16:50',\n",
       " '2012-07-21 04:33:02',\n",
       " '2012-10-23 11:13:59',\n",
       " '2012-12-05 19:26:06',\n",
       " '2012-02-15 05:06:28',\n",
       " '2010-12-09 22:00:07',\n",
       " '2012-12-12 15:40:54',\n",
       " '2012-04-24 10:46:34',\n",
       " '2012-11-05 12:58:36',\n",
       " '2013-08-28 13:33:57',\n",
       " '2011-07-18 11:06:50',\n",
       " '2013-05-31 11:19:30',\n",
       " '2012-04-24 10:18:34',\n",
       " '2012-04-13 14:50:44',\n",
       " '2013-01-23 00:32:33',\n",
       " '2011-07-05 21:46:58',\n",
       " '2011-10-25 20:59:20',\n",
       " '2014-08-12 18:58:40',\n",
       " '2012-08-15 17:19:58',\n",
       " '2012-07-22 11:15:41',\n",
       " '2011-11-03 19:01:30',\n",
       " '2012-06-22 16:09:26',\n",
       " '2012-12-13 23:14:43',\n",
       " '2012-10-29 08:20:44',\n",
       " '2013-07-13 20:00:01',\n",
       " '2012-06-14 12:40:04',\n",
       " '2011-05-09 17:53:55',\n",
       " '2011-02-26 21:45:46',\n",
       " '2013-08-23 23:09:55',\n",
       " '2013-08-30 22:39:37',\n",
       " '2012-02-14 04:57:19',\n",
       " '2012-06-14 23:19:21',\n",
       " '2012-08-14 12:37:39',\n",
       " '2012-11-21 16:54:16',\n",
       " '2012-02-15 18:02:41',\n",
       " '2010-11-22 15:43:17',\n",
       " '2011-08-22 01:26:44',\n",
       " '2011-03-20 10:11:32',\n",
       " '2012-07-20 20:39:25',\n",
       " '2013-01-21 12:04:00',\n",
       " '2012-02-23 10:38:06',\n",
       " '2012-10-16 21:41:09',\n",
       " '2011-11-12 18:43:17',\n",
       " '2013-02-21 20:53:08',\n",
       " '2011-08-02 06:06:02',\n",
       " '2012-06-29 05:39:47',\n",
       " '2012-09-25 20:49:27',\n",
       " '2012-03-11 06:54:20',\n",
       " '2011-04-07 14:29:27',\n",
       " '2012-12-24 16:58:27',\n",
       " '2014-01-02 21:46:14',\n",
       " '2012-12-14 20:30:58',\n",
       " '2013-01-03 22:21:38',\n",
       " '2012-09-04 15:46:53',\n",
       " '2012-12-09 11:26:20',\n",
       " '2011-02-02 11:01:33',\n",
       " '2011-04-28 19:01:45',\n",
       " '2012-06-15 16:54:02',\n",
       " '2012-07-31 08:06:54',\n",
       " '2011-05-26 07:33:00',\n",
       " '2012-05-23 06:45:24',\n",
       " '2012-05-25 02:37:33',\n",
       " '2010-09-25 10:59:17',\n",
       " '2010-11-09 01:25:20',\n",
       " '2011-01-18 15:07:26',\n",
       " '2012-07-19 18:29:20',\n",
       " '2014-04-19 14:07:46',\n",
       " '2012-09-21 00:24:40',\n",
       " '2012-06-17 10:20:56',\n",
       " '2012-08-10 19:59:18',\n",
       " '2012-05-22 17:21:53',\n",
       " '2012-07-04 21:56:45',\n",
       " '2012-12-28 21:23:12',\n",
       " '2010-09-07 10:18:19',\n",
       " '2012-12-06 05:01:45',\n",
       " '2011-07-19 15:50:11',\n",
       " '2014-08-01 09:19:50',\n",
       " '2011-09-19 06:27:50',\n",
       " '2012-11-24 13:53:21',\n",
       " '2012-07-10 06:15:09',\n",
       " '2012-06-08 14:03:46',\n",
       " '2012-02-19 11:37:44',\n",
       " '2012-08-01 19:40:34',\n",
       " '2010-11-24 12:47:15',\n",
       " '2012-11-12 16:20:22',\n",
       " '2012-01-23 08:51:41',\n",
       " '2011-12-29 05:05:22',\n",
       " '2012-10-09 17:23:37',\n",
       " '2012-03-23 16:56:31',\n",
       " '2010-08-25 20:49:17',\n",
       " '2011-02-27 20:09:28',\n",
       " '2011-02-25 22:54:12',\n",
       " '2013-08-23 02:49:02',\n",
       " '2012-09-26 17:53:46',\n",
       " '2011-12-03 22:12:15',\n",
       " '2014-04-02 14:02:44',\n",
       " '2012-04-23 01:22:32',\n",
       " '2012-03-22 09:40:59',\n",
       " '2012-09-29 22:27:01',\n",
       " '2013-10-25 22:07:58',\n",
       " '2010-08-25 22:57:35',\n",
       " '2012-01-01 14:35:26',\n",
       " '2012-06-04 07:26:43',\n",
       " '2012-07-20 07:03:49',\n",
       " '2012-11-02 16:52:57',\n",
       " '2010-11-25 07:42:10',\n",
       " '2011-03-22 04:50:36',\n",
       " '2010-11-24 20:35:21',\n",
       " '2011-08-26 19:52:58',\n",
       " '2012-04-20 12:46:39',\n",
       " '2012-05-31 15:35:28',\n",
       " '2012-11-26 03:41:39',\n",
       " '2012-02-01 12:42:44',\n",
       " '2012-03-24 09:05:12',\n",
       " '2012-05-12 22:09:58',\n",
       " '2012-09-14 16:17:54',\n",
       " '2012-10-15 13:14:57',\n",
       " '2011-11-29 22:51:46',\n",
       " '2012-06-23 18:09:19',\n",
       " '2012-06-19 20:42:32',\n",
       " '2012-05-31 19:43:34',\n",
       " '2011-03-27 08:46:49',\n",
       " '2013-09-06 09:38:35',\n",
       " '2012-12-13 23:22:16',\n",
       " '2011-08-21 11:13:21',\n",
       " '2011-12-29 13:39:35',\n",
       " '2012-11-21 07:19:19',\n",
       " '2012-02-22 23:37:33',\n",
       " '2013-01-10 12:43:52',\n",
       " '2011-05-08 20:43:07',\n",
       " '2012-06-19 12:26:17',\n",
       " '2012-08-20 10:44:45',\n",
       " '2011-06-08 03:33:07',\n",
       " '2012-01-27 00:12:44',\n",
       " '2013-12-05 19:21:11',\n",
       " '2011-08-21 08:18:15',\n",
       " '2012-04-07 08:56:49',\n",
       " '2011-08-10 18:51:31',\n",
       " '2012-09-06 15:40:23',\n",
       " '2012-10-09 08:19:14',\n",
       " '2011-08-26 09:39:51',\n",
       " '2012-06-21 19:02:37',\n",
       " '2011-10-21 00:18:46',\n",
       " '2012-06-11 15:25:56',\n",
       " '2012-05-15 19:29:09',\n",
       " '2012-08-11 08:07:12',\n",
       " '2011-11-01 16:17:29',\n",
       " '2012-05-04 12:57:39',\n",
       " '2013-07-09 17:24:04',\n",
       " '2012-06-28 10:04:43',\n",
       " '2011-03-31 21:58:28',\n",
       " '2012-04-18 14:36:43',\n",
       " '2012-05-24 22:18:22',\n",
       " '2012-09-20 08:12:16',\n",
       " '2013-01-13 05:58:11',\n",
       " '2012-07-12 19:53:05',\n",
       " '2012-11-09 09:46:42',\n",
       " '2011-05-19 22:10:05',\n",
       " '2011-11-02 15:40:50',\n",
       " '2012-02-24 09:04:43',\n",
       " '2011-09-27 04:24:40',\n",
       " '2011-10-06 20:26:41',\n",
       " '2012-01-05 23:30:50',\n",
       " '2011-08-16 13:36:59',\n",
       " '2011-04-08 11:28:14',\n",
       " '2010-10-21 16:27:24',\n",
       " '2011-12-13 22:08:35',\n",
       " '2012-02-23 13:56:39',\n",
       " '2013-01-20 20:22:28',\n",
       " '2012-11-27 00:22:14',\n",
       " '2014-01-06 00:11:43',\n",
       " '2011-04-01 10:44:32',\n",
       " '2012-05-28 17:20:12',\n",
       " '2013-01-17 19:38:15',\n",
       " '2010-07-26 16:57:59',\n",
       " '2011-10-05 04:47:19',\n",
       " '2011-03-11 08:06:25',\n",
       " '2012-06-22 08:29:46',\n",
       " '2011-02-10 17:53:56',\n",
       " '2011-02-20 02:37:07',\n",
       " '2012-12-14 21:30:47',\n",
       " '2011-10-07 02:13:18',\n",
       " '2011-03-09 16:32:43',\n",
       " '2012-09-21 15:22:25',\n",
       " '2012-05-24 04:09:02',\n",
       " '2013-01-12 21:18:01',\n",
       " '2012-07-06 21:01:13',\n",
       " '2012-03-07 05:58:41',\n",
       " '2011-12-03 21:37:29',\n",
       " '2013-07-20 23:49:38',\n",
       " '2012-08-27 18:37:15',\n",
       " '2012-12-04 04:02:17',\n",
       " '2012-04-19 11:41:51',\n",
       " '2013-07-02 08:29:44',\n",
       " '2011-04-16 08:02:51',\n",
       " '2012-06-05 13:58:15',\n",
       " '2011-10-27 19:25:35',\n",
       " '2011-10-13 02:04:57',\n",
       " '2014-01-10 22:54:17',\n",
       " '2011-05-18 04:52:58',\n",
       " '2012-06-13 07:02:44',\n",
       " '2012-11-11 16:01:44',\n",
       " '2012-09-24 11:07:26',\n",
       " '2012-12-09 11:50:36',\n",
       " '2011-07-09 09:49:27',\n",
       " '2011-10-21 16:48:09',\n",
       " '2012-06-27 07:10:41',\n",
       " '2011-08-31 09:38:46',\n",
       " '2012-10-02 05:08:56',\n",
       " '2012-08-31 14:00:04',\n",
       " '2013-01-21 23:40:51',\n",
       " '2010-09-19 17:28:17',\n",
       " '2011-09-26 16:36:08',\n",
       " '2011-07-24 06:46:53',\n",
       " '2011-10-12 10:23:27',\n",
       " '2012-11-27 15:30:34',\n",
       " '2012-06-08 12:05:27',\n",
       " '2011-04-29 00:20:08',\n",
       " '2012-06-07 09:35:12',\n",
       " '2011-04-18 20:45:20',\n",
       " '2012-02-24 22:38:09',\n",
       " '2013-01-06 22:47:09',\n",
       " '2011-06-22 17:59:07',\n",
       " '2012-12-06 21:15:15',\n",
       " '2012-04-24 08:49:02',\n",
       " '2012-04-24 13:23:29',\n",
       " '2010-12-24 13:49:58',\n",
       " '2011-08-07 22:05:56',\n",
       " '2012-10-03 21:25:18',\n",
       " '2012-07-05 12:03:15',\n",
       " '2010-12-02 12:44:04',\n",
       " '2013-01-17 16:04:53',\n",
       " '2012-09-19 12:39:00',\n",
       " '2011-09-15 02:17:32',\n",
       " '2010-11-13 21:10:22',\n",
       " '2011-03-19 15:32:26',\n",
       " '2012-09-27 00:07:07',\n",
       " '2012-11-27 23:28:17',\n",
       " '2012-04-05 17:23:27',\n",
       " '2012-01-11 09:30:03',\n",
       " '2011-08-22 07:40:24',\n",
       " '2011-08-06 12:59:53',\n",
       " '2012-05-24 18:41:41',\n",
       " '2010-12-17 07:48:12',\n",
       " '2012-06-09 08:33:03',\n",
       " '2012-07-03 20:46:10',\n",
       " '2014-02-11 12:18:34',\n",
       " '2012-05-15 16:04:48',\n",
       " '2013-01-17 14:23:12',\n",
       " '2011-12-10 18:17:05',\n",
       " '2013-01-11 11:52:12',\n",
       " '2012-09-19 03:22:48',\n",
       " '2011-07-22 09:40:56',\n",
       " '2012-01-13 00:32:52',\n",
       " '2011-07-21 17:48:04',\n",
       " '2011-08-28 20:09:18',\n",
       " '2012-12-04 02:30:00',\n",
       " '2012-07-03 11:22:17',\n",
       " '2012-05-24 14:40:50',\n",
       " '2013-03-08 18:58:33',\n",
       " '2011-07-27 10:50:24',\n",
       " '2012-03-24 09:11:34',\n",
       " '2011-07-06 05:42:20',\n",
       " '2012-03-20 20:49:30',\n",
       " '2012-07-16 11:28:13',\n",
       " '2012-08-17 15:04:23',\n",
       " '2012-03-06 03:59:19',\n",
       " '2011-08-02 10:31:38',\n",
       " '2012-08-06 15:35:23',\n",
       " '2010-08-07 17:52:12',\n",
       " '2012-05-03 14:44:00',\n",
       " '2013-04-11 08:04:47',\n",
       " '2014-06-15 03:58:03',\n",
       " '2012-05-11 03:00:45',\n",
       " '2012-08-08 01:26:26',\n",
       " '2011-05-21 05:25:53',\n",
       " '2012-03-13 21:59:45',\n",
       " '2012-05-08 09:55:39',\n",
       " '2012-08-23 15:44:21',\n",
       " '2013-01-20 12:18:09',\n",
       " '2012-08-21 17:49:21',\n",
       " '2012-11-16 21:06:21',\n",
       " '2013-01-08 19:44:03',\n",
       " '2011-07-18 12:25:23',\n",
       " '2012-06-29 00:17:57',\n",
       " '2012-10-05 06:47:46',\n",
       " '2012-09-19 15:25:35',\n",
       " '2013-07-20 23:11:16',\n",
       " '2012-10-17 21:34:14',\n",
       " '2012-10-17 23:35:44',\n",
       " '2011-08-02 18:18:46',\n",
       " '2012-07-27 13:35:12',\n",
       " '2010-11-25 10:31:03',\n",
       " '2013-03-06 12:36:00',\n",
       " '2011-10-26 06:54:38',\n",
       " '2012-07-28 14:53:38',\n",
       " '2013-09-04 14:00:05',\n",
       " '2011-04-18 07:01:02',\n",
       " '2012-05-31 08:31:27',\n",
       " '2011-10-09 14:01:17',\n",
       " '2012-12-25 17:18:41',\n",
       " '2012-11-24 02:44:31',\n",
       " '2011-10-17 10:57:44',\n",
       " '2012-01-22 22:21:20',\n",
       " '2012-10-25 15:25:52',\n",
       " '2011-09-16 18:46:24',\n",
       " '2013-01-28 21:07:45',\n",
       " '2011-11-28 03:33:58',\n",
       " '2011-10-29 22:27:08',\n",
       " '2012-10-09 15:50:12',\n",
       " '2012-03-22 16:27:57',\n",
       " '2011-08-29 10:28:15',\n",
       " '2011-11-14 15:55:00',\n",
       " '2011-06-29 09:59:08',\n",
       " '2012-12-27 16:30:58',\n",
       " '2012-10-30 19:13:17',\n",
       " '2012-11-20 17:38:10',\n",
       " '2010-09-08 02:54:16',\n",
       " '2011-02-02 19:11:03',\n",
       " '2012-11-02 13:36:35',\n",
       " '2012-12-08 22:08:19',\n",
       " '2011-01-13 22:10:04',\n",
       " '2014-06-02 12:46:19',\n",
       " '2012-11-26 22:31:28',\n",
       " '2013-07-21 11:38:10',\n",
       " '2011-11-10 16:38:14',\n",
       " '2012-04-08 16:16:01',\n",
       " '2013-01-07 08:45:35',\n",
       " '2012-07-30 19:44:47',\n",
       " '2011-11-18 20:13:03',\n",
       " '2012-05-11 13:21:06',\n",
       " '2013-08-06 00:01:14',\n",
       " '2010-09-27 22:05:45',\n",
       " '2012-03-26 14:22:43',\n",
       " '2012-11-11 21:52:48',\n",
       " '2012-10-08 11:56:32',\n",
       " '2011-09-01 18:41:16',\n",
       " '2010-08-03 00:57:51',\n",
       " '2011-03-18 09:15:16',\n",
       " '2014-05-11 07:46:23',\n",
       " '2012-02-05 16:15:32',\n",
       " '2012-02-28 21:27:48',\n",
       " '2011-08-10 01:21:26',\n",
       " '2012-06-21 06:15:41',\n",
       " '2011-05-04 19:31:30',\n",
       " '2011-06-20 16:43:49',\n",
       " '2011-12-08 07:39:24',\n",
       " '2011-02-12 11:26:39',\n",
       " '2012-08-14 06:32:24',\n",
       " '2011-03-07 06:01:22',\n",
       " '2012-08-15 22:28:52',\n",
       " '2011-12-27 07:06:38',\n",
       " '2012-08-15 21:10:42',\n",
       " '2011-11-11 22:46:45',\n",
       " '2011-08-17 16:35:11',\n",
       " '2012-01-15 16:31:58',\n",
       " '2011-11-13 16:15:47',\n",
       " '2011-03-27 16:02:58',\n",
       " '2011-04-09 12:57:05',\n",
       " '2012-02-08 22:57:24',\n",
       " '2010-11-09 11:51:48',\n",
       " '2012-08-07 21:07:27',\n",
       " '2012-03-28 23:58:09',\n",
       " '2010-12-10 20:26:42',\n",
       " '2012-06-29 12:48:54',\n",
       " '2010-09-09 19:55:14',\n",
       " '2012-04-29 21:03:54',\n",
       " '2011-07-28 09:19:02',\n",
       " '2013-06-19 02:11:08',\n",
       " '2013-01-17 09:24:03',\n",
       " '2011-02-01 00:14:02',\n",
       " '2012-10-02 19:52:19',\n",
       " '2013-01-11 00:36:51',\n",
       " '2013-08-31 20:14:41',\n",
       " '2010-12-13 03:21:06',\n",
       " '2012-01-17 16:34:52',\n",
       " '2012-10-10 12:55:56',\n",
       " '2013-07-01 13:16:27',\n",
       " '2011-03-19 12:26:25',\n",
       " '2012-07-02 14:26:51',\n",
       " '2013-06-27 07:10:58',\n",
       " '2010-10-28 15:44:12',\n",
       " '2011-11-30 13:44:06',\n",
       " '2012-05-29 14:21:14',\n",
       " '2012-06-30 11:09:41',\n",
       " '2012-11-26 16:18:47',\n",
       " '2012-08-15 17:09:27',\n",
       " '2013-07-16 22:21:25',\n",
       " '2011-11-27 09:53:04',\n",
       " '2011-09-16 18:10:08',\n",
       " '2011-08-01 19:25:14',\n",
       " '2013-01-11 12:54:28',\n",
       " '2012-04-22 21:21:02',\n",
       " '2013-09-04 15:59:09',\n",
       " '2012-08-18 17:51:00',\n",
       " '2013-01-15 12:53:06',\n",
       " '2014-03-19 15:51:42',\n",
       " '2012-02-22 22:09:45',\n",
       " '2011-03-24 21:05:57',\n",
       " '2011-02-03 05:54:20',\n",
       " '2010-12-21 21:18:12',\n",
       " '2012-02-06 10:06:14',\n",
       " '2011-03-13 19:22:24',\n",
       " '2012-08-20 19:21:17',\n",
       " '2012-07-22 08:28:55',\n",
       " '2011-08-08 18:35:35',\n",
       " '2012-02-08 09:51:12',\n",
       " '2012-11-27 16:19:20',\n",
       " '2010-11-30 16:45:51',\n",
       " '2012-09-03 11:33:45',\n",
       " '2012-12-05 17:09:50',\n",
       " '2012-09-30 12:15:13',\n",
       " '2010-10-12 21:23:24',\n",
       " '2011-01-14 15:24:06',\n",
       " '2012-12-03 13:09:56',\n",
       " '2011-10-04 08:02:23',\n",
       " '2012-07-23 16:05:53',\n",
       " '2012-06-18 19:23:06',\n",
       " '2012-04-26 07:52:03',\n",
       " '2012-07-25 23:15:40',\n",
       " '2011-04-12 13:32:31',\n",
       " '2012-03-22 22:44:42',\n",
       " '2012-11-05 13:48:54',\n",
       " '2010-09-09 21:37:22',\n",
       " '2012-03-26 17:33:00',\n",
       " '2012-10-16 20:13:54',\n",
       " '2011-04-27 10:36:23',\n",
       " '2012-07-26 11:48:55',\n",
       " '2012-11-19 19:36:59',\n",
       " '2011-08-24 20:20:00',\n",
       " '2012-06-26 01:02:01',\n",
       " '2010-11-11 23:48:57',\n",
       " '2010-12-06 05:02:30',\n",
       " '2012-10-17 07:01:12',\n",
       " '2011-10-04 07:06:49',\n",
       " '2011-09-05 12:32:36',\n",
       " '2012-08-09 16:02:17',\n",
       " '2010-09-13 20:26:11',\n",
       " '2012-07-08 08:54:17',\n",
       " '2011-05-04 03:51:50',\n",
       " '2012-12-07 17:43:11',\n",
       " '2010-11-23 21:09:56',\n",
       " '2011-04-19 15:42:39',\n",
       " '2012-02-12 20:33:55',\n",
       " '2012-08-13 13:43:33',\n",
       " '2012-10-07 08:23:26',\n",
       " '2012-11-02 11:12:11',\n",
       " '2010-10-17 20:12:38',\n",
       " '2012-01-14 16:41:44',\n",
       " '2011-09-12 08:38:59',\n",
       " '2011-12-10 22:30:20',\n",
       " '2012-05-16 09:05:26',\n",
       " '2011-06-04 03:41:33',\n",
       " '2012-05-26 17:28:00',\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'http://graph.facebook.com/100000071660800/picture?type=large',\n",
       " 'http://graph.facebook.com/100001122416350/picture?type=large',\n",
       " 'http://i.stack.imgur.com/0IgyC.png',\n",
       " 'http://i.stack.imgur.com/1b4QI.jpg',\n",
       " 'http://i.stack.imgur.com/1o7ij.jpg',\n",
       " 'http://i.stack.imgur.com/1sz79.png',\n",
       " 'http://i.stack.imgur.com/1uoPg.jpg',\n",
       " 'http://i.stack.imgur.com/22CD2.png',\n",
       " 'http://i.stack.imgur.com/2Je4F.jpg',\n",
       " 'http://i.stack.imgur.com/2NJpO.jpg',\n",
       " 'http://i.stack.imgur.com/36YTL.png',\n",
       " 'http://i.stack.imgur.com/3VnmB.jpg',\n",
       " 'http://i.stack.imgur.com/3lyag.png',\n",
       " 'http://i.stack.imgur.com/4LwOd.jpg',\n",
       " 'http://i.stack.imgur.com/4bZ01.jpg',\n",
       " 'http://i.stack.imgur.com/5Sqgt.png',\n",
       " 'http://i.stack.imgur.com/5Vkq8.jpg',\n",
       " 'http://i.stack.imgur.com/5ZzWR.png',\n",
       " 'http://i.stack.imgur.com/5aMmD.jpg',\n",
       " 'http://i.stack.imgur.com/5oWjh.jpg',\n",
       " 'http://i.stack.imgur.com/6BqAu.jpg',\n",
       " 'http://i.stack.imgur.com/6t0im.jpg',\n",
       " 'http://i.stack.imgur.com/73lQ8.jpg',\n",
       " 'http://i.stack.imgur.com/7A2m9.jpg',\n",
       " 'http://i.stack.imgur.com/7fpvh.jpg',\n",
       " 'http://i.stack.imgur.com/7hR5X.jpg',\n",
       " 'http://i.stack.imgur.com/7syBD.jpg',\n",
       " 'http://i.stack.imgur.com/9643V.jpg',\n",
       " 'http://i.stack.imgur.com/9H6nw.jpg',\n",
       " 'http://i.stack.imgur.com/B8ud0.png',\n",
       " 'http://i.stack.imgur.com/BF0y7.jpg',\n",
       " 'http://i.stack.imgur.com/Ba4iE.jpg',\n",
       " 'http://i.stack.imgur.com/BbqCG.jpg',\n",
       " 'http://i.stack.imgur.com/CHox8.png',\n",
       " 'http://i.stack.imgur.com/Cc7mM.jpg',\n",
       " 'http://i.stack.imgur.com/Cug0U.jpg',\n",
       " 'http://i.stack.imgur.com/DHumG.jpg',\n",
       " 'http://i.stack.imgur.com/DrRTV.jpg',\n",
       " 'http://i.stack.imgur.com/E5Eu7.jpg',\n",
       " 'http://i.stack.imgur.com/EuJtr.jpg',\n",
       " 'http://i.stack.imgur.com/F5xn2.png',\n",
       " 'http://i.stack.imgur.com/FSetO.png',\n",
       " 'http://i.stack.imgur.com/FluI8.jpg',\n",
       " 'http://i.stack.imgur.com/Frd3B.png',\n",
       " 'http://i.stack.imgur.com/GTHqI.jpg',\n",
       " 'http://i.stack.imgur.com/GU1xi.png',\n",
       " 'http://i.stack.imgur.com/Gmj2x.jpg',\n",
       " 'http://i.stack.imgur.com/Gzg7k.jpg',\n",
       " 'http://i.stack.imgur.com/HoUuO.png',\n",
       " 'http://i.stack.imgur.com/ILcTf.png',\n",
       " 'http://i.stack.imgur.com/IPbxC.jpg',\n",
       " 'http://i.stack.imgur.com/IjqM5.jpg',\n",
       " 'http://i.stack.imgur.com/Ik92X.jpg',\n",
       " 'http://i.stack.imgur.com/J2NM3.jpg',\n",
       " 'http://i.stack.imgur.com/J8e8y.jpg',\n",
       " 'http://i.stack.imgur.com/JsQBm.jpg',\n",
       " 'http://i.stack.imgur.com/KD6Fx.jpg',\n",
       " 'http://i.stack.imgur.com/KMYkT.jpg',\n",
       " 'http://i.stack.imgur.com/KNKIE.jpg',\n",
       " 'http://i.stack.imgur.com/KQ3YE.jpg?s=128&g=1',\n",
       " 'http://i.stack.imgur.com/Kua2Z.jpg',\n",
       " 'http://i.stack.imgur.com/LJ6vb.jpg',\n",
       " 'http://i.stack.imgur.com/LLgac.jpg',\n",
       " 'http://i.stack.imgur.com/LbRay.jpg',\n",
       " 'http://i.stack.imgur.com/M8Hr5.jpg',\n",
       " 'http://i.stack.imgur.com/MQ33X.jpg',\n",
       " 'http://i.stack.imgur.com/MTcDJ.jpg',\n",
       " 'http://i.stack.imgur.com/MjeHD.jpg',\n",
       " 'http://i.stack.imgur.com/MwRvs.jpg',\n",
       " 'http://i.stack.imgur.com/NeuGc.jpg',\n",
       " 'http://i.stack.imgur.com/O14Fi.jpg',\n",
       " 'http://i.stack.imgur.com/OqZrR.jpg',\n",
       " 'http://i.stack.imgur.com/PVlGM.jpg',\n",
       " 'http://i.stack.imgur.com/PXFXm.jpg',\n",
       " 'http://i.stack.imgur.com/Pmbuu.jpg',\n",
       " 'http://i.stack.imgur.com/Qnyua.jpg',\n",
       " 'http://i.stack.imgur.com/SxbWP.jpg',\n",
       " 'http://i.stack.imgur.com/TCyei.jpg',\n",
       " 'http://i.stack.imgur.com/TX6TC.jpg',\n",
       " 'http://i.stack.imgur.com/TvkT7.jpg',\n",
       " 'http://i.stack.imgur.com/U3gYU.jpg',\n",
       " 'http://i.stack.imgur.com/UWKcp.gif',\n",
       " 'http://i.stack.imgur.com/WtOby.jpg',\n",
       " 'http://i.stack.imgur.com/XFPSV.jpg',\n",
       " 'http://i.stack.imgur.com/Xwosz.jpg',\n",
       " 'http://i.stack.imgur.com/YJ0Ye.jpg?s=128&g=1',\n",
       " 'http://i.stack.imgur.com/YWmON.jpg',\n",
       " 'http://i.stack.imgur.com/YcSQq.jpg',\n",
       " 'http://i.stack.imgur.com/YwUTs.jpg',\n",
       " 'http://i.stack.imgur.com/ZU979.jpg',\n",
       " 'http://i.stack.imgur.com/Zhyym.jpg',\n",
       " 'http://i.stack.imgur.com/a0ycx.jpg',\n",
       " 'http://i.stack.imgur.com/a7yWR.jpg',\n",
       " 'http://i.stack.imgur.com/ao2Pm.jpg',\n",
       " 'http://i.stack.imgur.com/aqZSq.jpg',\n",
       " 'http://i.stack.imgur.com/axcPw.jpg',\n",
       " 'http://i.stack.imgur.com/bEHdQ.jpg',\n",
       " 'http://i.stack.imgur.com/bb8sB.jpg',\n",
       " 'http://i.stack.imgur.com/bbItm.png',\n",
       " 'http://i.stack.imgur.com/bmLJq.png',\n",
       " 'http://i.stack.imgur.com/bnim1.png',\n",
       " 'http://i.stack.imgur.com/c9A0L.png',\n",
       " 'http://i.stack.imgur.com/cCqdx.jpg',\n",
       " 'http://i.stack.imgur.com/cEINK.jpg',\n",
       " 'http://i.stack.imgur.com/dHsAN.jpg',\n",
       " 'http://i.stack.imgur.com/dcglq.jpg',\n",
       " 'http://i.stack.imgur.com/dqVjM.png',\n",
       " 'http://i.stack.imgur.com/e6fdD.jpg',\n",
       " 'http://i.stack.imgur.com/e6gnk.jpg',\n",
       " 'http://i.stack.imgur.com/e863w.jpg',\n",
       " 'http://i.stack.imgur.com/ePGCS.png',\n",
       " 'http://i.stack.imgur.com/f8n3X.jpg',\n",
       " 'http://i.stack.imgur.com/fbPUn.jpg',\n",
       " 'http://i.stack.imgur.com/g3YEU.jpg',\n",
       " 'http://i.stack.imgur.com/gIh82.jpg',\n",
       " 'http://i.stack.imgur.com/h60Xn.png',\n",
       " 'http://i.stack.imgur.com/hochp.png',\n",
       " 'http://i.stack.imgur.com/i8rnZ.jpg',\n",
       " 'http://i.stack.imgur.com/j7Ig7.jpg',\n",
       " 'http://i.stack.imgur.com/kc7hg.jpg',\n",
       " 'http://i.stack.imgur.com/km1pr.jpg',\n",
       " 'http://i.stack.imgur.com/knNVv.jpg',\n",
       " 'http://i.stack.imgur.com/lOS36.jpg',\n",
       " 'http://i.stack.imgur.com/mI2wg.png',\n",
       " 'http://i.stack.imgur.com/mIqXn.jpg',\n",
       " 'http://i.stack.imgur.com/mYIzP.png',\n",
       " 'http://i.stack.imgur.com/maMvF.jpg',\n",
       " 'http://i.stack.imgur.com/n6s2u.jpg',\n",
       " 'http://i.stack.imgur.com/nU36s.jpg?s=128&g=1',\n",
       " 'http://i.stack.imgur.com/nhtMQ.jpg',\n",
       " 'http://i.stack.imgur.com/oKsN6.jpg',\n",
       " 'http://i.stack.imgur.com/pSAVB.jpg',\n",
       " 'http://i.stack.imgur.com/pXbIh.gif',\n",
       " 'http://i.stack.imgur.com/qObut.jpg',\n",
       " 'http://i.stack.imgur.com/qfiK5.jpg?s=128&g=1',\n",
       " 'http://i.stack.imgur.com/qprPh.jpg',\n",
       " 'http://i.stack.imgur.com/r6e29.jpg',\n",
       " 'http://i.stack.imgur.com/rK2rh.png',\n",
       " 'http://i.stack.imgur.com/sQFX6.jpg',\n",
       " 'http://i.stack.imgur.com/sl6Bv.jpg',\n",
       " 'http://i.stack.imgur.com/stqX1.png',\n",
       " 'http://i.stack.imgur.com/szio3.jpg',\n",
       " 'http://i.stack.imgur.com/thOfN.gif',\n",
       " 'http://i.stack.imgur.com/tmmEr.jpg',\n",
       " 'http://i.stack.imgur.com/tyaCy.png',\n",
       " 'http://i.stack.imgur.com/u1sCz.jpg?s=128&g=1',\n",
       " 'http://i.stack.imgur.com/uKAHo.png',\n",
       " 'http://i.stack.imgur.com/uVGyl.jpg',\n",
       " 'http://i.stack.imgur.com/uimvC.gif?s=128&g=1',\n",
       " 'http://i.stack.imgur.com/uqNUS.jpg',\n",
       " 'http://i.stack.imgur.com/vyaaJ.jpg',\n",
       " 'http://i.stack.imgur.com/w5qzT.jpg',\n",
       " 'http://i.stack.imgur.com/w6rNs.jpg',\n",
       " 'http://i.stack.imgur.com/w8Goi.gif',\n",
       " 'http://i.stack.imgur.com/wMoWk.jpg',\n",
       " 'http://i.stack.imgur.com/wSfuj.png',\n",
       " 'http://i.stack.imgur.com/wmOYS.jpg?s=128&g=1',\n",
       " 'http://i.stack.imgur.com/wyuMa.jpg',\n",
       " 'http://i.stack.imgur.com/x7eKK.jpg',\n",
       " 'http://i.stack.imgur.com/xyy2Z.jpg',\n",
       " 'http://i.stack.imgur.com/zKvkR.jpg',\n",
       " 'http://i.stack.imgur.com/zSduQ.jpg',\n",
       " 'http://i.stack.imgur.com/zlV7q.jpg',\n",
       " 'http://www.gravatar.com/avatar/61c85d77bcaf5212b3a0aa2419c95f33?s=128&amp;d=identicon&amp;r=PG',\n",
       " 'http://www.gravatar.com/avatar/c7ba844e5aba2c764fe9499872deb54e?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/0f5337d47ece52ffa2a3ba366c2ca3a3?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/10950a808019775853972ea50bed1863?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/10d5751183c02f155eaedf7094174012?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/10db3ca2e511fe7b6851e3eb30aed981?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/178b62933f85027c9da370dba8ed8da3?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/2f822f8f82def56dc3237cc4ff794e9e?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/2fffdd4f8c494c29e613eefd78a6a084?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/33ff7f953a3fb4d98be320c2b71d86f8?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/37904cd3562d2f74241060ed46f4d97f?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/421afbe9424df7e10374caabccabbfd9?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/43310a8c36df3dbbca2c4d5ce158b5d0?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/43acd18dea0ebfe165c8bd6769a4bee9?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/513c598c9fd6df573ce5f4087a835560?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/56881de591adf646543e735ff28e77ea?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/569a31ff5fdd492dd39dd4bbfd9f86a7?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/5b60cd6e084f4a859eb2742a6a9ed3f6?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/5b95ed2b367e3b324597ba9271482338?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/5db52f950caf13b2702499daab09ddfe?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/62568666143087bfe90722ebeeff5609?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/669122b89437f78175d8eeaf4408c4e3?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/68b0e7c30787202b277343a9142ae0a3?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/6afa9f5c5c9ec18133611b198f106163?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/6fa70b96525f544b22fc6b051a8eb756?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/703d43e64e5d52bcaedba6af73a124e5?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/770348abdad4967b4246eaad81004f0f?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/78ac1c7e610953d897f25a1e00c7edbf?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/7d6524e509c8607ab65d9d8173071fe5?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/8d20d962d98c3b0d5be106eab8cc7ebc?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/8f4f015bc6c748fd2a4e7d700994c51e?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/91617c31b087fa08ec3a7948415f1044?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/9249785ded848233e985143ee28b08e5?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/9610fe59ef596064ac40a339343c7df6?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/98aa0766c4899d803cf24aab21221e9c?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/98fe597bf03d51503e48d2c312b4317f?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/9e51a2308408220f51b1327a4c3e0976?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/a3c50ef9c93c9ab17756f5ebea674989?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/abc177e73de48e3d81ef7afa1835bf57?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/ad6c889240f3a0301bcb17d7e63276c4?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/ad761f1a4145a1cb77f673f6efb1c453?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/b35eabc06494ff4c5b97db357e750ef0?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/b3c911f784d8fd40c687b9463c9ac3ba?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/b4a91a4f8e633988e44b64d9a4c8c5b8?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/be15d02ec5a08e675596561818f64272?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/c27fc35a92b68a498c77f020998b103b?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/c9cadf4928763c2284ec9c42a57d06a5?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/d1ff13442a2c2ae5617a92a093cdb6a9?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/e597fee4e2b9d63ef4a27829d1a183eb?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/ec1e94839a9f9b0308a5b5c243d75dc1?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/eeec80237213c91e5bab56b77ac5b8ca?s=128&d=identicon&r=PG',\n",
       " nan}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " '2010-08-22 09:47:37',\n",
       " '2010-08-22 10:02:58',\n",
       " '2010-11-08 16:06:40',\n",
       " '2010-11-03 04:01:10',\n",
       " '2010-08-04 14:02:21',\n",
       " '2011-01-28 11:07:15',\n",
       " '2012-05-15 01:58:24',\n",
       " '2010-09-09 02:35:31',\n",
       " '2012-02-09 15:13:30',\n",
       " '2012-08-21 15:17:25',\n",
       " '2011-05-20 18:21:06',\n",
       " '2012-03-28 10:08:10',\n",
       " '2011-01-16 13:48:53',\n",
       " '2013-01-13 20:45:07',\n",
       " '2012-07-26 12:21:14',\n",
       " '2010-10-04 15:04:32',\n",
       " '2010-08-10 22:39:06',\n",
       " '2011-11-09 10:39:13',\n",
       " '2010-08-28 07:11:23',\n",
       " '2010-08-06 03:14:53',\n",
       " '2010-08-14 01:41:18',\n",
       " '2010-07-27 09:25:00',\n",
       " '2012-01-25 10:55:31',\n",
       " '2010-08-08 04:15:18',\n",
       " '2010-07-27 06:51:24',\n",
       " '2012-10-29 08:07:38',\n",
       " '2011-08-15 13:52:43',\n",
       " '2012-09-26 10:10:10',\n",
       " '2012-04-26 18:59:18',\n",
       " '2010-12-13 18:39:58',\n",
       " '2011-04-11 18:45:10',\n",
       " '2011-01-20 21:55:22',\n",
       " '2011-01-27 13:29:56',\n",
       " '2012-09-07 16:57:11',\n",
       " '2011-04-08 21:33:23',\n",
       " '2012-01-01 14:27:22',\n",
       " '2010-08-14 00:58:21',\n",
       " '2011-11-04 10:38:00',\n",
       " '2010-08-31 17:41:55',\n",
       " '2012-06-18 17:49:08',\n",
       " '2010-07-20 18:16:17',\n",
       " '2010-07-21 04:19:00',\n",
       " '2011-12-06 09:09:33',\n",
       " '2011-12-30 02:21:15',\n",
       " '2011-01-27 06:46:32',\n",
       " '2011-03-12 23:55:47',\n",
       " '2010-08-06 02:30:56',\n",
       " '2010-09-01 18:23:56',\n",
       " '2012-03-28 01:34:16',\n",
       " '2010-07-19 21:19:43',\n",
       " '2012-08-27 07:28:03',\n",
       " '2012-07-24 11:40:10',\n",
       " '2012-04-03 15:32:35',\n",
       " '2010-11-20 19:19:08',\n",
       " '2010-08-18 10:56:44',\n",
       " '2010-09-18 21:46:28',\n",
       " '2011-02-14 07:39:27',\n",
       " '2012-05-30 02:39:29',\n",
       " '2012-07-17 12:44:21',\n",
       " '2010-12-15 17:28:47',\n",
       " '2012-07-28 01:40:34',\n",
       " '2010-10-01 19:23:05',\n",
       " '2012-03-04 23:57:02',\n",
       " '2010-08-14 01:02:31',\n",
       " '2011-05-26 08:46:13',\n",
       " '2011-07-01 07:05:58',\n",
       " '2010-08-21 04:42:39',\n",
       " '2010-07-30 02:42:38',\n",
       " '2012-01-25 18:55:21',\n",
       " '2011-09-20 18:06:16',\n",
       " '2012-01-23 13:02:54',\n",
       " '2010-07-29 22:35:28',\n",
       " '2011-04-10 14:26:46',\n",
       " '2011-05-07 18:40:18',\n",
       " '2011-12-13 16:07:49',\n",
       " '2012-03-25 18:37:57',\n",
       " '2010-10-01 17:21:51',\n",
       " '2012-05-29 14:53:20',\n",
       " '2012-10-02 16:37:50',\n",
       " '2010-07-22 11:33:47',\n",
       " '2010-12-07 07:46:56',\n",
       " '2012-05-05 08:28:16',\n",
       " '2012-10-04 19:39:57',\n",
       " '2013-06-23 09:57:40',\n",
       " '2011-02-14 21:19:12',\n",
       " '2010-07-21 13:50:08',\n",
       " '2010-08-13 22:35:50',\n",
       " '2010-07-31 05:49:03',\n",
       " '2010-11-08 15:05:19',\n",
       " '2010-09-12 08:12:21',\n",
       " '2012-04-11 11:52:24',\n",
       " '2012-01-05 19:05:58',\n",
       " '2010-07-27 07:38:19',\n",
       " '2010-08-19 07:05:16',\n",
       " '2010-08-19 04:47:46',\n",
       " '2010-08-14 09:41:44',\n",
       " '2012-12-10 22:29:47',\n",
       " '2011-02-25 13:25:01',\n",
       " '2010-07-20 11:59:56',\n",
       " '2010-09-29 15:57:22',\n",
       " '2010-12-04 11:27:56',\n",
       " '2011-01-25 14:43:09',\n",
       " '2010-07-21 15:23:53',\n",
       " '2010-07-30 12:11:18',\n",
       " '2010-08-06 16:10:48',\n",
       " '2011-04-05 13:34:02',\n",
       " '2010-09-03 18:51:15',\n",
       " '2011-06-22 02:03:50',\n",
       " '2010-07-20 09:44:11',\n",
       " '2012-03-23 03:15:35',\n",
       " '2011-07-05 21:24:45',\n",
       " '2010-12-01 23:57:48',\n",
       " '2012-02-01 01:10:21',\n",
       " '2012-04-19 17:44:13',\n",
       " '2012-04-19 16:38:42',\n",
       " '2012-08-28 18:22:39',\n",
       " '2011-07-05 20:15:25',\n",
       " '2012-03-07 16:16:36',\n",
       " '2010-10-09 16:02:58',\n",
       " '2012-02-14 19:46:30',\n",
       " '2012-12-25 00:08:00',\n",
       " '2011-08-12 20:30:10',\n",
       " '2010-11-04 10:02:45',\n",
       " '2010-08-06 13:52:50',\n",
       " '2011-08-17 20:15:31',\n",
       " '2012-05-02 12:24:49',\n",
       " '2010-08-09 13:16:41',\n",
       " '2010-08-02 20:35:41',\n",
       " '2012-10-06 09:36:10',\n",
       " '2010-07-21 17:17:45',\n",
       " '2012-09-06 14:43:03',\n",
       " '2012-02-29 08:43:18',\n",
       " '2011-07-05 08:57:48',\n",
       " '2010-10-13 15:07:55',\n",
       " '2011-11-07 13:54:30',\n",
       " '2011-07-07 09:58:49',\n",
       " '2013-01-12 21:14:16',\n",
       " '2011-02-05 18:31:42',\n",
       " '2012-01-20 01:30:01',\n",
       " '2010-09-18 02:43:41',\n",
       " '2011-03-01 07:17:43',\n",
       " '2010-12-14 08:48:45',\n",
       " '2012-01-04 08:04:10',\n",
       " '2011-05-16 22:32:27',\n",
       " '2010-08-29 08:58:43',\n",
       " '2010-11-17 14:17:55',\n",
       " '2010-08-06 14:12:21',\n",
       " '2010-07-19 23:27:36',\n",
       " '2010-09-01 11:31:31',\n",
       " '2010-11-21 09:44:03',\n",
       " '2010-09-02 18:56:34',\n",
       " '2012-09-02 17:11:30',\n",
       " '2012-09-03 17:14:41',\n",
       " '2014-01-11 23:27:07',\n",
       " '2010-07-29 22:38:21',\n",
       " '2011-01-27 06:06:18',\n",
       " '2012-01-27 06:35:49',\n",
       " '2012-11-21 04:04:55',\n",
       " '2010-09-28 01:25:32',\n",
       " '2010-07-28 14:06:35',\n",
       " '2010-07-21 01:35:13',\n",
       " '2012-12-08 20:35:03',\n",
       " '2011-10-12 21:36:03',\n",
       " '2010-08-13 23:58:11',\n",
       " '2012-02-14 19:29:09',\n",
       " '2011-05-03 02:40:27',\n",
       " '2011-06-30 05:50:35',\n",
       " '2011-04-06 08:35:46',\n",
       " '2012-01-12 18:45:40',\n",
       " '2011-10-30 03:14:13',\n",
       " '2012-06-09 06:06:23',\n",
       " '2011-01-21 08:38:17',\n",
       " '2010-08-25 08:02:51',\n",
       " '2012-05-09 21:50:34',\n",
       " '2010-08-09 10:29:38',\n",
       " '2010-08-23 14:58:22',\n",
       " '2010-08-04 11:38:07',\n",
       " '2011-11-08 04:46:44',\n",
       " '2011-11-09 15:07:13',\n",
       " '2011-10-29 04:45:09',\n",
       " '2011-10-18 16:14:31',\n",
       " '2010-10-08 01:39:26',\n",
       " '2012-01-05 19:49:03',\n",
       " '2010-08-06 01:29:56',\n",
       " '2010-08-21 21:36:07',\n",
       " '2010-12-14 23:13:34',\n",
       " '2010-09-09 22:52:29',\n",
       " '2010-11-03 19:01:28',\n",
       " '2011-12-08 00:40:59',\n",
       " '2010-07-28 18:17:46',\n",
       " '2010-10-04 12:45:23',\n",
       " '2012-04-11 20:19:32',\n",
       " '2011-03-09 10:22:58',\n",
       " '2010-11-23 07:50:07',\n",
       " '2012-11-27 20:13:45',\n",
       " '2010-10-19 14:34:44',\n",
       " '2011-01-20 17:50:00',\n",
       " '2010-08-16 06:19:27',\n",
       " '2012-09-06 14:48:44',\n",
       " '2012-04-26 07:10:09',\n",
       " '2011-02-22 16:13:39',\n",
       " '2010-11-30 06:18:04',\n",
       " '2010-10-02 23:58:10',\n",
       " '2010-07-26 12:53:22',\n",
       " '2010-07-27 18:33:56',\n",
       " '2010-07-19 21:58:51',\n",
       " '2011-01-27 23:45:50',\n",
       " '2010-09-21 23:40:39',\n",
       " '2011-06-26 01:49:18',\n",
       " '2011-05-05 21:04:31',\n",
       " '2012-05-02 20:00:15',\n",
       " '2010-08-06 11:23:15',\n",
       " '2010-12-04 03:37:32',\n",
       " '2011-05-07 18:56:38',\n",
       " '2011-05-26 21:39:40',\n",
       " '2011-04-06 10:55:04',\n",
       " '2012-03-29 23:16:33',\n",
       " '2011-09-22 19:40:00',\n",
       " '2011-05-10 23:50:24',\n",
       " '2011-01-27 07:51:33',\n",
       " '2010-12-09 21:31:56',\n",
       " '2010-07-19 20:36:47',\n",
       " '2010-11-18 10:57:43',\n",
       " '2011-11-07 14:37:39',\n",
       " '2010-08-19 14:13:27',\n",
       " '2012-10-02 16:35:10',\n",
       " '2012-03-16 00:26:15',\n",
       " '2013-10-02 15:14:49',\n",
       " '2010-10-04 10:16:23',\n",
       " '2011-01-29 05:40:49',\n",
       " '2011-02-19 19:25:19',\n",
       " '2010-09-03 16:27:56',\n",
       " '2010-07-19 19:26:13',\n",
       " '2011-05-27 15:30:01',\n",
       " '2010-11-30 13:19:58',\n",
       " '2010-07-27 15:05:38',\n",
       " '2012-04-20 01:29:44',\n",
       " '2010-07-19 19:14:43',\n",
       " '2010-08-03 02:49:32',\n",
       " '2010-12-31 10:46:17',\n",
       " '2012-01-11 20:30:02',\n",
       " '2011-02-19 19:47:16',\n",
       " '2010-12-04 00:36:08',\n",
       " '2011-03-16 22:34:19',\n",
       " '2010-08-25 22:55:22',\n",
       " '2010-09-24 04:09:04',\n",
       " '2010-08-06 04:50:59',\n",
       " '2011-11-22 14:26:25',\n",
       " '2011-05-08 11:53:51',\n",
       " '2010-09-30 14:04:56',\n",
       " '2012-05-05 17:27:18',\n",
       " '2012-12-19 18:50:04',\n",
       " '2010-09-09 08:59:38',\n",
       " '2010-07-21 07:15:54',\n",
       " '2010-08-19 11:06:22',\n",
       " '2010-08-04 18:29:57',\n",
       " '2012-05-31 14:35:14',\n",
       " '2010-11-30 20:22:45',\n",
       " '2012-11-21 00:03:27',\n",
       " '2011-01-21 09:42:03',\n",
       " '2010-12-17 22:00:34',\n",
       " '2010-09-13 13:16:33',\n",
       " '2010-09-27 07:25:32',\n",
       " '2010-09-16 10:21:36',\n",
       " '2010-07-30 10:03:37',\n",
       " '2011-05-04 20:12:48',\n",
       " '2011-01-27 04:24:01',\n",
       " '2010-08-13 21:22:56',\n",
       " '2010-10-04 15:40:44',\n",
       " '2011-11-04 13:18:01',\n",
       " '2011-10-04 00:41:57',\n",
       " '2012-10-09 20:46:38',\n",
       " '2011-08-17 18:38:07',\n",
       " '2011-01-18 22:11:11',\n",
       " '2012-01-12 19:58:30',\n",
       " '2010-08-29 17:15:22',\n",
       " '2010-11-17 13:49:06',\n",
       " '2011-05-20 18:39:55',\n",
       " '2011-12-29 14:00:12',\n",
       " '2012-01-31 09:53:25',\n",
       " '2012-02-01 00:27:23',\n",
       " '2013-01-23 04:29:40',\n",
       " '2010-10-21 12:17:31',\n",
       " '2010-07-19 20:35:34',\n",
       " '2010-07-27 14:14:10',\n",
       " '2011-05-27 08:11:49',\n",
       " '2010-09-11 15:37:55',\n",
       " '2010-11-03 15:49:57',\n",
       " '2010-08-10 13:47:10',\n",
       " '2010-08-04 22:36:49',\n",
       " '2010-08-04 12:32:08',\n",
       " '2011-05-16 20:51:58',\n",
       " '2012-03-28 23:53:12',\n",
       " '2010-07-27 09:00:42',\n",
       " '2010-12-15 20:06:07',\n",
       " '2010-07-20 09:43:23',\n",
       " '2012-04-10 11:54:34',\n",
       " '2012-05-26 06:33:22',\n",
       " '2011-03-15 12:03:42',\n",
       " '2012-02-28 09:36:41',\n",
       " '2011-12-29 17:15:43',\n",
       " '2012-03-27 05:13:34',\n",
       " '2010-10-04 04:48:07',\n",
       " '2010-09-08 10:28:33',\n",
       " '2010-11-18 10:10:55',\n",
       " '2012-08-31 08:37:07',\n",
       " '2010-10-05 15:08:21',\n",
       " '2010-12-15 20:10:26',\n",
       " '2010-10-11 20:31:15',\n",
       " '2010-07-27 06:26:04',\n",
       " '2010-08-19 17:14:39',\n",
       " '2010-09-02 10:25:32',\n",
       " '2010-10-08 11:30:29',\n",
       " '2010-11-30 06:19:58',\n",
       " '2010-11-13 14:00:03',\n",
       " '2011-03-22 17:21:14',\n",
       " '2011-05-19 13:31:00',\n",
       " '2012-06-27 21:58:26',\n",
       " '2010-09-16 16:27:02',\n",
       " '2012-05-09 21:55:24',\n",
       " '2011-01-27 13:22:27',\n",
       " '2010-08-20 18:16:10',\n",
       " '2012-01-19 01:24:06',\n",
       " '2010-08-20 23:58:49',\n",
       " '2012-02-14 19:15:29',\n",
       " '2011-10-28 08:23:36',\n",
       " '2011-09-23 04:36:06',\n",
       " '2011-07-06 15:37:34',\n",
       " '2011-12-06 09:02:17',\n",
       " '2010-07-26 20:09:20',\n",
       " '2010-12-15 20:08:32',\n",
       " '2010-07-20 21:05:35',\n",
       " '2010-08-30 01:22:20',\n",
       " '2011-10-07 17:23:13',\n",
       " '2010-10-16 01:09:31',\n",
       " '2012-05-02 22:56:48',\n",
       " '2011-12-07 03:12:54',\n",
       " '2010-08-14 13:06:45',\n",
       " '2010-11-15 20:28:53',\n",
       " '2011-11-02 04:50:12',\n",
       " '2012-11-03 00:22:09',\n",
       " '2010-07-21 00:54:29',\n",
       " '2013-01-11 05:29:09',\n",
       " '2011-12-25 10:12:47',\n",
       " '2010-07-31 15:46:05',\n",
       " '2010-11-06 19:47:38',\n",
       " '2012-07-26 15:51:08',\n",
       " '2010-08-12 13:30:08',\n",
       " '2010-10-21 14:42:28',\n",
       " '2010-11-21 21:02:25',\n",
       " '2011-02-28 07:39:54',\n",
       " '2010-09-03 19:22:30',\n",
       " '2012-08-12 13:10:19',\n",
       " '2010-08-19 13:09:50',\n",
       " '2011-09-01 03:05:59',\n",
       " '2010-12-13 22:10:48',\n",
       " '2010-07-21 22:57:58',\n",
       " '2011-04-01 04:40:52',\n",
       " '2010-11-21 02:00:15',\n",
       " '2010-07-26 15:31:39',\n",
       " '2010-08-30 23:04:11',\n",
       " '2010-08-21 00:04:13',\n",
       " '2011-02-12 12:47:14',\n",
       " '2011-02-18 00:06:24',\n",
       " '2012-02-21 13:06:45',\n",
       " '2010-08-02 16:57:33',\n",
       " '2012-01-19 19:35:28',\n",
       " '2012-12-21 02:15:37',\n",
       " '2011-01-22 00:50:41',\n",
       " '2012-02-14 19:22:31',\n",
       " '2010-08-31 15:30:46',\n",
       " '2011-10-03 15:41:31',\n",
       " '2010-09-01 15:02:00',\n",
       " '2010-07-19 23:06:43',\n",
       " '2012-09-29 23:06:37',\n",
       " '2010-07-19 21:01:35',\n",
       " '2012-03-07 13:27:39',\n",
       " '2011-03-17 10:53:40',\n",
       " '2011-07-08 08:51:21',\n",
       " '2012-12-20 22:55:54',\n",
       " '2012-02-09 14:52:45',\n",
       " '2011-07-13 02:37:11',\n",
       " '2011-10-21 20:00:14',\n",
       " '2010-07-26 15:52:05',\n",
       " '2013-07-29 14:48:26',\n",
       " '2012-07-27 15:17:42',\n",
       " '2010-07-19 21:50:44',\n",
       " '2011-11-17 00:00:16',\n",
       " '2010-08-22 15:08:47',\n",
       " '2011-08-09 06:45:27',\n",
       " '2012-05-12 22:06:25',\n",
       " '2012-12-19 18:45:34',\n",
       " '2012-01-25 07:01:54',\n",
       " '2011-02-17 21:56:24',\n",
       " '2012-12-30 12:48:32',\n",
       " '2014-06-16 18:42:37',\n",
       " '2010-07-28 17:43:49',\n",
       " '2010-08-19 13:46:41',\n",
       " '2010-11-30 08:02:28',\n",
       " '2010-09-12 05:04:04',\n",
       " '2012-11-28 06:04:24',\n",
       " '2010-08-02 08:46:38',\n",
       " '2012-12-20 17:12:04',\n",
       " '2011-07-12 15:26:57',\n",
       " '2012-08-08 17:40:36',\n",
       " '2010-08-09 13:05:50',\n",
       " '2010-09-11 13:50:33',\n",
       " '2011-08-18 17:27:32',\n",
       " '2010-11-16 07:52:02',\n",
       " '2010-11-02 12:04:30',\n",
       " '2012-04-25 00:13:45',\n",
       " '2010-10-14 13:05:32',\n",
       " '2012-01-13 16:49:07',\n",
       " '2012-07-27 17:51:25',\n",
       " '2012-01-12 22:18:51',\n",
       " '2010-08-06 01:08:05',\n",
       " '2010-09-18 18:15:49',\n",
       " '2011-02-06 04:09:24',\n",
       " '2010-08-15 08:59:45',\n",
       " '2010-07-22 12:17:20',\n",
       " '2010-08-31 15:58:42',\n",
       " '2010-10-22 14:03:02',\n",
       " '2012-04-24 03:43:25',\n",
       " '2011-01-29 08:53:30',\n",
       " '2010-07-19 20:33:26',\n",
       " '2010-12-04 03:38:44',\n",
       " '2012-01-19 07:12:05',\n",
       " '2014-05-14 15:17:07',\n",
       " '2010-09-16 21:32:16',\n",
       " '2010-08-06 11:15:17',\n",
       " '2010-10-02 19:21:19',\n",
       " '2010-08-09 13:51:29',\n",
       " '2010-12-04 01:32:33',\n",
       " '2012-05-11 13:22:34',\n",
       " '2012-01-25 08:48:43',\n",
       " '2010-11-23 10:22:21',\n",
       " '2010-08-03 03:02:57',\n",
       " '2010-07-19 21:24:58',\n",
       " '2010-10-20 19:28:41',\n",
       " '2011-07-08 03:50:26',\n",
       " '2010-07-21 21:04:03',\n",
       " '2012-01-16 11:22:45',\n",
       " '2010-09-05 08:36:31',\n",
       " '2010-07-28 09:35:43',\n",
       " '2010-09-05 19:19:03',\n",
       " '2012-01-12 15:37:43',\n",
       " '2010-07-21 16:53:35',\n",
       " '2011-12-07 17:38:33',\n",
       " '2010-11-15 07:06:37',\n",
       " '2010-08-16 13:45:34',\n",
       " '2012-02-28 17:12:23',\n",
       " '2011-01-27 11:08:52',\n",
       " '2012-02-09 15:24:27',\n",
       " '2010-10-14 10:31:18',\n",
       " '2012-04-10 17:38:02',\n",
       " '2010-09-20 20:42:21',\n",
       " '2010-08-12 20:25:12',\n",
       " '2012-03-28 02:27:13',\n",
       " '2010-08-21 21:19:39',\n",
       " '2010-08-04 03:28:53',\n",
       " '2010-12-04 10:54:08',\n",
       " '2012-10-08 09:31:51',\n",
       " '2010-08-17 02:01:44',\n",
       " '2010-12-04 03:48:36',\n",
       " '2010-11-30 06:54:37',\n",
       " '2011-07-29 13:58:24',\n",
       " '2010-12-14 21:11:31',\n",
       " '2010-07-21 07:04:26',\n",
       " '2012-05-13 02:05:17',\n",
       " '2010-12-14 09:37:12',\n",
       " '2012-07-28 01:19:18',\n",
       " '2010-07-27 06:23:57',\n",
       " '2012-05-23 22:55:29',\n",
       " '2012-02-14 19:06:16',\n",
       " '2010-08-06 08:06:13',\n",
       " '2011-04-09 16:49:38',\n",
       " '2010-09-10 10:48:33',\n",
       " '2013-01-12 17:52:32',\n",
       " '2010-08-03 03:49:10',\n",
       " '2012-12-12 05:49:44',\n",
       " '2010-09-18 16:14:21',\n",
       " '2012-05-12 16:33:46',\n",
       " '2012-05-30 02:16:51',\n",
       " '2010-08-23 14:55:35',\n",
       " '2010-08-20 12:28:09',\n",
       " '2012-02-04 17:49:55',\n",
       " '2010-08-19 04:35:15',\n",
       " '2010-10-04 12:05:20',\n",
       " '2010-09-05 11:43:35',\n",
       " '2010-07-29 22:30:13',\n",
       " '2012-01-22 07:33:36',\n",
       " '2011-06-02 03:51:31',\n",
       " '2011-05-07 16:26:44',\n",
       " '2011-12-30 17:10:38',\n",
       " '2012-08-01 04:44:46',\n",
       " '2011-03-29 13:28:06',\n",
       " '2012-02-08 22:05:54',\n",
       " '2012-02-06 19:18:52',\n",
       " '2010-09-07 08:58:26',\n",
       " '2010-08-06 01:20:36',\n",
       " '2011-05-20 09:23:56',\n",
       " '2010-08-20 22:35:55',\n",
       " '2011-05-21 16:04:11',\n",
       " '2010-08-19 21:44:04',\n",
       " '2011-05-27 18:19:55',\n",
       " '2012-07-25 00:59:27',\n",
       " '2010-09-07 02:04:47',\n",
       " '2011-10-19 18:52:31',\n",
       " '2011-07-05 22:16:55',\n",
       " '2010-10-07 23:27:47',\n",
       " '2012-03-29 22:41:43',\n",
       " '2011-01-06 15:28:05',\n",
       " '2012-01-24 14:03:02',\n",
       " '2012-01-25 05:42:43',\n",
       " '2011-03-15 11:28:53',\n",
       " '2010-09-05 09:22:57',\n",
       " '2012-04-13 14:21:53',\n",
       " '2012-11-09 22:24:45',\n",
       " '2010-08-17 02:52:02',\n",
       " '2011-05-16 21:48:29',\n",
       " '2010-07-21 22:35:38',\n",
       " '2011-02-18 22:29:42',\n",
       " '2011-05-20 10:56:14',\n",
       " '2010-07-21 03:48:55',\n",
       " '2012-08-22 22:05:48',\n",
       " '2010-12-17 11:13:27',\n",
       " '2011-05-20 21:19:25',\n",
       " '2010-09-02 13:10:52',\n",
       " '2010-10-04 09:13:27',\n",
       " '2012-10-07 17:26:36',\n",
       " '2010-08-19 11:21:55',\n",
       " '2010-08-12 20:51:36',\n",
       " '2012-02-23 10:35:22',\n",
       " '2011-08-17 17:18:56',\n",
       " '2010-07-21 15:21:33',\n",
       " '2010-09-24 04:13:25',\n",
       " '2010-09-11 15:18:26',\n",
       " '2012-06-08 13:01:00',\n",
       " '2010-07-27 08:42:23',\n",
       " '2010-08-31 10:13:21',\n",
       " '2012-02-14 19:39:35',\n",
       " '2012-02-29 07:17:57',\n",
       " '2011-07-06 23:08:50',\n",
       " '2010-12-01 20:17:08',\n",
       " '2011-01-30 04:04:07',\n",
       " '2010-09-20 15:19:18',\n",
       " '2012-06-20 14:14:15',\n",
       " '2010-09-02 09:35:37',\n",
       " '2010-09-08 22:23:19',\n",
       " '2011-03-17 00:09:23',\n",
       " '2011-05-08 07:16:23',\n",
       " '2011-10-31 07:10:37',\n",
       " '2010-09-20 20:44:51',\n",
       " '2011-09-22 01:48:05',\n",
       " '2010-12-13 23:21:55',\n",
       " '2010-12-04 00:12:46',\n",
       " '2012-12-11 03:48:22',\n",
       " '2012-01-22 10:35:00',\n",
       " '2010-07-20 00:06:20',\n",
       " '2012-01-25 07:24:39',\n",
       " '2010-07-19 19:13:28',\n",
       " '2012-02-08 19:54:04',\n",
       " '2010-10-19 06:16:41',\n",
       " '2010-07-20 18:17:25',\n",
       " '2011-02-19 19:49:46',\n",
       " '2010-09-02 07:18:38',\n",
       " '2010-08-19 13:45:36',\n",
       " '2010-07-29 18:59:01',\n",
       " '2011-08-17 19:48:27',\n",
       " '2011-03-22 00:36:10',\n",
       " '2010-08-07 02:33:35',\n",
       " '2010-11-16 13:30:48',\n",
       " '2011-08-16 15:34:39',\n",
       " '2010-07-19 23:19:44',\n",
       " '2010-08-29 12:04:51',\n",
       " '2010-08-01 18:56:25',\n",
       " '2010-09-07 17:02:12',\n",
       " '2010-08-22 04:16:03',\n",
       " '2012-02-03 16:12:56',\n",
       " '2010-11-17 09:14:40',\n",
       " '2014-02-27 19:03:27',\n",
       " '2010-08-29 08:44:54',\n",
       " '2011-10-12 14:29:43',\n",
       " '2010-12-13 23:50:20',\n",
       " '2010-08-04 12:32:12',\n",
       " '2010-08-19 06:28:05',\n",
       " '2010-10-06 12:22:32',\n",
       " '2011-10-07 09:45:54',\n",
       " '2011-02-14 22:09:41',\n",
       " '2010-08-08 20:22:58',\n",
       " '2012-04-10 01:06:11',\n",
       " '2010-11-09 22:31:31',\n",
       " '2010-08-02 20:31:22',\n",
       " '2010-09-01 12:21:52',\n",
       " '2010-11-27 18:14:10',\n",
       " '2011-05-26 12:05:30',\n",
       " '2011-10-15 12:53:56',\n",
       " '2011-04-05 15:21:14',\n",
       " '2011-04-12 19:43:01',\n",
       " '2011-06-24 20:39:48',\n",
       " '2010-07-21 15:38:46',\n",
       " '2010-08-06 01:15:52',\n",
       " '2010-11-18 10:36:54',\n",
       " '2012-04-24 20:50:35',\n",
       " '2012-04-03 14:05:28',\n",
       " '2010-10-20 19:12:23',\n",
       " '2011-04-13 15:39:52',\n",
       " '2010-11-17 14:15:22',\n",
       " '2012-06-15 20:18:41',\n",
       " '2011-08-05 13:32:37',\n",
       " '2010-07-19 21:18:12',\n",
       " '2010-07-20 15:20:38',\n",
       " '2012-01-28 20:58:33',\n",
       " '2012-04-17 03:22:37',\n",
       " '2011-10-08 00:01:28',\n",
       " '2010-08-19 10:45:32',\n",
       " '2010-11-23 18:37:54',\n",
       " '2010-10-22 12:52:59',\n",
       " '2010-10-20 20:23:38',\n",
       " '2011-06-09 15:53:24',\n",
       " '2010-08-06 00:29:25',\n",
       " '2011-05-06 12:13:40',\n",
       " '2011-05-02 03:02:45',\n",
       " '2010-08-13 22:29:08',\n",
       " '2010-08-11 06:42:21',\n",
       " '2011-05-04 23:29:40',\n",
       " '2011-12-26 21:24:29',\n",
       " '2010-12-31 13:17:04',\n",
       " '2011-07-06 00:37:17',\n",
       " '2012-07-31 01:06:30',\n",
       " '2010-08-06 01:26:54',\n",
       " '2011-11-08 11:34:10',\n",
       " '2014-07-30 20:09:14',\n",
       " '2010-07-19 23:48:50',\n",
       " '2010-07-21 16:50:35',\n",
       " '2010-09-07 03:19:29',\n",
       " '2010-07-20 16:13:03',\n",
       " '2010-07-30 12:15:45',\n",
       " '2010-08-20 20:55:10',\n",
       " '2010-08-21 00:00:26',\n",
       " '2012-08-07 20:05:09',\n",
       " '2010-10-02 22:51:17',\n",
       " '2011-05-04 20:52:28',\n",
       " '2010-07-27 14:04:13',\n",
       " '2010-12-14 14:01:47',\n",
       " '2010-07-27 08:43:15',\n",
       " '2010-08-17 03:47:24',\n",
       " '2011-03-01 07:40:34',\n",
       " '2011-08-12 20:30:18',\n",
       " '2012-11-20 23:53:03',\n",
       " '2012-08-03 22:11:26',\n",
       " '2010-07-27 09:19:53',\n",
       " '2010-08-30 04:55:48',\n",
       " '2010-07-23 08:18:52',\n",
       " '2011-09-21 18:09:23',\n",
       " '2012-05-04 21:14:06',\n",
       " '2012-04-09 23:18:08',\n",
       " '2011-06-01 17:43:40',\n",
       " '2010-08-22 10:07:23',\n",
       " '2010-07-26 12:38:40',\n",
       " '2010-10-05 13:54:13',\n",
       " '2012-02-29 06:38:15',\n",
       " '2010-07-22 14:10:29',\n",
       " '2010-09-21 03:00:47',\n",
       " '2010-07-21 09:02:11',\n",
       " '2011-11-02 02:33:57',\n",
       " '2010-07-26 19:53:10',\n",
       " '2011-05-08 19:03:21',\n",
       " '2012-02-05 00:52:00',\n",
       " '2011-12-06 09:05:02',\n",
       " '2012-01-14 22:11:00',\n",
       " '2011-11-07 04:54:13',\n",
       " '2010-11-03 15:51:11',\n",
       " '2011-12-13 20:36:53',\n",
       " '2011-09-30 01:37:39',\n",
       " '2012-07-24 01:09:45',\n",
       " '2012-01-22 22:24:38',\n",
       " '2010-07-21 08:01:54',\n",
       " '2010-08-22 18:15:09',\n",
       " '2010-08-22 00:34:17',\n",
       " '2012-02-15 08:26:53',\n",
       " '2012-06-18 18:25:36',\n",
       " '2010-07-19 19:19:03',\n",
       " '2010-10-04 04:31:35',\n",
       " '2010-07-22 14:32:20',\n",
       " '2010-12-14 03:02:22',\n",
       " '2010-09-03 03:49:48',\n",
       " '2010-08-23 14:53:02',\n",
       " '2012-04-04 21:14:58',\n",
       " '2012-10-11 09:43:05',\n",
       " '2013-01-10 04:46:19',\n",
       " '2010-08-05 14:32:53',\n",
       " '2010-07-21 15:53:59',\n",
       " '2011-12-06 09:05:52',\n",
       " '2012-01-18 05:23:21',\n",
       " '2010-07-27 14:03:52',\n",
       " '2010-07-28 19:34:00',\n",
       " '2011-08-08 19:24:17',\n",
       " '2010-09-17 11:11:41',\n",
       " '2011-03-13 12:26:51',\n",
       " '2010-10-19 20:50:18',\n",
       " '2010-07-19 21:23:20',\n",
       " '2012-08-11 06:24:41',\n",
       " '2012-06-09 20:22:52',\n",
       " '2011-06-05 14:57:27',\n",
       " '2010-12-16 23:13:28',\n",
       " '2012-06-19 18:53:00',\n",
       " '2010-08-25 20:09:26',\n",
       " '2010-09-10 07:40:27',\n",
       " '2011-03-13 00:58:39',\n",
       " '2012-01-05 16:56:42',\n",
       " '2010-09-13 14:07:11',\n",
       " '2010-10-06 20:10:59',\n",
       " '2010-07-20 08:38:38',\n",
       " '2011-06-19 23:56:44',\n",
       " '2012-01-12 12:58:51',\n",
       " '2010-07-26 20:22:30',\n",
       " '2010-12-05 05:18:37',\n",
       " '2012-02-10 21:49:04',\n",
       " '2010-07-28 19:22:44',\n",
       " '2011-02-20 07:03:34',\n",
       " '2012-10-02 23:59:50',\n",
       " '2010-08-29 19:54:09',\n",
       " '2010-07-27 09:16:48',\n",
       " '2011-12-08 06:45:33',\n",
       " '2012-08-07 08:57:41',\n",
       " '2010-07-24 20:15:20',\n",
       " '2012-10-29 17:08:40',\n",
       " '2010-07-27 16:04:42',\n",
       " '2010-10-02 18:10:23',\n",
       " '2012-03-28 01:07:46',\n",
       " '2012-11-27 21:59:46',\n",
       " '2011-01-02 11:22:43',\n",
       " '2011-04-18 18:56:29',\n",
       " '2012-01-22 21:28:30',\n",
       " '2012-04-04 14:19:55',\n",
       " '2013-11-06 21:59:59',\n",
       " '2012-12-17 13:18:50',\n",
       " '2011-05-11 12:33:30',\n",
       " '2010-11-17 11:14:47',\n",
       " '2012-11-09 12:38:15',\n",
       " '2012-05-23 13:17:26',\n",
       " '2010-09-23 06:10:48',\n",
       " '2012-01-06 00:22:14',\n",
       " '2013-01-18 19:50:12',\n",
       " '2010-08-27 09:36:24',\n",
       " '2010-12-14 23:17:09',\n",
       " '2010-11-17 11:51:36',\n",
       " '2010-12-14 09:17:59',\n",
       " '2010-08-19 12:45:55',\n",
       " '2010-08-06 09:52:51',\n",
       " '2011-05-20 19:55:14',\n",
       " '2010-11-21 19:28:40',\n",
       " '2010-07-30 15:43:28',\n",
       " '2011-05-08 02:26:21',\n",
       " '2012-12-18 01:39:58',\n",
       " '2010-07-21 16:07:20',\n",
       " '2011-05-07 14:19:42',\n",
       " '2010-08-19 13:25:53',\n",
       " '2011-05-20 20:15:32',\n",
       " '2010-12-04 20:42:26',\n",
       " '2010-11-14 17:50:08',\n",
       " '2011-03-30 11:35:21',\n",
       " '2012-05-05 01:58:28',\n",
       " '2010-07-21 22:02:27',\n",
       " '2010-08-20 02:25:32',\n",
       " '2010-08-30 18:02:22',\n",
       " '2010-08-19 18:43:16',\n",
       " '2010-10-19 20:36:30',\n",
       " '2011-11-16 20:17:12',\n",
       " '2011-01-03 23:01:01',\n",
       " '2010-09-20 20:39:08',\n",
       " '2010-08-22 06:37:15',\n",
       " '2010-08-27 15:07:18',\n",
       " '2010-11-16 19:10:24',\n",
       " '2010-11-08 17:07:23',\n",
       " '2011-06-25 22:37:57',\n",
       " '2010-08-16 14:38:47',\n",
       " '2012-12-30 11:40:18',\n",
       " '2010-08-26 21:24:56',\n",
       " '2010-11-18 09:52:47',\n",
       " '2012-10-23 19:47:03',\n",
       " '2010-12-31 14:56:21',\n",
       " '2011-01-18 21:34:07',\n",
       " '2011-03-13 11:54:36',\n",
       " '2012-04-06 13:34:20',\n",
       " '2012-04-05 13:12:17',\n",
       " '2010-08-17 16:17:00',\n",
       " '2012-03-28 23:57:13',\n",
       " '2010-07-26 19:41:11',\n",
       " '2010-07-21 20:53:54',\n",
       " '2010-08-14 01:16:16',\n",
       " '2012-01-12 13:58:53',\n",
       " '2011-12-25 21:55:38',\n",
       " '2012-01-03 17:44:36',\n",
       " '2011-01-03 23:15:30',\n",
       " '2013-01-23 09:37:19',\n",
       " '2011-03-01 08:01:54',\n",
       " '2010-09-16 12:57:26',\n",
       " '2012-12-16 21:59:01',\n",
       " '2010-12-09 17:11:31',\n",
       " '2010-10-14 18:00:23',\n",
       " '2010-08-13 16:11:20',\n",
       " '2011-03-01 14:19:56',\n",
       " '2012-04-27 14:49:04',\n",
       " '2010-08-19 11:22:51',\n",
       " '2010-07-30 20:00:45',\n",
       " '2011-02-11 18:44:38',\n",
       " '2012-11-02 15:17:00',\n",
       " '2011-08-09 14:53:54',\n",
       " '2011-01-28 19:20:55',\n",
       " '2010-12-14 20:00:01',\n",
       " '2011-10-13 16:41:40',\n",
       " '2010-07-27 09:09:25',\n",
       " '2010-08-30 14:19:24',\n",
       " '2010-08-06 05:27:57',\n",
       " '2010-10-04 18:24:36',\n",
       " '2012-08-21 01:17:10',\n",
       " '2010-07-20 09:13:29',\n",
       " '2010-07-27 14:38:34',\n",
       " '2010-08-09 15:36:26',\n",
       " '2011-05-16 20:26:08',\n",
       " '2010-08-04 10:55:31',\n",
       " '2011-07-01 06:27:39',\n",
       " '2011-05-15 20:20:23',\n",
       " '2011-05-26 08:34:15',\n",
       " '2011-11-16 00:43:24',\n",
       " '2012-10-10 14:37:08',\n",
       " '2012-05-29 14:16:58',\n",
       " '2010-08-18 08:54:23',\n",
       " '2013-01-03 12:31:58',\n",
       " '2012-10-13 06:30:43',\n",
       " '2012-03-31 20:21:21',\n",
       " '2010-07-21 15:43:44',\n",
       " '2011-11-22 20:39:28',\n",
       " '2010-07-26 19:51:45',\n",
       " '2011-02-22 02:08:30',\n",
       " '2010-08-19 13:01:46',\n",
       " '2010-08-09 11:14:34',\n",
       " '2010-08-05 02:42:11',\n",
       " '2011-10-06 07:48:29',\n",
       " '2010-09-27 12:01:56',\n",
       " '2011-01-19 19:03:46',\n",
       " '2012-02-16 22:03:09',\n",
       " '2011-09-20 20:42:50',\n",
       " '2010-07-19 21:53:02',\n",
       " '2011-03-01 16:16:25',\n",
       " '2012-02-15 06:20:32',\n",
       " '2012-12-14 15:27:00',\n",
       " '2010-09-18 21:07:17',\n",
       " '2012-08-20 14:58:40',\n",
       " '2010-11-07 22:41:34',\n",
       " '2012-01-05 21:14:03',\n",
       " '2010-07-31 15:36:12',\n",
       " '2012-06-13 12:59:19',\n",
       " '2010-08-08 23:05:17',\n",
       " '2011-07-13 17:12:44',\n",
       " '2011-10-06 23:34:08',\n",
       " '2010-11-08 23:28:28',\n",
       " '2010-08-01 14:21:18',\n",
       " '2010-11-20 19:17:18',\n",
       " '2010-08-12 20:20:11',\n",
       " '2010-08-15 17:26:03',\n",
       " '2010-09-17 09:40:03',\n",
       " '2010-08-27 14:38:12',\n",
       " '2010-12-09 17:39:07',\n",
       " '2010-08-19 13:47:24',\n",
       " '2011-10-30 22:51:18',\n",
       " '2011-10-12 08:35:08',\n",
       " '2010-07-26 15:37:25',\n",
       " '2010-07-29 10:57:18',\n",
       " '2012-05-29 15:04:32',\n",
       " '2012-04-10 13:48:27',\n",
       " '2010-09-30 13:14:28',\n",
       " '2011-08-17 01:00:24',\n",
       " '2011-07-02 19:44:26',\n",
       " '2011-10-08 10:35:16',\n",
       " '2012-02-01 01:31:25',\n",
       " '2010-12-03 16:23:28',\n",
       " '2012-02-09 10:48:37',\n",
       " '2012-01-14 02:49:40',\n",
       " '2010-11-16 12:19:21',\n",
       " '2010-10-01 01:04:39',\n",
       " '2012-04-11 20:45:59',\n",
       " '2012-02-29 04:34:02',\n",
       " '2011-03-29 13:26:09',\n",
       " '2012-01-22 17:59:10',\n",
       " '2012-06-09 16:41:00',\n",
       " '2010-08-19 11:03:39',\n",
       " '2010-09-27 10:48:47',\n",
       " '2010-11-10 21:11:31',\n",
       " '2011-11-06 12:31:39',\n",
       " '2010-10-25 14:59:22',\n",
       " '2010-08-19 12:50:42',\n",
       " '2011-10-29 01:17:11',\n",
       " '2011-05-07 12:32:39',\n",
       " '2010-11-14 14:49:35',\n",
       " '2011-02-26 19:58:58',\n",
       " '2011-04-08 21:02:38',\n",
       " '2010-09-16 12:08:07',\n",
       " '2010-08-19 10:29:27',\n",
       " '2011-06-02 11:49:17',\n",
       " '2012-03-13 21:45:55',\n",
       " '2010-08-06 11:09:39',\n",
       " '2012-07-22 19:19:55',\n",
       " '2010-07-21 16:04:18',\n",
       " '2013-01-26 03:25:13',\n",
       " '2010-08-28 17:07:59',\n",
       " '2011-02-02 14:41:39',\n",
       " '2012-08-20 20:11:30',\n",
       " '2011-10-24 21:02:24',\n",
       " '2010-08-19 13:44:40',\n",
       " '2012-01-20 09:04:04',\n",
       " '2010-10-01 18:31:56',\n",
       " '2011-12-30 18:28:19',\n",
       " '2010-07-26 15:37:10',\n",
       " '2010-07-29 10:50:53',\n",
       " '2012-01-23 16:38:09',\n",
       " '2012-05-15 05:29:51',\n",
       " '2010-09-27 10:11:37',\n",
       " '2010-07-27 16:50:51',\n",
       " '2010-10-19 11:07:08',\n",
       " '2010-07-19 20:36:01',\n",
       " '2011-01-27 17:57:12',\n",
       " '2012-10-05 15:54:06',\n",
       " '2012-05-02 11:58:26',\n",
       " '2010-09-05 19:18:58',\n",
       " '2011-11-04 12:49:03',\n",
       " '2013-01-03 21:32:53',\n",
       " '2010-07-19 23:00:30',\n",
       " '2010-08-30 11:48:13',\n",
       " '2012-08-30 11:14:58',\n",
       " '2011-05-07 17:43:25',\n",
       " '2011-05-31 11:07:15',\n",
       " '2010-10-04 07:15:14',\n",
       " '2010-10-31 05:14:12',\n",
       " '2010-08-03 21:19:22',\n",
       " '2012-08-09 21:13:56',\n",
       " '2011-06-10 12:53:28',\n",
       " '2011-02-11 15:01:37',\n",
       " '2010-07-29 11:25:13',\n",
       " '2011-11-12 20:20:17',\n",
       " '2010-12-09 15:05:19',\n",
       " '2011-05-07 17:20:21',\n",
       " '2010-09-18 02:32:42',\n",
       " '2010-07-20 16:12:08',\n",
       " '2010-10-25 15:54:27',\n",
       " '2011-06-02 11:09:11',\n",
       " '2010-09-25 15:45:57',\n",
       " '2012-10-11 03:18:01',\n",
       " '2014-01-27 14:38:45',\n",
       " '2010-10-20 19:13:42',\n",
       " '2011-06-17 12:29:47',\n",
       " '2012-04-04 15:57:04',\n",
       " '2011-01-30 00:05:34',\n",
       " '2011-05-07 19:25:34',\n",
       " '2011-01-29 18:37:00',\n",
       " '2010-07-20 09:45:06',\n",
       " '2011-01-21 13:40:53',\n",
       " '2012-05-05 16:59:11',\n",
       " '2011-08-16 15:52:51',\n",
       " '2010-07-21 20:29:12',\n",
       " '2012-06-26 22:45:54',\n",
       " '2011-10-07 17:21:35',\n",
       " '2010-08-24 16:17:53',\n",
       " '2011-09-22 14:02:14',\n",
       " '2011-03-16 23:59:43',\n",
       " '2011-03-30 04:20:40',\n",
       " '2010-08-07 06:03:39',\n",
       " '2010-07-26 20:08:09',\n",
       " '2011-12-06 09:31:52',\n",
       " '2010-11-18 12:42:26',\n",
       " '2010-08-06 11:11:00',\n",
       " '2011-12-29 21:38:05',\n",
       " '2011-06-17 02:29:37',\n",
       " '2012-12-20 04:33:25',\n",
       " '2010-09-03 14:42:15',\n",
       " '2012-11-20 17:15:17',\n",
       " '2012-05-10 11:54:42',\n",
       " '2012-02-16 15:26:53',\n",
       " '2010-12-13 18:20:38',\n",
       " '2011-01-01 22:49:18',\n",
       " '2010-08-27 11:15:23',\n",
       " '2014-08-01 21:37:48',\n",
       " '2010-07-19 21:40:02',\n",
       " '2012-11-18 00:37:39',\n",
       " '2010-08-31 13:02:43',\n",
       " '2010-07-22 13:49:37',\n",
       " '2012-02-28 18:05:40',\n",
       " '2010-09-10 23:50:38',\n",
       " '2010-09-19 03:24:26',\n",
       " '2011-11-25 15:52:54',\n",
       " '2011-07-06 23:18:06',\n",
       " '2010-07-27 06:20:38',\n",
       " '2011-10-07 23:59:04',\n",
       " '2011-12-08 11:04:19',\n",
       " '2010-08-20 21:52:06',\n",
       " '2010-09-06 23:56:59',\n",
       " '2010-08-16 12:03:55',\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'2010-07-19 20:19:46',\n",
       " '2010-07-20 00:21:48',\n",
       " '2010-07-29 12:38:50',\n",
       " '2010-07-29 23:43:25',\n",
       " '2010-07-29 23:45:11',\n",
       " '2010-08-03 07:58:26',\n",
       " '2010-08-03 09:14:27',\n",
       " '2010-08-05 13:06:12',\n",
       " '2010-08-19 15:10:48',\n",
       " '2010-09-01 16:59:06',\n",
       " '2010-09-16 07:09:04',\n",
       " '2010-10-07 01:30:50',\n",
       " '2010-10-14 08:57:22',\n",
       " '2010-10-14 09:00:42',\n",
       " '2010-10-20 17:16:08',\n",
       " '2010-10-22 16:04:22',\n",
       " '2010-10-24 21:24:43',\n",
       " '2010-10-25 23:52:05',\n",
       " '2010-11-30 16:38:57',\n",
       " '2010-12-08 09:15:39',\n",
       " '2011-01-09 12:50:39',\n",
       " '2011-02-06 13:02:49',\n",
       " '2011-03-02 06:16:43',\n",
       " '2011-03-04 21:40:11',\n",
       " '2011-03-11 04:09:55',\n",
       " '2011-03-11 22:18:47',\n",
       " '2011-03-23 17:06:37',\n",
       " '2011-03-24 21:11:34',\n",
       " '2011-03-29 21:45:14',\n",
       " '2011-04-08 15:18:21',\n",
       " '2011-04-11 15:04:34',\n",
       " '2011-04-29 20:13:18',\n",
       " '2011-05-13 20:14:36',\n",
       " '2011-05-24 16:26:12',\n",
       " '2011-05-28 20:11:23',\n",
       " '2011-05-31 21:06:46',\n",
       " '2011-06-17 13:28:08',\n",
       " '2011-06-19 16:36:27',\n",
       " '2011-06-24 11:18:30',\n",
       " '2011-06-26 22:07:50',\n",
       " '2011-06-29 12:49:22',\n",
       " '2011-06-29 15:58:28',\n",
       " '2011-07-11 16:03:44',\n",
       " '2011-07-29 10:18:27',\n",
       " '2011-07-29 16:48:36',\n",
       " '2011-07-31 14:12:19',\n",
       " '2011-07-31 19:40:29',\n",
       " '2011-08-03 13:56:26',\n",
       " '2011-08-04 13:34:19',\n",
       " '2011-08-08 17:33:16',\n",
       " '2011-08-10 01:42:02',\n",
       " '2011-08-12 02:44:42',\n",
       " '2011-08-15 17:32:32',\n",
       " '2011-08-15 23:14:42',\n",
       " '2011-08-23 09:26:16',\n",
       " '2011-08-24 14:05:47',\n",
       " '2011-08-25 19:13:01',\n",
       " '2011-09-06 19:45:37',\n",
       " '2011-09-08 13:27:07',\n",
       " '2011-09-08 15:23:15',\n",
       " '2011-09-13 16:31:02',\n",
       " '2011-09-15 18:48:52',\n",
       " '2011-09-16 16:30:41',\n",
       " '2011-09-16 17:48:00',\n",
       " '2011-09-17 14:19:21',\n",
       " '2011-09-19 08:38:35',\n",
       " '2011-09-19 15:10:56',\n",
       " '2011-09-22 21:10:08',\n",
       " '2011-09-23 15:11:02',\n",
       " '2011-09-23 16:17:59',\n",
       " '2011-09-23 18:16:31',\n",
       " '2011-09-26 13:48:30',\n",
       " '2011-09-26 16:00:33',\n",
       " '2011-10-01 22:24:44',\n",
       " '2011-10-05 22:02:12',\n",
       " '2011-10-06 18:21:48',\n",
       " '2011-10-07 14:36:36',\n",
       " '2011-10-12 07:11:43',\n",
       " '2011-10-12 20:43:23',\n",
       " '2011-10-16 16:54:57',\n",
       " '2011-10-17 13:31:39',\n",
       " '2011-10-18 20:10:27',\n",
       " '2011-10-18 20:12:24',\n",
       " '2011-11-03 04:49:37',\n",
       " '2011-11-11 12:16:26',\n",
       " '2011-11-18 15:25:49',\n",
       " '2011-11-21 20:42:57',\n",
       " '2011-11-22 22:41:08',\n",
       " '2011-11-25 21:59:41',\n",
       " '2011-11-28 19:35:10',\n",
       " '2011-12-10 09:44:15',\n",
       " '2011-12-13 06:07:32',\n",
       " '2011-12-13 20:34:11',\n",
       " '2011-12-19 00:35:20',\n",
       " '2011-12-19 15:14:31',\n",
       " '2011-12-20 18:04:15',\n",
       " '2011-12-23 03:32:41',\n",
       " '2011-12-23 13:59:16',\n",
       " '2011-12-23 14:12:10',\n",
       " '2011-12-27 14:40:39',\n",
       " '2011-12-30 08:53:36',\n",
       " '2012-01-01 15:01:58',\n",
       " '2012-01-03 13:44:56',\n",
       " '2012-01-03 15:38:23',\n",
       " '2012-01-04 15:40:50',\n",
       " '2012-01-09 00:13:04',\n",
       " '2012-01-09 15:02:02',\n",
       " '2012-01-10 14:02:42',\n",
       " '2012-01-11 19:40:42',\n",
       " '2012-01-12 21:13:45',\n",
       " '2012-01-20 10:35:10',\n",
       " '2012-01-30 11:55:34',\n",
       " '2012-01-31 08:26:25',\n",
       " '2012-02-03 15:03:49',\n",
       " '2012-02-11 08:37:19',\n",
       " '2012-02-12 20:11:06',\n",
       " '2012-02-17 15:02:28',\n",
       " '2012-02-19 06:43:26',\n",
       " '2012-02-21 16:01:16',\n",
       " '2012-02-22 14:02:31',\n",
       " '2012-02-22 21:19:53',\n",
       " '2012-02-22 23:35:05',\n",
       " '2012-02-22 23:50:15',\n",
       " '2012-02-27 04:49:15',\n",
       " '2012-02-29 08:38:07',\n",
       " '2012-03-06 19:10:53',\n",
       " '2012-03-12 13:21:27',\n",
       " '2012-03-13 17:23:23',\n",
       " '2012-03-14 22:37:12',\n",
       " '2012-03-15 16:32:53',\n",
       " '2012-03-17 00:07:38',\n",
       " '2012-03-18 17:06:10',\n",
       " '2012-03-20 15:51:10',\n",
       " '2012-03-22 11:05:40',\n",
       " '2012-03-24 08:59:01',\n",
       " '2012-03-24 09:06:35',\n",
       " '2012-03-24 20:31:39',\n",
       " '2012-03-26 17:25:30',\n",
       " '2012-03-29 11:04:53',\n",
       " '2012-04-01 12:15:05',\n",
       " '2012-04-02 19:17:41',\n",
       " '2012-04-10 20:14:16',\n",
       " '2012-04-11 08:19:31',\n",
       " '2012-04-13 14:27:28',\n",
       " '2012-04-13 16:13:33',\n",
       " '2012-04-16 15:05:04',\n",
       " '2012-04-17 17:31:52',\n",
       " '2012-04-17 19:00:13',\n",
       " '2012-04-20 17:22:54',\n",
       " '2012-04-25 08:35:24',\n",
       " '2012-05-02 10:54:44',\n",
       " '2012-05-03 14:56:56',\n",
       " '2012-05-09 16:46:29',\n",
       " '2012-05-10 07:49:25',\n",
       " '2012-05-14 19:15:34',\n",
       " '2012-05-16 15:58:31',\n",
       " '2012-05-16 20:38:14',\n",
       " '2012-05-18 14:11:07',\n",
       " '2012-05-18 14:17:07',\n",
       " '2012-05-22 14:25:22',\n",
       " '2012-05-22 19:24:15',\n",
       " '2012-05-23 13:37:15',\n",
       " '2012-05-24 12:55:29',\n",
       " '2012-05-24 14:16:59',\n",
       " '2012-05-28 17:32:46',\n",
       " '2012-05-29 14:16:08',\n",
       " '2012-05-31 13:53:54',\n",
       " '2012-06-01 12:15:12',\n",
       " '2012-06-01 12:28:28',\n",
       " '2012-06-03 18:04:59',\n",
       " '2012-06-04 17:37:36',\n",
       " '2012-06-05 13:57:13',\n",
       " '2012-06-06 14:42:25',\n",
       " '2012-06-07 16:05:18',\n",
       " '2012-06-07 21:44:55',\n",
       " '2012-06-10 13:44:45',\n",
       " '2012-06-11 21:17:43',\n",
       " '2012-06-14 13:12:56',\n",
       " '2012-06-14 13:55:05',\n",
       " '2012-06-15 10:40:36',\n",
       " '2012-06-15 15:18:08',\n",
       " '2012-06-18 22:42:51',\n",
       " '2012-06-22 18:54:32',\n",
       " '2012-06-25 14:45:41',\n",
       " '2012-06-27 03:11:39',\n",
       " '2012-06-29 14:38:26',\n",
       " '2012-07-01 10:07:50',\n",
       " '2012-07-05 13:36:54',\n",
       " '2012-07-07 09:47:44',\n",
       " '2012-07-10 12:16:26',\n",
       " '2012-07-10 12:34:19',\n",
       " '2012-07-11 16:15:53',\n",
       " '2012-07-12 14:57:46',\n",
       " '2012-07-12 16:44:55',\n",
       " '2012-07-12 18:58:17',\n",
       " '2012-07-12 22:23:31',\n",
       " '2012-07-18 12:40:43',\n",
       " '2012-07-18 12:42:29',\n",
       " '2012-07-18 15:47:39',\n",
       " '2012-07-18 18:53:06',\n",
       " '2012-07-20 15:51:03',\n",
       " '2012-07-24 14:00:08',\n",
       " '2012-07-25 09:07:39',\n",
       " '2012-07-26 11:52:35',\n",
       " '2012-07-26 12:28:06',\n",
       " '2012-07-27 13:10:49',\n",
       " '2012-07-31 02:05:44',\n",
       " '2012-07-31 02:11:46',\n",
       " '2012-08-02 14:00:33',\n",
       " '2012-08-02 19:40:28',\n",
       " '2012-08-03 13:12:34',\n",
       " '2012-08-04 10:53:08',\n",
       " '2012-08-05 22:20:29',\n",
       " '2012-08-07 14:36:18',\n",
       " '2012-08-07 15:48:43',\n",
       " '2012-08-08 20:03:09',\n",
       " '2012-08-08 22:29:00',\n",
       " '2012-08-09 20:08:07',\n",
       " '2012-08-09 23:16:04',\n",
       " '2012-08-10 17:09:37',\n",
       " '2012-08-14 12:05:12',\n",
       " '2012-08-14 12:08:44',\n",
       " '2012-08-14 12:14:58',\n",
       " '2012-08-14 12:16:51',\n",
       " '2012-08-14 12:19:02',\n",
       " '2012-08-14 12:20:07',\n",
       " '2012-08-14 12:30:06',\n",
       " '2012-08-14 12:37:39',\n",
       " '2012-08-14 12:38:09',\n",
       " '2012-08-14 12:38:31',\n",
       " '2012-08-14 12:41:56',\n",
       " '2012-08-14 12:42:15',\n",
       " '2012-08-14 12:49:34',\n",
       " '2012-08-14 12:50:15',\n",
       " '2012-08-14 12:50:27',\n",
       " '2012-08-14 12:54:38',\n",
       " '2012-08-14 12:56:02',\n",
       " '2012-08-14 12:56:41',\n",
       " '2012-08-14 12:56:53',\n",
       " '2012-08-14 13:00:49',\n",
       " '2012-08-14 13:00:57',\n",
       " '2012-08-14 20:28:12',\n",
       " '2012-08-14 20:33:32',\n",
       " '2012-08-14 20:35:34',\n",
       " '2012-08-14 20:36:08',\n",
       " '2012-08-14 20:37:33',\n",
       " '2012-08-14 20:38:11',\n",
       " '2012-08-14 20:40:00',\n",
       " '2012-08-14 20:40:58',\n",
       " '2012-08-14 20:44:28',\n",
       " '2012-08-14 20:46:00',\n",
       " '2012-08-14 20:49:38',\n",
       " '2012-08-17 21:27:28',\n",
       " '2012-08-18 19:41:06',\n",
       " '2012-08-21 16:37:43',\n",
       " '2012-08-21 19:26:31',\n",
       " '2012-08-22 21:34:16',\n",
       " '2012-08-24 01:55:31',\n",
       " '2012-08-25 21:59:13',\n",
       " '2012-08-25 22:00:40',\n",
       " '2012-08-26 07:34:30',\n",
       " '2012-08-27 09:02:34',\n",
       " '2012-08-27 18:39:17',\n",
       " '2012-08-28 14:32:07',\n",
       " '2012-08-28 14:39:13',\n",
       " '2012-08-28 19:20:14',\n",
       " '2012-08-28 19:43:42',\n",
       " '2012-08-30 13:54:29',\n",
       " '2012-08-31 12:42:15',\n",
       " '2012-08-31 16:52:57',\n",
       " '2012-09-02 08:15:24',\n",
       " '2012-09-02 16:05:52',\n",
       " '2012-09-02 16:12:23',\n",
       " '2012-09-02 16:16:30',\n",
       " '2012-09-03 09:37:51',\n",
       " '2012-09-03 09:49:54',\n",
       " '2012-09-03 16:09:16',\n",
       " '2012-09-04 12:24:22',\n",
       " '2012-09-05 12:26:10',\n",
       " '2012-09-05 16:03:21',\n",
       " '2012-09-05 22:17:53',\n",
       " '2012-09-06 15:04:11',\n",
       " '2012-09-07 12:25:49',\n",
       " '2012-09-07 13:32:57',\n",
       " '2012-09-07 22:01:46',\n",
       " '2012-09-08 21:45:15',\n",
       " '2012-09-10 20:11:46',\n",
       " '2012-09-11 14:20:48',\n",
       " '2012-09-12 15:49:44',\n",
       " '2012-09-13 16:45:45',\n",
       " '2012-09-13 16:46:03',\n",
       " '2012-09-13 16:46:23',\n",
       " '2012-09-14 15:48:41',\n",
       " '2012-09-16 15:58:16',\n",
       " '2012-09-16 15:58:42',\n",
       " '2012-09-16 16:03:56',\n",
       " '2012-09-20 13:41:44',\n",
       " '2012-09-20 13:45:11',\n",
       " '2012-09-20 13:45:50',\n",
       " '2012-09-21 08:19:59',\n",
       " '2012-09-21 12:56:44',\n",
       " '2012-09-22 15:51:57',\n",
       " '2012-09-23 21:15:12',\n",
       " '2012-09-24 01:56:12',\n",
       " '2012-09-24 01:58:52',\n",
       " '2012-09-24 14:11:43',\n",
       " '2012-09-24 14:34:12',\n",
       " '2012-09-25 20:09:10',\n",
       " '2012-09-25 21:05:25',\n",
       " '2012-09-25 21:28:02',\n",
       " '2012-09-27 00:34:58',\n",
       " '2012-09-27 14:24:46',\n",
       " '2012-09-28 20:26:10',\n",
       " '2012-09-28 22:56:23',\n",
       " '2012-09-29 07:10:24',\n",
       " '2012-09-29 20:45:20',\n",
       " '2012-09-29 21:44:00',\n",
       " '2012-09-30 22:29:07',\n",
       " '2012-09-30 22:30:47',\n",
       " '2012-09-30 22:39:02',\n",
       " '2012-09-30 23:33:52',\n",
       " '2012-10-02 04:47:11',\n",
       " '2012-10-02 05:07:56',\n",
       " '2012-10-02 16:21:11',\n",
       " '2012-10-02 19:51:14',\n",
       " '2012-10-03 13:48:21',\n",
       " '2012-10-07 18:52:42',\n",
       " '2012-10-07 19:23:59',\n",
       " '2012-10-08 09:58:41',\n",
       " '2012-10-08 15:33:31',\n",
       " '2012-10-08 15:37:37',\n",
       " '2012-10-09 19:16:23',\n",
       " '2012-10-10 10:55:26',\n",
       " '2012-10-10 10:56:20',\n",
       " '2012-10-10 15:44:24',\n",
       " '2012-10-11 10:14:29',\n",
       " '2012-10-11 10:29:06',\n",
       " '2012-10-11 10:35:55',\n",
       " '2012-10-11 14:27:38',\n",
       " '2012-10-11 21:24:31',\n",
       " '2012-10-15 12:08:48',\n",
       " '2012-10-15 12:10:22',\n",
       " '2012-10-15 12:11:57',\n",
       " '2012-10-17 16:13:43',\n",
       " '2012-10-20 17:59:01',\n",
       " '2012-10-21 03:35:59',\n",
       " '2012-10-21 13:47:21',\n",
       " '2012-10-21 17:01:03',\n",
       " '2012-10-23 17:33:51',\n",
       " '2012-10-23 20:56:02',\n",
       " '2012-10-24 13:51:05',\n",
       " '2012-10-25 10:27:57',\n",
       " '2012-10-25 15:53:41',\n",
       " '2012-10-25 17:57:06',\n",
       " '2012-10-29 19:14:37',\n",
       " '2012-10-30 01:02:51',\n",
       " '2012-10-30 23:52:23',\n",
       " '2012-10-31 14:07:58',\n",
       " '2012-11-01 17:31:26',\n",
       " '2012-11-01 23:18:34',\n",
       " '2012-11-03 20:27:15',\n",
       " '2012-11-04 11:33:23',\n",
       " '2012-11-04 17:34:17',\n",
       " '2012-11-04 20:40:23',\n",
       " '2012-11-04 20:41:01',\n",
       " '2012-11-05 08:31:57',\n",
       " '2012-11-07 16:29:57',\n",
       " '2012-11-11 21:13:31',\n",
       " '2012-11-12 17:27:10',\n",
       " '2012-11-15 16:40:19',\n",
       " '2012-11-15 21:49:11',\n",
       " '2012-11-16 12:27:18',\n",
       " '2012-11-18 22:15:04',\n",
       " '2012-11-20 18:22:19',\n",
       " '2012-11-25 11:55:19',\n",
       " '2012-11-25 21:03:00',\n",
       " '2012-11-27 19:02:45',\n",
       " '2012-12-01 15:51:14',\n",
       " '2012-12-01 21:02:53',\n",
       " '2012-12-05 01:51:18',\n",
       " '2012-12-05 14:50:07',\n",
       " '2012-12-07 19:49:06',\n",
       " '2012-12-08 15:27:55',\n",
       " '2012-12-11 16:18:09',\n",
       " '2012-12-11 17:42:44',\n",
       " '2012-12-13 02:44:48',\n",
       " '2012-12-13 15:35:10',\n",
       " '2012-12-14 17:58:58',\n",
       " '2012-12-17 07:22:05',\n",
       " '2012-12-17 15:49:56',\n",
       " '2012-12-20 14:45:39',\n",
       " '2012-12-20 15:40:04',\n",
       " '2012-12-20 20:11:14',\n",
       " '2012-12-21 23:15:04',\n",
       " '2012-12-26 08:56:18',\n",
       " '2012-12-27 15:58:48',\n",
       " '2012-12-27 18:57:43',\n",
       " '2012-12-28 05:30:59',\n",
       " '2012-12-30 14:46:33',\n",
       " '2012-12-31 14:35:16',\n",
       " '2013-01-03 19:28:05',\n",
       " '2013-01-04 00:08:21',\n",
       " '2013-01-04 09:48:00',\n",
       " '2013-01-04 10:47:48',\n",
       " '2013-01-06 12:19:18',\n",
       " '2013-01-07 14:29:48',\n",
       " '2013-01-08 20:43:45',\n",
       " '2013-01-09 13:25:40',\n",
       " '2013-01-09 13:51:10',\n",
       " '2013-01-10 08:21:02',\n",
       " '2013-01-11 15:45:23',\n",
       " '2013-01-13 16:56:31',\n",
       " '2013-01-13 16:58:24',\n",
       " '2013-01-15 13:15:24',\n",
       " '2013-01-15 14:35:04',\n",
       " '2013-01-15 15:25:35',\n",
       " '2013-01-17 16:05:33',\n",
       " '2013-01-20 10:51:02',\n",
       " '2013-01-21 15:49:52',\n",
       " '2013-01-21 18:29:11',\n",
       " '2013-01-22 18:12:08',\n",
       " '2013-01-23 15:29:42',\n",
       " '2013-01-24 13:34:38',\n",
       " '2013-01-26 11:56:19',\n",
       " '2013-01-29 12:38:24',\n",
       " '2013-01-30 21:40:10',\n",
       " '2013-02-07 14:43:26',\n",
       " '2013-02-07 14:47:36',\n",
       " '2013-02-07 20:59:40',\n",
       " '2013-02-11 18:19:14',\n",
       " '2013-02-12 23:07:15',\n",
       " '2013-02-13 15:09:23',\n",
       " '2013-02-13 15:36:11',\n",
       " '2013-02-13 15:36:35',\n",
       " '2013-02-14 16:37:21',\n",
       " '2013-02-14 16:37:32',\n",
       " '2013-02-15 13:32:21',\n",
       " '2013-02-15 16:21:19',\n",
       " '2013-02-17 22:11:26',\n",
       " '2013-02-18 17:58:02',\n",
       " '2013-02-27 00:36:24',\n",
       " '2013-03-01 22:35:12',\n",
       " '2013-03-07 18:27:16',\n",
       " '2013-04-03 19:39:22',\n",
       " '2013-04-05 13:24:46',\n",
       " '2013-04-24 13:38:54',\n",
       " '2013-04-26 04:51:32',\n",
       " '2013-04-30 14:38:58',\n",
       " '2013-05-06 21:42:10',\n",
       " '2013-05-17 23:20:46',\n",
       " '2013-05-22 19:28:12',\n",
       " '2013-05-30 18:37:30',\n",
       " '2013-06-18 03:39:53',\n",
       " '2013-06-20 03:05:29',\n",
       " '2013-06-20 14:08:50',\n",
       " '2013-06-20 14:09:08',\n",
       " '2013-06-21 20:36:04',\n",
       " '2013-06-28 19:20:40',\n",
       " '2013-06-30 09:12:59',\n",
       " '2013-07-06 12:17:41',\n",
       " '2013-07-18 19:01:48',\n",
       " '2013-07-21 10:56:15',\n",
       " '2013-07-22 08:03:56',\n",
       " '2013-08-03 19:17:05',\n",
       " '2013-08-05 01:05:53',\n",
       " '2013-08-19 19:39:04',\n",
       " '2013-08-22 18:11:28',\n",
       " '2013-08-23 19:02:08',\n",
       " '2013-08-26 08:19:37',\n",
       " '2013-08-28 09:17:38',\n",
       " '2013-08-29 17:09:07',\n",
       " '2013-09-02 15:19:30',\n",
       " '2013-09-03 15:34:18',\n",
       " '2013-09-09 15:42:20',\n",
       " '2013-09-16 23:41:12',\n",
       " '2013-09-19 22:15:27',\n",
       " '2013-09-27 16:24:39',\n",
       " '2013-10-02 14:22:50',\n",
       " '2013-10-02 14:23:17',\n",
       " '2013-10-22 15:04:01',\n",
       " '2013-11-07 13:12:47',\n",
       " '2013-11-14 07:50:25',\n",
       " '2013-11-14 15:30:19',\n",
       " '2013-11-21 10:45:54',\n",
       " '2013-11-29 10:18:11',\n",
       " '2013-12-10 11:15:20',\n",
       " '2013-12-31 14:16:04',\n",
       " '2014-01-15 22:05:35',\n",
       " '2014-01-26 10:13:50',\n",
       " '2014-01-28 18:56:51',\n",
       " '2014-02-09 19:20:50',\n",
       " '2014-02-12 01:23:48',\n",
       " '2014-02-26 16:04:21',\n",
       " '2014-02-27 21:30:26',\n",
       " '2014-02-27 21:31:05',\n",
       " '2014-02-27 21:31:53',\n",
       " '2014-02-27 21:32:30',\n",
       " '2014-03-08 22:01:28',\n",
       " '2014-03-24 07:24:04',\n",
       " '2014-04-20 13:17:01',\n",
       " '2014-04-20 22:19:48',\n",
       " '2014-04-21 22:12:50',\n",
       " '2014-05-14 17:23:17',\n",
       " '2014-05-29 22:18:49',\n",
       " '2014-06-01 07:24:29',\n",
       " '2014-07-02 13:12:05',\n",
       " '2014-07-29 23:11:30',\n",
       " '2014-08-02 14:01:10',\n",
       " '2014-08-05 12:21:53',\n",
       " '2014-08-07 20:48:13',\n",
       " '2014-08-14 21:14:00',\n",
       " nan}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'A.R',\n",
       " 'ALSTAT',\n",
       " 'Aarthi',\n",
       " 'Abhishek Shivkumar',\n",
       " 'Adam Bailey',\n",
       " 'Adam Bowen',\n",
       " 'Aina',\n",
       " 'Alberto',\n",
       " 'Alby',\n",
       " 'Alex',\n",
       " 'Alexander Galkin',\n",
       " 'Alexandre Passos',\n",
       " 'Alix Axel',\n",
       " 'Andreas Mueller',\n",
       " 'Andree',\n",
       " 'Andrew',\n",
       " 'Andrew Rosenberg',\n",
       " 'Angelo',\n",
       " 'Aniko',\n",
       " 'Anony-Mousse',\n",
       " 'Antonio Pedro Ramos',\n",
       " 'Arnold Neumaier',\n",
       " 'Artem Kaznatcheev',\n",
       " 'Arun',\n",
       " 'Arvin',\n",
       " 'Atilla Ozgur',\n",
       " 'AweSIM',\n",
       " 'B Seven',\n",
       " 'BB',\n",
       " 'BGreene',\n",
       " 'BYS2',\n",
       " 'Backlin',\n",
       " 'Ben',\n",
       " 'Ben Bolker',\n",
       " 'BenBarnes',\n",
       " 'BlueTrin',\n",
       " 'Bradford',\n",
       " 'Bryan',\n",
       " 'Bullmoose',\n",
       " 'CWT',\n",
       " 'Carl Witthoft',\n",
       " 'Cassie',\n",
       " 'CharlesM',\n",
       " 'Chase',\n",
       " 'Chris',\n",
       " 'Chris Farmer',\n",
       " 'Chris Taylor',\n",
       " 'ChrisArmstrong',\n",
       " 'ChrisS',\n",
       " 'Chthonic Project',\n",
       " 'DWin',\n",
       " 'Dail',\n",
       " 'Dan M.',\n",
       " 'Darren Young',\n",
       " 'David',\n",
       " 'David Roberts',\n",
       " 'David Robinson',\n",
       " 'Davide Giraudo',\n",
       " 'DevelBD',\n",
       " 'Developer',\n",
       " 'Dilip Sarwate',\n",
       " 'Dimitriy V. Masterov',\n",
       " 'Dirk Eddelbuettel',\n",
       " 'DonAndre',\n",
       " 'Dougal',\n",
       " 'Dov',\n",
       " 'DrewConway',\n",
       " 'Druss2k',\n",
       " 'ECII',\n",
       " 'EMS',\n",
       " 'EOL',\n",
       " 'Edward',\n",
       " 'Emre',\n",
       " 'EnergyNumbers',\n",
       " 'Ethan Shepherd',\n",
       " 'Faheem Mitha',\n",
       " 'Faith',\n",
       " 'Felipe',\n",
       " 'Francisco Roldán',\n",
       " 'Frank',\n",
       " 'Fred',\n",
       " 'Frederik Brinck Jensen',\n",
       " 'G. Grothendieck',\n",
       " 'GEdgar',\n",
       " 'Gang Liu',\n",
       " 'Garrith Graham',\n",
       " 'Gavin Simpson',\n",
       " 'Gerenuk',\n",
       " 'Girishkumar',\n",
       " 'Glen_b',\n",
       " 'Gonten',\n",
       " 'Graham Jones',\n",
       " 'GrayR',\n",
       " 'Guy',\n",
       " 'Henrik',\n",
       " 'Henry',\n",
       " 'Henry B.',\n",
       " 'Ian Stuart',\n",
       " 'Indrajit',\n",
       " 'Ismeet Kaur',\n",
       " 'Iterator',\n",
       " 'Ivan',\n",
       " 'Jacob',\n",
       " 'Jaime',\n",
       " 'James',\n",
       " 'James Bowery',\n",
       " 'James Waters',\n",
       " 'Jason S',\n",
       " 'Javad',\n",
       " 'Jawad',\n",
       " 'Jebego',\n",
       " 'Jens',\n",
       " 'Jeremy _',\n",
       " 'Jergason',\n",
       " 'Jim',\n",
       " 'John ',\n",
       " 'John A. Ramey',\n",
       " 'John Colby',\n",
       " 'John D. Cook',\n",
       " 'John Horton',\n",
       " 'John Leidegren',\n",
       " 'Jonas',\n",
       " 'Joris Meys',\n",
       " 'Jose',\n",
       " 'Josh',\n",
       " \"Josh O'Brien\",\n",
       " 'Joshua',\n",
       " 'Juanan',\n",
       " 'Julie',\n",
       " 'Kaish',\n",
       " 'Kaoukkos',\n",
       " 'Kavka',\n",
       " 'Kerry',\n",
       " 'Kevin',\n",
       " 'Kim N',\n",
       " 'Kit',\n",
       " 'Klas Lindbäck',\n",
       " 'Knightgu',\n",
       " 'KronoS',\n",
       " 'Kyle',\n",
       " 'Kyle Brandt',\n",
       " 'LPP',\n",
       " 'Largh',\n",
       " 'LazyCat',\n",
       " 'Legend',\n",
       " 'LonelyBear',\n",
       " 'Lori',\n",
       " 'Lostsoul',\n",
       " 'Lucas',\n",
       " 'Lukapple',\n",
       " 'Luke404',\n",
       " 'M C',\n",
       " 'MOLi',\n",
       " 'MRocklin',\n",
       " 'MYaseen208',\n",
       " 'Macro',\n",
       " 'Mahin',\n",
       " 'Mahmoud Ghandi ',\n",
       " 'MainMa',\n",
       " 'Marc Shivers',\n",
       " 'Marc in the box',\n",
       " 'Marcel',\n",
       " 'Marek Sebera',\n",
       " 'Mark Lavin',\n",
       " 'Martin Mystery',\n",
       " 'Martin Pohl',\n",
       " 'Mat',\n",
       " 'Mathematics',\n",
       " 'Mew 3.4',\n",
       " 'Michael Barker',\n",
       " 'Michael Chernick',\n",
       " 'Michael Hardy',\n",
       " 'Michael McGowan',\n",
       " 'Michael.Z',\n",
       " 'Mihran Hovsepyan',\n",
       " 'Mike Byrne',\n",
       " 'Mike Furlender',\n",
       " 'MikeRand',\n",
       " 'Ming-Chih Kao',\n",
       " 'Misha',\n",
       " 'MisterH',\n",
       " 'Mobius Pizza',\n",
       " 'Mohammad',\n",
       " 'Mohammed',\n",
       " 'Mutuelinvestor',\n",
       " 'NJH',\n",
       " 'Nick',\n",
       " 'Nick Sabbe',\n",
       " 'Nikratio',\n",
       " 'Noam Peled',\n",
       " 'OSlOlSO',\n",
       " 'Oleksandr Pshenychnyy',\n",
       " 'Oliver',\n",
       " 'Olivier',\n",
       " 'Owen',\n",
       " 'P Sellaz',\n",
       " 'PepsiCo',\n",
       " 'Peter',\n",
       " 'Peter H',\n",
       " 'Peter Shor ',\n",
       " 'PherricOxide',\n",
       " 'Philipp',\n",
       " 'Prasenjit',\n",
       " 'Programmer',\n",
       " 'Qin',\n",
       " 'Qnan',\n",
       " 'Quant Guy',\n",
       " 'Rachel',\n",
       " 'Ramnath',\n",
       " 'Ran',\n",
       " 'Ranabir',\n",
       " 'Raphael',\n",
       " 'Renato Dinhani Conceição',\n",
       " 'Ricardo González-Gil',\n",
       " 'Richie Cotton',\n",
       " 'Roland',\n",
       " 'Roman Luštrik',\n",
       " 'Ron Gejman',\n",
       " 'Ross Ahmed ',\n",
       " 'Ross Millikan',\n",
       " 'STW',\n",
       " 'S_H',\n",
       " 'Sachin Shekhar',\n",
       " 'Sam',\n",
       " 'Sam Roberts',\n",
       " 'Sander',\n",
       " 'Sanjay',\n",
       " 'Sasha',\n",
       " 'SaveTheRbtz',\n",
       " 'Seth',\n",
       " 'Seyhmus Güngören',\n",
       " 'Shahzad',\n",
       " 'Sidd',\n",
       " 'Sigvard',\n",
       " 'Sinan Ünür',\n",
       " 'Skiminok',\n",
       " 'Sklivvz',\n",
       " 'Soo',\n",
       " 'Sriram ',\n",
       " 'StasK',\n",
       " 'Strin',\n",
       " 'Surjya Narayana Padhi',\n",
       " 'Sycren',\n",
       " 'Szabolcs',\n",
       " 'TenaliRaman',\n",
       " 'ThePiachu',\n",
       " 'Thom Blake',\n",
       " 'Throwback1986',\n",
       " 'Timo',\n",
       " 'Tizz',\n",
       " 'Tom Au',\n",
       " 'Tom Ritter',\n",
       " 'Tomas T.',\n",
       " 'Tommy',\n",
       " 'Travis',\n",
       " 'Turukawa',\n",
       " 'Two Cents',\n",
       " 'Tyler Rinker',\n",
       " 'Ugo',\n",
       " 'Upul',\n",
       " 'User',\n",
       " 'Uwe Schmitt',\n",
       " 'Victor Lin',\n",
       " 'Victor May',\n",
       " 'Vighnesh',\n",
       " 'Vilx-',\n",
       " 'Vincent Zoonekynd',\n",
       " 'Vinterwoo',\n",
       " 'Volomike',\n",
       " 'Wee',\n",
       " 'Whymarrh',\n",
       " 'Wilduck',\n",
       " 'Xiaowen Li',\n",
       " 'Yang',\n",
       " 'Yiming Lu',\n",
       " 'Zach',\n",
       " 'aL3xa',\n",
       " 'aardvarkk',\n",
       " 'aaronjg',\n",
       " 'ablmf',\n",
       " 'adharris',\n",
       " 'aioobe',\n",
       " 'alang',\n",
       " 'alfa',\n",
       " 'amit',\n",
       " 'andreister',\n",
       " 'andrew cooke',\n",
       " 'andrija',\n",
       " 'anonymous_4322',\n",
       " 'apeescape',\n",
       " 'aramis',\n",
       " 'awshepard',\n",
       " 'banjollity',\n",
       " 'baz',\n",
       " 'bdh_dtu',\n",
       " 'ben',\n",
       " 'bgbg',\n",
       " 'bootech',\n",
       " 'brandon',\n",
       " 'burritoboy',\n",
       " 'caerolus',\n",
       " 'carlosdc',\n",
       " 'caseyr547',\n",
       " 'check123',\n",
       " 'chk',\n",
       " 'chutsu',\n",
       " 'cohoz',\n",
       " 'crible',\n",
       " 'cups',\n",
       " 'dannytoone',\n",
       " 'dark_charlie',\n",
       " 'datayoda',\n",
       " 'datta ram',\n",
       " 'dchandler',\n",
       " 'deinst',\n",
       " 'dmckee',\n",
       " 'doug',\n",
       " 'drknexus',\n",
       " 'dsimcha',\n",
       " 'duffymo',\n",
       " 'eznme',\n",
       " 'fabee',\n",
       " 'feelfree',\n",
       " 'ffriend',\n",
       " 'fred basset',\n",
       " 'gabriel',\n",
       " 'gisol',\n",
       " 'gorcee',\n",
       " 'gsk3',\n",
       " 'h.l.m',\n",
       " 'hackartist',\n",
       " 'heltonbiker',\n",
       " 'icobes',\n",
       " 'j0N45',\n",
       " 'jasonweiyi',\n",
       " 'jazzvibes',\n",
       " 'jb.',\n",
       " 'jeannot',\n",
       " 'jkg',\n",
       " 'johanbev',\n",
       " 'jonsca',\n",
       " 'joran',\n",
       " 'joriki',\n",
       " 'jthetzel',\n",
       " 'jwegner',\n",
       " 'keyboardP',\n",
       " 'kino',\n",
       " 'kmore',\n",
       " 'kohske',\n",
       " 'krammer',\n",
       " 'learner',\n",
       " 'lockedoff',\n",
       " 'maknelly',\n",
       " 'maple_shaft',\n",
       " 'maximus',\n",
       " 'mbq',\n",
       " 'metrobalderas',\n",
       " 'mga',\n",
       " 'mhoran_psprep',\n",
       " 'mihsathe',\n",
       " 'mosaic',\n",
       " 'mpiktas',\n",
       " 'mrdwab',\n",
       " 'muzhig',\n",
       " nan,\n",
       " 'nate',\n",
       " 'natorro',\n",
       " 'neuron',\n",
       " 'nibot',\n",
       " 'niko',\n",
       " 'nimrodm',\n",
       " 'nograpes',\n",
       " 'nostock',\n",
       " 'nullglob',\n",
       " 'ojblass',\n",
       " 'olchauvin',\n",
       " 'pater',\n",
       " 'petrelharp',\n",
       " 'petrichor',\n",
       " 'pharmine',\n",
       " 'picmate',\n",
       " 'pixel',\n",
       " 'pootzko',\n",
       " 'porst17',\n",
       " 'powerbar',\n",
       " 'prasenjit',\n",
       " 'prototoast',\n",
       " 'pyCthon',\n",
       " 'qmsource',\n",
       " 'quarkdown27',\n",
       " 'rahul',\n",
       " 'ralu',\n",
       " 'richardh',\n",
       " 'rnorberg',\n",
       " 'rumbleB',\n",
       " 's093294',\n",
       " 'sanity',\n",
       " 'sbg',\n",
       " 'sebhofer',\n",
       " 'sharky',\n",
       " 'shoonya',\n",
       " 'shootingstars',\n",
       " 'sinoTrinity',\n",
       " 'skyde',\n",
       " 'somori',\n",
       " 'stevenvh',\n",
       " 'subash',\n",
       " 'swiecki',\n",
       " 'sydeulissie',\n",
       " 'tdc',\n",
       " 'thedorkknight',\n",
       " 'thelatemail',\n",
       " 'thiton',\n",
       " 'tnaser',\n",
       " 'tutu',\n",
       " 'ulidtko',\n",
       " 'user1009703',\n",
       " 'user1134241',\n",
       " 'user1134516',\n",
       " 'user1140126',\n",
       " 'user1149913',\n",
       " 'user11667',\n",
       " 'user1172558',\n",
       " 'user1180428',\n",
       " 'user1203297',\n",
       " 'user1257313',\n",
       " 'user1265067',\n",
       " 'user12714',\n",
       " 'user1313',\n",
       " 'user1554592',\n",
       " 'user1569715',\n",
       " 'user1665220',\n",
       " 'user1727485',\n",
       " 'user1731927',\n",
       " 'user1738753',\n",
       " 'user1753987',\n",
       " 'user1785104',\n",
       " 'user1788005',\n",
       " 'user1873',\n",
       " 'user1911070',\n",
       " 'user1924327',\n",
       " 'user2760',\n",
       " 'user3125',\n",
       " 'user333',\n",
       " 'user3671',\n",
       " 'user3897',\n",
       " 'user54511',\n",
       " 'user5455',\n",
       " 'user570593',\n",
       " 'user594694',\n",
       " 'user697473',\n",
       " 'user714852',\n",
       " 'user779747',\n",
       " 'user862',\n",
       " 'user8968',\n",
       " 'user918967',\n",
       " 'user9325',\n",
       " 'user969113',\n",
       " 'user974896',\n",
       " 'user975904',\n",
       " 'user975964',\n",
       " 'user976991',\n",
       " 'user995434',\n",
       " 'user9964',\n",
       " 'utdiscant',\n",
       " 'vedran',\n",
       " 'vignesh kumar rathakumar',\n",
       " 'viki.omega9',\n",
       " 'vonPetrushev',\n",
       " 'vonjd',\n",
       " 'vzn',\n",
       " 'whuber',\n",
       " 'winwaed',\n",
       " 'wvoq',\n",
       " 'xan',\n",
       " 'xiaohan2012',\n",
       " 'yO2gO',\n",
       " 'zca0',\n",
       " 'zyx',\n",
       " 'ʞɔıu'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " 'user10525',\n",
       " 'user13253',\n",
       " 'user14071',\n",
       " 'user28',\n",
       " 'user5644',\n",
       " 'user7997',\n",
       " 'user9410'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Fill this columns\n",
    "#strings with empty string ' ' \n",
    "display(set(users_posts['DisplayName']))\n",
    "display(set(users_posts['WebsiteUrl']))\n",
    "display(set(users_posts['Location']))\n",
    "display(set(users_posts['AboutMe']))\n",
    "display(set(users_posts['Body']))\n",
    "display(set(users_posts['Title']))\n",
    "display(set(users_posts['Tags']))\n",
    "#numeric with 0\n",
    "display(set(users_posts['Age']))\n",
    "display(set(users_posts['AcceptedAnswerId']))\n",
    "display(set(users_posts['ViewCount']))\n",
    "display(set(users_posts['AnswerCount']))\n",
    "display(set(users_posts['FavoriteCount']))\n",
    "display(set(users_posts['LastEditorUserId']))\n",
    "display(set(users_posts['ParentId']))\n",
    "#Min datetime\n",
    "display(set(users_posts['LastEditDate']))\n",
    "\n",
    "#clean\n",
    "#drop columns\n",
    "display(set(users_posts['ProfileImageUrl']))\n",
    "display(set(users_posts['CommunityOwnedDate']))\n",
    "display(set(users_posts['ClosedDate']))\n",
    "display(set(users_posts['OwnerDisplayName']))\n",
    "display(set(users_posts['LastEditorDisplayName']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill strings\n",
    "users_posts[['DisplayName','WebsiteUrl','Location','AboutMe','Body','Title','Tags']] = users_posts[['DisplayName','WebsiteUrl','Location','AboutMe','Body','Title','Tags']].fillna(' ')\n",
    "#Fill numeric\n",
    "users_posts[['Age','AcceptedAnswerId','ViewCount','AnswerCount','FavoriteCount','LastEditorUserId','ParentId']] = users_posts[['Age','AcceptedAnswerId','ViewCount','AnswerCount','FavoriteCount','LastEditorUserId','ParentId']].fillna(0)\n",
    "\n",
    "#Fill datetime\n",
    "import datetime\n",
    "\n",
    "users_posts['LastEditDate'] = users_posts['LastEditDate'].fillna(datetime.datetime.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>percent_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ProfileImageUrl</th>\n",
       "      <td>ProfileImageUrl</td>\n",
       "      <td>92.749346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CommunityOwnedDate</th>\n",
       "      <td>CommunityOwnedDate</td>\n",
       "      <td>95.313382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ClosedDate</th>\n",
       "      <td>ClosedDate</td>\n",
       "      <td>98.688466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OwnerDisplayName</th>\n",
       "      <td>OwnerDisplayName</td>\n",
       "      <td>98.105847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LastEditorDisplayName</th>\n",
       "      <td>LastEditorDisplayName</td>\n",
       "      <td>99.463580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 column_name  percent_missing\n",
       "ProfileImageUrl              ProfileImageUrl        92.749346\n",
       "CommunityOwnedDate        CommunityOwnedDate        95.313382\n",
       "ClosedDate                        ClosedDate        98.688466\n",
       "OwnerDisplayName            OwnerDisplayName        98.105847\n",
       "LastEditorDisplayName  LastEditorDisplayName        99.463580"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_missing = users_posts.isnull().sum() * 100 / len(users_posts)\n",
    "missing_value_df = pd.DataFrame({'column_name': users_posts.columns,\n",
    "                                 'percent_missing': percent_missing})\n",
    "missing_value_df[(percent_missing>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns\n",
    "\n",
    "users_posts = users_posts.drop(['ProfileImageUrl','CommunityOwnedDate','ClosedDate','OwnerDisplayName','LastEditorDisplayName'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>percent_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [column_name, percent_missing]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_missing = users_posts.isnull().sum() * 100 / len(users_posts)\n",
    "missing_value_df = pd.DataFrame({'column_name': users_posts.columns,\n",
    "                                 'percent_missing': percent_missing})\n",
    "missing_value_df[(percent_missing>0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Adjust the data types in order to avoid future issues. Which ones should be changed? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId                int64\n",
       "Reputation            int64\n",
       "CreationDate         object\n",
       "DisplayName          object\n",
       "LastAccessDate       object\n",
       "WebsiteUrl           object\n",
       "Location             object\n",
       "AboutMe              object\n",
       "Views                 int64\n",
       "UpVotes               int64\n",
       "DownVotes             int64\n",
       "AccountId             int64\n",
       "Age                 float64\n",
       "postId                int64\n",
       "PostTypeId            int64\n",
       "AcceptedAnswerId    float64\n",
       "CreaionDate          object\n",
       "Score                 int64\n",
       "ViewCount           float64\n",
       "Body                 object\n",
       "LasActivityDate      object\n",
       "Title                object\n",
       "Tags                 object\n",
       "AnswerCount         float64\n",
       "CommentCount          int64\n",
       "FavoriteCount       float64\n",
       "LastEditorUserId    float64\n",
       "LastEditDate         object\n",
       "ParentId            float64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(users_posts.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>...</th>\n",
       "      <th>Body</th>\n",
       "      <th>LasActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>ParentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [userId, Reputation, CreationDate, DisplayName, LastAccessDate, WebsiteUrl, Location, AboutMe, Views, UpVotes, DownVotes, AccountId, Age, postId, PostTypeId, AcceptedAnswerId, CreaionDate, Score, ViewCount, Body, LasActivityDate, Title, Tags, AnswerCount, CommentCount, FavoriteCount, LastEditorUserId, LastEditDate, ParentId]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>...</th>\n",
       "      <th>Body</th>\n",
       "      <th>LasActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>ParentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [userId, Reputation, CreationDate, DisplayName, LastAccessDate, WebsiteUrl, Location, AboutMe, Views, UpVotes, DownVotes, AccountId, Age, postId, PostTypeId, AcceptedAnswerId, CreaionDate, Score, ViewCount, Body, LasActivityDate, Title, Tags, AnswerCount, CommentCount, FavoriteCount, LastEditorUserId, LastEditDate, ParentId]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>...</th>\n",
       "      <th>Body</th>\n",
       "      <th>LasActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>ParentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [userId, Reputation, CreationDate, DisplayName, LastAccessDate, WebsiteUrl, Location, AboutMe, Views, UpVotes, DownVotes, AccountId, Age, postId, PostTypeId, AcceptedAnswerId, CreaionDate, Score, ViewCount, Body, LasActivityDate, Title, Tags, AnswerCount, CommentCount, FavoriteCount, LastEditorUserId, LastEditDate, ParentId]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>...</th>\n",
       "      <th>Body</th>\n",
       "      <th>LasActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>ParentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [userId, Reputation, CreationDate, DisplayName, LastAccessDate, WebsiteUrl, Location, AboutMe, Views, UpVotes, DownVotes, AccountId, Age, postId, PostTypeId, AcceptedAnswerId, CreaionDate, Score, ViewCount, Body, LasActivityDate, Title, Tags, AnswerCount, CommentCount, FavoriteCount, LastEditorUserId, LastEditDate, ParentId]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>...</th>\n",
       "      <th>Body</th>\n",
       "      <th>LasActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>ParentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [userId, Reputation, CreationDate, DisplayName, LastAccessDate, WebsiteUrl, Location, AboutMe, Views, UpVotes, DownVotes, AccountId, Age, postId, PostTypeId, AcceptedAnswerId, CreaionDate, Score, ViewCount, Body, LasActivityDate, Title, Tags, AnswerCount, CommentCount, FavoriteCount, LastEditorUserId, LastEditDate, ParentId]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>...</th>\n",
       "      <th>Body</th>\n",
       "      <th>LasActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>ParentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [userId, Reputation, CreationDate, DisplayName, LastAccessDate, WebsiteUrl, Location, AboutMe, Views, UpVotes, DownVotes, AccountId, Age, postId, PostTypeId, AcceptedAnswerId, CreaionDate, Score, ViewCount, Body, LasActivityDate, Title, Tags, AnswerCount, CommentCount, FavoriteCount, LastEditorUserId, LastEditDate, ParentId]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>...</th>\n",
       "      <th>Body</th>\n",
       "      <th>LasActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>ParentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [userId, Reputation, CreationDate, DisplayName, LastAccessDate, WebsiteUrl, Location, AboutMe, Views, UpVotes, DownVotes, AccountId, Age, postId, PostTypeId, AcceptedAnswerId, CreaionDate, Score, ViewCount, Body, LasActivityDate, Title, Tags, AnswerCount, CommentCount, FavoriteCount, LastEditorUserId, LastEditDate, ParentId]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(users_posts[((users_posts['Age']%1)!=0)])\n",
    "display(users_posts[((users_posts['AcceptedAnswerId']%1)!=0)])\n",
    "display(users_posts[((users_posts['ViewCount']%1)!=0)])\n",
    "display(users_posts[((users_posts['AnswerCount']%1)!=0)])\n",
    "display(users_posts[((users_posts['FavoriteCount']%1)!=0)])\n",
    "display(users_posts[((users_posts['LastEditorUserId']%1)!=0)])\n",
    "display(users_posts[((users_posts['ParentId']%1)!=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId                int64\n",
       "Reputation            int64\n",
       "CreationDate         object\n",
       "DisplayName          object\n",
       "LastAccessDate       object\n",
       "WebsiteUrl           object\n",
       "Location             object\n",
       "AboutMe              object\n",
       "Views                 int64\n",
       "UpVotes               int64\n",
       "DownVotes             int64\n",
       "AccountId             int64\n",
       "Age                 float64\n",
       "postId                int64\n",
       "PostTypeId            int64\n",
       "AcceptedAnswerId    float64\n",
       "CreaionDate          object\n",
       "Score                 int64\n",
       "ViewCount           float64\n",
       "Body                 object\n",
       "LasActivityDate      object\n",
       "Title                object\n",
       "Tags                 object\n",
       "AnswerCount         float64\n",
       "CommentCount          int64\n",
       "FavoriteCount       float64\n",
       "LastEditorUserId    float64\n",
       "LastEditDate         object\n",
       "ParentId            float64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "userId               int64\n",
       "Reputation           int64\n",
       "CreationDate        object\n",
       "DisplayName         object\n",
       "LastAccessDate      object\n",
       "WebsiteUrl          object\n",
       "Location            object\n",
       "AboutMe             object\n",
       "Views                int64\n",
       "UpVotes              int64\n",
       "DownVotes            int64\n",
       "AccountId            int64\n",
       "Age                  int64\n",
       "postId               int64\n",
       "PostTypeId           int64\n",
       "AcceptedAnswerId     int64\n",
       "CreaionDate         object\n",
       "Score                int64\n",
       "ViewCount            int64\n",
       "Body                object\n",
       "LasActivityDate     object\n",
       "Title               object\n",
       "Tags                object\n",
       "AnswerCount          int64\n",
       "CommentCount         int64\n",
       "FavoriteCount        int64\n",
       "LastEditorUserId     int64\n",
       "LastEditDate        object\n",
       "ParentId             int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Should change these, because are integer but within data type float\n",
    "\n",
    "display(users_posts.dtypes)\n",
    "users_posts = users_posts.astype({'Age':'int','AcceptedAnswerId':'int','ViewCount':'int','AnswerCount':'int','FavoriteCount':'int','LastEditorUserId':'int','ParentId':'int'})\n",
    "display(users_posts.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>DownVotes</th>\n",
       "      <th>AccountId</th>\n",
       "      <th>Age</th>\n",
       "      <th>postId</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>ParentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>3.896200e+04</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.00000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6079.063087</td>\n",
       "      <td>7281.091679</td>\n",
       "      <td>1400.648016</td>\n",
       "      <td>914.799677</td>\n",
       "      <td>43.841050</td>\n",
       "      <td>6.490214e+05</td>\n",
       "      <td>12.975078</td>\n",
       "      <td>22960.799651</td>\n",
       "      <td>1.639290</td>\n",
       "      <td>4259.335224</td>\n",
       "      <td>4.083081</td>\n",
       "      <td>472.509573</td>\n",
       "      <td>0.627586</td>\n",
       "      <td>2.014630</td>\n",
       "      <td>0.61098</td>\n",
       "      <td>2318.515117</td>\n",
       "      <td>11951.882167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5224.896435</td>\n",
       "      <td>15164.527714</td>\n",
       "      <td>3423.886887</td>\n",
       "      <td>2296.527060</td>\n",
       "      <td>161.797079</td>\n",
       "      <td>5.958926e+05</td>\n",
       "      <td>18.804939</td>\n",
       "      <td>13696.932471</td>\n",
       "      <td>0.595892</td>\n",
       "      <td>11038.875897</td>\n",
       "      <td>6.561843</td>\n",
       "      <td>2423.961137</td>\n",
       "      <td>1.514881</td>\n",
       "      <td>2.674018</td>\n",
       "      <td>3.52762</td>\n",
       "      <td>4673.215379</td>\n",
       "      <td>14494.357588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1317.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.565530e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11325.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4856.000000</td>\n",
       "      <td>909.000000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.567810e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22373.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4620.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9651.000000</td>\n",
       "      <td>7931.000000</td>\n",
       "      <td>1050.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.091370e+06</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>33688.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2116.000000</td>\n",
       "      <td>22468.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>55226.000000</td>\n",
       "      <td>87393.000000</td>\n",
       "      <td>20932.000000</td>\n",
       "      <td>11442.000000</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>2.840547e+06</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>48325.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>114363.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>175495.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>233.00000</td>\n",
       "      <td>53985.000000</td>\n",
       "      <td>48314.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             userId    Reputation         Views       UpVotes     DownVotes  \\\n",
       "count  38962.000000  38962.000000  38962.000000  38962.000000  38962.000000   \n",
       "mean    6079.063087   7281.091679   1400.648016    914.799677     43.841050   \n",
       "std     5224.896435  15164.527714   3423.886887   2296.527060    161.797079   \n",
       "min       -1.000000      1.000000      0.000000      0.000000      0.000000   \n",
       "25%     1317.000000    147.000000     16.000000      4.000000      0.000000   \n",
       "50%     4856.000000    909.000000    124.000000     65.000000      1.000000   \n",
       "75%     9651.000000   7931.000000   1050.000000    582.000000     16.000000   \n",
       "max    55226.000000  87393.000000  20932.000000  11442.000000   1920.000000   \n",
       "\n",
       "          AccountId           Age        postId    PostTypeId  \\\n",
       "count  3.896200e+04  38962.000000  38962.000000  38962.000000   \n",
       "mean   6.490214e+05     12.975078  22960.799651      1.639290   \n",
       "std    5.958926e+05     18.804939  13696.932471      0.595892   \n",
       "min   -1.000000e+00      0.000000      1.000000      1.000000   \n",
       "25%    1.565530e+05      0.000000  11325.250000      1.000000   \n",
       "50%    4.567810e+05      0.000000  22373.500000      2.000000   \n",
       "75%    1.091370e+06     29.000000  33688.500000      2.000000   \n",
       "max    2.840547e+06     94.000000  48325.000000      7.000000   \n",
       "\n",
       "       AcceptedAnswerId         Score      ViewCount   AnswerCount  \\\n",
       "count      38962.000000  38962.000000   38962.000000  38962.000000   \n",
       "mean        4259.335224      4.083081     472.509573      0.627586   \n",
       "std        11038.875897      6.561843    2423.961137      1.514881   \n",
       "min            0.000000    -19.000000       0.000000      0.000000   \n",
       "25%            0.000000      1.000000       0.000000      0.000000   \n",
       "50%            0.000000      2.000000       0.000000      0.000000   \n",
       "75%            0.000000      5.000000     253.000000      1.000000   \n",
       "max       114363.000000    192.000000  175495.000000    136.000000   \n",
       "\n",
       "       CommentCount  FavoriteCount  LastEditorUserId      ParentId  \n",
       "count  38962.000000    38962.00000      38962.000000  38962.000000  \n",
       "mean       2.014630        0.61098       2318.515117  11951.882167  \n",
       "std        2.674018        3.52762       4673.215379  14494.357588  \n",
       "min        0.000000        0.00000         -1.000000      0.000000  \n",
       "25%        0.000000        0.00000          0.000000      0.000000  \n",
       "50%        1.000000        0.00000          0.000000   4620.000000  \n",
       "75%        3.000000        0.00000       2116.000000  22468.000000  \n",
       "max       45.000000      233.00000      53985.000000  48314.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OUTLIERS\n",
    "\n",
    "users_posts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId               -11184.000\n",
       "Reputation           -11529.000\n",
       "Views                 -1535.000\n",
       "UpVotes                -863.000\n",
       "DownVotes               -24.000\n",
       "AccountId          -1245672.500\n",
       "Age                     -43.500\n",
       "postId               -22219.625\n",
       "PostTypeId               -0.500\n",
       "AcceptedAnswerId          0.000\n",
       "Score                    -5.000\n",
       "ViewCount              -379.500\n",
       "AnswerCount              -1.500\n",
       "CommentCount             -4.500\n",
       "FavoriteCount             0.000\n",
       "LastEditorUserId      -3174.000\n",
       "ParentId             -33702.000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "userId                22152.000\n",
       "Reputation            19607.000\n",
       "Views                  2601.000\n",
       "UpVotes                1449.000\n",
       "DownVotes                40.000\n",
       "AccountId           2493595.500\n",
       "Age                      72.500\n",
       "postId                67233.375\n",
       "PostTypeId                3.500\n",
       "AcceptedAnswerId          0.000\n",
       "Score                    11.000\n",
       "ViewCount               632.500\n",
       "AnswerCount               2.500\n",
       "CommentCount              7.500\n",
       "FavoriteCount             0.000\n",
       "LastEditorUserId       5290.000\n",
       "ParentId              56170.000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Q3 = users_posts.quantile(0.75)\n",
    "Q1 = users_posts.quantile(0.25)\n",
    "IQR = Q3 - Q1\n",
    "lower_limit = Q1 - (IQR * 1.5) \n",
    "upper_limit = Q3 + (IQR * 1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24736/790282299.py:1: FutureWarning: Automatic reindexing on DataFrame vs Series comparisons is deprecated and will raise ValueError in a future version. Do `left, right = left.align(right, axis=1, copy=False)` before e.g. `left == right`\n",
      "  outliers = users_posts[((users_posts < (Q1 - 1.5 * IQR)) |(users_posts > (Q3 + 1.5 * IQR))).any(axis=1)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>...</th>\n",
       "      <th>Body</th>\n",
       "      <th>LasActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>ParentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;CrossValidated&lt;/strong&gt; is for stat...</td>\n",
       "      <td>2013-01-10 19:43:24</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2014-04-23 13:43:43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>2011-03-21 17:40:28</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2011-03-21 17:40:28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>2011-03-21 17:46:43</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2011-03-21 17:46:43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;p&gt;\"Statistics\" can refer variously to the (wi...</td>\n",
       "      <td>2011-03-30 19:23:14</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>919</td>\n",
       "      <td>2011-03-30 19:23:14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>...</td>\n",
       "      <td>This generic tag is only rarely suitable; use ...</td>\n",
       "      <td>2011-03-30 19:23:14</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>919</td>\n",
       "      <td>2011-03-30 19:23:14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38957</th>\n",
       "      <td>45934</td>\n",
       "      <td>11</td>\n",
       "      <td>2014-05-21 17:13:23</td>\n",
       "      <td>jasonweiyi</td>\n",
       "      <td>2014-06-08 13:36:47</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;p&gt;I built a suggestion system, which provides...</td>\n",
       "      <td>2014-02-09 12:34:41</td>\n",
       "      <td>Statistical test for a gradually self-improvin...</td>\n",
       "      <td>&lt;hypothesis-testing&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>805</td>\n",
       "      <td>2013-05-13 04:41:41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38958</th>\n",
       "      <td>46192</td>\n",
       "      <td>36</td>\n",
       "      <td>2014-05-26 15:29:30</td>\n",
       "      <td>user1738753</td>\n",
       "      <td>2014-09-10 19:52:34</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;p&gt;Currently, I am working on a count data set...</td>\n",
       "      <td>2012-10-18 15:11:09</td>\n",
       "      <td>Testing for Spatial Autocorrelation in a Negat...</td>\n",
       "      <td>&lt;r&gt;&lt;spatial&gt;&lt;correlation&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>930</td>\n",
       "      <td>2012-10-18 15:07:40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38959</th>\n",
       "      <td>46522</td>\n",
       "      <td>235</td>\n",
       "      <td>2014-06-01 17:25:18</td>\n",
       "      <td>Andy Blankertz</td>\n",
       "      <td>2014-09-13 18:03:07</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>&lt;p&gt;Actuary for a life insurance company.&lt;/p&gt;\\r\\n</td>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;p&gt;I'm studying a data set in R using both reg...</td>\n",
       "      <td>2011-10-24 12:21:54</td>\n",
       "      <td>Mismatch between significant variables from lo...</td>\n",
       "      <td>&lt;r&gt;&lt;modeling&gt;&lt;classification&gt;&lt;regression&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0001-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38960</th>\n",
       "      <td>52371</td>\n",
       "      <td>221</td>\n",
       "      <td>2014-07-19 13:36:58</td>\n",
       "      <td>Karel Petranek</td>\n",
       "      <td>2014-07-20 09:32:16</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;p&gt;When doing research in Economy, one frequen...</td>\n",
       "      <td>2012-04-27 13:48:30</td>\n",
       "      <td>What are the most useful sources of economics ...</td>\n",
       "      <td>&lt;sources&gt;</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0001-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38961</th>\n",
       "      <td>55226</td>\n",
       "      <td>119</td>\n",
       "      <td>2014-09-04 07:34:48</td>\n",
       "      <td>Klas Lindbäck</td>\n",
       "      <td>2014-09-08 11:07:33</td>\n",
       "      <td>http://N/A</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>&lt;p&gt;Working with a wide range of languages, but...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;p&gt;How about making a solution based on the Si...</td>\n",
       "      <td>2011-09-28 07:26:03</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0001-01-01 00:00:00</td>\n",
       "      <td>16174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23664 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  Reputation         CreationDate     DisplayName  \\\n",
       "0          -1           1  2010-07-19 06:55:26       Community   \n",
       "1          -1           1  2010-07-19 06:55:26       Community   \n",
       "2          -1           1  2010-07-19 06:55:26       Community   \n",
       "3          -1           1  2010-07-19 06:55:26       Community   \n",
       "4          -1           1  2010-07-19 06:55:26       Community   \n",
       "...       ...         ...                  ...             ...   \n",
       "38957   45934          11  2014-05-21 17:13:23      jasonweiyi   \n",
       "38958   46192          36  2014-05-26 15:29:30     user1738753   \n",
       "38959   46522         235  2014-06-01 17:25:18  Andy Blankertz   \n",
       "38960   52371         221  2014-07-19 13:36:58  Karel Petranek   \n",
       "38961   55226         119  2014-09-04 07:34:48   Klas Lindbäck   \n",
       "\n",
       "            LastAccessDate                      WebsiteUrl  \\\n",
       "0      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "1      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "2      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "3      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "4      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "...                    ...                             ...   \n",
       "38957  2014-06-08 13:36:47                                   \n",
       "38958  2014-09-10 19:52:34                                   \n",
       "38959  2014-09-13 18:03:07                                   \n",
       "38960  2014-07-20 09:32:16                                   \n",
       "38961  2014-09-08 11:07:33                      http://N/A   \n",
       "\n",
       "                 Location                                            AboutMe  \\\n",
       "0      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "1      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "2      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "3      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "4      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "...                   ...                                                ...   \n",
       "38957                                                                          \n",
       "38958                                                                          \n",
       "38959                       <p>Actuary for a life insurance company.</p>\\r\\n   \n",
       "38960                                                                          \n",
       "38961              Sweden  <p>Working with a wide range of languages, but...   \n",
       "\n",
       "       Views  UpVotes  ...                                               Body  \\\n",
       "0          0     5007  ...  <p><strong>CrossValidated</strong> is for stat...   \n",
       "1          0     5007  ...                                                      \n",
       "2          0     5007  ...                                                      \n",
       "3          0     5007  ...  <p>\"Statistics\" can refer variously to the (wi...   \n",
       "4          0     5007  ...  This generic tag is only rarely suitable; use ...   \n",
       "...      ...      ...  ...                                                ...   \n",
       "38957      1        0  ...  <p>I built a suggestion system, which provides...   \n",
       "38958      1        0  ...  <p>Currently, I am working on a count data set...   \n",
       "38959     13       27  ...  <p>I'm studying a data set in R using both reg...   \n",
       "38960      2        0  ...  <p>When doing research in Economy, one frequen...   \n",
       "38961      2        3  ...  <p>How about making a solution based on the Si...   \n",
       "\n",
       "           LasActivityDate                                              Title  \\\n",
       "0      2013-01-10 19:43:24                                                      \n",
       "1      2011-03-21 17:40:28                                                      \n",
       "2      2011-03-21 17:46:43                                                      \n",
       "3      2011-03-30 19:23:14                                                      \n",
       "4      2011-03-30 19:23:14                                                      \n",
       "...                    ...                                                ...   \n",
       "38957  2014-02-09 12:34:41  Statistical test for a gradually self-improvin...   \n",
       "38958  2012-10-18 15:11:09  Testing for Spatial Autocorrelation in a Negat...   \n",
       "38959  2011-10-24 12:21:54  Mismatch between significant variables from lo...   \n",
       "38960  2012-04-27 13:48:30  What are the most useful sources of economics ...   \n",
       "38961  2011-09-28 07:26:03                                                      \n",
       "\n",
       "                                            Tags  AnswerCount  CommentCount  \\\n",
       "0                                                           0             0   \n",
       "1                                                           0             0   \n",
       "2                                                           0             0   \n",
       "3                                                           0             0   \n",
       "4                                                           0             0   \n",
       "...                                          ...          ...           ...   \n",
       "38957                       <hypothesis-testing>            1             2   \n",
       "38958                  <r><spatial><correlation>            1             2   \n",
       "38959  <r><modeling><classification><regression>            3             0   \n",
       "38960                                  <sources>            7             5   \n",
       "38961                                                       0             0   \n",
       "\n",
       "      FavoriteCount  LastEditorUserId         LastEditDate ParentId  \n",
       "0                 0                -1  2014-04-23 13:43:43        0  \n",
       "1                 0                -1  2011-03-21 17:40:28        0  \n",
       "2                 0                -1  2011-03-21 17:46:43        0  \n",
       "3                 0               919  2011-03-30 19:23:14        0  \n",
       "4                 0               919  2011-03-30 19:23:14        0  \n",
       "...             ...               ...                  ...      ...  \n",
       "38957             0               805  2013-05-13 04:41:41        0  \n",
       "38958             1               930  2012-10-18 15:07:40        0  \n",
       "38959             0                 0  0001-01-01 00:00:00        0  \n",
       "38960             8                 0  0001-01-01 00:00:00        0  \n",
       "38961             0                 0  0001-01-01 00:00:00    16174  \n",
       "\n",
       "[23664 rows x 29 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers = users_posts[((users_posts < (Q1 - 1.5 * IQR)) |(users_posts > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
