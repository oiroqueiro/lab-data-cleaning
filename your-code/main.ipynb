{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Import users table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patool: Extracting datasets.rar ...\n",
      "patool: running /usr/bin/unrar x -- \"/home/roque/01. IronHack/00. Data Analytics/01. Course/05. Week 2 - Day 2/git/lab-data-cleaning/your-code/datasets.rar\"\n",
      "patool:     with cwd='./Unpack_rjk9aesl'\n",
      "patool: ... datasets.rar extracted to `datasets2' (multiple files in root).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'datasets2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import patoolib\n",
    "patoolib.extract_archive('datasets.rar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>DownVotes</th>\n",
       "      <th>AccountId</th>\n",
       "      <th>Age</th>\n",
       "      <th>ProfileImageUrl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>1920</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "      <td>2010-07-19 14:01:36</td>\n",
       "      <td>Geoff Dalgas</td>\n",
       "      <td>2013-11-12 22:07:23</td>\n",
       "      <td>http://stackoverflow.com</td>\n",
       "      <td>Corvallis, OR</td>\n",
       "      <td>&lt;p&gt;Developer on the StackOverflow team.  Find ...</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>101</td>\n",
       "      <td>2010-07-19 15:34:50</td>\n",
       "      <td>Jarrod Dixon</td>\n",
       "      <td>2014-08-08 06:42:58</td>\n",
       "      <td>http://stackoverflow.com</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>&lt;p&gt;&lt;a href=\"http://blog.stackoverflow.com/2009...</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>101</td>\n",
       "      <td>2010-07-19 19:03:27</td>\n",
       "      <td>Emmett</td>\n",
       "      <td>2014-01-02 09:31:02</td>\n",
       "      <td>http://minesweeperonline.com</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>&lt;p&gt;currently at a startup in SF&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1998</td>\n",
       "      <td>28.0</td>\n",
       "      <td>http://i.stack.imgur.com/d1oHX.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6792</td>\n",
       "      <td>2010-07-19 19:03:57</td>\n",
       "      <td>Shane</td>\n",
       "      <td>2014-08-13 00:23:47</td>\n",
       "      <td>http://www.statalgo.com</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>&lt;p&gt;Quantitative researcher focusing on statist...</td>\n",
       "      <td>1145</td>\n",
       "      <td>662</td>\n",
       "      <td>5</td>\n",
       "      <td>54503</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40320</th>\n",
       "      <td>55743</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-09-13 21:03:50</td>\n",
       "      <td>AussieMeg</td>\n",
       "      <td>2014-09-13 21:18:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5026902</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://graph.facebook.com/665821703/picture?ty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40321</th>\n",
       "      <td>55744</td>\n",
       "      <td>6</td>\n",
       "      <td>2014-09-13 21:39:30</td>\n",
       "      <td>Mia Maria</td>\n",
       "      <td>2014-09-13 21:39:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5026998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40322</th>\n",
       "      <td>55745</td>\n",
       "      <td>101</td>\n",
       "      <td>2014-09-13 23:45:27</td>\n",
       "      <td>tronbabylove</td>\n",
       "      <td>2014-09-13 23:45:27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>481766</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.gravatar.com/avatar/faa7a3fdbd8308...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40323</th>\n",
       "      <td>55746</td>\n",
       "      <td>106</td>\n",
       "      <td>2014-09-14 00:29:41</td>\n",
       "      <td>GPP</td>\n",
       "      <td>2014-09-14 02:05:17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Stats noobie, product, marketing &amp;amp; medi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>976289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.gravatar.com/avatar/6d9e9fa6b783a3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40324</th>\n",
       "      <td>55747</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-09-14 01:01:44</td>\n",
       "      <td>Shivam Agrawal</td>\n",
       "      <td>2014-09-14 01:19:04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>India</td>\n",
       "      <td>&lt;p&gt;Maths Enthusiast &lt;/p&gt;\\r\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5027354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://lh4.googleusercontent.com/-ZsXhwVaFmiY...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40325 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id  Reputation         CreationDate     DisplayName  \\\n",
       "0         -1           1  2010-07-19 06:55:26       Community   \n",
       "1          2         101  2010-07-19 14:01:36    Geoff Dalgas   \n",
       "2          3         101  2010-07-19 15:34:50    Jarrod Dixon   \n",
       "3          4         101  2010-07-19 19:03:27          Emmett   \n",
       "4          5        6792  2010-07-19 19:03:57           Shane   \n",
       "...      ...         ...                  ...             ...   \n",
       "40320  55743           1  2014-09-13 21:03:50       AussieMeg   \n",
       "40321  55744           6  2014-09-13 21:39:30       Mia Maria   \n",
       "40322  55745         101  2014-09-13 23:45:27    tronbabylove   \n",
       "40323  55746         106  2014-09-14 00:29:41             GPP   \n",
       "40324  55747           1  2014-09-14 01:01:44  Shivam Agrawal   \n",
       "\n",
       "            LastAccessDate                      WebsiteUrl  \\\n",
       "0      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "1      2013-11-12 22:07:23        http://stackoverflow.com   \n",
       "2      2014-08-08 06:42:58        http://stackoverflow.com   \n",
       "3      2014-01-02 09:31:02    http://minesweeperonline.com   \n",
       "4      2014-08-13 00:23:47         http://www.statalgo.com   \n",
       "...                    ...                             ...   \n",
       "40320  2014-09-13 21:18:52                             NaN   \n",
       "40321  2014-09-13 21:39:30                             NaN   \n",
       "40322  2014-09-13 23:45:27                             NaN   \n",
       "40323  2014-09-14 02:05:17                             NaN   \n",
       "40324  2014-09-14 01:19:04                             NaN   \n",
       "\n",
       "                 Location                                            AboutMe  \\\n",
       "0      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "1           Corvallis, OR  <p>Developer on the StackOverflow team.  Find ...   \n",
       "2            New York, NY  <p><a href=\"http://blog.stackoverflow.com/2009...   \n",
       "3       San Francisco, CA  <p>currently at a startup in SF</p>\\r\\n\\r\\n<p>...   \n",
       "4            New York, NY  <p>Quantitative researcher focusing on statist...   \n",
       "...                   ...                                                ...   \n",
       "40320                 NaN                                                NaN   \n",
       "40321                 NaN                                                NaN   \n",
       "40322       United States                                                NaN   \n",
       "40323                 NaN  <p>Stats noobie, product, marketing &amp; medi...   \n",
       "40324               India                       <p>Maths Enthusiast </p>\\r\\n   \n",
       "\n",
       "       Views  UpVotes  DownVotes  AccountId   Age  \\\n",
       "0          0     5007       1920         -1   NaN   \n",
       "1         25        3          0          2  37.0   \n",
       "2         22       19          0          3  35.0   \n",
       "3         11        0          0       1998  28.0   \n",
       "4       1145      662          5      54503  35.0   \n",
       "...      ...      ...        ...        ...   ...   \n",
       "40320      0        0          0    5026902   NaN   \n",
       "40321      1        0          0    5026998   NaN   \n",
       "40322      0        0          0     481766   NaN   \n",
       "40323      1        0          0     976289   NaN   \n",
       "40324      0        0          0    5027354   NaN   \n",
       "\n",
       "                                         ProfileImageUrl  \n",
       "0                                                    NaN  \n",
       "1                                                    NaN  \n",
       "2                                                    NaN  \n",
       "3                     http://i.stack.imgur.com/d1oHX.jpg  \n",
       "4                                                    NaN  \n",
       "...                                                  ...  \n",
       "40320  http://graph.facebook.com/665821703/picture?ty...  \n",
       "40321                                                NaN  \n",
       "40322  https://www.gravatar.com/avatar/faa7a3fdbd8308...  \n",
       "40323  https://www.gravatar.com/avatar/6d9e9fa6b783a3...  \n",
       "40324  https://lh4.googleusercontent.com/-ZsXhwVaFmiY...  \n",
       "\n",
       "[40325 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = pd.read_csv('datasets/users_table.csv')\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Rename Id column to userId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>DownVotes</th>\n",
       "      <th>AccountId</th>\n",
       "      <th>Age</th>\n",
       "      <th>ProfileImageUrl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>1920</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "      <td>2010-07-19 14:01:36</td>\n",
       "      <td>Geoff Dalgas</td>\n",
       "      <td>2013-11-12 22:07:23</td>\n",
       "      <td>http://stackoverflow.com</td>\n",
       "      <td>Corvallis, OR</td>\n",
       "      <td>&lt;p&gt;Developer on the StackOverflow team.  Find ...</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>101</td>\n",
       "      <td>2010-07-19 15:34:50</td>\n",
       "      <td>Jarrod Dixon</td>\n",
       "      <td>2014-08-08 06:42:58</td>\n",
       "      <td>http://stackoverflow.com</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>&lt;p&gt;&lt;a href=\"http://blog.stackoverflow.com/2009...</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>101</td>\n",
       "      <td>2010-07-19 19:03:27</td>\n",
       "      <td>Emmett</td>\n",
       "      <td>2014-01-02 09:31:02</td>\n",
       "      <td>http://minesweeperonline.com</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>&lt;p&gt;currently at a startup in SF&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1998</td>\n",
       "      <td>28.0</td>\n",
       "      <td>http://i.stack.imgur.com/d1oHX.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6792</td>\n",
       "      <td>2010-07-19 19:03:57</td>\n",
       "      <td>Shane</td>\n",
       "      <td>2014-08-13 00:23:47</td>\n",
       "      <td>http://www.statalgo.com</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>&lt;p&gt;Quantitative researcher focusing on statist...</td>\n",
       "      <td>1145</td>\n",
       "      <td>662</td>\n",
       "      <td>5</td>\n",
       "      <td>54503</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40320</th>\n",
       "      <td>55743</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-09-13 21:03:50</td>\n",
       "      <td>AussieMeg</td>\n",
       "      <td>2014-09-13 21:18:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5026902</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://graph.facebook.com/665821703/picture?ty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40321</th>\n",
       "      <td>55744</td>\n",
       "      <td>6</td>\n",
       "      <td>2014-09-13 21:39:30</td>\n",
       "      <td>Mia Maria</td>\n",
       "      <td>2014-09-13 21:39:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5026998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40322</th>\n",
       "      <td>55745</td>\n",
       "      <td>101</td>\n",
       "      <td>2014-09-13 23:45:27</td>\n",
       "      <td>tronbabylove</td>\n",
       "      <td>2014-09-13 23:45:27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>481766</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.gravatar.com/avatar/faa7a3fdbd8308...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40323</th>\n",
       "      <td>55746</td>\n",
       "      <td>106</td>\n",
       "      <td>2014-09-14 00:29:41</td>\n",
       "      <td>GPP</td>\n",
       "      <td>2014-09-14 02:05:17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Stats noobie, product, marketing &amp;amp; medi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>976289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.gravatar.com/avatar/6d9e9fa6b783a3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40324</th>\n",
       "      <td>55747</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-09-14 01:01:44</td>\n",
       "      <td>Shivam Agrawal</td>\n",
       "      <td>2014-09-14 01:19:04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>India</td>\n",
       "      <td>&lt;p&gt;Maths Enthusiast &lt;/p&gt;\\r\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5027354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://lh4.googleusercontent.com/-ZsXhwVaFmiY...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40325 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  Reputation         CreationDate     DisplayName  \\\n",
       "0          -1           1  2010-07-19 06:55:26       Community   \n",
       "1           2         101  2010-07-19 14:01:36    Geoff Dalgas   \n",
       "2           3         101  2010-07-19 15:34:50    Jarrod Dixon   \n",
       "3           4         101  2010-07-19 19:03:27          Emmett   \n",
       "4           5        6792  2010-07-19 19:03:57           Shane   \n",
       "...       ...         ...                  ...             ...   \n",
       "40320   55743           1  2014-09-13 21:03:50       AussieMeg   \n",
       "40321   55744           6  2014-09-13 21:39:30       Mia Maria   \n",
       "40322   55745         101  2014-09-13 23:45:27    tronbabylove   \n",
       "40323   55746         106  2014-09-14 00:29:41             GPP   \n",
       "40324   55747           1  2014-09-14 01:01:44  Shivam Agrawal   \n",
       "\n",
       "            LastAccessDate                      WebsiteUrl  \\\n",
       "0      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "1      2013-11-12 22:07:23        http://stackoverflow.com   \n",
       "2      2014-08-08 06:42:58        http://stackoverflow.com   \n",
       "3      2014-01-02 09:31:02    http://minesweeperonline.com   \n",
       "4      2014-08-13 00:23:47         http://www.statalgo.com   \n",
       "...                    ...                             ...   \n",
       "40320  2014-09-13 21:18:52                             NaN   \n",
       "40321  2014-09-13 21:39:30                             NaN   \n",
       "40322  2014-09-13 23:45:27                             NaN   \n",
       "40323  2014-09-14 02:05:17                             NaN   \n",
       "40324  2014-09-14 01:19:04                             NaN   \n",
       "\n",
       "                 Location                                            AboutMe  \\\n",
       "0      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "1           Corvallis, OR  <p>Developer on the StackOverflow team.  Find ...   \n",
       "2            New York, NY  <p><a href=\"http://blog.stackoverflow.com/2009...   \n",
       "3       San Francisco, CA  <p>currently at a startup in SF</p>\\r\\n\\r\\n<p>...   \n",
       "4            New York, NY  <p>Quantitative researcher focusing on statist...   \n",
       "...                   ...                                                ...   \n",
       "40320                 NaN                                                NaN   \n",
       "40321                 NaN                                                NaN   \n",
       "40322       United States                                                NaN   \n",
       "40323                 NaN  <p>Stats noobie, product, marketing &amp; medi...   \n",
       "40324               India                       <p>Maths Enthusiast </p>\\r\\n   \n",
       "\n",
       "       Views  UpVotes  DownVotes  AccountId   Age  \\\n",
       "0          0     5007       1920         -1   NaN   \n",
       "1         25        3          0          2  37.0   \n",
       "2         22       19          0          3  35.0   \n",
       "3         11        0          0       1998  28.0   \n",
       "4       1145      662          5      54503  35.0   \n",
       "...      ...      ...        ...        ...   ...   \n",
       "40320      0        0          0    5026902   NaN   \n",
       "40321      1        0          0    5026998   NaN   \n",
       "40322      0        0          0     481766   NaN   \n",
       "40323      1        0          0     976289   NaN   \n",
       "40324      0        0          0    5027354   NaN   \n",
       "\n",
       "                                         ProfileImageUrl  \n",
       "0                                                    NaN  \n",
       "1                                                    NaN  \n",
       "2                                                    NaN  \n",
       "3                     http://i.stack.imgur.com/d1oHX.jpg  \n",
       "4                                                    NaN  \n",
       "...                                                  ...  \n",
       "40320  http://graph.facebook.com/665821703/picture?ty...  \n",
       "40321                                                NaN  \n",
       "40322  https://www.gravatar.com/avatar/faa7a3fdbd8308...  \n",
       "40323  https://www.gravatar.com/avatar/6d9e9fa6b783a3...  \n",
       "40324  https://lh4.googleusercontent.com/-ZsXhwVaFmiY...  \n",
       "\n",
       "[40325 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = users.rename(columns={'Id':'userId'})\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Import posts table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>CreaionDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Body</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>LasActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>...</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>CommunityOwnedDate</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>ClosedDate</th>\n",
       "      <th>OwnerDisplayName</th>\n",
       "      <th>LastEditorDisplayName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2010-07-19 19:12:12</td>\n",
       "      <td>23</td>\n",
       "      <td>1278.0</td>\n",
       "      <td>&lt;p&gt;How should I elicit prior distributions fro...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2010-09-15 21:08:26</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2010-07-19 19:12:57</td>\n",
       "      <td>22</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>&lt;p&gt;In many different statistical methods there...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2012-11-12 09:21:54</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2010-08-07 17:56:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2010-07-19 19:13:28</td>\n",
       "      <td>54</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>&lt;p&gt;What are some valuable Statistical Analysis...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2013-05-27 14:48:36</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4</td>\n",
       "      <td>36.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2011-02-12 05:50:03</td>\n",
       "      <td>2010-07-19 19:13:28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>135.0</td>\n",
       "      <td>2010-07-19 19:13:31</td>\n",
       "      <td>13</td>\n",
       "      <td>5224.0</td>\n",
       "      <td>&lt;p&gt;I have two groups of data.  Each with a dif...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2010-09-08 03:00:19</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-07-19 19:14:43</td>\n",
       "      <td>81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;The R-project&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\"http://www...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2010-07-19 19:21:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2010-07-19 19:21:15</td>\n",
       "      <td>2010-07-19 19:14:43</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>48321</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-23 09:00:01</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;you can use the matlab codes for svm and co...</td>\n",
       "      <td>19966.0</td>\n",
       "      <td>2013-01-23 09:00:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45118.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>48322</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-23 09:09:34</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;I use &lt;a href=\"http://www.gnu.org/software/...</td>\n",
       "      <td>892.0</td>\n",
       "      <td>2013-01-23 13:13:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>892.0</td>\n",
       "      <td>2013-01-23 13:13:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48311.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>48323</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-23 09:16:44</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;If I understand your question correctly, yo...</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>2013-01-23 09:16:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48247.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>48324</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-23 09:36:07</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Doesn't really help you with your question,...</td>\n",
       "      <td>19914.0</td>\n",
       "      <td>2013-01-23 09:36:07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48297.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>48325</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-23 09:44:07</td>\n",
       "      <td>-1</td>\n",
       "      <td>116.0</td>\n",
       "      <td>&lt;p&gt;I have 10 vectors each having 100,000 point...</td>\n",
       "      <td>19968.0</td>\n",
       "      <td>2013-02-22 11:23:54</td>\n",
       "      <td>are data sets obtained from a Normal distribut...</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id  PostTypeId  AcceptedAnswerId          CreaionDate  Score  \\\n",
       "0          1           1              15.0  2010-07-19 19:12:12     23   \n",
       "1          2           1              59.0  2010-07-19 19:12:57     22   \n",
       "2          3           1               5.0  2010-07-19 19:13:28     54   \n",
       "3          4           1             135.0  2010-07-19 19:13:31     13   \n",
       "4          5           2               NaN  2010-07-19 19:14:43     81   \n",
       "...      ...         ...               ...                  ...    ...   \n",
       "39995  48321           2               NaN  2013-01-23 09:00:01      0   \n",
       "39996  48322           2               NaN  2013-01-23 09:09:34      3   \n",
       "39997  48323           2               NaN  2013-01-23 09:16:44      1   \n",
       "39998  48324           2               NaN  2013-01-23 09:36:07      3   \n",
       "39999  48325           1               NaN  2013-01-23 09:44:07     -1   \n",
       "\n",
       "       ViewCount                                               Body  \\\n",
       "0         1278.0  <p>How should I elicit prior distributions fro...   \n",
       "1         8198.0  <p>In many different statistical methods there...   \n",
       "2         3613.0  <p>What are some valuable Statistical Analysis...   \n",
       "3         5224.0  <p>I have two groups of data.  Each with a dif...   \n",
       "4            NaN  <p>The R-project</p>\\n\\n<p><a href=\"http://www...   \n",
       "...          ...                                                ...   \n",
       "39995        NaN  <p>you can use the matlab codes for svm and co...   \n",
       "39996        NaN  <p>I use <a href=\"http://www.gnu.org/software/...   \n",
       "39997        NaN  <p>If I understand your question correctly, yo...   \n",
       "39998        NaN  <p>Doesn't really help you with your question,...   \n",
       "39999      116.0  <p>I have 10 vectors each having 100,000 point...   \n",
       "\n",
       "       OwnerUserId      LasActivityDate  \\\n",
       "0              8.0  2010-09-15 21:08:26   \n",
       "1             24.0  2012-11-12 09:21:54   \n",
       "2             18.0  2013-05-27 14:48:36   \n",
       "3             23.0  2010-09-08 03:00:19   \n",
       "4             23.0  2010-07-19 19:21:15   \n",
       "...            ...                  ...   \n",
       "39995      19966.0  2013-01-23 09:00:01   \n",
       "39996        892.0  2013-01-23 13:13:30   \n",
       "39997       2020.0  2013-01-23 09:16:44   \n",
       "39998      19914.0  2013-01-23 09:36:07   \n",
       "39999      19968.0  2013-02-22 11:23:54   \n",
       "\n",
       "                                                   Title  ... AnswerCount  \\\n",
       "0                          Eliciting priors from experts  ...         5.0   \n",
       "1                                     What is normality?  ...         7.0   \n",
       "2      What are some valuable Statistical Analysis op...  ...        19.0   \n",
       "3      Assessing the significance of differences in d...  ...         5.0   \n",
       "4                                                    NaN  ...         NaN   \n",
       "...                                                  ...  ...         ...   \n",
       "39995                                                NaN  ...         NaN   \n",
       "39996                                                NaN  ...         NaN   \n",
       "39997                                                NaN  ...         NaN   \n",
       "39998                                                NaN  ...         NaN   \n",
       "39999  are data sets obtained from a Normal distribut...  ...         2.0   \n",
       "\n",
       "       CommentCount  FavoriteCount  LastEditorUserId         LastEditDate  \\\n",
       "0                 1           14.0               NaN                  NaN   \n",
       "1                 1            8.0              88.0  2010-08-07 17:56:44   \n",
       "2                 4           36.0             183.0  2011-02-12 05:50:03   \n",
       "3                 2            2.0               NaN                  NaN   \n",
       "4                 3            NaN              23.0  2010-07-19 19:21:15   \n",
       "...             ...            ...               ...                  ...   \n",
       "39995             0            NaN               NaN                  NaN   \n",
       "39996             2            NaN             892.0  2013-01-23 13:13:30   \n",
       "39997             0            NaN               NaN                  NaN   \n",
       "39998             0            NaN               NaN                  NaN   \n",
       "39999             4            NaN               NaN                  NaN   \n",
       "\n",
       "        CommunityOwnedDate ParentId  ClosedDate OwnerDisplayName  \\\n",
       "0                      NaN      NaN         NaN              NaN   \n",
       "1                      NaN      NaN         NaN              NaN   \n",
       "2      2010-07-19 19:13:28      NaN         NaN              NaN   \n",
       "3                      NaN      NaN         NaN              NaN   \n",
       "4      2010-07-19 19:14:43      3.0         NaN              NaN   \n",
       "...                    ...      ...         ...              ...   \n",
       "39995                  NaN  45118.0         NaN              NaN   \n",
       "39996                  NaN  48311.0         NaN              NaN   \n",
       "39997                  NaN  48247.0         NaN              NaN   \n",
       "39998                  NaN  48297.0         NaN              NaN   \n",
       "39999                  NaN      NaN         NaN              NaN   \n",
       "\n",
       "      LastEditorDisplayName  \n",
       "0                       NaN  \n",
       "1                       NaN  \n",
       "2                       NaN  \n",
       "3                       NaN  \n",
       "4                       NaN  \n",
       "...                     ...  \n",
       "39995                   NaN  \n",
       "39996                   NaN  \n",
       "39997                   NaN  \n",
       "39998                   NaN  \n",
       "39999                   NaN  \n",
       "\n",
       "[40000 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = pd.read_csv('datasets/posts_table.csv')\n",
    "posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Rename Id column to postId and OwnerUserId to userId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postId</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>CreaionDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Body</th>\n",
       "      <th>userId</th>\n",
       "      <th>LasActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>...</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>CommunityOwnedDate</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>ClosedDate</th>\n",
       "      <th>OwnerDisplayName</th>\n",
       "      <th>LastEditorDisplayName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2010-07-19 19:12:12</td>\n",
       "      <td>23</td>\n",
       "      <td>1278.0</td>\n",
       "      <td>&lt;p&gt;How should I elicit prior distributions fro...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2010-09-15 21:08:26</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2010-07-19 19:12:57</td>\n",
       "      <td>22</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>&lt;p&gt;In many different statistical methods there...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2012-11-12 09:21:54</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2010-08-07 17:56:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2010-07-19 19:13:28</td>\n",
       "      <td>54</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>&lt;p&gt;What are some valuable Statistical Analysis...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2013-05-27 14:48:36</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4</td>\n",
       "      <td>36.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2011-02-12 05:50:03</td>\n",
       "      <td>2010-07-19 19:13:28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>135.0</td>\n",
       "      <td>2010-07-19 19:13:31</td>\n",
       "      <td>13</td>\n",
       "      <td>5224.0</td>\n",
       "      <td>&lt;p&gt;I have two groups of data.  Each with a dif...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2010-09-08 03:00:19</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-07-19 19:14:43</td>\n",
       "      <td>81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;The R-project&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\"http://www...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2010-07-19 19:21:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2010-07-19 19:21:15</td>\n",
       "      <td>2010-07-19 19:14:43</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>48321</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-23 09:00:01</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;you can use the matlab codes for svm and co...</td>\n",
       "      <td>19966.0</td>\n",
       "      <td>2013-01-23 09:00:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45118.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>48322</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-23 09:09:34</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;I use &lt;a href=\"http://www.gnu.org/software/...</td>\n",
       "      <td>892.0</td>\n",
       "      <td>2013-01-23 13:13:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>892.0</td>\n",
       "      <td>2013-01-23 13:13:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48311.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>48323</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-23 09:16:44</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;If I understand your question correctly, yo...</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>2013-01-23 09:16:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48247.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>48324</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-23 09:36:07</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Doesn't really help you with your question,...</td>\n",
       "      <td>19914.0</td>\n",
       "      <td>2013-01-23 09:36:07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48297.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>48325</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-23 09:44:07</td>\n",
       "      <td>-1</td>\n",
       "      <td>116.0</td>\n",
       "      <td>&lt;p&gt;I have 10 vectors each having 100,000 point...</td>\n",
       "      <td>19968.0</td>\n",
       "      <td>2013-02-22 11:23:54</td>\n",
       "      <td>are data sets obtained from a Normal distribut...</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       postId  PostTypeId  AcceptedAnswerId          CreaionDate  Score  \\\n",
       "0           1           1              15.0  2010-07-19 19:12:12     23   \n",
       "1           2           1              59.0  2010-07-19 19:12:57     22   \n",
       "2           3           1               5.0  2010-07-19 19:13:28     54   \n",
       "3           4           1             135.0  2010-07-19 19:13:31     13   \n",
       "4           5           2               NaN  2010-07-19 19:14:43     81   \n",
       "...       ...         ...               ...                  ...    ...   \n",
       "39995   48321           2               NaN  2013-01-23 09:00:01      0   \n",
       "39996   48322           2               NaN  2013-01-23 09:09:34      3   \n",
       "39997   48323           2               NaN  2013-01-23 09:16:44      1   \n",
       "39998   48324           2               NaN  2013-01-23 09:36:07      3   \n",
       "39999   48325           1               NaN  2013-01-23 09:44:07     -1   \n",
       "\n",
       "       ViewCount                                               Body   userId  \\\n",
       "0         1278.0  <p>How should I elicit prior distributions fro...      8.0   \n",
       "1         8198.0  <p>In many different statistical methods there...     24.0   \n",
       "2         3613.0  <p>What are some valuable Statistical Analysis...     18.0   \n",
       "3         5224.0  <p>I have two groups of data.  Each with a dif...     23.0   \n",
       "4            NaN  <p>The R-project</p>\\n\\n<p><a href=\"http://www...     23.0   \n",
       "...          ...                                                ...      ...   \n",
       "39995        NaN  <p>you can use the matlab codes for svm and co...  19966.0   \n",
       "39996        NaN  <p>I use <a href=\"http://www.gnu.org/software/...    892.0   \n",
       "39997        NaN  <p>If I understand your question correctly, yo...   2020.0   \n",
       "39998        NaN  <p>Doesn't really help you with your question,...  19914.0   \n",
       "39999      116.0  <p>I have 10 vectors each having 100,000 point...  19968.0   \n",
       "\n",
       "           LasActivityDate                                              Title  \\\n",
       "0      2010-09-15 21:08:26                      Eliciting priors from experts   \n",
       "1      2012-11-12 09:21:54                                 What is normality?   \n",
       "2      2013-05-27 14:48:36  What are some valuable Statistical Analysis op...   \n",
       "3      2010-09-08 03:00:19  Assessing the significance of differences in d...   \n",
       "4      2010-07-19 19:21:15                                                NaN   \n",
       "...                    ...                                                ...   \n",
       "39995  2013-01-23 09:00:01                                                NaN   \n",
       "39996  2013-01-23 13:13:30                                                NaN   \n",
       "39997  2013-01-23 09:16:44                                                NaN   \n",
       "39998  2013-01-23 09:36:07                                                NaN   \n",
       "39999  2013-02-22 11:23:54  are data sets obtained from a Normal distribut...   \n",
       "\n",
       "       ... AnswerCount  CommentCount  FavoriteCount  LastEditorUserId  \\\n",
       "0      ...         5.0             1           14.0               NaN   \n",
       "1      ...         7.0             1            8.0              88.0   \n",
       "2      ...        19.0             4           36.0             183.0   \n",
       "3      ...         5.0             2            2.0               NaN   \n",
       "4      ...         NaN             3            NaN              23.0   \n",
       "...    ...         ...           ...            ...               ...   \n",
       "39995  ...         NaN             0            NaN               NaN   \n",
       "39996  ...         NaN             2            NaN             892.0   \n",
       "39997  ...         NaN             0            NaN               NaN   \n",
       "39998  ...         NaN             0            NaN               NaN   \n",
       "39999  ...         2.0             4            NaN               NaN   \n",
       "\n",
       "              LastEditDate   CommunityOwnedDate ParentId  ClosedDate  \\\n",
       "0                      NaN                  NaN      NaN         NaN   \n",
       "1      2010-08-07 17:56:44                  NaN      NaN         NaN   \n",
       "2      2011-02-12 05:50:03  2010-07-19 19:13:28      NaN         NaN   \n",
       "3                      NaN                  NaN      NaN         NaN   \n",
       "4      2010-07-19 19:21:15  2010-07-19 19:14:43      3.0         NaN   \n",
       "...                    ...                  ...      ...         ...   \n",
       "39995                  NaN                  NaN  45118.0         NaN   \n",
       "39996  2013-01-23 13:13:30                  NaN  48311.0         NaN   \n",
       "39997                  NaN                  NaN  48247.0         NaN   \n",
       "39998                  NaN                  NaN  48297.0         NaN   \n",
       "39999                  NaN                  NaN      NaN         NaN   \n",
       "\n",
       "      OwnerDisplayName LastEditorDisplayName  \n",
       "0                  NaN                   NaN  \n",
       "1                  NaN                   NaN  \n",
       "2                  NaN                   NaN  \n",
       "3                  NaN                   NaN  \n",
       "4                  NaN                   NaN  \n",
       "...                ...                   ...  \n",
       "39995              NaN                   NaN  \n",
       "39996              NaN                   NaN  \n",
       "39997              NaN                   NaN  \n",
       "39998              NaN                   NaN  \n",
       "39999              NaN                   NaN  \n",
       "\n",
       "[40000 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = posts.rename(columns={'Id':'postId','OwnerUserId':'userId'})\n",
    "posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Define new dataframes for users and posts with the following selected columns:\n",
    "    **users columns**: userId, Reputation,Views,UpVotes,DownVotes\n",
    "    **posts columns**: postId, Score,userId,ViewCount,CommentCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_new = users[['userId','Reputation','Views','UpVotes','DownVotes']]\n",
    "posts_new = posts[['postId','Score','userId','ViewCount','CommentCount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           -1\n",
       "1            2\n",
       "2            3\n",
       "3            4\n",
       "4            5\n",
       "         ...  \n",
       "40320    55743\n",
       "40321    55744\n",
       "40322    55745\n",
       "40323    55746\n",
       "40324    55747\n",
       "Name: userId, Length: 40325, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_new.head()\n",
    "users_new['userId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postId</th>\n",
       "      <th>Score</th>\n",
       "      <th>userId</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>CommentCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1278.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5224.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>81</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>48321</td>\n",
       "      <td>0</td>\n",
       "      <td>19966.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>48322</td>\n",
       "      <td>3</td>\n",
       "      <td>892.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>48323</td>\n",
       "      <td>1</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>48324</td>\n",
       "      <td>3</td>\n",
       "      <td>19914.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>48325</td>\n",
       "      <td>-1</td>\n",
       "      <td>19968.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       postId  Score   userId  ViewCount  CommentCount\n",
       "0           1     23      8.0     1278.0             1\n",
       "1           2     22     24.0     8198.0             1\n",
       "2           3     54     18.0     3613.0             4\n",
       "3           4     13     23.0     5224.0             2\n",
       "4           5     81     23.0        NaN             3\n",
       "...       ...    ...      ...        ...           ...\n",
       "39995   48321      0  19966.0        NaN             0\n",
       "39996   48322      3    892.0        NaN             2\n",
       "39997   48323      1   2020.0        NaN             0\n",
       "39998   48324      3  19914.0        NaN             0\n",
       "39999   48325     -1  19968.0      116.0             4\n",
       "\n",
       "[40000 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postId</th>\n",
       "      <th>Score</th>\n",
       "      <th>userId</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>CommentCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1278.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5224.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>81</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>48321</td>\n",
       "      <td>0</td>\n",
       "      <td>19966.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>48322</td>\n",
       "      <td>3</td>\n",
       "      <td>892.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>48323</td>\n",
       "      <td>1</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>48324</td>\n",
       "      <td>3</td>\n",
       "      <td>19914.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>48325</td>\n",
       "      <td>-1</td>\n",
       "      <td>19968.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       postId  Score   userId  ViewCount  CommentCount\n",
       "0           1     23      8.0     1278.0             1\n",
       "1           2     22     24.0     8198.0             1\n",
       "2           3     54     18.0     3613.0             4\n",
       "3           4     13     23.0     5224.0             2\n",
       "4           5     81     23.0        NaN             3\n",
       "...       ...    ...      ...        ...           ...\n",
       "39995   48321      0  19966.0        NaN             0\n",
       "39996   48322      3    892.0        NaN             2\n",
       "39997   48323      1   2020.0        NaN             0\n",
       "39998   48324      3  19914.0        NaN             0\n",
       "39999   48325     -1  19968.0      116.0             4\n",
       "\n",
       "[40000 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postId</th>\n",
       "      <th>Score</th>\n",
       "      <th>userId</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>CommentCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1278.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5224.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>81</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>48321</td>\n",
       "      <td>0</td>\n",
       "      <td>19966.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>48322</td>\n",
       "      <td>3</td>\n",
       "      <td>892.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>48323</td>\n",
       "      <td>1</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>48324</td>\n",
       "      <td>3</td>\n",
       "      <td>19914.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>48325</td>\n",
       "      <td>-1</td>\n",
       "      <td>19968.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38962 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       postId  Score   userId  ViewCount  CommentCount\n",
       "0           1     23      8.0     1278.0             1\n",
       "1           2     22     24.0     8198.0             1\n",
       "2           3     54     18.0     3613.0             4\n",
       "3           4     13     23.0     5224.0             2\n",
       "4           5     81     23.0        NaN             3\n",
       "...       ...    ...      ...        ...           ...\n",
       "39995   48321      0  19966.0        NaN             0\n",
       "39996   48322      3    892.0        NaN             2\n",
       "39997   48323      1   2020.0        NaN             0\n",
       "39998   48324      3  19914.0        NaN             0\n",
       "39999   48325     -1  19968.0      116.0             4\n",
       "\n",
       "[38962 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "postId            int64\n",
       "Score             int64\n",
       "userId          float64\n",
       "ViewCount       float64\n",
       "CommentCount      int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "postId            int64\n",
       "Score             int64\n",
       "userId            int64\n",
       "ViewCount       float64\n",
       "CommentCount      int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(posts_new)\n",
    "\n",
    "posts_new = posts_new[((posts_new['userId']%1)==0)]\n",
    "display(posts_new)\n",
    "\n",
    "display(posts_new.dtypes)\n",
    "posts_new = posts_new.astype({'userId':'int'})\n",
    "display(posts_new.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postId</th>\n",
       "      <th>Score</th>\n",
       "      <th>userId</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>CommentCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>1278.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>8198.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>18</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>5224.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>81</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>48321</td>\n",
       "      <td>0</td>\n",
       "      <td>19966</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>48322</td>\n",
       "      <td>3</td>\n",
       "      <td>892</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>48323</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>48324</td>\n",
       "      <td>3</td>\n",
       "      <td>19914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>48325</td>\n",
       "      <td>-1</td>\n",
       "      <td>19968</td>\n",
       "      <td>116.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38962 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       postId  Score  userId  ViewCount  CommentCount\n",
       "0           1     23       8     1278.0             1\n",
       "1           2     22      24     8198.0             1\n",
       "2           3     54      18     3613.0             4\n",
       "3           4     13      23     5224.0             2\n",
       "4           5     81      23        NaN             3\n",
       "...       ...    ...     ...        ...           ...\n",
       "39995   48321      0   19966        NaN             0\n",
       "39996   48322      3     892        NaN             2\n",
       "39997   48323      1    2020        NaN             0\n",
       "39998   48324      3   19914        NaN             0\n",
       "39999   48325     -1   19968      116.0             4\n",
       "\n",
       "[38962 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(posts_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Merge both dataframes, users and posts. \n",
    "You will need to make a [merge](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) of posts and users dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>...</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>CommunityOwnedDate</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>ClosedDate</th>\n",
       "      <th>OwnerDisplayName</th>\n",
       "      <th>LastEditorDisplayName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2014-04-23 13:43:43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2011-03-21 17:40:28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2011-03-21 17:46:43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>919.0</td>\n",
       "      <td>2011-03-30 19:23:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>919.0</td>\n",
       "      <td>2011-03-30 19:23:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38957</th>\n",
       "      <td>45934</td>\n",
       "      <td>11</td>\n",
       "      <td>2014-05-21 17:13:23</td>\n",
       "      <td>jasonweiyi</td>\n",
       "      <td>2014-06-08 13:36:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>805.0</td>\n",
       "      <td>2013-05-13 04:41:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jasonweiyi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38958</th>\n",
       "      <td>46192</td>\n",
       "      <td>36</td>\n",
       "      <td>2014-05-26 15:29:30</td>\n",
       "      <td>user1738753</td>\n",
       "      <td>2014-09-10 19:52:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>930.0</td>\n",
       "      <td>2012-10-18 15:07:40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>user1738753</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38959</th>\n",
       "      <td>46522</td>\n",
       "      <td>235</td>\n",
       "      <td>2014-06-01 17:25:18</td>\n",
       "      <td>Andy Blankertz</td>\n",
       "      <td>2014-09-13 18:03:07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Actuary for a life insurance company.&lt;/p&gt;\\r\\n</td>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>user1009703</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38960</th>\n",
       "      <td>52371</td>\n",
       "      <td>221</td>\n",
       "      <td>2014-07-19 13:36:58</td>\n",
       "      <td>Karel Petranek</td>\n",
       "      <td>2014-07-20 09:32:16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-07-30 20:09:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dark_charlie</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38961</th>\n",
       "      <td>55226</td>\n",
       "      <td>119</td>\n",
       "      <td>2014-09-04 07:34:48</td>\n",
       "      <td>Klas Lindbäck</td>\n",
       "      <td>2014-09-08 11:07:33</td>\n",
       "      <td>http://N/A</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>&lt;p&gt;Working with a wide range of languages, but...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16174.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Klas Lindbäck</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38962 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  Reputation         CreationDate     DisplayName  \\\n",
       "0          -1           1  2010-07-19 06:55:26       Community   \n",
       "1          -1           1  2010-07-19 06:55:26       Community   \n",
       "2          -1           1  2010-07-19 06:55:26       Community   \n",
       "3          -1           1  2010-07-19 06:55:26       Community   \n",
       "4          -1           1  2010-07-19 06:55:26       Community   \n",
       "...       ...         ...                  ...             ...   \n",
       "38957   45934          11  2014-05-21 17:13:23      jasonweiyi   \n",
       "38958   46192          36  2014-05-26 15:29:30     user1738753   \n",
       "38959   46522         235  2014-06-01 17:25:18  Andy Blankertz   \n",
       "38960   52371         221  2014-07-19 13:36:58  Karel Petranek   \n",
       "38961   55226         119  2014-09-04 07:34:48   Klas Lindbäck   \n",
       "\n",
       "            LastAccessDate                      WebsiteUrl  \\\n",
       "0      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "1      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "2      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "3      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "4      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "...                    ...                             ...   \n",
       "38957  2014-06-08 13:36:47                             NaN   \n",
       "38958  2014-09-10 19:52:34                             NaN   \n",
       "38959  2014-09-13 18:03:07                             NaN   \n",
       "38960  2014-07-20 09:32:16                             NaN   \n",
       "38961  2014-09-08 11:07:33                      http://N/A   \n",
       "\n",
       "                 Location                                            AboutMe  \\\n",
       "0      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "1      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "2      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "3      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "4      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "...                   ...                                                ...   \n",
       "38957                 NaN                                                NaN   \n",
       "38958                 NaN                                                NaN   \n",
       "38959                 NaN   <p>Actuary for a life insurance company.</p>\\r\\n   \n",
       "38960                 NaN                                                NaN   \n",
       "38961              Sweden  <p>Working with a wide range of languages, but...   \n",
       "\n",
       "       Views  UpVotes  ...  AnswerCount  CommentCount  FavoriteCount  \\\n",
       "0          0     5007  ...          NaN             0            NaN   \n",
       "1          0     5007  ...          NaN             0            NaN   \n",
       "2          0     5007  ...          NaN             0            NaN   \n",
       "3          0     5007  ...          NaN             0            NaN   \n",
       "4          0     5007  ...          NaN             0            NaN   \n",
       "...      ...      ...  ...          ...           ...            ...   \n",
       "38957      1        0  ...          1.0             2            NaN   \n",
       "38958      1        0  ...          1.0             2            1.0   \n",
       "38959     13       27  ...          3.0             0            NaN   \n",
       "38960      2        0  ...          7.0             5            8.0   \n",
       "38961      2        3  ...          NaN             0            NaN   \n",
       "\n",
       "      LastEditorUserId         LastEditDate   CommunityOwnedDate  ParentId  \\\n",
       "0                 -1.0  2014-04-23 13:43:43                  NaN       NaN   \n",
       "1                 -1.0  2011-03-21 17:40:28                  NaN       NaN   \n",
       "2                 -1.0  2011-03-21 17:46:43                  NaN       NaN   \n",
       "3                919.0  2011-03-30 19:23:14                  NaN       NaN   \n",
       "4                919.0  2011-03-30 19:23:14                  NaN       NaN   \n",
       "...                ...                  ...                  ...       ...   \n",
       "38957            805.0  2013-05-13 04:41:41                  NaN       NaN   \n",
       "38958            930.0  2012-10-18 15:07:40                  NaN       NaN   \n",
       "38959              NaN                  NaN                  NaN       NaN   \n",
       "38960              NaN                  NaN  2014-07-30 20:09:14       NaN   \n",
       "38961              NaN                  NaN                  NaN   16174.0   \n",
       "\n",
       "      ClosedDate  OwnerDisplayName  LastEditorDisplayName  \n",
       "0            NaN               NaN                    NaN  \n",
       "1            NaN               NaN                    NaN  \n",
       "2            NaN               NaN                    NaN  \n",
       "3            NaN               NaN                    NaN  \n",
       "4            NaN               NaN                    NaN  \n",
       "...          ...               ...                    ...  \n",
       "38957        NaN        jasonweiyi                    NaN  \n",
       "38958        NaN       user1738753                    NaN  \n",
       "38959        NaN       user1009703                    NaN  \n",
       "38960        NaN      dark_charlie                    NaN  \n",
       "38961        NaN     Klas Lindbäck                    NaN  \n",
       "\n",
       "[38962 rows x 34 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "users_posts = users.merge(posts,left_on='userId',right_on='userId')\n",
    "display(users_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. How many missing values do you have in your merged dataframe? On which columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484716"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DisplayName                  9\n",
       "WebsiteUrl               21496\n",
       "Location                 18057\n",
       "AboutMe                  18488\n",
       "Age                      24862\n",
       "ProfileImageUrl          36137\n",
       "AcceptedAnswerId         31813\n",
       "ViewCount                23572\n",
       "Body                       115\n",
       "Title                    23572\n",
       "Tags                     23572\n",
       "AnswerCount              23572\n",
       "FavoriteCount            31906\n",
       "LastEditorUserId         19624\n",
       "LastEditDate             19434\n",
       "CommunityOwnedDate       37136\n",
       "ParentId                 15923\n",
       "ClosedDate               38451\n",
       "OwnerDisplayName         38224\n",
       "LastEditorDisplayName    38753\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sum(users_posts.isnull().sum()))\n",
    "null_cols = users_posts.isnull().sum()\n",
    "display(null_cols[null_cols>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. You will need to make something with missing values.  Will you clean or filling them? Explain. \n",
    "**Remember** to check the results of your code before passing to the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>percent_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DisplayName</th>\n",
       "      <td>DisplayName</td>\n",
       "      <td>0.023099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <td>WebsiteUrl</td>\n",
       "      <td>55.171706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <td>Location</td>\n",
       "      <td>46.345157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AboutMe</th>\n",
       "      <td>AboutMe</td>\n",
       "      <td>47.451363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>Age</td>\n",
       "      <td>63.810893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProfileImageUrl</th>\n",
       "      <td>ProfileImageUrl</td>\n",
       "      <td>92.749346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <td>AcceptedAnswerId</td>\n",
       "      <td>81.651353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ViewCount</th>\n",
       "      <td>ViewCount</td>\n",
       "      <td>60.499974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Body</th>\n",
       "      <td>Body</td>\n",
       "      <td>0.295159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Title</th>\n",
       "      <td>Title</td>\n",
       "      <td>60.499974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>Tags</td>\n",
       "      <td>60.499974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AnswerCount</th>\n",
       "      <td>AnswerCount</td>\n",
       "      <td>60.499974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FavoriteCount</th>\n",
       "      <td>FavoriteCount</td>\n",
       "      <td>81.890047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <td>LastEditorUserId</td>\n",
       "      <td>50.367024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LastEditDate</th>\n",
       "      <td>LastEditDate</td>\n",
       "      <td>49.879370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CommunityOwnedDate</th>\n",
       "      <td>CommunityOwnedDate</td>\n",
       "      <td>95.313382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ParentId</th>\n",
       "      <td>ParentId</td>\n",
       "      <td>40.868025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ClosedDate</th>\n",
       "      <td>ClosedDate</td>\n",
       "      <td>98.688466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OwnerDisplayName</th>\n",
       "      <td>OwnerDisplayName</td>\n",
       "      <td>98.105847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LastEditorDisplayName</th>\n",
       "      <td>LastEditorDisplayName</td>\n",
       "      <td>99.463580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 column_name  percent_missing\n",
       "DisplayName                      DisplayName         0.023099\n",
       "WebsiteUrl                        WebsiteUrl        55.171706\n",
       "Location                            Location        46.345157\n",
       "AboutMe                              AboutMe        47.451363\n",
       "Age                                      Age        63.810893\n",
       "ProfileImageUrl              ProfileImageUrl        92.749346\n",
       "AcceptedAnswerId            AcceptedAnswerId        81.651353\n",
       "ViewCount                          ViewCount        60.499974\n",
       "Body                                    Body         0.295159\n",
       "Title                                  Title        60.499974\n",
       "Tags                                    Tags        60.499974\n",
       "AnswerCount                      AnswerCount        60.499974\n",
       "FavoriteCount                  FavoriteCount        81.890047\n",
       "LastEditorUserId            LastEditorUserId        50.367024\n",
       "LastEditDate                    LastEditDate        49.879370\n",
       "CommunityOwnedDate        CommunityOwnedDate        95.313382\n",
       "ParentId                            ParentId        40.868025\n",
       "ClosedDate                        ClosedDate        98.688466\n",
       "OwnerDisplayName            OwnerDisplayName        98.105847\n",
       "LastEditorDisplayName  LastEditorDisplayName        99.463580"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_missing = users_posts.isnull().sum() * 100 / len(users_posts)\n",
    "missing_value_df = pd.DataFrame({'column_name': users_posts.columns,\n",
    "                                 'percent_missing': percent_missing})\n",
    "missing_value_df[(percent_missing>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " 'Giuseppe',\n",
       " 'Masood_mj',\n",
       " 'Robert Price',\n",
       " 'Andrew Latham',\n",
       " 'thelatemail',\n",
       " 'Momo',\n",
       " 'Daniel Wessel',\n",
       " 'Ravi',\n",
       " 'Mahin',\n",
       " 'Gaetan Lion',\n",
       " 'Lauren',\n",
       " 'Dimitar Vouldjeff',\n",
       " 'Anne Z.',\n",
       " 'user3303',\n",
       " 'Kniganapolke',\n",
       " 'MG1',\n",
       " 'Wilson',\n",
       " 'JSnf2012',\n",
       " 'r.e.s.',\n",
       " 'Jennifer Tye',\n",
       " 'Blah',\n",
       " 'user4167',\n",
       " 'Tera',\n",
       " 'PCalderon',\n",
       " 'Affine',\n",
       " 'Feng Mai',\n",
       " 'Mimi',\n",
       " 'Yug',\n",
       " 'Johann Hibschman',\n",
       " 'Tiago',\n",
       " 'Wenhao.SHE',\n",
       " 'karen murray',\n",
       " 'Cathy Ames',\n",
       " 'VitalStatistix',\n",
       " 'colithium',\n",
       " 'ching',\n",
       " 'user4262',\n",
       " 'Joshua Frank',\n",
       " 'ADEpt',\n",
       " 'Aps',\n",
       " 'Abraham Flaxman',\n",
       " 'Idris Adamu Alhaji',\n",
       " 'dbenton',\n",
       " 'Tyler',\n",
       " 'Rob Gathercole',\n",
       " 'Louisa Grey',\n",
       " 'nkormanik',\n",
       " 'Tom Reilly',\n",
       " 'madness',\n",
       " 'Mark Adler',\n",
       " 'Charles Menguy',\n",
       " 'eastafri',\n",
       " 'Shlomo Argamon',\n",
       " 'Deepak',\n",
       " 'orangepips',\n",
       " 'Daniel L',\n",
       " 'Matteo',\n",
       " 'newbie3',\n",
       " 'Lisa Ann',\n",
       " 'mahendra',\n",
       " 'mmgm',\n",
       " 'gozero',\n",
       " 'steadyfish',\n",
       " 'niko',\n",
       " 'CAL',\n",
       " 'Toni Bulleti',\n",
       " 'Adam Lynch',\n",
       " 'fgregg',\n",
       " 'faramir',\n",
       " 'Jamse',\n",
       " 'Ilik',\n",
       " 'luffe',\n",
       " 'Julius',\n",
       " 'mili',\n",
       " 'Jan-Henk',\n",
       " 'Victor',\n",
       " 'akaphenom',\n",
       " 'Eduardo Leoni',\n",
       " 'Noureddine',\n",
       " 'Xavier',\n",
       " 'snape_ar',\n",
       " 'Aryan',\n",
       " 'Khader Shameer',\n",
       " 'metdos',\n",
       " 'biomed',\n",
       " 'Robert Smith',\n",
       " 'Elvis',\n",
       " 'Galois Theory',\n",
       " 'mmacx',\n",
       " 'sviatoslav hong',\n",
       " 'Juha-Matti S.',\n",
       " 'Fox-Kid',\n",
       " 'csfowler',\n",
       " 'shawnny',\n",
       " 'Rushdi Shams',\n",
       " 'Michael Schwartz',\n",
       " 'Epifunky',\n",
       " 'mfk534',\n",
       " 'ftonti',\n",
       " 'colinfang',\n",
       " 'John Gunnar Carlsson',\n",
       " 'Peter Prettenhofer',\n",
       " 'izzy',\n",
       " 'Icarus',\n",
       " 'user1375871',\n",
       " 'Digital Gal',\n",
       " 'AcidNynex',\n",
       " 'user12290',\n",
       " 'Nic',\n",
       " 'conjugateprior',\n",
       " 'murrekatt',\n",
       " 'Andrzej Kosinski',\n",
       " 'Gil Kalai',\n",
       " 'Amil',\n",
       " 'Anony-Mousse',\n",
       " 'Ziel',\n",
       " 'Victor P',\n",
       " 'qdjm',\n",
       " 'Lindsay',\n",
       " 'Kamilski81',\n",
       " 'Emily',\n",
       " 'ibid',\n",
       " 'ezbentley',\n",
       " 'John Chow',\n",
       " 'Tom Carter',\n",
       " 'John D. Cook',\n",
       " 'Jimmy',\n",
       " 'manjovial',\n",
       " 'user1453477',\n",
       " 'Natasha',\n",
       " 'CodeGuy',\n",
       " 'Rasmus',\n",
       " 'fiona',\n",
       " 'Daniel Stark',\n",
       " 'Oeufcoque Penteano',\n",
       " 'JCV',\n",
       " 'MathematicalOrchid',\n",
       " 'Zach',\n",
       " 'Maria',\n",
       " 'Andy HP',\n",
       " 'Tommy r',\n",
       " 'user2839',\n",
       " 'yayu',\n",
       " 'Edgar Glark',\n",
       " 'weez13',\n",
       " 'ako',\n",
       " 'Tomas',\n",
       " 'Johann McBricks',\n",
       " 'nathan',\n",
       " 'drapkin11',\n",
       " 'Zivko Juznic-Zonta',\n",
       " 'ivez',\n",
       " 'learner',\n",
       " 'Elvin',\n",
       " 'inspectorG4dget',\n",
       " 'Ju Sun',\n",
       " 'KE.',\n",
       " 'errence',\n",
       " 'Jdub',\n",
       " 'Rob123',\n",
       " 'Petr',\n",
       " 'grajlord',\n",
       " 'atlantis',\n",
       " 'Alex Pineda',\n",
       " 'Josh Davis',\n",
       " 'greenpen',\n",
       " 'Pantera',\n",
       " 'Beltrame',\n",
       " 'Krish',\n",
       " 'user18262',\n",
       " 'user16453',\n",
       " 'user1366',\n",
       " 'Aleksandr Zubatyi',\n",
       " 'Masood Moshref Javadi',\n",
       " 'Thomas K',\n",
       " 'Ddavid',\n",
       " 'roboren',\n",
       " 'tzeH',\n",
       " 'Poik',\n",
       " 'Sevenless',\n",
       " 'Alfred Inselberg',\n",
       " 'AlcubierreDrive',\n",
       " 'gizgok',\n",
       " 'eric.a.booth',\n",
       " 'martinus',\n",
       " 'MartinW',\n",
       " 'Paul McMillan',\n",
       " 'Griffith Rees',\n",
       " 'nerdpod',\n",
       " 'Botond Sipos',\n",
       " 'onur güngör',\n",
       " 'Aleks',\n",
       " 'Biomathjdaily',\n",
       " 'Jeff Delezen',\n",
       " 'Rob Penridge',\n",
       " 'Philena Farmer',\n",
       " 'kathy',\n",
       " 'SlimJim',\n",
       " 'Julian',\n",
       " 'Demian',\n",
       " 'user1837966',\n",
       " 'Mike G',\n",
       " 'Adhesh Josh',\n",
       " 'user300811',\n",
       " 'oort',\n",
       " 'Cassie Hazell',\n",
       " 'ray',\n",
       " 'Maxime',\n",
       " 'Jazz Man',\n",
       " 'daemonfire300',\n",
       " 'Mike Flynn',\n",
       " 'Luca V',\n",
       " 'gerrit',\n",
       " 'Rosh',\n",
       " 'leonbloy',\n",
       " 'Anthony',\n",
       " 'Siddharth',\n",
       " 'Aerik',\n",
       " 'Chris Beeley',\n",
       " 'user512826',\n",
       " 'Bill Amos',\n",
       " 'guenis',\n",
       " \"Kristian D'Amato\",\n",
       " 'Andreea',\n",
       " 'Paddy',\n",
       " 'Marvin',\n",
       " 'Gautam Thakur',\n",
       " 'pipie314',\n",
       " 'DPS',\n",
       " 'Farbod',\n",
       " 'Willem',\n",
       " 'timbp',\n",
       " 'Greg Aponte',\n",
       " 'Sami',\n",
       " 'blJOg',\n",
       " 'Skylar Saveland',\n",
       " 'Vegard Larsen',\n",
       " 'Mike Mazur',\n",
       " 'ziggystar',\n",
       " 'Speldosa',\n",
       " 'Maisu',\n",
       " 'My_name_is_name',\n",
       " 'Arman',\n",
       " 'jb.',\n",
       " 'user18010',\n",
       " 'giorgio_v',\n",
       " 'zaxtax',\n",
       " 'dfrankow',\n",
       " 'user12559',\n",
       " 'Noam Kremen',\n",
       " 'daydo',\n",
       " 'adam.888',\n",
       " 'Virginia',\n",
       " 'hans0l0',\n",
       " 'anujk',\n",
       " 'psychometriko',\n",
       " 'Sarah',\n",
       " 'Damien',\n",
       " 'pate',\n",
       " 'Mark Greenaway',\n",
       " 'Leon J',\n",
       " 'user9448',\n",
       " 'Peteris',\n",
       " 'Ittay Weiss',\n",
       " 'Milen',\n",
       " 'nba',\n",
       " 'user1753987',\n",
       " 'Sheriff',\n",
       " 'Isabelle Mazerie',\n",
       " 'mariana soffer',\n",
       " 'StudentEcon',\n",
       " 'random_forest_fanatic',\n",
       " 'Egon Willighagen',\n",
       " 'user14583',\n",
       " 'CHCH',\n",
       " 'Kat Martin',\n",
       " 'user17160',\n",
       " 'David J. Harris',\n",
       " 'Sam Cruickshank',\n",
       " 'Serena',\n",
       " 'Dnaiel',\n",
       " 'Rein',\n",
       " 'Ringold',\n",
       " 'Farzad',\n",
       " 'Vik',\n",
       " 'Thierry Silbermann',\n",
       " 'Iasonas',\n",
       " 'RandomAnswer',\n",
       " 'Alex Blakemore',\n",
       " 'liuliu',\n",
       " 'deepsky',\n",
       " 'user54511',\n",
       " 'doofuslarge',\n",
       " 'nix',\n",
       " 'Ilya Tolstikhin',\n",
       " 'Marzieh',\n",
       " 'user3136',\n",
       " 'dino',\n",
       " 'aaronclauset',\n",
       " 'user12125',\n",
       " 'hapagolucky',\n",
       " 'David Lee',\n",
       " 'PeterR',\n",
       " 'Shafi',\n",
       " 'Lazer',\n",
       " 'Georgia',\n",
       " 'Thylacoleo',\n",
       " 'Yadira González',\n",
       " 'mohana',\n",
       " 'Robert Kuykendall',\n",
       " 'EOF',\n",
       " 'Sharon',\n",
       " 'half-pass',\n",
       " 'JudoWill',\n",
       " 'Jacob Eggers',\n",
       " 'dogmatic69',\n",
       " 'ektrules',\n",
       " 'AlanD',\n",
       " 'Valerian',\n",
       " 'Jim',\n",
       " 'MarkSAlen',\n",
       " 'user12234',\n",
       " 'Kshitij Wagh',\n",
       " 'Robert Dodd',\n",
       " 'tseo',\n",
       " 'Olivia Grigg',\n",
       " 'wyrobekj',\n",
       " 'D.A.',\n",
       " 'David Shih',\n",
       " 'Ady',\n",
       " 'nmu',\n",
       " 'Victor Lin',\n",
       " 'jmcejuela',\n",
       " 'Thell',\n",
       " 'Josh Hemann',\n",
       " 'mef',\n",
       " 'mare',\n",
       " 'dr.bunsen',\n",
       " 'Random',\n",
       " 'ikkin',\n",
       " 'Karl Hallowell',\n",
       " 'WOW',\n",
       " 'dentstu',\n",
       " 'Marloes',\n",
       " 'Chloe',\n",
       " 'Michael Hoffman',\n",
       " 'dude1981',\n",
       " 'Roy Yip',\n",
       " 'jdennison',\n",
       " 'pamela',\n",
       " 'Bioinformatics',\n",
       " 'j lieberman',\n",
       " 'Wam',\n",
       " 'CLS',\n",
       " 'Eugene Osovetsky',\n",
       " 'Alice',\n",
       " 'GVstats',\n",
       " 'M C',\n",
       " 'Tauf',\n",
       " 'Yossi Gil',\n",
       " 'iamreddave',\n",
       " 'Goz',\n",
       " 'skhullar',\n",
       " 'Scott Weinstein',\n",
       " 'Oreotrephes',\n",
       " 'Eric Thoma',\n",
       " 'mohit khanna',\n",
       " 'DavidA',\n",
       " 'richiemorrisroe',\n",
       " 'jake hadas',\n",
       " 'Frank Barry',\n",
       " 'Ste Trevors',\n",
       " 'Karen',\n",
       " 'nona',\n",
       " 'Trey',\n",
       " 'Pacific 231',\n",
       " 'user4528',\n",
       " 'Aaron Statham',\n",
       " 'BabOu Sunshine',\n",
       " 'charles madison',\n",
       " 'Patrick McCarthy',\n",
       " 'dubhousing',\n",
       " 'wdenton',\n",
       " 'Penz',\n",
       " 'Daи',\n",
       " 'estimator',\n",
       " 'c4il',\n",
       " 'Tunc Jamgocyan',\n",
       " 'user568458',\n",
       " 'wmmurrah',\n",
       " 'kiki',\n",
       " 'Jakub Hampl',\n",
       " 'Scott',\n",
       " 'Deniz',\n",
       " 'Bob Durrant',\n",
       " 'Francesco',\n",
       " 'far away',\n",
       " 'Beth',\n",
       " 'auretaure',\n",
       " 'Krisrs1128',\n",
       " 'Ivan Diaz',\n",
       " 'fra',\n",
       " 'Tianyang Li',\n",
       " 'Betsy',\n",
       " 'RubenGeert',\n",
       " 'ptocquin',\n",
       " 'Scott Manderson',\n",
       " 'phaneron',\n",
       " 'gianluca',\n",
       " 'Shan',\n",
       " 'James Diggans',\n",
       " 'Adal',\n",
       " 'RJ Wirth',\n",
       " 'Gern Blankston',\n",
       " 'ttnphns',\n",
       " 'user601',\n",
       " 'mike',\n",
       " 'deftfyodor',\n",
       " 'Hongweng',\n",
       " 'Ish',\n",
       " 'Rakesh Chalasani',\n",
       " 'genesiss',\n",
       " 'julia wijnmaalen',\n",
       " 'mona',\n",
       " 'danilofreire',\n",
       " 'user254694',\n",
       " 'user7032',\n",
       " 'JustCurious',\n",
       " 'Nam',\n",
       " 'pancapekarfait',\n",
       " 'user1111261',\n",
       " 'Aras',\n",
       " 'Ana Maria Popescu',\n",
       " 'M. Tibbits',\n",
       " 'Python_R',\n",
       " 'Robert Jacobson',\n",
       " 'statskeptic',\n",
       " 'Jon Pedersen',\n",
       " 'user1728853',\n",
       " 'Rodney Polkinghorne',\n",
       " 'Jodie',\n",
       " 'Wendy Alfaro',\n",
       " 'neo123',\n",
       " 'JEquihua',\n",
       " 'Fr.',\n",
       " 'aaronsw',\n",
       " 'John Mecky',\n",
       " 'Vishal',\n",
       " 'Orsino',\n",
       " 'tony gabay',\n",
       " 'scw',\n",
       " 'br69',\n",
       " 'jjepsuomi',\n",
       " 'Elaine',\n",
       " 'Ahmet Altun',\n",
       " 'Matthias Studer',\n",
       " 'drewh',\n",
       " 'Will Jagy',\n",
       " 'GriffinEvo',\n",
       " 'user5292',\n",
       " 'Peter M',\n",
       " 'skyde',\n",
       " 'Leo Vasquez',\n",
       " 'Valentin Ruano',\n",
       " 'Bossykena',\n",
       " 'user5518',\n",
       " 'JJ O',\n",
       " 'argon1024',\n",
       " 'Orkun Sahmali',\n",
       " 'chet',\n",
       " 'Akshay Sabib',\n",
       " 'tereško',\n",
       " 'silent',\n",
       " 'Peter Shor ',\n",
       " 'LadyM',\n",
       " 'meepmeep',\n",
       " 'Cassie',\n",
       " 'CHarma',\n",
       " 'Arek Paterek',\n",
       " 'R. Schumacher',\n",
       " 'felix',\n",
       " 'Mobius Pizza',\n",
       " 'Ido Tamir',\n",
       " 'Judite',\n",
       " 'jacky',\n",
       " 'Slak',\n",
       " 'dacongy',\n",
       " 'user1731029',\n",
       " 'Akavall',\n",
       " 'Felix S',\n",
       " 'bliako',\n",
       " 'robert',\n",
       " 'Omnium',\n",
       " 'Oksana',\n",
       " 'Cathy',\n",
       " 'N Brouwer',\n",
       " 'orderstats',\n",
       " 'flxlex',\n",
       " 'aule',\n",
       " 'user1546029',\n",
       " 'Mike Jenkins',\n",
       " 'Abhishek ',\n",
       " 'Philip Oakley',\n",
       " 'Luke Braidwood',\n",
       " 'EKG',\n",
       " 'Jake Fisher',\n",
       " 'gelraen',\n",
       " 'screechOwl',\n",
       " 'cupoftea',\n",
       " 'BurninLeo',\n",
       " 'reek',\n",
       " 'D L Dahly',\n",
       " 'Fletcher Duran',\n",
       " 'sayan dasgupta',\n",
       " 'br8',\n",
       " 'Gilead',\n",
       " 'thomas',\n",
       " 'Pankaj More',\n",
       " 'justspamjustin',\n",
       " 'puzzle',\n",
       " 'Rotich',\n",
       " 'MrGomez',\n",
       " 'Kaye',\n",
       " 'Bogaso',\n",
       " 'Brett',\n",
       " 'Saber CN',\n",
       " 'adamo',\n",
       " 'cpuguru',\n",
       " 'tunnuz',\n",
       " 'stats_student',\n",
       " 'Mark Lister',\n",
       " 'ln22',\n",
       " 'user963386',\n",
       " 'emilie rankine',\n",
       " 'beck',\n",
       " 'user19874',\n",
       " 'george',\n",
       " 'Dav Clark',\n",
       " 'Alexander Wolff',\n",
       " 'Aldous Wong',\n",
       " 'Vera',\n",
       " 'zsero',\n",
       " 'anurag',\n",
       " 'bshor',\n",
       " 'dan',\n",
       " 'Peter_F',\n",
       " 'Lavy',\n",
       " 'Mark Salmon',\n",
       " 'mac389',\n",
       " 'DrDom',\n",
       " 'Tracy Lu',\n",
       " 'Ejs',\n",
       " 'Mark Heckmann',\n",
       " 'Scott Silvi',\n",
       " 'Deah',\n",
       " 'kjetil b halvorsen',\n",
       " 'user3629',\n",
       " 'stan',\n",
       " 'backflip',\n",
       " 'Smandoli',\n",
       " 'Max',\n",
       " 'Keith Winstein',\n",
       " 'mzuba',\n",
       " 'Razan Paul',\n",
       " 'geneorama',\n",
       " 'PKP',\n",
       " 'user1554592',\n",
       " 'Pascal',\n",
       " 'dcl',\n",
       " 'user1173951',\n",
       " 'Mien',\n",
       " 'Mhc',\n",
       " 'mindless.panda',\n",
       " 'James Thompson',\n",
       " 'FrenchKheldar',\n",
       " 'Prachi',\n",
       " 'user56380',\n",
       " 'R_usr',\n",
       " 'twerdster',\n",
       " 'Magnus Lie Hetland',\n",
       " 'Ron Romero',\n",
       " 'teucer',\n",
       " 'vonjd',\n",
       " 'lero',\n",
       " 'tutu',\n",
       " 'Bac',\n",
       " 'Juicebox',\n",
       " 'Jack Schmidt',\n",
       " 'nico',\n",
       " 'MvG',\n",
       " 'maliky0_o',\n",
       " 'patrick d',\n",
       " 'dpkingma',\n",
       " 'zaphod',\n",
       " 'jamiroquai',\n",
       " 'King',\n",
       " 'paul',\n",
       " 'hoju',\n",
       " 'mstringer',\n",
       " 'wvoq',\n",
       " 'Dimitrios Athanasakis',\n",
       " 'hwyneken',\n",
       " 'HairyBeast',\n",
       " 'VVV',\n",
       " 'Marco A',\n",
       " 'gutto',\n",
       " 'user4269',\n",
       " 'agmao',\n",
       " 'ali amidi',\n",
       " 'user4229',\n",
       " 'user918967',\n",
       " 'excray',\n",
       " 'Derrick Coetzee',\n",
       " 'Seth Rogers',\n",
       " 'Chalie Her',\n",
       " 'Gleno',\n",
       " 'Hiwa',\n",
       " 'user2779',\n",
       " 'Joshua',\n",
       " 'Kas',\n",
       " 'A. Brandmaier',\n",
       " 'Anurag H',\n",
       " 'siegfried',\n",
       " 'Matthew Leingang',\n",
       " 'Tarantula',\n",
       " 'Emil H',\n",
       " 'Adam Greenhall',\n",
       " 'Mike Z.',\n",
       " 'Todd',\n",
       " 'Sanjay Manohar',\n",
       " 'Carson Myers',\n",
       " 'ACD',\n",
       " 'David Joubert',\n",
       " 'Amit',\n",
       " 'sebhofer',\n",
       " 'kellyjeglum',\n",
       " 'Minh',\n",
       " 'd.putto',\n",
       " 'Iris Priest',\n",
       " 'Christin',\n",
       " 'Ashal',\n",
       " 'J.A.F',\n",
       " 'Eric Siegel',\n",
       " 'user18075',\n",
       " 'Vera Jindrova',\n",
       " 'Jonas Heidelberg',\n",
       " 'araroot',\n",
       " 'Joanna',\n",
       " 'frin',\n",
       " 'lgbi',\n",
       " 'arandomlypickedname',\n",
       " 'Vicente Cartas',\n",
       " 'Björn Malmgren',\n",
       " 'Esther Lee',\n",
       " 'Addsy',\n",
       " 'leopino',\n",
       " 'sally moustafa',\n",
       " 'trs',\n",
       " 'David Roberts',\n",
       " 'tyranitar',\n",
       " 'Emsnoel',\n",
       " 'Janet Reno',\n",
       " 'Jeremy',\n",
       " 'psr',\n",
       " 'Dr. Eric',\n",
       " 'Rob',\n",
       " 'tetragrammaton',\n",
       " 'Polshgiant',\n",
       " 'Niousha',\n",
       " 'mussi',\n",
       " 'Shruti',\n",
       " 'Bandrush',\n",
       " 'mingzuheng',\n",
       " 'cybele',\n",
       " 'jazzvibes',\n",
       " 'docstudent',\n",
       " 'Sidmitra',\n",
       " 'BillyHx',\n",
       " 'Ali Ok',\n",
       " 'alexeigor',\n",
       " 'alittleboy',\n",
       " 'rnorberg',\n",
       " 'Thomas O',\n",
       " 'alm',\n",
       " 'Craig Wright',\n",
       " 'Hank',\n",
       " 'lcl23',\n",
       " 'TLS',\n",
       " 'Yuval Peres',\n",
       " 'Mona Rifaat',\n",
       " 'calejero',\n",
       " 'Yoda',\n",
       " '163ka',\n",
       " 'Lythimus',\n",
       " 'waldog',\n",
       " 'Deepti',\n",
       " 'om_henners',\n",
       " 'Alan James Salmoni',\n",
       " 'Richard Willey',\n",
       " 'CWT',\n",
       " 'neophyte',\n",
       " 'ffriend',\n",
       " 'sam',\n",
       " 'Mark Sale',\n",
       " 'Superbest',\n",
       " 'Mark Lavin',\n",
       " 'pacomet',\n",
       " 'Andreas Eckner',\n",
       " 'Ciarán',\n",
       " 'IrishStat',\n",
       " 'Gray',\n",
       " 'Mike Davie',\n",
       " 'Jeromy Anglim',\n",
       " 'ShreevatsaR',\n",
       " 'Ondrej',\n",
       " 'coen',\n",
       " 'Hans Rudel',\n",
       " 'jakub',\n",
       " 'ru_di',\n",
       " 'Rossinante',\n",
       " 'Sohail Akram',\n",
       " 'Gail Palubiak',\n",
       " 'Zylfj',\n",
       " 'Robert Jones',\n",
       " 'Kams',\n",
       " 'user43812',\n",
       " 'Jason',\n",
       " 'Haley Rosenfeld',\n",
       " 'ileitch',\n",
       " 'Hbar',\n",
       " 'Richard Herron',\n",
       " 'blz',\n",
       " 'Steve Lianoglou',\n",
       " 'B Seven',\n",
       " 'lokheart',\n",
       " 'lmsasu',\n",
       " 'vanilla',\n",
       " 'atad',\n",
       " 'John Forbes',\n",
       " 'sajni',\n",
       " 'ebtg',\n",
       " 'julien stirnemann',\n",
       " 'fangly',\n",
       " 'AM2',\n",
       " 'user2168',\n",
       " 'user14862',\n",
       " 'CptanPanic',\n",
       " 'Javad',\n",
       " 'user9821',\n",
       " 'cing',\n",
       " 'Dhruv',\n",
       " 'paras_doshi',\n",
       " 'Nate',\n",
       " 'DG1',\n",
       " 'Peter Hull',\n",
       " 'Monica',\n",
       " 'Matt Hall',\n",
       " 'bwalenz',\n",
       " 'yelh',\n",
       " 'user531',\n",
       " 'Yuck',\n",
       " 'biofreezer',\n",
       " 'Mike Richardson',\n",
       " 'Davorak',\n",
       " 'Hello',\n",
       " 'Anastasia',\n",
       " 'nidhi',\n",
       " 'Fabian',\n",
       " 'remo',\n",
       " 'Xiaowen Li',\n",
       " 'user824624',\n",
       " 'nkint',\n",
       " 'Hernan_L',\n",
       " 'Golo Roden',\n",
       " 'Blaise Egan',\n",
       " 'user16204',\n",
       " 'donguzman',\n",
       " 'vizzero',\n",
       " 'Ole Henrik Skogstrøm',\n",
       " 'Augusto',\n",
       " 'Chen',\n",
       " 'James Holland',\n",
       " 'Atilla Ozgur',\n",
       " 'navaneeth',\n",
       " 'Rosarch',\n",
       " 'Lynn',\n",
       " 'Mark',\n",
       " 'bmc',\n",
       " 'JackL',\n",
       " 'erik',\n",
       " 'Stacey',\n",
       " 'Jean-Victor Côté',\n",
       " 'user1308532',\n",
       " '404Dreamer_ML',\n",
       " 'ansate',\n",
       " 'Joachim Ziemssen',\n",
       " 'jphoward',\n",
       " 'Will Newton',\n",
       " '130490868091234',\n",
       " 'ECII',\n",
       " 'iopsych',\n",
       " 'viki.omega9',\n",
       " 'Bernardo Mendoza',\n",
       " 'user4341',\n",
       " 'krazycat',\n",
       " 'Teresa Spence',\n",
       " 'Ama',\n",
       " 'Adam Bailey',\n",
       " 'user1061210',\n",
       " 'Martina Ozan',\n",
       " 'DavidShor',\n",
       " 'joker',\n",
       " 'DKK',\n",
       " 'Jamie Hall',\n",
       " 'Bradford',\n",
       " 'tobias mguire',\n",
       " 'Juanan',\n",
       " 'Billy',\n",
       " 'Lyra',\n",
       " 'pingi',\n",
       " 'user11562',\n",
       " 'Amol Pande',\n",
       " 'Jergason',\n",
       " 'Nuzhi',\n",
       " 'Queequeg',\n",
       " 'user566',\n",
       " 'TVT',\n",
       " 'Jan Hannig',\n",
       " 'Karien Lomans',\n",
       " 'Michael Barker',\n",
       " 'Andrew Arnold',\n",
       " 'JV Phillips',\n",
       " 'Dason',\n",
       " 'user4594',\n",
       " 'means-to-meaning',\n",
       " 'Anne Hansen',\n",
       " 'ruben baetens',\n",
       " 'berkay',\n",
       " 'Scott Wood',\n",
       " 'Marco De Mattia',\n",
       " 'tams',\n",
       " 'TaeHeon Kim',\n",
       " 'Prabhu M',\n",
       " 'Roger',\n",
       " 'TCAllen07',\n",
       " 'Abdel',\n",
       " 'gina turco',\n",
       " 'Paul Biggar',\n",
       " 'Philipp G. Sinicyn',\n",
       " 'Anna',\n",
       " 'Tony ejnrjekr',\n",
       " 'soren.qvist',\n",
       " 'persistence911',\n",
       " 'Oddthinking',\n",
       " 'contr.error',\n",
       " 'Matt Hurley',\n",
       " 'steffen',\n",
       " 'Phonon',\n",
       " 'Tony Redpath',\n",
       " 'TQEF',\n",
       " 'Javier',\n",
       " 'vanguard2k',\n",
       " 'Luana',\n",
       " 'user969113',\n",
       " 'Ken Miller',\n",
       " 'Flanfl',\n",
       " 'counfounded',\n",
       " 'almondovar',\n",
       " 'crs',\n",
       " 'MrPricklepants',\n",
       " 'William Gunn',\n",
       " 'Benny',\n",
       " 'matt',\n",
       " 'ALV',\n",
       " 'DanielOfTaebl',\n",
       " 'clairec',\n",
       " 'DrewConway',\n",
       " 'user17079',\n",
       " 'Jon Peltier',\n",
       " 'even',\n",
       " 'Alexandre Passos',\n",
       " 'zinou021',\n",
       " 'evt',\n",
       " 'ThePiachu',\n",
       " 'Skolnick',\n",
       " 'Joao Azevedo',\n",
       " 'Ales',\n",
       " 'burnmp3s',\n",
       " 'Tiddo',\n",
       " 'Randy M.',\n",
       " 'Keller Scholl',\n",
       " 'Jake Westfall',\n",
       " 'user13283',\n",
       " 'troger19',\n",
       " 'Rich Kenefic',\n",
       " 'Rosa',\n",
       " 'ting',\n",
       " 'user1643060',\n",
       " 'Jon Peck',\n",
       " 'Pinar Donmez',\n",
       " 'Tomasz',\n",
       " 'asksw0rder',\n",
       " 'amcolian',\n",
       " 'dynamo',\n",
       " 'Fabio F.',\n",
       " 'Pablo Marin-Garcia',\n",
       " 't l',\n",
       " 'Robert E Mealey',\n",
       " 'cammil',\n",
       " 'TheLostOne',\n",
       " \"Josh O'Brien\",\n",
       " 'severin',\n",
       " 'Bucsa Lucian',\n",
       " 'user2654',\n",
       " 'Rossella',\n",
       " 'gobbble',\n",
       " 'B_Miner',\n",
       " 'mfc',\n",
       " 'Terri',\n",
       " 'Omar Kooheji',\n",
       " 'yuk',\n",
       " 'Andy McKenzie',\n",
       " 'Tiffany',\n",
       " 'user1873',\n",
       " 'user17330',\n",
       " 'Anusha',\n",
       " 'robbisg',\n",
       " 'wondering-girl',\n",
       " 'ZPQ',\n",
       " 'Hamed',\n",
       " 'Dean Eckles',\n",
       " 'sdemyanov',\n",
       " 'sundancer',\n",
       " 'Margaret',\n",
       " 'Waqar Muhammad Khan',\n",
       " 'Iva Rashkova',\n",
       " 'Martin Hue',\n",
       " 'dram',\n",
       " 'Orlando Mezquita',\n",
       " 'RWFarley',\n",
       " 'Beh',\n",
       " 'Nick Alger',\n",
       " 'Calvin',\n",
       " 'growse',\n",
       " 'francogrex',\n",
       " 'Julie',\n",
       " 'Andrey',\n",
       " 'sachin',\n",
       " 'Andrew Warner',\n",
       " 'xate',\n",
       " 'Jessica',\n",
       " 'user1121',\n",
       " 'Greg Slodkowicz',\n",
       " 'mikebmassey',\n",
       " 'Adam Bowen',\n",
       " 'locke14',\n",
       " 'MRocklin',\n",
       " 'Harsh',\n",
       " 'Phi-dot',\n",
       " 'mirror2image',\n",
       " 'kriegar',\n",
       " 'mindmatters',\n",
       " 'Marco Tulio',\n",
       " 'Diego Bilski',\n",
       " 'Dmitrij Celov',\n",
       " 'vqv',\n",
       " 'Mostafa Mahdieh',\n",
       " 'mathieu_r',\n",
       " 'Alex Stoddard',\n",
       " 'Femi',\n",
       " 'Andrzej',\n",
       " 'tayf',\n",
       " 'Gregory Piatetsky',\n",
       " 'bnjmn',\n",
       " 'tonykejrkkejrenrk',\n",
       " 'Chunming Wang',\n",
       " 'Chill Penguin',\n",
       " 'kspacja',\n",
       " 'Ben Goodrich',\n",
       " 'user11170',\n",
       " 'Paul Hiemstra',\n",
       " 'Pulkit Sinha',\n",
       " 'studenteconomics',\n",
       " 'Juan Fernandez Tajes',\n",
       " 'KrishKalyan',\n",
       " 'alenyb',\n",
       " 'wolf.rauch',\n",
       " 'user1705135',\n",
       " 'Ben Curren',\n",
       " 'hgcrpd',\n",
       " 'Travis R',\n",
       " 'euki',\n",
       " 'Skiminok',\n",
       " 'Erad',\n",
       " 'Daniel Lee',\n",
       " 'Mansour Aldosari',\n",
       " 'davidshen84',\n",
       " 'Chris',\n",
       " 'Seyhmus Güngören',\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " 'http://stla.github.io/stlapblog/',\n",
       " 'http://www.ouviemsp.com.br',\n",
       " 'http://people.math.osu.edu/edgar.2/',\n",
       " 'http://tushman.com',\n",
       " 'http://www.moviri.com',\n",
       " 'http://underconstruction',\n",
       " 'http://cyrussamii.com',\n",
       " 'http://gordonwatts.wordpress.com',\n",
       " 'http://www.souhaib-bentaieb.com',\n",
       " 'http://ziroby.com',\n",
       " 'http://kdl.cs.umass.edu/people/jensen/',\n",
       " 'http://tormodboe.wordpress.com/',\n",
       " 'http://clickdamage.com',\n",
       " 'http://sct.tl',\n",
       " 'http://createdthings.blogspot.com/',\n",
       " 'http://gotbim.com',\n",
       " 'http://www.dinhani.com.br/',\n",
       " 'http://about.me/paul.mason',\n",
       " 'http://www.gmge.org',\n",
       " 'http://www.biologie.uni-hamburg.de/bzf/fbda005/fbda005_e.htm',\n",
       " 'http://twitter.com/kshameer',\n",
       " 'http://peteriserins.com',\n",
       " 'http://kermit.epska.org',\n",
       " 'http://www.cse.ust.hk/~derekhh/',\n",
       " 'http://aerik.com',\n",
       " 'http://homepages.ulb.ac.be/~alabarre/',\n",
       " 'http://ch.linkedin.com/in/jnlopes',\n",
       " 'http://www.cse.ohio-state.edu/~satuluri/',\n",
       " 'http://nicolas.kruchten.com/',\n",
       " 'http://www.cdeszaq.com',\n",
       " 'http://www.yuan-shuai.info',\n",
       " 'http://thomblake.com',\n",
       " 'http://Google',\n",
       " 'http://ram.rachum.com',\n",
       " 'http://grahamcookson.com',\n",
       " 'http://www.cs.mcgill.ca/~pmangg',\n",
       " 'http://www.uzbozor.com',\n",
       " 'http://www.peterbloem.nl',\n",
       " 'http://patrickmd.net/blog/',\n",
       " 'http://aramk.com',\n",
       " 'http://carljoseph.com.au',\n",
       " 'http://www.rsginc.com',\n",
       " 'http://cometapps.com',\n",
       " 'http://www.inbo.be',\n",
       " 'http://abstractlabor.blogspot.com',\n",
       " 'http://www.google.com/profiles/utunga#about',\n",
       " 'http://eshioji.co.uk',\n",
       " 'http://n.a',\n",
       " 'http://www.oddskool.net',\n",
       " 'http://mindcode.org',\n",
       " 'http://koaning.com',\n",
       " 'http://www.ics.uci.edu/~duboisc',\n",
       " 'http://www.jirilukavsky.info',\n",
       " 'http://www.flickr.com/photos/nimrodm/',\n",
       " 'http://www.altosresearch.com/',\n",
       " 'http://Olexe.com',\n",
       " 'http://www.growse.com',\n",
       " 'http://vkuzo.com',\n",
       " 'http://ceit.aut.ac.ir/~isaac',\n",
       " 'http://www.indolering.com',\n",
       " 'https://github.com/mmparker',\n",
       " 'http://mike.axiak.net',\n",
       " 'http://mikestumpf.com',\n",
       " 'http://matteodefelice.name/research',\n",
       " 'https://wemarsh.com/',\n",
       " 'http://aws.amazon.com/elasticbeanstalk/',\n",
       " 'https://sites.google.com/site/rknmodn/',\n",
       " 'http://lepo.it.da.ut.ee/~timo_p',\n",
       " 'http://zjffdu.blogspot.com',\n",
       " 'http://google.com.au',\n",
       " 'http://scidav.org',\n",
       " 'http://www.ieortools.com',\n",
       " 'http://www.linkedin.com/pub/oleksandr-pavlyk/4/403/39a',\n",
       " 'http://remembr.in',\n",
       " 'http://www.buffalo.edu/~lovegren',\n",
       " 'http://dipanmehta.wordpress.com',\n",
       " 'http://gravatar.com/sagarnikam123',\n",
       " 'http://synthesis.williamgunn.org',\n",
       " 'http://www.biostat.wisc.edu/~kbroman',\n",
       " 'http://www.themindfulperformer.com',\n",
       " 'http://stackoverflow.com/users/edit/1003565',\n",
       " 'http://www.control.isy.liu.se/~andrecb',\n",
       " 'http://vzn1.wordpress.com/',\n",
       " 'http://www.blog4work.com',\n",
       " 'http://hypergeometric.wordpress.com',\n",
       " 'http://web.mit.edu',\n",
       " 'http://www.etchsoftware.com',\n",
       " 'http://randomalgorithmoftheweek.wordpress.com/',\n",
       " 'http://danachandler.com',\n",
       " 'http://www.empirica-systeme.de',\n",
       " 'http://www.applieddatalabs.com',\n",
       " 'http://www.setzkorn.eu',\n",
       " 'http://www.cs.bilkent.edu.tr/~cant',\n",
       " 'http://twitter.com/muzhig',\n",
       " 'http://aminpractice.blogspot.com',\n",
       " 'https://sbarnea.com',\n",
       " 'http://www.watermarquee.com',\n",
       " 'http://samueljlivingstone.wix.com/webpage',\n",
       " 'http://timabe.tumblr.com',\n",
       " 'http://dropbearcode.blogspot.com/',\n",
       " 'http://jakebowers.org',\n",
       " 'http://feliperiveroll.name/',\n",
       " 'http://ariddell.org',\n",
       " 'http://www.netconstructor.com',\n",
       " 'http://www.ccc.uga.edu',\n",
       " 'http://tuwien.ac.at',\n",
       " 'http://www.broadinstitute.org/~mghandi',\n",
       " 'http://www.beavercreekfire.org',\n",
       " 'http://research.sourcebyte.com',\n",
       " 'http://freerice.org',\n",
       " 'http://reader.differentialist.info',\n",
       " 'http://list.seqfan.eu/',\n",
       " 'http://khakieconomist.com',\n",
       " 'http://sysadmin.jprice.org',\n",
       " 'http://www.mendeley.com/profiles/ruben-baetens/',\n",
       " 'http://about.me/mkozhevnikov',\n",
       " 'http://www.hightechrealm.com',\n",
       " 'http://www.talkstats.com',\n",
       " 'http://android-argentina.blogspot.com',\n",
       " 'http://panictank.net',\n",
       " 'http://micans.org/stijn/',\n",
       " 'http://erikvold.com/index.cfm',\n",
       " 'http://www.tellesfera.com/',\n",
       " 'http://stackexchange.com/users/158078/wok',\n",
       " 'http://Y',\n",
       " 'http://www.s2temsc.org/staffprofiles/gregorymacdougall',\n",
       " 'http://vision.cs.arizona.edu/ksimek',\n",
       " 'http://stackoverflow.com/users/1049893/vutukuri',\n",
       " 'http://www.analyticstraining.in',\n",
       " 'http://moderntoolmaking.blogspot.com/',\n",
       " 'http://www.staff.ncl.ac.uk/d.j.wilkinson/',\n",
       " 'http://telliott99.blogspot.com',\n",
       " 'http://radekstepan.com',\n",
       " 'https://sites.google.com/site/abdelhakmahmoudi/home',\n",
       " 'http://www.thinkinator.com',\n",
       " 'http://www.datamilk.com',\n",
       " 'http://twitter.com/jholloway7',\n",
       " 'http://swig505.com',\n",
       " 'http://www.andrea-zellner.com',\n",
       " 'http://www.benjaminhowarth.com',\n",
       " 'http://rpostcard.blogspot.com/2014/03/my-missing-countries.html',\n",
       " 'http://www.robbie.eugraph.com',\n",
       " 'https://github.com/mkemnetz',\n",
       " 'http://probabilicity.wordpress.com',\n",
       " 'http://www.coachingwithkati.co.za',\n",
       " 'http://in.linkedin.com/in/abhishekclearforest',\n",
       " 'http://folk.uio.no/plison',\n",
       " 'http://inside.mines.edu/~ddanford',\n",
       " 'http://noldorin.com/',\n",
       " 'http://www.xmwconsulting.co.uk',\n",
       " 'http://milktrader.net',\n",
       " 'http://zmjones.com',\n",
       " 'http://http//twitter.com/nitin',\n",
       " 'http://pantheon.yale.edu/~ejw3/',\n",
       " 'http://errorstatistics.com',\n",
       " 'http://www.linkedin.com/in/crntaylor',\n",
       " 'http://www.luigip.com/',\n",
       " 'http://i-coded.blogspot.com',\n",
       " 'http://twitter.com/RaniNelken',\n",
       " 'http://casa.colorado.edu/~ginsbura/',\n",
       " 'http://dafeda.wordpress.com',\n",
       " 'http://www.cise.ufl.edu/~gsthakur',\n",
       " 'http://mattnworb.com',\n",
       " 'http://jhoward.fastmail.fm',\n",
       " 'http://www.linkedin.com/in/mtodor',\n",
       " 'http://www.kieranhealy.org',\n",
       " 'http://jessejacobanderson.com',\n",
       " 'http://www.rommil.com',\n",
       " 'http://www.lastyeah.dk',\n",
       " 'http://felixcloutier.com/',\n",
       " 'http://www.darkrho.com',\n",
       " 'http://www.univ-rouen.fr/LMRS/Persopage/Giraudo/',\n",
       " 'http://bentilly.blogspot.com/',\n",
       " 'http://www.r-bloggers.com',\n",
       " 'http://www.salford-systems.com/',\n",
       " 'http://www.oslolso.tumblr.com',\n",
       " 'http://Noneatm',\n",
       " 'http://anthonykong.tumblr.com/',\n",
       " 'http://taghawi-nejad.de',\n",
       " 'http://rguha.net',\n",
       " 'http://www.annezelenka.com',\n",
       " 'http://treasurecoastresearch.com',\n",
       " 'http://akshayl.posterous.com',\n",
       " 'http://www.sentient.nl/',\n",
       " 'http://creativeskillsinstitute.net',\n",
       " 'http://HeroCalc.com',\n",
       " 'http://sessrumnir.wikidot.com',\n",
       " 'http://chil.rice.edu/jzemla',\n",
       " 'http://drpaulbrewer.com',\n",
       " 'http://snyke.net',\n",
       " 'http://notebookonthewebs.tumblr.com/',\n",
       " 'http://www.bakerlab.org/',\n",
       " 'http://www.fisica.unam.mx/ifunam_english/clusters_english/',\n",
       " 'http://www.lordabbett.com/',\n",
       " 'http://jamesxli.blogspot.com/2010/10/principal-components-analysis-pca-of.html',\n",
       " 'http://Idonthaveahomepage.',\n",
       " 'http://yandex.ru/search',\n",
       " 'http://Gallagher',\n",
       " 'http://www.organizingcreativity.com/',\n",
       " 'http://www.quantum.at',\n",
       " 'http://www.kevinjspring.com',\n",
       " 'http://whiteraptor.deviantart.com/',\n",
       " 'http://pelliptic.blogspot.com/',\n",
       " 'http://www.topjaklont.org/',\n",
       " 'http://www.cs.ucr.edu/~eamonn/',\n",
       " 'http://www.kirkouimet.com/',\n",
       " 'http://www.ms.unimelb.edu.au/~andrewpr',\n",
       " 'http://suitdummy.blogspot.com',\n",
       " 'http://typeandflow.blogspot.com',\n",
       " 'http://matan.name',\n",
       " 'http://ua.linkedin.com/pub/maxim-ivanov/21/3a/440',\n",
       " 'http://Itsprivate...',\n",
       " 'http://www.cs.utah.edu/~moeller',\n",
       " 'http://www.dbruhn.de',\n",
       " 'http://www.about.me/thiagosteiner',\n",
       " 'https://github.com/bontibon',\n",
       " 'http://redhardsupra.blogspot.com',\n",
       " 'http://linuxcs.com',\n",
       " 'http://blog.tomtung.com/',\n",
       " 'http://www.fabriziobianchi.it',\n",
       " 'http://www2.hawaii.edu/~senin',\n",
       " 'http://www.johnnylogic.org/',\n",
       " 'http://www.iam.fmph.uniba.sk/ospm/Lacko',\n",
       " 'http://www.linkedin.com/in/bjansen',\n",
       " 'http://cloudera.com/',\n",
       " 'http://stevenljohnson.org',\n",
       " 'http://www.stats.stackexchange.com',\n",
       " 'http://nkls.schmckr.de',\n",
       " 'http://aioo.be',\n",
       " 'http://www.sbitzer.eu',\n",
       " 'http://codebox.org.uk',\n",
       " 'http://visual.ly',\n",
       " 'http://walkytalky.net/',\n",
       " 'http://twitter.com/alexablag',\n",
       " 'http://onertipaday.blogspot.com',\n",
       " 'http://313n.com',\n",
       " 'http://www.sparrowtail.com',\n",
       " 'http://it.linkedin.com/in/luigisaggese',\n",
       " 'http://sindevel.com',\n",
       " 'http://www.stanford.edu/~justso1',\n",
       " 'http://www.blueraja.com/blog',\n",
       " 'http://tonysharp.me',\n",
       " 'http://ali.moeeny.com',\n",
       " 'http://gplus.to/dlras2/about',\n",
       " 'http://github.com/ariofrio',\n",
       " 'http://dougal.me',\n",
       " 'http://sds.podval.org',\n",
       " 'http://www.stat.purdue.edu/~jdatta',\n",
       " 'http://www.graphpad.com',\n",
       " 'http://creatapreneur.com',\n",
       " 'http://www.inthehaystack.com',\n",
       " 'http://www.angelmansucks.com',\n",
       " 'http://aaronlevin.ca',\n",
       " 'http://www.ics.uci.edu/~yganjisa',\n",
       " 'http://kracken.cs.ucla.edu',\n",
       " 'http://fsteeg.wordpress.com',\n",
       " 'http://quantcorner.wordpress.com',\n",
       " 'http://www.vanheusden.com/',\n",
       " 'http://onyx.brandmaier.de',\n",
       " 'http://fluxicon.com',\n",
       " 'http://www.connor-johnson.com',\n",
       " 'http://www.fosstrading.com',\n",
       " 'http://stats.stackexchange.com/users/5494',\n",
       " 'http://venturesindatascience.blogspot.com/',\n",
       " 'http://www.google.com/profiles/ivan.mashchenko',\n",
       " 'http://www.cs.ru.nl/~gori/',\n",
       " 'http://j-rednote.blogspot.com/',\n",
       " 'http://statsoft.com',\n",
       " 'http://singmann.org',\n",
       " 'http://thatjohnbarnes.blogspot.com',\n",
       " 'http://twitter.com/brocktibert',\n",
       " 'http://swolter.sdf1.org',\n",
       " 'http://crsouza.com',\n",
       " 'http://www.linkedin.com/in/ralphwinters',\n",
       " 'http://preetiedul.wordpress.com/',\n",
       " 'http://aramis.hostman.us',\n",
       " 'http://www.cse.iitb.ac.in/~salilj',\n",
       " 'http://celenius.com',\n",
       " 'http://kenmankoff',\n",
       " 'http://johannes.jakeapp.com/blog/',\n",
       " 'http://john.jersdesk.com',\n",
       " 'http://kev.inburke.com',\n",
       " 'http://cwick.co.nz',\n",
       " 'http://blog.zemox.com',\n",
       " 'http://bschwehn.de',\n",
       " 'http://dlaptev.org',\n",
       " 'http://www.johnsjolander.com',\n",
       " 'http://www.cpdomina.net',\n",
       " 'http://www.mas.ncl.ac.uk/~ncsg3/',\n",
       " 'http://mkhamis.com',\n",
       " 'http://ryouready.wordpress.com',\n",
       " 'http://hlangeveld.nl',\n",
       " 'http://mdswanson.com',\n",
       " 'http://axiomofcats.com',\n",
       " 'http://www.nmt.edu/~borchers',\n",
       " 'http://danielswan.net',\n",
       " 'http://www.centerspace.net',\n",
       " 'http://dalkescientific.com/writings/',\n",
       " 'http://www.deslondesoftware.com',\n",
       " 'http://www.csse.unimelb.edu.au/~mlui',\n",
       " 'http://qmaxim.com',\n",
       " 'http://www.huumanquestion.com',\n",
       " 'http://www.rhsmith.umd.edu/faculty/gshmueli/',\n",
       " 'http://www.cs.brown.edu/~matteo',\n",
       " 'http://twitter.com/shabbychef',\n",
       " 'http://www.csi.ucd.ie/users/thomas-holz',\n",
       " 'http://longwood.edu/staff/fortinok',\n",
       " 'http://blog.tiney.com',\n",
       " 'http://www.ianlangmore.com',\n",
       " 'http://allencch.wordpress.com/',\n",
       " 'http://dat-berger.de',\n",
       " 'http://memming.wordpress.com',\n",
       " 'http://www.hamiltongross.com',\n",
       " 'http://www.stefvanbuuren.nl',\n",
       " 'http://gossetsstudent.wordpress.com/',\n",
       " 'http://www.parasdoshi.com',\n",
       " 'http://www.select-statistics.co.uk',\n",
       " 'http://www.meac.go.tz',\n",
       " 'http://www.scitemplar.wordpress.com',\n",
       " 'http://xeoncross.com',\n",
       " 'http://www.linkedin.com/profile/view?id=155522394&trk=nav_responsive_tab_profile',\n",
       " 'http://energynumbers.info/?ref=stackexchange',\n",
       " 'http://pavel.kabir.ru',\n",
       " 'http://www.celtic-knot-creator.com',\n",
       " 'http://twitter.com/saneef',\n",
       " 'https://github.com/skyl',\n",
       " 'http://amyglen.wordpress.com',\n",
       " 'http://mypetprojects.blogspot.com/',\n",
       " 'http://www.stubbornmule.net',\n",
       " 'http://www.econinfo.de',\n",
       " 'http://scriptcrafty.com',\n",
       " 'http://www.john-joseph-horton.com/',\n",
       " 'http://www.researchremix.org',\n",
       " 'http://scholar.google.com/citations?user=L5t2HBYAAAAJ&hl=en',\n",
       " 'http://www.consultingstatistics.org',\n",
       " 'http://www.linkedin.com/in/andrewweinert',\n",
       " 'http://www.linkedin.com/in/guidogarcia',\n",
       " 'http://fjordist.wordpress.com',\n",
       " 'http://bergin.se',\n",
       " 'http://www.astro.puc.cl/~nespino',\n",
       " 'http://www.mauvedeity.com/',\n",
       " 'http://www.hrnutshell.com',\n",
       " 'http://cpd.wustl.edu',\n",
       " 'http://martin.von-gagern.net/',\n",
       " 'http://ismailari.com',\n",
       " 'http://adini.me',\n",
       " 'http://Grieu',\n",
       " 'http://book.drupalfun.com',\n",
       " 'http://aquaya.org',\n",
       " 'http://www.squobble.com',\n",
       " 'http://phil-wise.com/',\n",
       " 'http://www.cherhan.net',\n",
       " 'http://www.bioss.ac.uk/students/alexm.html',\n",
       " 'http://gael-varoquaux.info',\n",
       " 'http://Unnown',\n",
       " 'http://meta.stackexchange.com/',\n",
       " 'http://statisfaction.wordpress.com/',\n",
       " 'http://wiki.github.com/joelthelion/autojump',\n",
       " 'http://www.romeovansnick.be',\n",
       " 'http://keithsheppard.name',\n",
       " 'http://yaroslavvb.blogspot.com',\n",
       " 'http://goo.gl/0TJHX',\n",
       " 'http://www.andrewherbst.net',\n",
       " 'http://LeanEntrepreneur.co',\n",
       " 'http://circleci.com',\n",
       " 'http://yourpsyche.org',\n",
       " 'http://www.scheme.dk/blog/',\n",
       " 'http://www.Kormanik.com',\n",
       " 'http://peltiertech.com/WordPress/',\n",
       " 'http://www.ics.uci.edu/~amkulkar',\n",
       " 'http://leidegren.se/',\n",
       " 'http://ron.gejman.com',\n",
       " 'http://frontalattackonanenglishwriter.com',\n",
       " 'http://www.endycahyono.com/',\n",
       " 'http://lrandroid.com',\n",
       " 'http://edild.github.com/',\n",
       " 'http://www.kalaharilionresearch.org',\n",
       " 'http://livejournal.com/~_navi_',\n",
       " 'http://skila.pl',\n",
       " 'https://github.com/alixaxel/',\n",
       " 'http://logfc.wordpress.com',\n",
       " 'http://prognoz.ck.ua',\n",
       " 'http://sychopx.wordpress.com',\n",
       " 'http://thebiobucket.blogspot.com/',\n",
       " 'http://scholar.google.com/citations?user=J72tQy4AAAAJ',\n",
       " 'http://www.mat.univie.ac.at/~neum/',\n",
       " 'http://crschmidt.net/',\n",
       " 'http://www.streamerone.it',\n",
       " 'http://www.iiasa.ac.at/Research/POP/Staff/wazir.html',\n",
       " 'http://zv.github.com',\n",
       " 'http://www.johndierks.com',\n",
       " 'http://about.me/stephen.rush',\n",
       " 'http://statisticaconr.blogspot.com',\n",
       " 'http://en.wikipedia.org/wiki/Gongsun_Long',\n",
       " 'http://spencerboucher.com',\n",
       " 'http://vytautas.s.blogspot.com',\n",
       " 'http://www.jdxyw.com',\n",
       " 'http://www.atmos.uw.edu/~akchen0/',\n",
       " 'http://alexott.net/en/',\n",
       " 'http://www.montegraphia.com',\n",
       " 'http://www.stefan-koch.name/',\n",
       " 'http://akgupta.ca',\n",
       " 'http://thomasjowens.com/',\n",
       " 'http://nl.linkedin.com/in/jochemdonkers',\n",
       " 'https://www.researchgate.net/profile/Richard_Warnung/imgsrc=https://www.researchgate.net/images/public/profile_share_badge.pngalt=RichardWarnung//a',\n",
       " 'http://mikegioia@gmail.com',\n",
       " 'http://www.google.com/profiles/timothy.s.lau',\n",
       " 'http://bangor.academia.edu/BronsonHarry',\n",
       " 'http://rocknrblog.wordpress.com/',\n",
       " 'http://renaudr.posterous.com/',\n",
       " 'http://robteszka.wordpress.com',\n",
       " 'http://statigrafix',\n",
       " 'http://mustafaatik.com',\n",
       " 'http://gameschannelblog.wordpress.com/',\n",
       " 'http://econ.ucsd.edu/~dkaplan',\n",
       " 'http://www.sharpsteen.net',\n",
       " 'http://www.caspershouse.com',\n",
       " 'http://www.geekymedia.com',\n",
       " 'http://www.joopox.net',\n",
       " 'http://www.bhjones.com',\n",
       " 'http://benhamner.com',\n",
       " 'http://mako.cc',\n",
       " 'http://bpetrushev.appspot.com',\n",
       " 'http://www.andrewmao.net',\n",
       " 'http://www.cims.nyu.edu/~leingang/',\n",
       " 'http://www.elegantdesigns.ca',\n",
       " 'http://www.tweelingenregister.org',\n",
       " 'http://oswco.com',\n",
       " 'http://www.sumsar.net',\n",
       " 'http://www.almostsure.com',\n",
       " 'http://ben.com',\n",
       " 'http://www.thomasrehman.net/',\n",
       " 'http://www.google.com/',\n",
       " 'http://www.baltimark.com',\n",
       " 'http://www.wegnerdesign.com',\n",
       " 'http://tweetvest.com',\n",
       " 'http://compbiome.com',\n",
       " 'http://twitter.com/ogrisel',\n",
       " 'http://blog.aarthid.me/',\n",
       " 'http://brenocon.com',\n",
       " 'http://not_yet...',\n",
       " 'http://pherricoxide.wordpress.com',\n",
       " 'http://www.su3analytics.com',\n",
       " 'http://biostat.mc.vanderbilt.edu/FrankHarrell',\n",
       " 'http://neurosail.com/',\n",
       " 'http://blog.rootle.it',\n",
       " 'http://www.cs.berkeley.edu/~dcoetzee/',\n",
       " 'http://en.wikipedia.org/wiki/User:MikeDunlavey',\n",
       " 'http://www.ime.usp.br/~patriota',\n",
       " 'http://numericalrecipes.wordpress.com',\n",
       " 'http://www.reglue.org',\n",
       " 'http://koool',\n",
       " 'http://www.enac.fr/recherche/leea/Steve%20Lawford/steve_site1.html',\n",
       " 'https://www.researchgate.net/profile/Kaveh_Vakili/?ev=hdr_xprf',\n",
       " 'http://www.u-psud.fr',\n",
       " 'http://www.chrisgagne.com',\n",
       " 'http://amanahuja.me',\n",
       " 'http://www.crmportals.com',\n",
       " 'http://Nonewhatsoever',\n",
       " 'http://kalx.net',\n",
       " 'http://about.me/prakhar9',\n",
       " 'http://yz.mit.edu/',\n",
       " 'http://www.thomas-steinbrenner.net',\n",
       " 'http://dotnetpanda.blogspot.com',\n",
       " 'http://www.enigmaportal.com',\n",
       " 'http://jamisondance.com/',\n",
       " 'http://people.reed.edu/~swansone',\n",
       " 'http://twitter.com/omitevski',\n",
       " 'http://geneorama.com',\n",
       " 'https://twitter.com/ConstantPieters',\n",
       " 'http://www.chrisamiller.com',\n",
       " 'http://linbaba.wordpress.com/',\n",
       " 'http://www.berkustun.com',\n",
       " 'http://birdventure.blogspot.com',\n",
       " 'http://www.stiglerdiet.com',\n",
       " 'http://yokozar.org/',\n",
       " 'http://healthyalgorithms.wordpress.com',\n",
       " 'http://blog.aylett.co.uk',\n",
       " 'http://prod.campuscruiser.com/cruiser/widener/maalexander',\n",
       " 'http://www.cqlcorp.com',\n",
       " 'http://dominionstrategy.com',\n",
       " 'http://jon-coleman.net',\n",
       " 'http://twitter.com/zjelveh',\n",
       " 'http://procbits.com',\n",
       " 'http://www.cs.princeton.edu/~asuleime/',\n",
       " 'http://twitter.com/marcgg',\n",
       " 'http://lamontconsulting.com',\n",
       " 'http://www.r-statistics.com',\n",
       " 'http://www.kaybensoft.com',\n",
       " 'http://dantalus.org',\n",
       " 'http://pelicandd.com/',\n",
       " 'http://VisuMap.com',\n",
       " 'http://www.newsh.it',\n",
       " 'http://rushdishams.googlepages.com',\n",
       " 'http://www.pflaquerre.ca',\n",
       " 'http://blogs.sas.com/content/iml',\n",
       " 'http://go.helms-net.de',\n",
       " 'http://joshuawiley.com',\n",
       " 'http://www.hutchinsonlee.com',\n",
       " 'http://www.thegollys.com',\n",
       " 'http://johnmetta.com',\n",
       " 'https://twitter.com/#!/ziyuang',\n",
       " 'http://Yahoo',\n",
       " 'http://fwusher.com',\n",
       " 'http://www.physioguru.com',\n",
       " 'http://www.globalccs.net/~vanstat',\n",
       " 'http://www.tuhh.de',\n",
       " 'https://sites.google.com/site/johnjvickers/',\n",
       " 'http://heliosphan.org/',\n",
       " 'http://iigs.dreamhosters.com',\n",
       " 'http://cpuguru.net',\n",
       " 'http://gandrusz.blogspot.com',\n",
       " 'http://chetvericov.ru',\n",
       " 'http://www.icmc.usp.br/~alceufc/',\n",
       " 'http://www.junuxx.net',\n",
       " 'https://plus.google.com/108351331401552240682/posts',\n",
       " 'https://sites.google.com/site/mathsworkmusic/',\n",
       " 'http://www.astro.rug.nl/~jones/',\n",
       " 'http://f.briatte.org/',\n",
       " 'http://www.lathamcity.com',\n",
       " 'http://profiles.google.com/114451807329956829391/about',\n",
       " 'http://careers.stackoverflow.com/dougybarbo',\n",
       " 'http://peekabo-vision.blogspot.com',\n",
       " 'http://www.bellomyresearch.com',\n",
       " 'http://www.chenyuzhao.net',\n",
       " 'http://deaneckles.com/blog',\n",
       " 'http://google.com',\n",
       " 'http://www.med.govt.nz/sectors-industries/tourism/tourism-research-data',\n",
       " 'http://www.gillesmaes.be',\n",
       " 'http://pckben.blogspot.com',\n",
       " 'http://twitter.com/venkasub',\n",
       " 'http://www.wyzant.com/Tutors/NY/Hollis/7894492/',\n",
       " 'http://sethpaxton.tumblr.com',\n",
       " 'http://in.linkedin.com/pub/shashank-gupta/28/2bb/518',\n",
       " 'http://richcooks.blogspot.com',\n",
       " 'http://cfarmerga.myopenid.com/',\n",
       " 'http://github.com/aomidpanah',\n",
       " 'http://mormon.org/me/38ZB',\n",
       " 'https://sites.google.com/site/arunirc/',\n",
       " 'http://fa.bianp.net',\n",
       " 'http://reasoniamhere.com',\n",
       " 'http://harrak.net',\n",
       " 'http://paulmurray.id.au',\n",
       " 'http://theatavism.blogspot.com',\n",
       " 'http://cloudartisan.com',\n",
       " 'http://alexholcombe.wordpress.com/',\n",
       " 'http://www.santafe.edu/~aaronc/',\n",
       " 'http://www.cse.ohio-state.edu/~khatchad',\n",
       " 'http://vishview.com',\n",
       " 'http://sites.google.com/site/barisaydinoz/',\n",
       " 'http://www.paulhurley.co.uk',\n",
       " 'http://www.danieljhocking.worpress.com',\n",
       " 'http://lambert.geek.nz/',\n",
       " 'http://ianmackinnon.co.uk',\n",
       " 'http://www.martindueren.de',\n",
       " 'http://www.rabidotter.com',\n",
       " 'http://soma.denkt.org',\n",
       " 'http://www.danclarke.com',\n",
       " 'http://NUR',\n",
       " 'http://na',\n",
       " 'https://twitter.com/mvholmes',\n",
       " 'http://mcfromnz.wordpress.com/',\n",
       " 'http://www.ucl.ac.uk/energy',\n",
       " 'http://www.reedcopsey.com',\n",
       " 'http://dropbit.com',\n",
       " 'http://math.pugetsound.edu/~mspivey/',\n",
       " 'http://ncatlab.org/davidroberts/show/HomePage',\n",
       " 'http://www.slimjim.name',\n",
       " 'http://bow.web.id',\n",
       " 'http://www.mymindleaks.com',\n",
       " 'http://about.me/drit',\n",
       " 'http://gatton.uky.edu/gradstudents/cress',\n",
       " 'http://www.morethannothing.co.uk',\n",
       " 'http://harshhpareek.com',\n",
       " 'http://davidshen84.appspot.com/blog',\n",
       " 'http://github.com/beyeran',\n",
       " 'https://twitter.com/sameersoi',\n",
       " 'http://biology.unm.edu/mmfuller/',\n",
       " 'http://student.johnpdaigle.com',\n",
       " 'http://www.cloudturkiye.com',\n",
       " 'http://antipaucity.com',\n",
       " 'http://lessoned.blogspot.com/',\n",
       " 'http://jcl.posterous.com',\n",
       " 'http://akshayshah.org',\n",
       " 'http://stackoverflow.com/users/1209279/',\n",
       " 'http://jamesscottbrown.com',\n",
       " 'http://www.kalebpederson.com/',\n",
       " 'http://www.rkuykendall.com/',\n",
       " 'http://www.kalisch.biz',\n",
       " 'http://openwetware.org/wiki/User:Thaman_Chand',\n",
       " 'http://hirejamesbradshaw.com',\n",
       " 'http://www.bytemining.com',\n",
       " 'https://sites.google.com/a/g.rit.edu/drogalis/',\n",
       " 'http://www.joelonsoftware.com/',\n",
       " 'http://cerebralmastication.com',\n",
       " 'http://gforge.se',\n",
       " 'http://www.jcachat.com',\n",
       " 'http://web.engr.illinois.edu/~mvakili2/',\n",
       " 'http://www.samuels-art.com',\n",
       " 'http://pratikdeoghare.github.com',\n",
       " 'http://www.facebook.com',\n",
       " 'http://www.kcl.ac.uk/iop/depts/neuroimaging/people/Ginestet,Cedric.aspx',\n",
       " 'http://stevetjoa.com',\n",
       " 'http://jking.ca',\n",
       " 'http://aatrujillo.wordpress.com',\n",
       " 'http://www.greatPixelHunt.com',\n",
       " 'http://stanford.edu/~mbloem/',\n",
       " 'http://vouldjeff.freewildart.com',\n",
       " 'http://themanthursday.com',\n",
       " 'http://davharris.github.io',\n",
       " 'http://scausa.com',\n",
       " 'http://blog.hansdude.com',\n",
       " 'http://aleczopf.com',\n",
       " 'http://sapir.us',\n",
       " 'http://www.mintuz.co.uk',\n",
       " 'http://www.nicolaromano.net',\n",
       " 'http://www.cs.uml.edu/~kuttecht/',\n",
       " 'http://nonerightnow',\n",
       " 'http://twitter.com/#!/imbenzene',\n",
       " 'http://www.navsea.navy.mil/nswc/Corona',\n",
       " 'http://socialgraphpaper.blogspot.com',\n",
       " 'http://thetarzan.wordpress.com',\n",
       " 'http://blog.explainmydata.com',\n",
       " 'http://www.multiplyleadership.com',\n",
       " 'http://jatinpatni.co.nr',\n",
       " 'http://www.samgledhill.com',\n",
       " 'http://matthewrathbone.com',\n",
       " 'http://ayushb.blogspot.com',\n",
       " 'http://www.twitter.com/paulsalvaggio',\n",
       " 'http://mortoray.com/',\n",
       " 'https://heatma.ps',\n",
       " 'http://twitter.com/#!/ET_13',\n",
       " 'http://blog.programet.org',\n",
       " 'http://tungwaiyip.info/',\n",
       " 'http://www.vpgcentral,com',\n",
       " 'http://jason.skepsi.net',\n",
       " 'http://aurimas.eu',\n",
       " 'http://codevanced.net/',\n",
       " 'http://migdal.wikidot.com/en',\n",
       " 'http://jplehmann.com',\n",
       " 'http://maximzaslavsky.com',\n",
       " 'http://csferrie.com',\n",
       " 'http://blog.thegrandlocus.com',\n",
       " 'http://stackoverflow.com/users/717441/benjamin',\n",
       " 'http://aaboyles.com',\n",
       " 'http://nass.usda.gov',\n",
       " 'http://www.yangzhiping.com',\n",
       " 'http://kitmonisit.com',\n",
       " 'http://footballthoughtsfromsweden.wordpress.com',\n",
       " 'http://lemire.me/en/',\n",
       " 'http://ritter.vg',\n",
       " 'http://jayraj.net',\n",
       " 'http://www.yossale.com',\n",
       " 'http://whathecode.wordpress.com/',\n",
       " 'http://faculty.washington.edu/mrich',\n",
       " 'http://www.dandunn.org',\n",
       " 'http://blog.meta-spaces.com',\n",
       " 'http://anton.korobeynikov.info',\n",
       " 'http://peternewhook.com',\n",
       " 'http://www.jocund.net/',\n",
       " 'http://www.statisticaladvisor.com',\n",
       " 'http://www.acooke.org',\n",
       " 'http://nsaunders.wordpress.com',\n",
       " 'http://dadadom.de',\n",
       " 'http://www.labescape.com/',\n",
       " 'http://luispedro.org',\n",
       " 'http://dnsdebug.com',\n",
       " 'http://ie.linkedin.com/in/danielvassallo',\n",
       " 'http://www.stat.berkeley.edu/~drosen/',\n",
       " 'http://www.CopperThoughts.com',\n",
       " 'http://www.jameskoppel.com',\n",
       " 'http://www.calvin.edu/~mkh2',\n",
       " 'http://www.rosshartshorn.net',\n",
       " 'https://whymarrh.com',\n",
       " 'http://theInPUT.org',\n",
       " 'http://coding.pressbin.com',\n",
       " 'http://www.usemarkup.com',\n",
       " 'http://gregguida.com/',\n",
       " 'http://www.ic.unicamp.br/~tachard/',\n",
       " 'http://mgaudet.ca',\n",
       " 'http://www.aaronsw.com/',\n",
       " 'http://www.ccl.gatech.edu/',\n",
       " 'https://www.cs.tcd.ie/~mcaulejj',\n",
       " 'http://www.cquotient.com',\n",
       " 'http://myprofile.cos.com/gilder',\n",
       " 'http://i9606.blogspot.com',\n",
       " 'http://salebarn.com',\n",
       " 'http://gameroom.io',\n",
       " 'http://arek-paterek.com',\n",
       " 'https://twitter.com/#!/technipion',\n",
       " 'http://www.pramodc.wordpress.com',\n",
       " 'http://www.economicdroplets.com',\n",
       " 'http://xyz',\n",
       " 'http://n/a',\n",
       " 'http://6v8.gamboni.org',\n",
       " 'http://www.terrymatula.com',\n",
       " 'http://aeshin.org/',\n",
       " 'http://onur-gungor.com',\n",
       " 'http://almost-published.com',\n",
       " 'http://www.implicit.ugent.be/index.php?position=1x3x3#.UFic5WFuItk',\n",
       " 'http://www.markmfredrickson.com',\n",
       " 'http://www.benjamindeschamps.ca',\n",
       " 'http://haking',\n",
       " 'http://egonw.github.com',\n",
       " 'http://www.arielburone.com.ar',\n",
       " 'http://spiritzhang.com',\n",
       " 'http://www.princeton.edu/~sims',\n",
       " 'http://gettinggeneticsdone.blogspot.com/',\n",
       " 'http://adamlynch.ie',\n",
       " 'http://bedatadriven.com',\n",
       " 'http://blogs.wiki-dot.net/alaudo',\n",
       " 'http://tora.us.fm/erelsgl',\n",
       " 'http://li-tianyang.com',\n",
       " 'https://www.google.com/+BrentWorden',\n",
       " 'http://refactor.it',\n",
       " 'http://www.kmctrampolineteam.com',\n",
       " 'http://sharedcount.com',\n",
       " 'http://berkeleybiolabs.com',\n",
       " 'http://kurthaeusler.wordpress.com',\n",
       " 'http://weblogs.asp.net/sweinstein',\n",
       " 'http://www.optimisationbeacon.com/',\n",
       " 'http://ruivieira.org',\n",
       " 'http://www.cse.ohio-state.edu/~yan',\n",
       " 'http://about.me/vinuct',\n",
       " 'http://www.facebook.com/madeleine.barratt',\n",
       " 'http://wog.com.es',\n",
       " 'http://www.thenewsbeforethenews.com',\n",
       " 'http://wikipedia.org',\n",
       " 'http://www.bangequals.net',\n",
       " 'http://vince.vu',\n",
       " 'http://www.otago.ac.nz/wellington/departments/biostatisticalservices/staff/otago019097.html',\n",
       " 'http://www.gallamine.com',\n",
       " 'http://folk.uio.no/sverrej/',\n",
       " 'http://www.solimano.org',\n",
       " 'http://ctxmodel.net',\n",
       " 'http://biomath.ugent.be/biomath/index.php',\n",
       " 'http://jfcoder.com',\n",
       " 'http://cdn2.knowyourmeme.com/i/5378/original/zero.jpg?1247534859',\n",
       " 'http://www.jay.fm',\n",
       " 'http://www.cc.gatech.edu/~sbakhshi/',\n",
       " 'http://Toobusy',\n",
       " 'http://www.ices.utexas.edu/~poulson',\n",
       " 'http://sjuvekar.appspot.com/blog',\n",
       " 'http://www.linkedin.com/in/gd047',\n",
       " 'http://bradleygermain.com',\n",
       " 'http://4recipes.ru',\n",
       " 'http://hetland.org',\n",
       " 'http://www.jkndrkn.com',\n",
       " 'http://arencambre.com',\n",
       " 'http://www.remin.pl',\n",
       " 'http://www.drbunsen.org',\n",
       " 'http://www.linkedin.com/pub/oscar-forth/8/857/22',\n",
       " 'http://stackoverflow.com/users/1493368/metrics',\n",
       " 'http://www.seizurerobots.com/',\n",
       " 'http://matthias.vallentin.net',\n",
       " 'http://www.kbrandt.com',\n",
       " 'http://idris.heroku.com',\n",
       " 'http://joefu.net/blog',\n",
       " 'http://jeroen.vangoey.be',\n",
       " 'https://github.com/markusian',\n",
       " 'http://adamgreenhall.com',\n",
       " 'http://braintrace.ru',\n",
       " 'http://jcazevedo.net',\n",
       " 'http://tullo.ch',\n",
       " 'http://michal.illich.cz/',\n",
       " 'http://yannick.poulet.org',\n",
       " 'http://joelreyesnoche.wordpress.com/tag/mathematics/',\n",
       " 'http://SeanKilleen.com',\n",
       " 'http://www.lutralutra.co.uk',\n",
       " 'http://www.traviswolfe.com',\n",
       " 'http://www.nbr-graphs.com',\n",
       " 'http://www.businessforecastblog.com',\n",
       " 'http://jermdemo.blogspot.com',\n",
       " 'http://www.tomdiethe.com',\n",
       " 'http://www.communitywiki.org/en/EmileKroeger',\n",
       " 'http://had.co.nz',\n",
       " 'http://www.ivuedesign.com',\n",
       " 'http://visionazulyoro.tumblr.com/',\n",
       " 'http://dogmatic69.com',\n",
       " 'http://vincentdavis.net',\n",
       " 'http://www.geektrading.com',\n",
       " 'http://www.linkedin.com/in/jdanielnd',\n",
       " 'http://bloggingabout.net/blogs/vagif/default.aspx',\n",
       " 'http://eren.0fess.net',\n",
       " 'http://taufmonster.blogspot.com',\n",
       " 'http://christianjauv.in',\n",
       " 'http://www.thiloschneider.net',\n",
       " 'http://www.aaronmcdaid.com',\n",
       " 'http://jacquestardie.org',\n",
       " 'http://www.tristancartledge.com',\n",
       " 'http://jakewestfall.org',\n",
       " 'http://www.zloto-jubiler.pl',\n",
       " 'http://slvsef.org',\n",
       " 'http://kartones.net/blogs/jadengine',\n",
       " 'http://codeonipad.com/',\n",
       " 'http://www.circulumvite.com',\n",
       " 'http://www.cs.bham.ac.uk',\n",
       " 'http://gr.linkedin.com/in/eriksperling',\n",
       " 'http://www.bertelsen.ca',\n",
       " 'http://yes',\n",
       " 'http://ericjensen.com',\n",
       " 'http://lingcog.iit.edu',\n",
       " 'http://www.onestop.co.uk',\n",
       " 'http://cmenguy.github.io',\n",
       " 'http://noWebsite.com',\n",
       " 'http://osiloke.com',\n",
       " 'http://www.kishinmanglani.com',\n",
       " 'http://www.pariser.me/',\n",
       " 'http://jspha.com',\n",
       " 'http://www.firstheartland.com',\n",
       " 'http://www.ichinco.com',\n",
       " 'http://www.redf.org',\n",
       " 'http://cran.r-project.org/web/packages/excel.link/index.html',\n",
       " 'http://manasto.info',\n",
       " 'http://nym.se/',\n",
       " 'http://www.markfisherevolution.com',\n",
       " 'http://about.me/LiaoGongYi',\n",
       " 'https://sites.google.com/site/miro165sabo/',\n",
       " 'http://twitter.com/mike_jenkins',\n",
       " 'http://objektorient.blogspot.com/',\n",
       " 'https://twitter.com/#!/geracleous',\n",
       " 'http://www.statalgo.com',\n",
       " 'http://grey.colorado.edu/mingus',\n",
       " 'http://dirknachbar.com',\n",
       " 'http://Notes',\n",
       " 'http://www.faciletek.com',\n",
       " 'http://sidmitra.com',\n",
       " 'http://www.di.uoa.gr/~std07150',\n",
       " 'http://dahtah.wordpress.com',\n",
       " 'http://www.eamann.com',\n",
       " 'http://singyourownlullaby.blogspot.com',\n",
       " 'http://www.smanohar.com',\n",
       " 'http://bit.ly/dcantor',\n",
       " 'http://facebook.com/nickcowans',\n",
       " 'http://www.embeddedrelated.com/blogs-1/nf/Jason_Sachs.php',\n",
       " 'http://thefoundation.de/michael',\n",
       " 'http://codethink.no-ip.org',\n",
       " 'http://www.mrcorey.com',\n",
       " 'http://www.equate-design.com',\n",
       " 'http://idebamarketing.com',\n",
       " 'http://www.psycho.uni-duesseldorf.de/abteilungen/mkp/Arbeitsgruppe',\n",
       " 'http://twitter.com/scw',\n",
       " 'http://www.crimmer.co.uk',\n",
       " 'http://www.reilurekrytointi.fi/video/sepp%C3%A4l%C3%A4-jaakko',\n",
       " 'http://stackoverflow.com',\n",
       " 'http://edgarhassler.com',\n",
       " 'http://therandomtexan.wordpress.com/',\n",
       " 'http://www.pharmacology.unimelb.edu.au/statboss/home.html',\n",
       " 'http://scienceblogs.com/developingintelligence',\n",
       " 'http://math.mit.edu/~shor',\n",
       " 'http://nan',\n",
       " 'http://www.JoalahDesigns.com',\n",
       " 'http://sozluk.sourtimes.org/show.asp?t=ssg+bunu+okuyosan+topsun+olm+top',\n",
       " 'http://quantdevel.com/public',\n",
       " 'http://gua.com',\n",
       " 'http://recologia.com.br',\n",
       " 'http://www.cs.mcgill.ca/~akazna/',\n",
       " 'https://sites.google.com/site/zhfan555',\n",
       " 'http://www.quantisan.com',\n",
       " 'http://acgrama.github.io',\n",
       " 'http://vene.ro',\n",
       " 'http://stat.org.ua',\n",
       " 'http://www.cse.iitb.ac.in/~aruniyer/',\n",
       " 'http://beza1e1.tuxen.de',\n",
       " 'http://mikekr.blogspot.com',\n",
       " 'http://dxjones.com',\n",
       " 'http://liveatthewitchtrials.blogspot.',\n",
       " 'http://blog.ouseful.info',\n",
       " 'http://www.epernicus.com/pho',\n",
       " 'http://herbsusmann.com',\n",
       " 'http://www.ramhiser.com',\n",
       " 'http://socialdatablog.com',\n",
       " 'http://alansalmoni.com/portfolio.html',\n",
       " 'http://www.wolfsheadsoftware.co.uk',\n",
       " 'http://www.andrewheiss.com',\n",
       " 'http://dcook.org/work/',\n",
       " 'http://bobobobo.wordpress.com',\n",
       " 'https://developers.google.com/',\n",
       " 'http://navarroj.com/latex',\n",
       " 'https://christopherdardis.wordpress.com/',\n",
       " 'http://people.csail.mit.edu/rivest',\n",
       " 'http://www.1450.me',\n",
       " 'http://www.cs.mu',\n",
       " 'http://www.dcsc.tudelft.nl/~itkachev/',\n",
       " 'http://www.linkedin.com/in/panagiotispalladinos',\n",
       " 'http://www.rileydutton.com/',\n",
       " 'http://Plotkowiak',\n",
       " 'http://blog.kilotrader.com',\n",
       " 'http://www.ninjaPixel.eu',\n",
       " 'http://djpirtu.deviantart.com',\n",
       " 'http://vancouverdata.blogspot.com',\n",
       " 'http://127.0.0.1/index.html',\n",
       " 'http://foxfirekitty.wordpress.com',\n",
       " 'http://www.patternwebsolutions.com',\n",
       " 'http://uosis.mif.vu.lt/~celov',\n",
       " 'http://www.clsnyder.com',\n",
       " 'http://www.getdorm.com',\n",
       " 'http://www-personal.umich.edu/~carmelos/',\n",
       " 'http://stackexchange.com/users/c447702a-0df2-432e-bf56-b289f2f0fcb4?tab=accounts',\n",
       " 'http://bazzilic.me/',\n",
       " 'http://blue-feet.com',\n",
       " 'http://isomorphismes.tumblr.com',\n",
       " 'http://vkreynin.wordpress.com',\n",
       " 'http://snowl.net',\n",
       " 'http://mlstat.wordpress.com',\n",
       " 'http://www.ellipsix.net',\n",
       " 'http://www.whatsupinamsterdam.co.nl',\n",
       " 'http://www.looper.hu',\n",
       " 'https://www.randomthoughtsonr.wordpress.com',\n",
       " 'http://matrukripa.com',\n",
       " 'http://www.cmikavac.net',\n",
       " 'http://hybridlogic.co.uk/',\n",
       " 'http://eeecon.uibk.ac.at/~zeileis/',\n",
       " 'http://diariolinux.com',\n",
       " 'http://slashhome.be',\n",
       " 'http://vzemlys.wordpress.com',\n",
       " 'http://www.archimetrics.com',\n",
       " 'http://thereasonableprogressive.org',\n",
       " 'http://www.twitter.com/palanski',\n",
       " 'http://website.com',\n",
       " 'http://lse.academia.edu/GeorgeMichaelides',\n",
       " 'http://mark.reid.name',\n",
       " 'http://www.johnmyleswhite.com',\n",
       " 'http://www.thenativeweb.io',\n",
       " 'http://samsaffron.com',\n",
       " 'http://www.riskmath.eu',\n",
       " 'http://tdwright.co.uk',\n",
       " 'http://siyandawrites.wordpress.com',\n",
       " 'http://volomike.com',\n",
       " 'http://schissel.org',\n",
       " 'http://www.unur.com/',\n",
       " 'http://nlp.stanford.edu/~manning/',\n",
       " 'http://www.pebble-watch.nl',\n",
       " 'http://www.8164.org',\n",
       " 'https://github.com/vegard',\n",
       " 'http://obeautifulcode.com/',\n",
       " 'http://biomath.ugent.be/',\n",
       " 'http://www.burtonsys.com/email/',\n",
       " 'http://conjugateprior.org',\n",
       " 'http://www.softwest.com',\n",
       " 'http://kuk.ac.in',\n",
       " 'http://eduardoleoni.com',\n",
       " 'http://neurov.is/on',\n",
       " 'http://blog.locut.us/',\n",
       " 'http://gampleman.eu',\n",
       " 'http://www.newtnotes.com',\n",
       " 'http://www.kdnuggets.com/gps.html',\n",
       " 'http://1',\n",
       " 'https://sites.google.com/site/antonioirpino/',\n",
       " 'http://bronzebeard.wordpress.com',\n",
       " 'http://www.utdiscant.dk',\n",
       " 'http://www.DrAnkush.com',\n",
       " 'http://web.pas.rochester.edu/~tobin/',\n",
       " 'http://www.tcm.phy.cam.ac.uk/~gz218',\n",
       " 'http://rivita.ru/spssmacros_en.shtml',\n",
       " 'http://www.StatisticsDoc.com',\n",
       " 'http://www.mendeley.com/profiles/marcus-morrisey/',\n",
       " 'http://dorserg.com',\n",
       " 'http://miningtext.blogspot.com',\n",
       " 'http://chrisbeeley.net',\n",
       " 'http://berndblume.com',\n",
       " 'http://stackoverflow.com/users/281413',\n",
       " 'http://www.tau.ac.il/~saharon',\n",
       " 'http://www.statisticalanalysisconsulting.com/',\n",
       " 'http://tirandocodigo.net',\n",
       " 'http://michaelmbishop.github.com/',\n",
       " 'http://www.margotbrown.com',\n",
       " 'http://lorinhochstein.org',\n",
       " 'http://www.fromthebottomoftheheap.net',\n",
       " 'http://op.to/sO+',\n",
       " 'http://www.bipede.de',\n",
       " 'http://www.parsprogrammers.ir',\n",
       " 'http://djhurio.wordpress.com/',\n",
       " 'http://thecity2.com',\n",
       " 'http://ryandillon.net',\n",
       " 'http://matthewrocklin.com',\n",
       " 'https://plus.google.com/109439195292129836824',\n",
       " 'http://steinbock.org',\n",
       " 'http://www.Leviton.com',\n",
       " 'http://craicpropagation.blogspot.com/',\n",
       " 'http://www.seamusbradley.net',\n",
       " 'http://www.nestorsulikowski.com',\n",
       " 'http://plindenbaum.blogspot.com',\n",
       " 'http://hsigrist.github.io/blog',\n",
       " 'http://jaheruddin.nl',\n",
       " 'http://www.jabsy.com',\n",
       " 'http://twitter.com/pchalasani',\n",
       " 'http://www.swanamb.com',\n",
       " 'http://www.DavidDLewis.com',\n",
       " 'http://about.me/brycethomas',\n",
       " 'http://geogrouper.appspot.com/',\n",
       " 'http://magicbeanlab.com',\n",
       " 'http://tech-disasters.blogspot.com/',\n",
       " 'http://drfloob.com',\n",
       " 'http://www.stats.ox.ac.uk/~evans',\n",
       " 'http://www.stat.umn.edu/~arendahl/',\n",
       " 'http://artax.karlin.mff.cuni.cz/~tucev2am/',\n",
       " 'http://www.EralpBayraktar.com',\n",
       " 'http://samuelhulick.com',\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " '94025',\n",
       " 'AHMEDABD,INDIA',\n",
       " 'Aachen, Germany',\n",
       " 'Aalto University, Finland',\n",
       " 'Abersychan, United Kingdom',\n",
       " 'Acton Vale, Canada',\n",
       " 'Adelaide, Australia',\n",
       " 'Aiken, SC',\n",
       " 'Alabama',\n",
       " 'Alamo, CA',\n",
       " 'Alaska',\n",
       " 'Albany, NY',\n",
       " 'Alberta, Canada',\n",
       " 'Albuquerque, NM',\n",
       " 'Alexandria, Egypt',\n",
       " 'Altadena, CA',\n",
       " 'America',\n",
       " 'Ames, IA',\n",
       " 'Amherst, MA',\n",
       " 'Amritsar, India',\n",
       " 'Amsterdam',\n",
       " 'Amsterdam, Netherlands',\n",
       " 'Amsterdam, The Netherlands',\n",
       " 'Ankara, Turkey',\n",
       " 'Ann Arbor, MI',\n",
       " 'Antwerp, Belgium',\n",
       " 'Antwerp/Leuven, Belgium',\n",
       " 'Arcadia, CA',\n",
       " 'Arenys de Mar, Spain',\n",
       " 'Argentina',\n",
       " 'Arizona',\n",
       " 'Arkansas',\n",
       " 'Arlington, MA',\n",
       " 'Arlington, VA',\n",
       " 'Ashburn, VA',\n",
       " 'Asheville, NC',\n",
       " 'Asia',\n",
       " 'Athens, GA',\n",
       " 'Athens, Greece',\n",
       " 'Atlanta',\n",
       " 'Atlanta, GA',\n",
       " 'Auckland, New Zealand',\n",
       " 'Austin, TX',\n",
       " 'Australia',\n",
       " 'Austria',\n",
       " 'Baden-Wurttemberg, Germany',\n",
       " 'Bahia, Brazil',\n",
       " 'Baltimore, MD',\n",
       " 'Bangalore',\n",
       " 'Bangalore, India',\n",
       " 'Bangladesh',\n",
       " 'Bangor University',\n",
       " 'Barcelona, Spain',\n",
       " 'Basel, Switzerland',\n",
       " 'Basingstoke, United Kingdom',\n",
       " 'Bath',\n",
       " 'Bath, United Kingdom',\n",
       " 'Bay Area, CA',\n",
       " 'Bayreuth, Germany',\n",
       " 'Beavercreek, Ohio',\n",
       " 'Beijing',\n",
       " 'Beijing, China',\n",
       " 'Belgium',\n",
       " 'Bellevue, WA',\n",
       " 'Bellingham, WA',\n",
       " 'Belo Horizonte, Brazil',\n",
       " 'Bergen, Norway',\n",
       " 'Bergen-Norway',\n",
       " 'Berkeley, CA',\n",
       " 'Berkeley, CA, USA',\n",
       " 'Berkeley, California',\n",
       " 'Berlin',\n",
       " 'Berlin, Germany',\n",
       " 'Berne, Switzerland',\n",
       " 'Bethel Park, PA',\n",
       " 'Bethesda, MD',\n",
       " 'Between Glen_b and cardinal',\n",
       " 'Beverly Hills, CA',\n",
       " 'Bieberville, NM',\n",
       " 'Billings, MO',\n",
       " 'Birmingham, AL',\n",
       " 'Birmingham, UK',\n",
       " 'Birmingham, United Kingdom',\n",
       " 'Blacksburg, VA',\n",
       " 'Bloomington, IN',\n",
       " 'Bogota, Colombia',\n",
       " 'Bologna, Italy',\n",
       " 'Bolton, United Kingdom',\n",
       " 'Bolzano, Italy',\n",
       " 'Bonn, Germany',\n",
       " 'Boston MA',\n",
       " 'Boston, MA',\n",
       " 'Botswana',\n",
       " 'Boulder, CO',\n",
       " 'Boyle, Ireland',\n",
       " 'Bozeman, MT',\n",
       " 'Brasilia, Brazil',\n",
       " 'Bratislava, Slovakia',\n",
       " 'Braunschweig/Hannover, Germany',\n",
       " 'Brazil',\n",
       " 'Bremen, Germany',\n",
       " 'Brighton',\n",
       " 'Brighton, United Kingdom',\n",
       " 'Brisbane, Australia',\n",
       " 'Bristol, United Kingdom',\n",
       " 'British Columbia',\n",
       " 'Brno, Czech Republic',\n",
       " 'Bronx, NY',\n",
       " 'Brookline, MA',\n",
       " 'Brooklyn, AL',\n",
       " 'Brooklyn, NY',\n",
       " 'Brownbackistan (Kansas)',\n",
       " 'Brussels',\n",
       " 'Brussels, Belgium',\n",
       " 'Bucharest, Romania',\n",
       " 'Budapest, Hungary',\n",
       " 'Buenos Aires',\n",
       " 'Buenos Aires, Argentina',\n",
       " 'Buffalo, NY',\n",
       " 'Burlington, VT',\n",
       " 'CA',\n",
       " 'Cairo, Egypt',\n",
       " 'Calgary, Canada',\n",
       " 'Calgary, Canaday',\n",
       " 'California',\n",
       " 'Camarines Sur, Philippines',\n",
       " 'Cambridge, ID',\n",
       " 'Cambridge, MA',\n",
       " 'Cambridge, UK',\n",
       " 'Cambridge, United Kingdom',\n",
       " 'Cambridgeshire, United Kingdom',\n",
       " 'Campinas, Brazil',\n",
       " 'Can',\n",
       " 'Canada',\n",
       " 'Canberra, Australia',\n",
       " 'Cape Town, South Africa',\n",
       " 'Caracas, Venezuela',\n",
       " 'Cardiff, UK',\n",
       " 'Carnegie Mellon University, PA',\n",
       " 'Cartagena, Spain',\n",
       " 'Cary, NC',\n",
       " 'Castelldefels, Spain',\n",
       " 'Catalonia, Spain',\n",
       " 'Cayman Islands',\n",
       " 'Cedarville University, OH',\n",
       " 'Chalon-sur-Saône, France',\n",
       " 'Chambery, France',\n",
       " 'Champaign, IL',\n",
       " 'Channel Islands, United Kingdom',\n",
       " 'Chapel Hill, NC',\n",
       " 'Charlotte, NC',\n",
       " 'Charlottesville, VA',\n",
       " 'Chelmsford, MA',\n",
       " 'Cheltenham, United Kingdom',\n",
       " 'Chennai, India',\n",
       " 'Chester, United Kingdom',\n",
       " 'Chicago, IL',\n",
       " 'Chicagoland',\n",
       " 'Chile',\n",
       " 'China',\n",
       " 'Chisinau, Moldova',\n",
       " 'Christiansburg, VA',\n",
       " 'Cincinnati, OH',\n",
       " 'Class',\n",
       " 'Classified',\n",
       " 'Cleveland, OH',\n",
       " 'Cleveland, Ohio',\n",
       " 'Coimbatore,India',\n",
       " 'Coimbra, Portugal',\n",
       " 'College Station, TX',\n",
       " 'Cologne, Germany',\n",
       " 'Colombo, Sri Lanka',\n",
       " 'Colorado',\n",
       " 'Columbia, MO',\n",
       " 'Columbia, Maryland',\n",
       " 'Columbus, OH',\n",
       " 'Connecticut',\n",
       " 'Copenhagen, Denmark',\n",
       " 'Copenhagen/Warsaw',\n",
       " 'Cork, Ireland',\n",
       " 'Coruscant',\n",
       " 'Costa Rica',\n",
       " 'Coventry, United Kingdom',\n",
       " 'Covington, KY',\n",
       " 'Croatia',\n",
       " 'Crosslake, MN',\n",
       " 'Curitiba, Brazil',\n",
       " 'Cyberjaya, Malaysia',\n",
       " 'Czech Republic',\n",
       " 'Cáceres, Spain',\n",
       " 'DE',\n",
       " 'Dallas, TX',\n",
       " 'Dallas, TX, USA',\n",
       " 'Davis, CA',\n",
       " 'Dayton, OH',\n",
       " 'Delaware',\n",
       " 'Delft, Netherlands',\n",
       " 'Delhi',\n",
       " 'Delhi, India',\n",
       " 'Denmark',\n",
       " 'Denver',\n",
       " 'Denver, CO',\n",
       " 'Dingli, Malta',\n",
       " 'District of Columbia',\n",
       " 'District of Columbia Metro area',\n",
       " 'Distrito Federal, Mexico',\n",
       " 'Dubai, United Arab Emirates',\n",
       " 'Dublin',\n",
       " 'Dublin, Ireland',\n",
       " 'Duchy of Grand Fenwick',\n",
       " 'Duluth, MN',\n",
       " 'Dundee, Scotland',\n",
       " 'Dunedin',\n",
       " 'Dunedin, New Zealand',\n",
       " 'Durham',\n",
       " 'Durham, NC',\n",
       " 'Düsseldorf, Germany',\n",
       " 'Earth',\n",
       " 'Earth, TX',\n",
       " 'East Anglia',\n",
       " 'East Coast',\n",
       " 'East Greenwich, RI',\n",
       " 'Eau Claire, WI',\n",
       " 'Edinburgh, United Kingdom',\n",
       " 'Edmonton, Canada',\n",
       " 'Egypt',\n",
       " 'Eindhoven, Netherlands',\n",
       " 'Elgin, IL',\n",
       " 'England',\n",
       " 'England, United Kingdom',\n",
       " 'Espoo, Finland',\n",
       " 'Essen, Germany',\n",
       " 'Essex, UK',\n",
       " 'Estonia',\n",
       " 'Europe',\n",
       " 'FL',\n",
       " 'Fair Oaks, CA',\n",
       " 'Fairfax, CA',\n",
       " 'Fairfax, VA',\n",
       " 'Falcon Heights, MN',\n",
       " 'Ferrara, Italy',\n",
       " 'Fiji',\n",
       " 'Finland',\n",
       " 'Flagstaff, AZ',\n",
       " 'Flanders, Belgium',\n",
       " 'Florence, Italy',\n",
       " 'Florianópolis, Brazil',\n",
       " 'Florida',\n",
       " 'Flower Mound, Texas',\n",
       " 'Formiga, Brazil',\n",
       " 'France',\n",
       " 'Francia',\n",
       " 'Frankfurt, Germany',\n",
       " 'Fredericton, Canada',\n",
       " 'Fredrikstad, Norway',\n",
       " 'Freiburg, Germany',\n",
       " 'GB',\n",
       " 'Gainesville, AL',\n",
       " 'Gainesville, FL',\n",
       " 'Gainesville, Florida',\n",
       " 'Garner, NC',\n",
       " 'Gelderland',\n",
       " 'Geneva, Switzerland',\n",
       " 'Genoa, Italy',\n",
       " 'Georgia',\n",
       " 'Germantown, MD',\n",
       " 'Germany',\n",
       " 'Ghent, Belgium',\n",
       " 'Giessen, Germany',\n",
       " 'Gilbert, AZ',\n",
       " 'Glasgow, United Kingdom',\n",
       " 'Gold Coast',\n",
       " 'Golden, CO',\n",
       " 'Google, Inc.',\n",
       " 'Gothenburg, Sweden',\n",
       " 'Gouda, Netherlands',\n",
       " 'Grand Rapids, MI',\n",
       " 'Great Cacapon, WV',\n",
       " 'Greece',\n",
       " 'Greenville, SC',\n",
       " 'Grimsö, Sweden',\n",
       " 'Gryon, Switzerland',\n",
       " 'Gurgaon, India',\n",
       " 'Göttingen, Germany',\n",
       " 'Halifax, Canada',\n",
       " 'Hamburg, Germany',\n",
       " 'Hamburg/Kiel/Flensburg, Germany',\n",
       " 'Hamilton, Canada',\n",
       " 'Hamilton, New Zealand',\n",
       " 'Hanover, Germany',\n",
       " 'Havre de Grace, MD',\n",
       " 'Heidelberg, Germany',\n",
       " 'Hellas',\n",
       " 'Hobart, Australia',\n",
       " 'Hollywood, FL',\n",
       " 'Honduras',\n",
       " 'Hong Kong',\n",
       " 'Hong Kong, Hong Kong',\n",
       " 'Honolulu, HI',\n",
       " 'Houston',\n",
       " 'Houston, TX',\n",
       " 'Hungary',\n",
       " 'Hyderabad, India',\n",
       " 'Hyrule',\n",
       " 'I am here',\n",
       " \"I'm right here\",\n",
       " 'ITALY',\n",
       " 'Iligan City, Philippines',\n",
       " 'Illinois',\n",
       " 'India',\n",
       " 'Indiana',\n",
       " 'Indianapolis',\n",
       " 'Indianapolis, IN',\n",
       " 'Indonesia',\n",
       " 'Innsbruck, Austria',\n",
       " 'Internet Cloud',\n",
       " 'Iowa',\n",
       " 'Iowa City, IA',\n",
       " 'Iran',\n",
       " 'Ireland',\n",
       " 'Irvine, CA',\n",
       " 'Irving, TX',\n",
       " 'Israel',\n",
       " 'Istanbul, Turkey',\n",
       " 'Istanbul, Turkiye',\n",
       " 'Italy',\n",
       " 'Ithaca, NY',\n",
       " 'Jakarta, Indonesia',\n",
       " 'Japan',\n",
       " 'Jersey City, NJ',\n",
       " 'Jerusalem',\n",
       " 'Jerusalem, Israel',\n",
       " 'Jordan',\n",
       " 'Jupiter, FL',\n",
       " 'Kailua Kona, HI',\n",
       " 'Kailua, HI',\n",
       " 'Kaiserslautern, Germany',\n",
       " 'Kaneohe, HI',\n",
       " 'Kansas',\n",
       " 'Kansas City, MO',\n",
       " 'Kaohsiung, Taiwan',\n",
       " 'Karachi, Pakistan',\n",
       " 'Karlsruhe, Germany',\n",
       " 'Kassel, Germany',\n",
       " 'Kaunas, Republic of Lithuania',\n",
       " 'Kenya',\n",
       " 'Kerala, India',\n",
       " 'Kiel, Germany',\n",
       " 'Kiev, Ukraine',\n",
       " 'Kilifi, Kenya',\n",
       " \"King's College London, United Kingdom\",\n",
       " 'Kingdom of Zhao',\n",
       " 'Kingston, Canada',\n",
       " 'Kitchener, Canada',\n",
       " 'Knoxville, TN',\n",
       " 'Kolkata, India',\n",
       " 'Korea',\n",
       " 'Kuala Lumpur, Malysia.',\n",
       " 'Kuopio, Finland',\n",
       " 'Kyoto, Japan',\n",
       " 'Kyoto-shi, Japan',\n",
       " 'La Jolla',\n",
       " 'La Jolla, CA',\n",
       " 'Lahore, Pakistan',\n",
       " 'Lakewood, CO',\n",
       " 'Lancaster, United Kingdom',\n",
       " 'Las Vegas, NV',\n",
       " 'Latvia',\n",
       " 'Laurel, MD',\n",
       " 'Lausanne, Switzerland',\n",
       " 'Leamington, United Kingdom',\n",
       " 'Leeds, United Kingdom',\n",
       " 'Leicester, United Kingdom',\n",
       " 'Leiculeşti, Romania',\n",
       " 'Leiden, Netherlands',\n",
       " 'Leuven, Belgium',\n",
       " 'Lexington, KY',\n",
       " 'Leyte',\n",
       " 'Lille, France',\n",
       " 'Lima, Peru',\n",
       " 'Lincoln, NE',\n",
       " 'Linz, Austria',\n",
       " 'Lisbon, Portugal',\n",
       " 'Lithuania',\n",
       " 'Lititz, PA',\n",
       " 'Little Rock, AR',\n",
       " 'Little Venice',\n",
       " 'Liverpool, United Kingdom',\n",
       " 'Livingston, LA',\n",
       " 'Ljubljana, Slovenia',\n",
       " 'Lodz, Poland',\n",
       " 'Lomas De Zamora, Argentina',\n",
       " 'London',\n",
       " 'London, Canada',\n",
       " 'London, Europe',\n",
       " 'London, UK',\n",
       " 'London, United Kingdom',\n",
       " 'London, uk',\n",
       " 'London,UK',\n",
       " 'Longwood University, VA',\n",
       " 'Los Angeles',\n",
       " 'Los Angeles, CA',\n",
       " 'Los Gatos, CA',\n",
       " 'Louisiana',\n",
       " 'Ludwigsburg, Germany',\n",
       " 'Lund, Sweden',\n",
       " 'Luxembourg & Heidelberg',\n",
       " 'Lviv, Ukraine',\n",
       " 'Maastricht, Netherlands',\n",
       " 'Macedonia',\n",
       " 'Macquarie University, Australia',\n",
       " 'Madison, WI',\n",
       " 'Madison, Wisconsin',\n",
       " 'Madrid, Spain',\n",
       " 'Madurai, India',\n",
       " 'Maine',\n",
       " 'Malaysia',\n",
       " 'Malta',\n",
       " 'Manchester, United Kingdom',\n",
       " 'Manhattan, KS',\n",
       " 'Manila, Philippines',\n",
       " 'Mansfield, CT',\n",
       " 'Mars',\n",
       " 'Maryland',\n",
       " 'Massachusetts',\n",
       " 'Mcmaster University, Canada',\n",
       " 'Medellin, Colombia',\n",
       " 'Melboure, Australia',\n",
       " 'Melbourne',\n",
       " 'Melbourne, Australia',\n",
       " 'Memphis, TN',\n",
       " 'Menlo Park, CA',\n",
       " 'Mexico',\n",
       " 'Mexico City, Mexico',\n",
       " 'Michigan',\n",
       " 'Michigan State University, MI',\n",
       " 'Mid West, US',\n",
       " 'Midland, TX',\n",
       " 'Milan, Italy',\n",
       " 'Milton, GA',\n",
       " 'Milwaukee, WI',\n",
       " 'Minga',\n",
       " 'Minneapolis, MN',\n",
       " 'Minnesota',\n",
       " 'Missouri',\n",
       " 'Monterey, CA',\n",
       " 'Montreal',\n",
       " 'Montreal, Canada',\n",
       " 'Montreal, QC',\n",
       " 'Montreal, Quebec',\n",
       " 'Morgantown, WV',\n",
       " 'Morocco',\n",
       " 'Moscow, Russia',\n",
       " 'Mount Sinai, NY',\n",
       " 'Mountain View, CA',\n",
       " 'Mountain View, California',\n",
       " 'Mumbai, India',\n",
       " 'Munich',\n",
       " 'Munich, Germany',\n",
       " 'Myrtle Beach, SC',\n",
       " 'NC, USA',\n",
       " 'ND',\n",
       " 'NE UK',\n",
       " 'NY',\n",
       " 'NYC',\n",
       " 'NYC, WHOI, or Antarctica',\n",
       " 'Nairobi, Kenya',\n",
       " 'Naples, Italy',\n",
       " 'Nashua, NH',\n",
       " 'Nashville, TN',\n",
       " 'Natick, MA',\n",
       " 'National University Of Singapore',\n",
       " 'Nepal',\n",
       " 'Nerdvana',\n",
       " 'Netherlands',\n",
       " 'New England',\n",
       " 'New Haven, CT',\n",
       " 'New Haven, CT, USA',\n",
       " 'New Jersey',\n",
       " 'New Mexico',\n",
       " 'New Orleans, LA',\n",
       " 'New Orleans, Louisiana',\n",
       " 'New York',\n",
       " 'New York, NY',\n",
       " 'New York, New York',\n",
       " 'New York, United States',\n",
       " 'New Zealand',\n",
       " 'Newark, DE',\n",
       " 'Newcastle UK',\n",
       " 'Newcastle, CA',\n",
       " 'Newcastle, United Kingdom',\n",
       " 'Newport Beach, CA',\n",
       " 'Nish, Serbia',\n",
       " 'Nomad, MI',\n",
       " 'Norrtälje, Sweden',\n",
       " 'North America',\n",
       " 'North Carolina',\n",
       " 'North Pole',\n",
       " 'North Salt Lake, UT',\n",
       " 'Northeastern US',\n",
       " 'Northern Europe',\n",
       " 'Norway',\n",
       " 'Norwich',\n",
       " 'Not from around here',\n",
       " 'Nottingham, United Kingdom',\n",
       " 'Nova Scotia, Canada',\n",
       " 'Oakland, CA',\n",
       " 'Odessa, Ukraine',\n",
       " 'Oklahoma',\n",
       " 'Omaha, NE',\n",
       " 'Ontario',\n",
       " 'Ontario, Canada',\n",
       " 'Oregon',\n",
       " 'Oregon/Washington',\n",
       " 'Orlando',\n",
       " 'Orlando, FL',\n",
       " 'Osaka-shi, Japan',\n",
       " 'Oslo',\n",
       " 'Oslo, Norway',\n",
       " 'Ottawa',\n",
       " 'Ottawa, Canada',\n",
       " 'Oxford, UK',\n",
       " 'Oxford, United Kingdom',\n",
       " 'Oz',\n",
       " 'PVD, RI',\n",
       " 'Pacific Northwest',\n",
       " 'Pacific Palisades, CA',\n",
       " 'Pakistan',\n",
       " 'Palm Beach, FL',\n",
       " 'Palo Alto, CA',\n",
       " 'Panama',\n",
       " 'Paris France',\n",
       " 'Paris, France',\n",
       " 'Passau, Germany',\n",
       " 'Pennsylvania',\n",
       " 'Persian Gulf',\n",
       " 'Perth, Australia',\n",
       " 'Petaluma, CA',\n",
       " 'Peterborough, Canada',\n",
       " 'Phase space',\n",
       " 'Philadelphia, PA',\n",
       " 'Philippines',\n",
       " 'Phillips, ME',\n",
       " 'Phoenix, AZ',\n",
       " 'Phoenix, AZ, USA',\n",
       " 'Piliscsaba, Hungary',\n",
       " 'Pilsen, Czech Republic',\n",
       " 'Piracicaba, SP, Brazil',\n",
       " 'Pisa (Italy)',\n",
       " 'Pittsburgh, PA',\n",
       " 'Plymouth, United Kingdom',\n",
       " 'Poland',\n",
       " 'Portland, OR',\n",
       " 'Portland, Oregon',\n",
       " 'Portland,OR',\n",
       " 'Porto Alegre, Brazil',\n",
       " 'Porto, Portugal',\n",
       " 'Portugal',\n",
       " 'Potchefstroom, South Africa',\n",
       " 'Potsdam, Germany',\n",
       " 'Poznan, Poland',\n",
       " 'Prague',\n",
       " 'Prague, Czech Republic',\n",
       " 'Pratteln, Switzerland',\n",
       " 'Pretoria, South Africa',\n",
       " 'Princeton, NJ',\n",
       " 'Providence RI',\n",
       " 'Providence, RI',\n",
       " 'Provo, UT',\n",
       " 'Pryor, OK',\n",
       " 'Pune, India',\n",
       " 'Qatar',\n",
       " 'Quebec City, Canada',\n",
       " 'Queen Mary University London',\n",
       " 'Rabat, Morocco',\n",
       " 'Raleigh, NC',\n",
       " 'Redmond, WA',\n",
       " 'Redmond, WA, United States',\n",
       " 'Redwood City, CA',\n",
       " 'Regina, Canada',\n",
       " 'Rennes, France',\n",
       " 'Research Triangle Park',\n",
       " 'Reston, VA',\n",
       " 'Reykjavik, Iceland',\n",
       " 'Rhode Island',\n",
       " 'Richardson, TX',\n",
       " 'Riegel am Kaiserstuhl, Germany',\n",
       " 'Riga, Republic of Latvia',\n",
       " 'Rio De Janeiro, Brazil',\n",
       " 'Rio de Janeiro',\n",
       " 'Rio de Janeiro, Brazil',\n",
       " 'Riverside, CA',\n",
       " 'Rochester, MN',\n",
       " 'Rochester, NY',\n",
       " 'Rockefeller University, NY',\n",
       " 'Romania',\n",
       " 'Rome, Italy',\n",
       " 'Rotterdam',\n",
       " 'Rotterdam, Netherlands',\n",
       " 'Rotterdam, The Netherlands',\n",
       " 'Rouen, France',\n",
       " 'Russia',\n",
       " 'Russia, Moscow, Yandex',\n",
       " 'SF',\n",
       " 'Saarbrücken, Germany',\n",
       " 'Saint Moscow',\n",
       " 'Salem, OR',\n",
       " 'Salt Lake City, UT',\n",
       " 'Salt Lake City, Utah',\n",
       " 'Salto, Brazil',\n",
       " 'Salvador, Brazil',\n",
       " 'San Antonio, TX',\n",
       " 'San Diego',\n",
       " 'San Diego, CA',\n",
       " 'San Francisco Bay Area',\n",
       " 'San Francisco, CA',\n",
       " 'San Jose, CA',\n",
       " 'San Marcos, CA',\n",
       " 'San Marcos, TX',\n",
       " 'San Mateo, CA',\n",
       " \"San Michele all'Adige, Italy\",\n",
       " 'San francisco',\n",
       " 'Santa Barbara, CA',\n",
       " 'Santa Clara, CA',\n",
       " 'Santa Cruz, CA',\n",
       " 'Santa Fe, NM',\n",
       " 'Santa Monica CA',\n",
       " 'Santa Monica, CA',\n",
       " 'Santiago, Chile',\n",
       " 'Sao Paulo, Brazil',\n",
       " 'Sapporo-shi, Japan',\n",
       " 'Sarasota, FL',\n",
       " 'Sarrebruck, Germany',\n",
       " 'Saskatoon, Canada',\n",
       " 'Scotland, United Kingdom',\n",
       " 'Seattle',\n",
       " 'Seattle, WA',\n",
       " 'Seattle, Washington',\n",
       " 'Senegal',\n",
       " 'Seoul, Korea',\n",
       " 'Seoul, South Korea',\n",
       " \"Sevastopol', Ukraine\",\n",
       " 'Shandong, China',\n",
       " 'Shanghai',\n",
       " 'Shanghai, China',\n",
       " 'Sheffield, United Kingdom',\n",
       " 'Sherbrooke, Canada',\n",
       " 'Silicon Valley',\n",
       " 'Silver Spring, MD, US, Earth',\n",
       " 'Singapore',\n",
       " 'Singapore, Singapore',\n",
       " 'Slovenia',\n",
       " 'Socorro, NM',\n",
       " 'Sofia, Bulgaria',\n",
       " 'Somerville, Massachusetts',\n",
       " 'Sorocaba, Brazil',\n",
       " 'South Africa',\n",
       " 'South Dakota',\n",
       " 'South Korea',\n",
       " 'Southern California',\n",
       " 'Spain',\n",
       " 'Spanish Fork, UT',\n",
       " 'Spokane, WA',\n",
       " 'Springfield, MO',\n",
       " 'Sri Lanka',\n",
       " 'St Louis, MO',\n",
       " 'St Paul, MN',\n",
       " 'St Petersburg, FL',\n",
       " 'St. Gallen',\n",
       " \"St. John's, Canada\",\n",
       " 'St. Louis',\n",
       " 'St. Louis, MO',\n",
       " 'St. Paul, MN',\n",
       " 'St. Petersburg, Russia',\n",
       " 'Stanford, CA',\n",
       " 'State College, PA',\n",
       " 'Stellenbosch, South Africa',\n",
       " 'Steyr, Austria',\n",
       " 'Stockholm',\n",
       " 'Stockholm, Sweden',\n",
       " 'Stony Brook, NY',\n",
       " 'Subang Jaya, Malaysia',\n",
       " 'Sundsvall, Sweden',\n",
       " 'Sunnyvale, CA',\n",
       " 'Surrey, United Kingdom',\n",
       " 'Swansea MA',\n",
       " 'Swansea, United Kingdom',\n",
       " 'Sweden',\n",
       " 'Switzerland',\n",
       " 'Sydney, Australia',\n",
       " 'São Carlos, Brazil',\n",
       " 'São Paulo - Brazil',\n",
       " 'Tacoma, WA',\n",
       " 'Taipei & Hong Kong',\n",
       " 'Taiwan',\n",
       " 'Taloon',\n",
       " 'Tampa, FL',\n",
       " 'Tartu, Estonia',\n",
       " 'Taunton, United Kingdom',\n",
       " 'Tbilisi, Georgia',\n",
       " 'Tehran, Iran',\n",
       " 'Tel Aviv',\n",
       " 'Tel Aviv, Israel',\n",
       " 'Tempe, AZ',\n",
       " 'Tennessee',\n",
       " 'Texas',\n",
       " 'Thailand',\n",
       " 'The Earth',\n",
       " 'The Hague',\n",
       " 'The Netherlands',\n",
       " 'The Woodlands, TX',\n",
       " 'Thessaloniki, Greece',\n",
       " 'Thunder Bay, Canada',\n",
       " 'Tiburon, CA',\n",
       " 'Tokyo',\n",
       " 'Tokyo, Japan',\n",
       " 'Tornado, WV',\n",
       " 'Toronto, Canada',\n",
       " 'Toronto, Ontario, Canada',\n",
       " 'Toulouse, France',\n",
       " 'Townsville, Australia',\n",
       " 'Trento, Italy',\n",
       " 'Trieste, Italy',\n",
       " 'Tucson, AZ',\n",
       " 'Tunis, Tunisia',\n",
       " 'Turin, Italy',\n",
       " 'Turkey',\n",
       " 'Turku, Finland',\n",
       " 'Tønsberg, Norway',\n",
       " 'Tübingen, Germany',\n",
       " 'U.S.',\n",
       " 'UCL, United Kingdom',\n",
       " 'UCSF',\n",
       " 'UK',\n",
       " 'UK, Iran',\n",
       " 'US',\n",
       " 'USA',\n",
       " 'Ukraine',\n",
       " 'United Kingdom',\n",
       " 'United States',\n",
       " 'United States of America',\n",
       " 'University of Chicago, IL',\n",
       " 'University of Colorado, Boulder',\n",
       " 'University of Connecticut',\n",
       " 'University of Detroit Mercy',\n",
       " 'University of Southern California, CA',\n",
       " 'University of Waterloo, Canada',\n",
       " 'Uppsala, Sweden',\n",
       " 'Urbana, IL',\n",
       " 'Utah',\n",
       " 'Utah, United States',\n",
       " 'Uzhhorod, Ukraine',\n",
       " 'Valladolid, Spain',\n",
       " 'Vancouver',\n",
       " 'Vancouver, B.C.',\n",
       " 'Vancouver, Canada',\n",
       " 'Vathorst,Netherlands',\n",
       " 'Vay, France',\n",
       " 'Venezuela',\n",
       " 'Versailles, KY',\n",
       " 'Vienna',\n",
       " 'Vienna, Austria',\n",
       " 'Vilnius, Lithuania',\n",
       " 'Virginia',\n",
       " 'Walnut Creek, CA',\n",
       " 'Warsaw, Poland',\n",
       " 'Washington D.C.',\n",
       " 'Washington DC, United States',\n",
       " 'Washington, DC',\n",
       " 'Washington, District of Columbia, United States',\n",
       " 'Washington, United States',\n",
       " 'Waterloo, Canada',\n",
       " 'Weimar, Germany',\n",
       " 'Wellesley, MA',\n",
       " 'Wellington, New Zealand',\n",
       " 'Wenatchee, WA',\n",
       " 'West Chester, PA',\n",
       " 'West Kelowna, Canada',\n",
       " 'West Lafayette, IN',\n",
       " 'West Virginia',\n",
       " 'Westmont, IL',\n",
       " 'Westwood, MA',\n",
       " 'Wichita, KS',\n",
       " 'Williamsport, PA',\n",
       " 'Willich, Germany',\n",
       " 'Winnipeg, Canada',\n",
       " 'Winston-Salem, NC',\n",
       " 'Wisconsin',\n",
       " 'Worcester, MA',\n",
       " 'Wuhan, China',\n",
       " 'Wurzburg, Germany',\n",
       " 'Yaounde, Cameroon',\n",
       " 'Yerevan, Armenia',\n",
       " 'Youngstown, OH',\n",
       " 'Zagreb, Croatia',\n",
       " 'Zaragoza, Spain',\n",
       " 'Ziqim, Israel',\n",
       " 'Zurich, Switzerland',\n",
       " 'amsterdam',\n",
       " 'berlin.de',\n",
       " 'if it was your ass you would know',\n",
       " 'los angeles',\n",
       " nan,\n",
       " 'on the server farm',\n",
       " 'paris',\n",
       " 'san diego',\n",
       " 'seattle',\n",
       " 'Île Maurice',\n",
       " 'Östersund, Sweden',\n",
       " 'İstanbul, Turkey'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " 'PhD student',\n",
       " \"<p>I'm on an MD/PhD training fellowship at UCLA. My PhD research was in the departments of Neurology and Biomedical Engineering, and focuses on the field of developmental neuroscience. We use MRI to study how the brain develops structurally, and how this relates to our developing cognitive abilities.</p>\\r\\n\\r\\n<p>Now I am heading back to the last part medical school, but still like to come on here to keep my skills sharp and stay current on new topics.</p>\\r\\n\",\n",
       " '<p>Hi!</p>\\r\\n\\r\\n<p>sorry - I am C/C++ noobe, and I am reading a book<code>=)</code></p>\\r\\n\\r\\n<p>verse 4 rfc1925</p>\\r\\n\\r\\n<blockquote>\\r\\n  <p>Some things in life can never be fully appreciated nor\\r\\n          understood unless experienced firsthand. Some things in\\r\\n          networking can never be fully understood by someone who neither\\r\\n          builds commercial networking equipment nor runs an operational\\r\\n          network.</p>\\r\\n</blockquote>\\r\\n',\n",
       " \"I'm student in cs.I'm picking up Perl and Python recently.\",\n",
       " '<p>At various times a user of python (esp. django, tornado, and numpy/scipy), javascript (esp. jquery), Cognos, and statistical analysis packages similar to the programming language R.  Also did a few years in a mainframe gig.</p>\\r\\n',\n",
       " '<p>Love C++ and Python programming, and mess with mathematics and algorithms.</p>\\r\\n',\n",
       " '<p>I provide statistical consulting for clients in private firms, public agencies, and academic institutions.  I have had over 25 years of experience as a consultant and a Research Professor. Please visit my website to learn more about obtaining personalized consultation services.</p>\\r\\n',\n",
       " '<p>I work under the propulsion directorate in the Air Force Research Laboratory aka (AFRL).  Currently working on an iPad app, just picked up Xcode and Objective-C so this is one of my more challenging projects.  </p>\\r\\n',\n",
       " '<p>Master\\'s Degree Student at University of Bergen, Norway</p>\\r\\n\\r\\n<p><a href=\"http://www.uib.no/\" rel=\"nofollow\">http://www.uib.no/</a></p>\\r\\n',\n",
       " '<p>Electronics and Communication Engineering</p>\\r\\n',\n",
       " '<p>Pretty much new to everything but highly enjoy c# and asp. </p>\\r\\n',\n",
       " \"<p>I'm a data scientist in Stockholm who love linux, climbing, coding and statistics.</p>\\r\\n\",\n",
       " '<p>Psychology, Stats</p>\\r\\n',\n",
       " '<p>Passionate Ruby lover, machine learning and data science wannabe.</p>\\r\\n',\n",
       " '<p>www.linkedin.com/in/jalospinoso/</p>\\r\\n',\n",
       " '<p>Arrest this man he talks in maths</p>\\r\\n',\n",
       " '<p>Saved, husband, father, experimental test pilot, statistician.</p>\\r\\n',\n",
       " '<p>My <em>about me</em> is currently blank</p>\\r\\n',\n",
       " '<p>UK computer networking student running several websites</p>\\r\\n\\r\\n<p><a href=\"http://www.facetimedirectory.net\" rel=\"nofollow\">Facetime Social Network</a></p>\\r\\n\\r\\n<p><a href=\"http://www.mintuz.co.uk\" rel=\"nofollow\">UK Web designer</a></p>\\r\\n\\r\\n<p>I am learning in my spare time objective-c cocoa touch and c#. Planning to make my own app dev company.</p>\\r\\n',\n",
       " '<p>i sometimes worry about the coming robot uprising.</p>\\r\\n',\n",
       " 'Not a programmer but not afraid to use simple programming / macroing such as manipulate and analyze data in R or write 3-10 line autohotkey commands.',\n",
       " '<p>Physician, programmer, beginner statistician and data miner, fan of Above and Beyond and Ferry Corsten! Yeas ;)</p>\\r\\n',\n",
       " '<p>loves python, cooking, yoga, math and loud electrical guitars.</p>\\r\\n',\n",
       " '<p>No information avaiable I want anybody to see openly in the internet.</p>\\r\\n',\n",
       " '<p>I teach probability and statistics at the University of Alberta.</p>\\r\\n',\n",
       " '<p>Mechanical and Aerospace Engineer</p>\\r\\n',\n",
       " 'I mostly code in R, MATLAB and C#.',\n",
       " '<p>Experience as a Windows/Linux/Unix Sysadmin as well as an Integration(EDI) programmer.</p>\\r\\n',\n",
       " 'Statistics, blogging, and the hope for a happy long life.',\n",
       " '<p>Purdue Statistics Ph.D Candidate</p>\\r\\n',\n",
       " '<p>Mainly interested in Python and R but with heart-warming emphasis on Logo and all its derivatives. </p>\\r\\n',\n",
       " '<p><a href=\"http://en.wikipedia.org/wiki/Big_ball_of_mud\" rel=\"nofollow\">Big Ball of Mud</a> maintainer (mostly Java, C++, and SQL - Perl, Python, and JavaScript as necessary) currently living in Charlotte, NC.  I also tutor Java, Python, and JavaScript part-time at <a href=\"http://www.franklin.edu/\" rel=\"nofollow\">Franklin University</a>.<br/></p>\\r\\n\\r\\n<p>I blog about programming, math, learning, and technology at <a href=\"http://www.BillTheLizard.com\" rel=\"nofollow\">BillTheLizard.com</a>.</p>\\r\\n\\r\\n<p>You can follow me on Twitter, <a href=\"http://twitter.com/lizardbill\" rel=\"nofollow\">@lizardbill</a>.  (If I recognize you from SO or meta, I\\'ll probably follow you back.)  You can also find me on <a href=\"http://gplus.to/lizardbill\" rel=\"nofollow\">Google+</a> and on <a href=\"https://github.com/BillCruise\" rel=\"nofollow\">GitHub</a>. (<strong>Questions about moderator actions on Stack Overflow should still be posted on <a href=\"http://meta.stackoverflow.com/\">Meta Stack Overflow</a>.</strong>)</p>\\r\\n\\r\\n<p>I also wrote <a href=\"https://twitter.com/BountyBot\" rel=\"nofollow\">@BountyBot</a>, a Twitter bot that posts new and interesting bounty questions from Stack Overflow. You can <a href=\"https://github.com/BillCruise/BountyBot\" rel=\"nofollow\">view the source on GitHub</a>.</p>\\r\\n',\n",
       " '<p>When things get tough, the tough get going</p>\\r\\n',\n",
       " '<p>a research student</p>\\r\\n',\n",
       " '<p>Procastinating is fun.</p>\\r\\n',\n",
       " '<p>Graduate research student /\\r\\nTerrestrial Ecohydrology /\\r\\nClimate Change Cluster /\\r\\nFaculty of Science /\\r\\nUniversity of Technology Sydney</p>\\r\\n',\n",
       " '<p>Here.</p>\\r\\n',\n",
       " \"<p>“It is not that I'm so smart. But I stay with the questions much longer.”\\r\\n― Albert Einstein</p>\\r\\n\\r\\n<p>“The more I read, the more I acquire, the more certain I am that I know nothing.”\\r\\n― Voltaire</p>\\r\\n\\r\\n<p>“Study hard what interests you the most in the most undisciplined, irreverent and original manner possible.”\\r\\n― Richard P. Feynman</p>\\r\\n\\r\\n<p>“I think the big mistake in schools is trying to teach children anything, and by using fear as the basic motivation. Fear of getting failing grades, fear of not staying with your class, etc. Interest can produce learning on a scale compared to fear as a nuclear explosion to a firecracker.”\\r\\n― Stanley Kubrick</p>\\r\\n\\r\\n<p>“We are all failures- at least the best of us are.”\\r\\n― J.M. Barrie</p>\\r\\n\\r\\n<p>“Learning is not child's play; we cannot learn without pain.”\\r\\n― Aristotle</p>\\r\\n\\r\\n<p>“Learning never exhausts the mind.”\\r\\n― Leonardo da Vinci</p>\\r\\n\\r\\n<p>“I’ve seen how you can’t learn anything when you’re trying to look like the smartest person in the room.”\\r\\n― Barbara Kingsolver</p>\\r\\n\\r\\n<p>“The authority of those who teach is often an obstacle to those who want to learn.”\\r\\n― Cicero</p>\\r\\n\\r\\n<p>“Real learning comes about when the competitive spirit has ceased.”\\r\\n― Jiddu Krishnamurti</p>\\r\\n\\r\\n<p>“For the best return on your money, pour your purse into your head.”\\r\\n― Benjamin Franklin</p>\\r\\n\\r\\n<p>“The formulation of the problem is often more essential than its solution, which may be merely a matter of mathematical or experimental skill.”\\r\\n― Albert Einstein</p>\\r\\n\",\n",
       " '<p>I am a research scientist interested in the computational modeling of biological phenomenon.</p>\\r\\n',\n",
       " '<p>About Me.</p>\\r\\n',\n",
       " '<p>I am a ninja.</p>\\r\\n',\n",
       " '<p>Econometrician</p>\\r\\n',\n",
       " '<pre><code>\\u3000\\r\\n\\u3000\\r\\n\\u3000\\r\\n                         (__)\\r\\n                         (--) ... ( *&gt;YAWN&lt;* )\\r\\n                   /------\\\\\\\\/\\r\\n                  /|     ||\\r\\n                 * ||----||\\r\\n                   ~~    ~~\\r\\n\\u3000\\r\\n\\u3000\\r\\n\\u3000\\r\\n\\u3000\\r\\n</code></pre>\\r\\n\\r\\n\\r\\n',\n",
       " '<p>Web Developer specializing in PHP, JavaScript, Ajax, JQuery and CSS3 with 3 years of experience creating and maintaining professional web applications. Experience developing technical specifications, documenting code and procedures. Effectively develop well structured, easily maintainable applications, web components, pages, style sheets.</p>\\r\\n',\n",
       " '<p>I do a bunch of stuff.  Lately a lot of Scala, Play Framework, Hadoop/Hbase/Pig/, Redshift, and some python.</p>\\r\\n',\n",
       " \"<p>Three years ago I start scripting - since then I've spent endless hours learning. I don't think I've had a single day in those three years that I haven't learnt something new about computers. And of course, there is no better way to learn than to do - probably why I'd happily rewrite the wheel to find out how it works. Of course my personal favourite is maths. Studying Further Maths for my AS levels gives me plenty to do implementing various theories, particularly in decision maths.</p>\\r\\n\",\n",
       " \"<p>I predict US bank financials and generate the world's largest body of US bank analytics, worldwide.</p>\\r\\n\",\n",
       " '<p>Former mechanical engineer who migrated from FORTRAN to C, now software developer who went from C++ to Java.   Trying to break out of the \"curly brace\" mold by learning Python.  90th to earn Legendary badge.</p>\\r\\n',\n",
       " '<p>2nd year PhD student at UT Austin</p>\\r\\n',\n",
       " \"<p>I'm a computer science PhD student, with a severe lack of practical computing ability. No-one has called me on it yet, but I'm sure the day is only just around the corner...</p>\\r\\n\",\n",
       " '<p>I\\'m a PhD candidate in Quantitative and Computational Biology at Princeton University.</p>\\r\\n\\r\\n<p><a href=\"https://github.com/dgrtwo\" rel=\"nofollow\">My GitHub</a></p>\\r\\n',\n",
       " '<p>Ben Hanowell (aka Brash Equilibrium) is an anthropologist and Fulbrighter. He lives in Seattle and is the virgin birth of an android ninja.</p>\\r\\n',\n",
       " '<p>Interests:Statistical Learning Theory, Kernel Methods, Robust Statistics,Nonparametric Probability Density Estimation</p>\\r\\n',\n",
       " '<p>Software Engineering PhD Student at the University of Illinois at Urbana-Champaign</p>\\r\\n',\n",
       " '<blockquote>\\r\\n  <p>[...] from this point of view, when you contemplate what to do about\\r\\n  outliers, you’re like Marlow, in “Heart of Darkness,” when he is\\r\\n  travelling up-river to find Kurtz. “Watching a coast as it slips by\\r\\n  the ship,” Conrad writes,</p>\\r\\n  \\r\\n  <blockquote>\\r\\n    <p>is like thinking about an enigma. There it is before you—smiling,\\r\\n    frowning, inviting, grand, mean, insipid, or savage, and always mute\\r\\n    with an air of whispering, ‘Come and find out.’</p>\\r\\n  </blockquote>\\r\\n</blockquote>\\r\\n',\n",
       " '<p><a href=\"http://gordoncluster.wordpress.com\" rel=\"nofollow\">http://gordoncluster.wordpress.com</a></p>\\r\\n',\n",
       " \"I teach high school math in Oakland, CA.  I studied some computer science in college, and I eventually plan to get an MS in CS.  I dabble in Java, Perl, C++, and Objective-C, and I've held an unhealthy interest in Joel and Jeff's podcast since Stack Overflow's inception.\",\n",
       " '<p>I like statistical modeling and its applications. I am good in R.</p>\\r\\n\\r\\n<p><img src=\"http://i.stack.imgur.com/WDsug.jpg\" alt=\"enter image description here\"></p>\\r\\n',\n",
       " 'Graduate student in Physics',\n",
       " 'Bioinformatics reasearch, mostly with MATLAB.',\n",
       " \"<p>Academic in the Dept of Communication &amp; Systems at The Open University. I'm not a statistician, but I am a public open data junkie...</p>\\r\\n\",\n",
       " '<p>Happily geeking away in Birmingham, UK</p>\\r\\n',\n",
       " '<p>I work in computational biology in New York and Boston.  I do a great deal of numerical analysis, optimization and parallel algorithm design.</p>\\r\\n\\r\\n<p>You can learn more about my professional research and work at <a href=\"http://andrewmatteson.com\" rel=\"nofollow\">andrewmatteson.com</a>.  I host a blog about all things math, programming and biology at <a href=\"http://axiomofcats.com\" rel=\"nofollow\">axiomofcats.com</a>.</p>\\r\\n',\n",
       " '<p><em>Researcher</em> in Intellectual Data Mining department at <strong>DATADVANCE lls.</strong> (an EADS company).</p>\\r\\n',\n",
       " '<p>I always wondered what this box was for...</p>\\r\\n',\n",
       " '<p>J2EE Developer and full time Ubuntu Linux user.\\r\\nWell versed and or certified in some of the following:\\r\\nAnt, Linux admin, Windows administration/deployments, Java, C, php, Apache, MySQL, javascript, HTML, XML</p>\\r\\n',\n",
       " '<p><a href=\"http://jaheruddin.nl\" rel=\"nofollow\">http://jaheruddin.nl</a></p>\\r\\n',\n",
       " \"<p>I'm a historian who also happens to tinker around in code for my research. My interests involve textual analysis, web scraping, and the like.</p>\\r\\n\",\n",
       " '<p>Software Engineer with a Video Game Background</p>\\r\\n',\n",
       " '<p>Monte Carlo, risk, QMC, statistical efficiency, high dimensional approximation...</p>\\r\\n',\n",
       " '<p><a href=\"https://heatma.ps\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/hXd1e.png\" alt=\"enter image description here\"></a></p>\\r\\n\\r\\n<h1><a href=\"https://heatma.ps\" rel=\"nofollow\">Find out where they touch your app!</a></h1>\\r\\n',\n",
       " '<p>I am PHD student in international black sea university,city Tbilisi,capital of georgia,my interest field in my PHD degree is signal processing,additionaly calculus,linear algebra ,functional analysis,topology and combinatoric</p>\\r\\n',\n",
       " '<p><a href=\"http://www.twitter.com/keyboardp\" rel=\"nofollow\">Twitter - @keyboardP</a></p>\\r\\n',\n",
       " '<p>Just another geek</p>\\r\\n',\n",
       " \"<p>I don't really know yet.</p>\\r\\n\",\n",
       " '<p>Solving problems for fun and for profit since the last century.</p>\\r\\n',\n",
       " \"<p>I'm a graduate in computational linguistics and received my master's degree from Saarland University, Saarbrücken, Germany. </p>\\r\\n\\r\\n<p>I'm interested in answering questions related to Python programming and web development, preferably using Django and other Python-based technologies. Linguistics is always nice as well, of course.</p>\\r\\n\\r\\n<p>Jobwise, I've been working with Adobe ColdFusion very intensively, so I might ask or answer questions related to ColdFusion as well.</p>\\r\\n\",\n",
       " \"<p>If you don't ask, you won't know. </p>\\r\\n\",\n",
       " '<p>Doctoral student in computational linguistics. Hoping to make machines understand natural language since human beings are not doing a great job on this front.</p>\\r\\n',\n",
       " '<p>I am a student.</p>\\r\\n',\n",
       " '<p>Principal - Game Analytics @ Activision</p>\\r\\n',\n",
       " '<p><a href=\"http://stackexchange.com/users/232418\">\\r\\n<img src=\"http://stackexchange.com/users/flair/232418.png\" width=\"208\" height=\"58\" alt=\"profile for Simon Hayward on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Simon Hayward on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\\r\\n</a></p>\\r\\n\\r\\n<p>Hi everyone!</p>\\r\\n\\r\\n<p>I\\'m a statistician working in online media, and I\\'m largely self taught in programming, I like working things through for myself which is why I\\'m always asking about guides to do things.</p>\\r\\n\\r\\n<p>I have more ideas than I have time to develop so these sites are always a huge help.</p>\\r\\n',\n",
       " \"<p>I'm an Assistant Professor in the Department of Political Science at the University of Rochester. I help develop the Amelia package for the R statistical platform. </p>\\r\\n\",\n",
       " '<p>GIS, data analyst and more</p>\\r\\n',\n",
       " '<p>Systems Engineer, with a Masters in Computational Finance from Carnegie Mellon University. Creator of the backtesting software called \"Indixio\" (formerly code-named SAMOA): www.nestorsulikowski.com\\r\\n<br/>\\r\\nFounder of Achronos Capital Management (main users of Indixio): www.achronoscm.com\\r\\n<br/>\\r\\n<br/>\\r\\n<strong>\\r\\nWriting software is one of the pleasures of life!\\r\\n</strong></p>\\r\\n',\n",
       " '<p>Some physics material I wrote:</p>\\r\\n\\r\\n<p><LI><A HREF=\"http://www.mat.univie.ac.at/~neum/physfaq/physics-faq.html\" rel=\"nofollow\">\\r\\nA theoretical physics FAQ</A>\\r\\n<LI><A HREF=\"http://de.arxiv.org/abs/0810.1019\" rel=\"nofollow\">\\r\\nClassical and Quantum Mechanics via Lie algebras</A>\\r\\n<LI><A HREF=\"http://www.mat.univie.ac.at/~neum/ms/ren.pdf\" rel=\"nofollow\"> \\r\\nRenormalization without infinities - an elementary tutorial</A>\\r\\n<LI><A HREF=\"http://www.mat.univie.ac.at/~neum/ms/lightslides.pdf\" rel=\"nofollow\"> \\r\\nClassical and quantum field aspects of light</A>\\r\\n<LI><A HREF=\"http://www.mat.univie.ac.at/~neum/ms/optslides.pdf\" rel=\"nofollow\"> \\r\\nOptical models for quantum mechanics</A>\\r\\n<LI><A HREF=\"http://www.mat.univie.ac.at/~neum/ms/protein.pdf\" rel=\"nofollow\"> \\r\\nMolecular modeling of proteins and mathematical prediction of \\r\\nprotein structure</A></p>\\r\\n',\n",
       " '<p>I graduated with a <a href=\"http://www.se.rit.edu/\" rel=\"nofollow\">Bachelor of Science in Software Engineering</a> from the <a href=\"http://www.rit.edu\" rel=\"nofollow\">Rochester Institute of Technology</a> in May 2011. I currently work for <a href=\"http://utcaerospacesystems.com/\" rel=\"nofollow\">UTC Aerospace Systems</a> in Massachusetts. My full CV is available on <a href=\"http://careers.stackoverflow.com/thomasjowens\">StackOverflow Careers</a> and <a href=\"http://www.linkedin.com/in/thomasjowens\" rel=\"nofollow\">LinkedIn</a>.</p>\\r\\n\\r\\n<p>My professional interests include software project management, software engineering process, software measurements and metrics, leadership, and professionalism in software engineering. I\\'m also a casual student of psychology and sociology, especially as they apply to a business context. Personally, I have taken up photography as a hobby. I\\'m also a casual gamer.</p>\\r\\n',\n",
       " '<p>I am a recent electrical engineering graduate from Rutgers University, and a employee at <a href=\"http://pulsetor.com/\" rel=\"nofollow\">PulseTor</a> doing mostly FPGA programming.</p>\\r\\n\\r\\n<p>My advice for R programmers: <a href=\"https://svn.r-project.org/R/trunk/\" rel=\"nofollow\">look under the hood</a>. Most of the R interpreter source makes a lot of sense and you can learn a lot about how the language works.</p>\\r\\n\\r\\n<p><a href=\"http://www.facebook.com/ellbur\" rel=\"nofollow\">facebook</a></p>\\r\\n',\n",
       " '<p>Engineer working with Java, C, C++, assembler and embedded hardware.</p>\\r\\n',\n",
       " '<p>A GNU/Linux user and Python developer<br>\\r\\nUse much PyGTK, PyQt, and some Django<br>\\r\\nInsterested in Math and specially Geometry  </p>\\r\\n\\r\\n<p>My Projects:<br>\\r\\n<a href=\"http://sourceforge.net/projects/starcal\" rel=\"nofollow\">http://sourceforge.net/projects/starcal</a><br>\\r\\n<a href=\"http://sourceforge.net/projects/pyglossary\" rel=\"nofollow\">http://sourceforge.net/projects/pyglossary</a></p>\\r\\n',\n",
       " '<p>Хаммага салом</p>\\r\\n',\n",
       " '<p>I\\'m a PhD student in  Computer Science at <a href=\"http://bit.ly/1lugQ1M\" rel=\"nofollow\">ETH Zurich</a> doing research on Bitcoin.</p>\\r\\n\\r\\n<p><strong>Publications</strong></p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://bit.ly/1emqRvE\" rel=\"nofollow\">Information Propagation in the Bitcoin Network</a> (<a href=\"http://bit.ly/18UcYqu\" rel=\"nofollow\">bibtex</a>)</li>\\r\\n<li><a href=\"http://bit.ly/1pc9GQZ\" rel=\"nofollow\">Bitcoin Transaction Malleability and MtGox</a> (<a href=\"http://bit.ly/YACGNc\" rel=\"nofollow\">bibtex</a>)</li>\\r\\n<li><a href=\"http://bit.ly/18Ud1Ts\" rel=\"nofollow\">Have a Snack, Pay with Bitcoins</a> (<a href=\"http://bit.ly/1kWdp6F\" rel=\"nofollow\">bibtex</a>)</li>\\r\\n<li><a href=\"http://bit.ly/1tCz9Hl\" rel=\"nofollow\">BlueWallet: The secure Bitcoin Wallet</a> (<a href=\"http://bit.ly/1xP5kts\" rel=\"nofollow\">bibtex</a>)</li>\\r\\n</ul>\\r\\n',\n",
       " \"<p>I'm a Linux user</p>\\r\\n\",\n",
       " '<p>I work in the market insights and strategies practice at Resource Systems Group. I specialize in discrete choice model estimation and application used to support the analysis of consumer behavior and value product drivers in a wide variety of settings for Fortune 500 clients.</p>\\r\\n',\n",
       " '<p>I currently combine my scientific background with my software engineering skills, and apply them to high-end R&amp;D projects. My specialities are data analysis, software engineering, and clear communication of complex subjects. </p>\\r\\n\\r\\n<p>I am fluent in R and Python, and comfortable programming IDL, Fortran and Bash. In addition, I have some experience in C++ and JavaScript.</p>\\r\\n\\r\\n<p>For more details on my work, publications and software, please checkout <a href=\"http://www.numbertheory.nl\" rel=\"nofollow\">my website</a>. In addition, <a href=\"https://bitbucket.org/paulhiemstra\" rel=\"nofollow\">my bitbucket account</a> contains a number of software projects I worked on. For a more complete breakdown of my career path until now, I refer to my <a href=\"http://nl.linkedin.com/pub/paul-hiemstra/20/30b/770/\" rel=\"nofollow\">linkedin profile</a>.</p>\\r\\n',\n",
       " '<p>I study (not enough) maths, but mainly probability.</p>\\r\\n',\n",
       " '<p>stats student</p>\\r\\n',\n",
       " \"<p>I'm an eternal stat-R apprentice. </p>\\r\\n\",\n",
       " '<p>x</p>\\r\\n',\n",
       " \"<p>Studying cyber security (theory of cryptographic protocols) for M.Sc. in the Applied Logic And Security (ALAS) lab at Worcester Polytechnic Institute. I am learning how to do research mathematics.</p>\\r\\n\\r\\n<p>I love to take real-world problems and model them with formal structures and computer programs.</p>\\r\\n\\r\\n<p>I'm practiced with picking up new languages quickly for one-off projects.</p>\\r\\n\\r\\n<p>Hobbies: problem solving, critical reasoning, rhetoric, math (esp. graph theory!), economics, cognitive science, political science, and theoretical physics.</p>\\r\\n\\r\\n<p>Interested in machine learning, especially fields with practical applications that can make a difference in the world such as economics, knowledge sharing, healthcare, bioinformatics, and ecoinformatics.</p>\\r\\n\\r\\n<p>Deep relationship with Python;\\r\\nPretty skillful with C++, PHP, Javascript, MySQL, Java, Haskell, Matlab.</p>\\r\\n\",\n",
       " \"<p>I'm JR Galia, a JEE programmer, a web developer, a certified PhilNITS IT professional and a FOSS advocate.</p>\\r\\n\",\n",
       " '<blockquote>\\r\\n  <p>\"Live a good life. If there are gods and they are just, they will not\\r\\n  care how devout you have been, but will welcome you based on the\\r\\n  values you have lived by. If there are gods, but unjust, then you\\r\\n  should not want to worship them. If there are no gods, then you will\\r\\n  be gone, but will have lived a noble life that will live on in the\\r\\n  memory of your loved ones. I am not afraid.\"</p>\\r\\n</blockquote>\\r\\n\\r\\n<ul>\\r\\n<li>Unknown</li>\\r\\n</ul>\\r\\n',\n",
       " \"<p>As one is not supposed to thank another for a helpful answer,\\r\\nas it may be quite annoying if you just read the post,\\r\\nI decided to generally thank the people who work through the questions,\\r\\nas until now all the questions I had were solved here.\\r\\nIt helped me a lot, but one can not upvote another as much as one sometimes maybe feels like.\\r\\nSome answers are extremely helpful and I've been struggling with a few for several days when I finally found a related question and answer here. So.. thanks.</p>\\r\\n\",\n",
       " '<p>A scientist who focuses on applied machine learning, information retrieval and data mining.</p>\\r\\n',\n",
       " '<p>I am a computer scientist working on the relationship between climate and energy sector. </p>\\r\\n',\n",
       " '<p>My background is in Computer Science, specializing in Machine Learning :)</p>\\r\\n',\n",
       " 'Some Java guy',\n",
       " '<p>Sr. statistician at Complete Genomics, Inc., Mountain View, CA.</p>\\r\\n',\n",
       " '<p>Grad student at UCLA.</p>\\r\\n',\n",
       " '<p>We once had a Nuclear Engineering proposal on Area 51, but it failed.  The space technology proposal might be interesting.</p>\\r\\n\\r\\n<p>My academic interests are in power plants of all types, shapes, and sizes, and how well they play with each other.  My hobbyist interest are kind of all over the place.  I like the maker movement, I\\'m most interested in the sensors and data, the \"internet of things\" kind of stuff.  In physics, I\\'m probably the most curious about general relativity topics, but my curiosities wind up all over the place.</p>\\r\\n',\n",
       " '<p>I am an economics student.</p>\\r\\n',\n",
       " '<p>15% charimastic and the other part composed of pure working ethos.</p>\\r\\n',\n",
       " '<p>astrophysics student at University of Sydney</p>\\r\\n',\n",
       " '<p>Author of NumPy Beginner\\'s Guide <a href=\"http://www.packtpub.com/numpy-1-5-using-real-world-examples-beginners-guide/book\" rel=\"nofollow\">http://www.packtpub.com/numpy-1-5-using-real-world-examples-beginners-guide/book</a></p>\\r\\n',\n",
       " '<p>person</p>\\r\\n',\n",
       " '<p>I am a twenty-something comp sci grad from University of Illinois Urbana-Champaign. I am currently working as an ASP.NET full-stack web developer in downtown Chicago. My spare time is currently spent tinkering on personal web app projects as well as playing Pathfinder and Minecraft.</p>\\r\\n',\n",
       " '<p>MSc in Geomatics. BSc in Environmental science.</p>\\r\\n',\n",
       " '<p>Expert chess player</p>\\r\\n\\r\\n<p>Model UN president</p>\\r\\n\\r\\n<p>B.S./M.S. Computer Science, 2015</p>\\r\\n\\r\\n<p>Case Western Reserve University</p>\\r\\n\\r\\n<p><a href=\"http://stackexchange.com/users/954104\">\\r\\n<img src=\"http://stackexchange.com/users/flair/954104.png\" width=\"208\" height=\"58\" alt=\"profile for Andrew Latham on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Andrew Latham on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\\r\\n</a></p>\\r\\n',\n",
       " '<p>Hello! My concentration is statistical genetics. I also like statistical computing (mainly) using R and other open-source software packages.</p>\\r\\n',\n",
       " '<p>Bioinformaticist, hobbyist iPhone developer</p>\\r\\n',\n",
       " '<p>I\\'m broadly interested in <a href=\"http://www.johndcook.com/veryappliedmath.html\" rel=\"nofollow\">very applied math</a>. I try to apply ideas from mathematics, statistics, machine learning, formal systems and computer science to solve real-world problems. Mostly in applied finance/quantitative trading, but in other areas if the mood takes me.</p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p>The following is for my own use, but feel free to borrow it -</p>\\r\\n\\r\\n<p>Hi. It looks like you are new here. We are generally very willing to help, but we like users to show the work that they\\'ve done towards solving the problem on their own first. If you can edit your question to show the code you\\'ve written so far, and where you are stuck, then you will get a much better response.</p>\\r\\n',\n",
       " '<p>I\\'m a security analyst and consultant, I\\'m here to learn and help others out. </p>\\r\\n\\r\\n<p>Opinions are my own. Advice provided with no warranty.</p>\\r\\n\\r\\n<p>Find me on  <a href=\"http://cloud101.eu\" rel=\"nofollow\">http://cloud101.eu</a></p>\\r\\n',\n",
       " '<p><strong>argonaut:</strong> A person who is engaged in a dangerous but potentially rewarding quest.</p>\\r\\n',\n",
       " '<p>Professor of Statistics, Monash University, Australia. Have used R and LaTeX for more than 20 years.</p>\\r\\n',\n",
       " '<p>Business systems analyst.</p>\\r\\n',\n",
       " '<p>I am a professional with interests in data mining, machine learning and statistical inference</p>\\r\\n',\n",
       " '<p>Forest Resource Analyst</p>\\r\\n',\n",
       " '<p>Credit risk modeler  <br />\\r\\nMSc in Mathematics</p>\\r\\n',\n",
       " '<p>Vision scientist interested in human perception, attention and visual memory.</p>\\r\\n',\n",
       " '<p>Sayin this and that cause this and that was missin.</p>\\r\\n',\n",
       " \"<p>I'm an analyst trained in frequentist statistics looking to learn more about ever-present Bayesian statistics - and to catch hold of the coat tails of machine learning that's come speeding out of nowhere and looks set to eclipse everything I yawned my way through at university. </p>\\r\\n\\r\\n<p>I've worked on poverty data for the UK government and clinical trials for pharmaceutical's.  Am currently working alongside epidemiologists on large, medical, observational data sets.</p>\\r\\n\",\n",
       " 'C# software developer, generally specialising in financial markets.',\n",
       " '<p>PhD Student in the UK.</p>\\r\\n',\n",
       " '<p>Member of <a href=\"http://heal.heuristiclab.com\" rel=\"nofollow\">HEAL</a> Heuristics and Evolutionary Algorithm Laboratory in Hagenberg, Austria. We\\'re doing research on metaheuristic algorithms (Genetic Algorithm, Evolution Strategy, Simulated Annealing, Tabu Search, Variable Neighborhood Search, Particle Swarm Optimization, etc.) and application to optimization problems (Facility Layout and Assignment, Storage Location Assignment, Vehicle Routing/Pickup&amp;Delivery, Job Shop Scheduling, Data Mining).</p>\\r\\n\\r\\n<p>Co-Architect and Developer of <a href=\"http://dev.heuristiclab.com\" rel=\"nofollow\">HeuristicLab</a>.</p>\\r\\n',\n",
       " '<p>Consultant and Software programmer by day.</p>\\r\\n\\r\\n<p>Superhero by night. </p>\\r\\n\\r\\n<p>Speaks fluently Java. Works agile and test driven, or at least feels appropriately ashamed when failing to do so. Current job title is Eclipse specialist and java developer. Which basically means try keep the Eclipseboat afloat and working at optimum performance for some 400+ users.</p>\\r\\n',\n",
       " \"<p>Software developer originally from New Zealand. I'm an ultra-runner and have completed at least 8 50km, a 50 mile and a 100km.</p>\\r\\n\",\n",
       " '<p>I am a neuroscientist with a love of Statistics, R, and data visualization. </p>\\r\\n',\n",
       " '<p>My formal education is in economics. My programming skills and computer knowledge are self-taught. I have an uncanny ability to find answers to questions outside my immediate expertise. I have a deep love of learning and I enjoy building / fixing all kinds of things -- software, computer hardware, engines, etc.</p>\\r\\n',\n",
       " '<p>I love JavaScript, Ruby and Web Development.</p>\\r\\n\\r\\n<p>I work at Etsy.</p>\\r\\n',\n",
       " \"My name is Sergio Díaz. I am known as Seichleon(a.k.a. Seich) on the internet. I am a young-self taught Honduran Web Developer and Software designer. I've been working with code for around 5 years and I've been in love with computers since I was 4 and I love it.\",\n",
       " '<p>Website and Youtube Channel:</p>\\r\\n\\r\\n<p><a href=\"http://revgr.com\" rel=\"nofollow\">http://revgr.com</a></p>\\r\\n\\r\\n<p><a href=\"https://www.youtube.com/user/revgro\" rel=\"nofollow\">https://www.youtube.com/user/revgro</a></p>\\r\\n',\n",
       " '<p>Swiss, 17, Web Developer.</p>\\r\\n',\n",
       " '<pre><code>Hello World!!\\r\\nHello friends!!\\r\\n</code></pre>\\r\\n',\n",
       " '<p>Statistician; part of the development team for Stan (http://www.mc-stan.org). </p>\\r\\n\\r\\n<p>You can reach me at daniel.lee@hutchinsonlee.com or bearlee@alum.mit.edu.</p>\\r\\n',\n",
       " '<p>Mathematician, MSc in Machine Learning and Data Mining, working in Financial Services</p>\\r\\n',\n",
       " '<p>@JnBrymn</p>\\r\\n\\r\\n<ul>\\r\\n<li>I do Solr </li>\\r\\n<li>I do Cassandra </li>\\r\\n<li>I do Python </li>\\r\\n<li>I do Java </li>\\r\\n<li>I do Software Architecture</li>\\r\\n<li>I do Data Mining</li>\\r\\n</ul>\\r\\n\\r\\n<p>... and I do them all well</p>\\r\\n',\n",
       " '<p>CS grad student, coder, wannabe entrepreneur.</p>\\r\\n',\n",
       " '<p>Software developer and tester.  Mostly Ruby, Bash, C/C++...</p>\\r\\n',\n",
       " '<p>I am a psychology student currently completing my degree. </p>\\r\\n',\n",
       " '<p>I am an independent ICT entrepeneur from the Netherlands. My main interests are PHP and Android development.</p>\\r\\n',\n",
       " '<p>I\\'m a data scientist at <a href=\"https://www.highmark.com/\" rel=\"nofollow\">Highmark, Inc</a>, working as the manager of predictive analystics and data management within Operations. I earned my PhD doing computational neuroscience, working with decision making processes and investigating functional connectivity between different parts of the human brain. I\\'ve also worked as a financial quant, freelance web developer, and IT consultant. One of my favorite pastimes is <a href=\"http://www.playpoi.com/why-play-poi\" rel=\"nofollow\">poi spinning</a>.</p>\\r\\n\\r\\n<p>You can learn more about me on my <a href=\"http://www.linkedin.com/in/eykanal\" rel=\"nofollow\">LinkedIn page</a>. Feel free to contact me about anything and everything at eykanal at erikdev dot com.</p>\\r\\n',\n",
       " '<p>Here is a sample of my work.  I hope that it will prove useful...\\r\\n\"Nothing has been accomplished if something remains to be done.\"\\r\\n(The Works of Gauss vol. 5, quot. from Worbs, 1955,  p. 43)</p>\\r\\n',\n",
       " '<p>Formerly a webdev, then a failed postgrad, now shifting focus to more data and maths projects</p>\\r\\n',\n",
       " '<p>I do security for the Python and Django projects.</p>\\r\\n',\n",
       " '<p>Studying for a degree in Computer Science and AI</p>\\r\\n',\n",
       " 'Hardware engineer turned programmer',\n",
       " '<p><strong><code>M-A-K-E . S-T-U-F-F</code></strong></p>\\r\\n',\n",
       " '<p>Working in JavaScript, Google Analytics, Python, PHP, MySQL, HTML/CSS, jQuery, Google App Engine, Git, NodeJS, Heroku, EC2.</p>\\r\\n\\r\\n<p>Find me on <a href=\"http://twitter.com/yahel\" rel=\"nofollow\">Twitter</a>.</p>\\r\\n',\n",
       " '<p><a href=\"http://www.linkedin.com/in/arjenkruithof\" rel=\"nofollow\">LinkedIn - Arjen Kruithof</a><br/>\\r\\n<a href=\"http://www.facebook.com/arjenkruithof\" rel=\"nofollow\">Facebook - Arjen Kruithof</a><br/>\\r\\n<a href=\"http://www.google.com/reader/shared/arjen.kruithof\" rel=\"nofollow\">Google Reader Shared - Arjen Kruithof</a><br/>\\r\\n<a href=\"http://twitter.com/arjenkruithof\" rel=\"nofollow\">Twitter - Arjen Kruithof</a><br/>\\r\\n<a href=\"https://www.sellaband.com/en/believer/arjen-kruithof\" rel=\"nofollow\">Sellaband - Arjen Kruithof</a><br/>\\r\\n<a href=\"http://www.nu.nl/internet/921730/lasttv-en-treinvizier-zijn-beter-dan-microsoft.html\" rel=\"nofollow\">NU.nl - TreinVizier</a><br/>\\r\\n<a href=\"http://picasaweb.google.com/arjen.kruithof\" rel=\"nofollow\">Picasa - Arjen Kruithof</a><br/>\\r\\n<a href=\"http://youtube.com/user/arjenkruithof\" rel=\"nofollow\">YouTube - Arjen Kruithof</a><br/>\\r\\n<a href=\"http://www.tamtam.nl/arjenkruithof\" rel=\"nofollow\">Tam Tam - Arjen Kruithof</a><br/>\\r\\n<a href=\"http://www.myspace.com/arjen_kruithof\" rel=\"nofollow\">Myspace - Arjen Kruithof</a><br/>\\r\\n<a href=\"http://swerl.tudelft.nl/bin/view/Main/PastAndCurrentMScProjects\" rel=\"nofollow\">TU Delft, EEMCS, SERG - Arjen Kruithof</a><br/></p>\\r\\n',\n",
       " '<p>My <a href=\"http://www.linkedin.com/pub/jeff-hansberry/58/351/a77\" rel=\"nofollow\">LinkedIn</a> profile. I work on diagnostics for cancer and other diseases. Any views expressed are my own. They should not be interpreted as reflecting the views of my employers.</p>\\r\\n',\n",
       " '<ul>\\r\\n<li>PhD in CS/AI/Machine Learning/Cognitive Modeling from UIUC</li>\\r\\n<li>Post-doctoral work in Computational Cognitive Science at Columbia, UConn, NYU</li>\\r\\n<li>Statistical Data Scientist in private industry</li>\\r\\n</ul>\\r\\n\\r\\n<p>Twitter: @harlanh</p>\\r\\n',\n",
       " '<p>I am having a challenge in creating a regression model for some data. I have a dataset with nominal(Both binary - gender and 3 valued) , ordinal and continuous variables. I want to set up a regression model to include all these variable. What is the best approach??</p>\\r\\n',\n",
       " '<p>Prefers to stay anonymous and eat mousse au chocolat.</p>\\r\\n',\n",
       " '<p>Master Student in Machine Learning and Data Mining at the <a href=\"http://www.tkk.fi/en\" rel=\"nofollow\">Aalto Univerity School of Science</a> (formerly TKK / Helsinki University of Technology) in Helsinki, Finland.</p>\\r\\n\\r\\n<p>Currently working as <a href=\"http://users.ics.tkk.fi/peter/\" rel=\"nofollow\">Research Assistant</a> in the <a href=\"http://research.ics.tkk.fi/speech/\" rel=\"nofollow\">Speech Group</a> of the <a href=\"http://www.cis.hut.fi/research/\" rel=\"nofollow\">Adaptive Informatics Research Centre</a> of the <a href=\"http://www.tkk.fi/en\" rel=\"nofollow\">Aalto Univerity School of Science</a> (formerly TKK / Helsinki University of Technology) in Helsinki, Finland.</p>\\r\\n',\n",
       " \"<p>I'm a graduate student in computer science at University of California, Berkeley. I also spent 5 years as a Software Development Engineer at Microsoft, 2 in a product group and 3 as a Research Developer for the Microsoft Research Operating Systems Group. I'm also an administrator on the English Wikipedia and Wikimedia Commons, and run several websites including LiteratePrograms wiki.</p>\\r\\n\",\n",
       " '<p>Jedidiah is nearing the completion of his DPhil in statistics at Oxford University, where he applies machine learning techniques to learn about the patterns of evolution in the HIV virus.</p>\\r\\n\\r\\n<p>He is interested in far too many things at the same time, cannot seem to learn enough and his curiosity often gets him into trouble.</p>\\r\\n\\r\\n<p>In his spare time you can find him hacking, tinkering, coding and kick boxing.</p>\\r\\n',\n",
       " \"<p>Hobbes: What's the point of attaching a number to everything you do ?<br>\\r\\nCalvin: If your numbers go up, it means you're having more fun.</p>\\r\\n\",\n",
       " '<p>Dave (dave@autobox.com) is currently a Senior Consultant at Automatic Forecasting Systems  (215-675-0652). His doctorate dissertation on automatic modeling of time series was the basis for the technological advances in time series analysis. One of the founding principals of Automatic Forecasting Systems (AFS)( <a href=\"http://www.autobox.com\" rel=\"nofollow\">http://www.autobox.com</a> ), his innovational leadership and statistical expertise is at the core of the success of AFS. He has over 40 years of experience in statistical consulting and expert system product development. Under his leadership and vision, AFS has lead the way in championing expert time series analysis this making these tools available to all. He has held teaching positions at Penn State University and the Drexel University Graduate School. Dave is well-founded both theoretically and in the practice of Management Science. Before founding AFS he held Senior management positions at General Electric, Celanese Corporation and The American Stock Exchange. Dave holds a B.A. in Statistics from CCNY, an M.S. in Statistics from Villanova University and an A.B.D. in Applied Economics from the University of Pennsylvania.</p>\\r\\n',\n",
       " '<p><a href=\"http://www.linkedin.com/in/hughperkins\" rel=\"nofollow\">http://www.linkedin.com/in/hughperkins</a></p>\\r\\n',\n",
       " '<p>Statistics self-study!</p>\\r\\n',\n",
       " '<p>about:me ;)</p>\\r\\n',\n",
       " '<p>23 year old, 1st class BSc in Computer Science from UEA, Software Engineer at Blinkx</p>\\r\\n',\n",
       " \"<p>Name intended to be a vocal exercise :)</p>\\r\\n\\r\\n<p>email: i'll put a tor redirect eventually</p>\\r\\n\",\n",
       " '<p>Currently playing <a href=\"/questions/tagged/wfrp-3e\" class=\"post-tag\" title=\"show questions tagged \\'wfrp-3e\\'\" rel=\"tag\">wfrp-3e</a> with an amoral sell-sword. Possibly read too much Song of Ice and Fire recently.</p>\\r\\n\\r\\n<p>Looking to run a game set in Ankh-Morpork using <a href=\"/questions/tagged/fate\" class=\"post-tag\" title=\"show questions tagged \\'fate\\'\" rel=\"tag\">fate</a>.</p>\\r\\n\\r\\n<p>Got way too much time on his hands and too many opinions in his head.</p>\\r\\n',\n",
       " '<p>Yet another quant guy...</p>\\r\\n',\n",
       " 'When I\\'m not managing a data warehouse for a mid-sized financial institution I\\'m <a href=\"http://www.poolplayers.com/\" rel=\"nofollow\">shooting pool</a> and <a href=\"http://www.boardgamegeek.com/\" rel=\"nofollow\">playing boardgames</a>.<br><br>I\\'m also crazy about <a href=\"http://picasaweb.google.com/lh/photo/sYqOUAf0CUszgJlgCKZs1A\" rel=\"nofollow\">my little shaver</a>.',\n",
       " \"<p>I'm a software engineering student. What more is there to say?</p>\\r\\n\",\n",
       " '<p>Developing features for the Casebook 2, an innovative Rails-based case management and social analytics web-app that assists social workers for the State of Indiana\\'s Department of Child Services (<a href=\"http://www.in.gov/dcs\" rel=\"nofollow\">http://www.in.gov/dcs</a>) in tracking, assessing, and managing their child welfare cases. My peers and I are working out of Pivotal Labs NYC and leveraging Agile/XP and Rails to ensure better outcomes for children in foster care through better tools for Indiana DCS. Working on multidiscipline teams of Designers, Rails Developers, and DevOps pairs from Case Commons and Pivotal Labs to continuously refine our XP process and extend our large scale agile development capabilities.</p>\\r\\n',\n",
       " '<p>phd student in sociology.  your assistance is sincerely appreciated.  seriously, the existence of stackoverflow is inspiring.</p>\\r\\n',\n",
       " 'Independent developer',\n",
       " '<p>Professor of Stochastic Modelling, Newcastle University, UK.</p>\\r\\n',\n",
       " '<p>Wanting to program in order to be a better middleman between humans and coders.</p>\\r\\n\\r\\n<p>And of course to create to be able to put my ideas in practice! Which mainly focus on new interaction possibilties between man and environment. </p>\\r\\n\\r\\n<p>Always up for new collaborations!!</p>\\r\\n',\n",
       " '<p><a href=\"http://stackexchange.com/users/9ed9aac6bf454e0b94afd129f78ecf6b\">\\r\\n<img src=\"http://stackexchange.com/users/flair/9ed9aac6bf454e0b94afd129f78ecf6b.png\" width=\"208\" height=\"58\" alt=\"profile for Seamus on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Seamus on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\\r\\n</a></p>\\r\\n\\r\\n<p>I am a postdoc in philosophy.</p>\\r\\n\\r\\n<p>I contribute to a blog for philosophers who use LaTeX. If you fall in to that niche, check it out: <a href=\"http://www.charlietanksley.net/philtex/\" rel=\"nofollow\">PhilTeX</a>. (The blog will be of use to all kind of humanities scholars using LaTeX, I imagine, but it was started by, and is run by philosophers...) The blog is currently defunct, but may be resurrected soon.</p>\\r\\n\\r\\n<p>I made this <a href=\"https://github.com/scmbradley/Beamer-colour-change-project\" rel=\"nofollow\">beamer colour change package</a> that slowly changes the colour of structure elements of beamer presentations. Feedback welcome.\\r\\nI also made this <a href=\"https://github.com/scmbradley/moreenum\" rel=\"nofollow\"><code>moreenum</code></a> package which adds new enumeration options.\\r\\nThe <a href=\"http://www.seamusbradley.net/tex.html\" rel=\"nofollow\">TeX goodies</a> page of my website includes some other bits and bobs I\\'ve done.</p>\\r\\n',\n",
       " '<p>Statistics Lecturer</p>\\r\\n',\n",
       " '<p>A great econometrics enthusiastic</p>\\r\\n',\n",
       " '<p>Software engineer\\r\\n@ Data Analytix, Skopje, Macedonia.\\r\\nMy opinions does not reflect my company\\'s policy in any matter.\\r\\nFor my programming blog, visit <a href=\"http://bpetrushev.appspot.com\" rel=\"nofollow\">http://bpetrushev.appspot.com</a>. </p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"https://github.com/petrushev/nlmk\" rel=\"nofollow\">nlmk</a> - library for Natural Language Processing in Macedonian.</li>\\r\\n<li><a href=\"https://github.com/petrushev/txwebbi\" rel=\"nofollow\">txwebbi</a> - web microframework using <code>twisted</code>, <code>jinja2</code> templates and <code>werkzeug</code> routing</li>\\r\\n<li><a href=\"https://github.com/petrushev/txcrawl\" rel=\"nofollow\">txcrawl</a> - microcrawler written in <code>twisted</code></li>\\r\\n<li><a href=\"https://github.com/petrushev/zerospider\" rel=\"nofollow\">zerospider</a> - parallel crawler using <code>zeromq</code></li>\\r\\n</ul>\\r\\n',\n",
       " '<p>NYU faculty.  Social science statistics.</p>\\r\\n',\n",
       " 'I\\'m a graduate student in computational biology working in the lab of Dr. David Baker. I develop code for <a href=\"http://boinc.bakerlab.org/\" rel=\"nofollow\">Rosetta@Home</a>, and I am interested in using computers to learn about the world. ',\n",
       " '<p>Just beginning with R, not programmer though!</p>\\r\\n',\n",
       " '<p>New to programming, but trying to learn as much as I can.</p>\\r\\n',\n",
       " '<p>\\r\\nI am ... if you really want to know, check my website ;-)\\r\\n</p>\\r\\n',\n",
       " '<p>a new user of WinBUGS</p>\\r\\n',\n",
       " '<p>Physicist, Physics Teacher, Researcher</p>\\r\\n',\n",
       " '<p>MBA Finance<br />\\r\\nBA Econ<br />\\r\\nData Mining<br />\\r\\nC#, Javascript, VB, SQL</p>\\r\\n',\n",
       " '<p>I write code for all kinds of things, embedded devices, desktops.</p>\\r\\n',\n",
       " \"<p>I'm interested in latent variable modeling. I work with structural equation models, item response theory models, and latent variable mixture models on a variety of applied problems, mostly in psychological research.</p>\\r\\n\",\n",
       " '<p>Software Developer at Epic Systems</p>\\r\\n',\n",
       " \"<p>I'm currently working on my Masters at West Virginia University in Morgantown, WV.  My thesis-work is in Artificial Intelligence and Data Mining -- meaning I do most of my coding in Lisp, sometimes Python.</p>\\r\\n\\r\\n<p>My friends and I head up a Free Software organization at the University and spend a lot of our time staring at screens.</p>\\r\\n\",\n",
       " '<p>Physician, statistician</p>\\r\\n',\n",
       " 'Maths Teacher, Computer Programmer, Web Designer.',\n",
       " '<p>Research Interests: Data mining, refactoring, software engineering, metrics, knowledge representation.</p>\\r\\n\\r\\n<p>25+ years software development experience in Java, C++, Lisp, Prolog, Ada,...</p>\\r\\n',\n",
       " '<p>Currently working on my master Thesis at Maastricht University. Master of Accounting.</p>\\r\\n',\n",
       " '<p>“Daughter of the Night, she walks again. The ancient war, she yet fights. Her new lover she seeks, who shall serve her and die, yet serve still. Who shall stand against her coming? The Shining Walls shall kneel. Blood feeds blood. Blood calls blood. Blood is, and blood was, and blood shall ever be.”</p>\\r\\n',\n",
       " '<p>Im a Mathematics &amp; Statistics teacher at Universidad de Aconcagua and teach mathematics at school, Chile. Work with LaTeX (XeTeX+Sublime Text+TexLive in OSX), R, SageMath, python, IPython. Im vegan &amp; practice zen-buddhist.</p>\\r\\n',\n",
       " '<p>metrologist [<em>not a weatherman, a measurement engineer]</em>, saver of heirloom seeds, terrible chef</p>\\r\\n',\n",
       " \"<p>I'm a PhD student in economics.</p>\\r\\n\",\n",
       " '<p>BBA Finance/Economics,\\r\\nMSc Statistics, and\\r\\ncurrently pursuing PhD in Finance</p>\\r\\n',\n",
       " '<p>Computationally Intensive ...</p>\\r\\n',\n",
       " \"<p>I'm a neuroscience grad student.\\r\\n:`D</p>\\r\\n\",\n",
       " '<p>Mathematical statistician</p>\\r\\n',\n",
       " \"<p>I'm a software developer with a passion for data science. Major interests:</p>\\r\\n\\r\\n<ul>\\r\\n<li>Machine Learning</li>\\r\\n<li>Natural Language Processing</li>\\r\\n<li>Web Scraping</li>\\r\\n</ul>\\r\\n\\r\\n<p>You bring the data, I'll bring the algorithms, it'll be a party.</p>\\r\\n\",\n",
       " 'I am a Python Developer in a text analytics startup, based out of chennai, India.  ',\n",
       " \"<p>My name is Manuel Ebert and I'm a neuroscientist, web developer, designer and musician (not necessarily in this order) living in San Francisco.</p>\\r\\n\",\n",
       " '<p>I am a php developer, mostly using cakephp and mysql. have also played with many other languages</p>\\r\\n',\n",
       " '<p>UX Architect in London</p>\\r\\n',\n",
       " '<p>A high-school sophomore, deeply interested in math</p>\\r\\n',\n",
       " '<p>Econ grad student at MIT. Interested in development economics and labor, have lived in various places in South America and enjoys contributing to public goods if they involve knowledge (ie, SE or Quora).</p>\\r\\n',\n",
       " '<p>I work at a small mechanical engineering company, where I develop software and image processing algorithms for camera-based inspection machines.</p>\\r\\n',\n",
       " '<p>Hello c:</p>\\r\\n',\n",
       " '<p>I\\'m a graduate student in physics doing research in high-energy particle physics. I also have a hobby interest in computer programming.</p>\\r\\n\\r\\n<p>You can find me on <a href=\"http://twitter.com/ellipsix\" rel=\"nofollow\">Twitter</a>, or check out my <a href=\"http://www.ellipsix.net\" rel=\"nofollow\">blog and personal website</a>!</p>\\r\\n\\r\\n<p>For matters <em>not related to Stack Exchange</em>, I can be contacted by email at <strong>stack@ellipsix.net</strong>.</p>\\r\\n\\r\\n\\r\\n',\n",
       " '<p>Full time J2EE &amp; Jave EE developer / Part time PhD candidate<br>\\r\\nI am really keen on <strong>computational linguistics</strong>.</p>\\r\\n',\n",
       " 'Java, Python',\n",
       " '<p>I\\'m a Lecturer at University of Waikato. I am interested mainly in mitigating (computational and inferential) issues associated with working with high-dimensional data. I recently completed my PhD at the University of Birmingham (England), supervised by Ata Kaban. You can read my thesis on randomly-projected linear discriminants <a href=\"http://www.cs.bham.ac.uk/~durranrj/Bob_Durrant_Thesis.pdf\" rel=\"nofollow\">here.</a></p>\\r\\n',\n",
       " '<p>neverending student ;)</p>\\r\\n',\n",
       " '<p>Engineer</p>\\r\\n',\n",
       " '<p>PhD Student facing preliminary defense issues...</p>\\r\\n',\n",
       " \"<p>Intermediate statistics, mostly self taught so don't take me <em>too</em> seriously.</p>\\r\\n\",\n",
       " '<p>I am an economics PhD student, originally from Brazil, but currently in Australia. </p>\\r\\n\\r\\n<p><p>I am trying to learn Emacs (and enjoying the process), but it is challenging. It doesn\\'t help that most online help assumes knowledge I don\\'t have (\"change your variable PATH\" - what are you talking about??, \"open your ~/.emacs\" - what does ~ mean?, \"type this code\" - sure, but where?). </p>\\r\\n\\r\\n<p><p>I am getting there slowly, though, and I am happy with how much I have been learning!</p>\\r\\n\\r\\n<p><p> I am also interested in LaTeX, Matlab, and plan on learning R (soonish) and hopefully Python (not anytime soon!).</p>\\r\\n',\n",
       " '<p>Just Getting started...</p>\\r\\n\\r\\n<p>Interest in: Machine Learning, Database Sys, and Bioinformatics.</p>\\r\\n\\r\\n<p>Know: Perl, Ruby on Rails, Java, AjAX, JS, CSS. </p>\\r\\n',\n",
       " '<p><a href=\"http://stackexchange.com/users/54832\">\\r\\n<img src=\"http://stackexchange.com/users/flair/54832.png\" width=\"208\" height=\"58\" alt=\"profile for hhh on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for hhh on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\\r\\n</a></p>\\r\\n\\r\\n<p><strong>Great!</strong></p>\\r\\n\\r\\n<blockquote>\\r\\n  <ul>\\r\\n  <li><p><em>Ungeduld ist Angst.</em></p></li>\\r\\n  <li><p><a href=\"http://www.youtube.com/watch?v=JCQVnSOFqfM\" rel=\"nofollow\"><em>It\\'s not time to make a change. Just relax, take it easy. You\\'re still young, that\\'s your fault. There\\'s so much you have to know.</em></a></p></li>\\r\\n  <li><p><em>It is nice to be important but more important to be nice.</em> ~Roger Federer</p></li>\\r\\n  <li><p><em>Be yourself; everyone else is already taken.</em> - Oscar Wilde</p></li>\\r\\n  <li><p><em>I can\\'t control the wind but I can adjust the sail.</em> - Ricky Skaggs</p></li>\\r\\n  <li><p><em>If you want your children to be intelligent, read them fairy tales. If you want them to be more intelligent, read them more fairy tales.</em> - Albert Einstein</p></li>\\r\\n  <li><p><em>You swing your best when you have the fewest things to think about.</em> - Bobby Jones</p></li>\\r\\n  <li><p><em>It is the mark of an educated mind to be able to entertain a thought without accepting it.</em> - Aristotle</p></li>\\r\\n  <li><p><em>We can do better. We have to do better.</em> - Teemu Selänne</p></li>\\r\\n  <li><p><em>Real artists ship.</em> - Steve Jobs</p></li>\\r\\n  <li><p><em>The only thing you can control is your attitude toward the next shot.</em> - Mark McCumber</p></li>\\r\\n  <li><p><em>Simplicity is not the goal. It is the by-product of a good idea and modest expectations.</em> -Paul Rand</p></li>\\r\\n  <li><p><em><a href=\"http://www.inc.com/jeff-haden/the-8-qualities-of-remarkable-employees.html\" rel=\"nofollow\">\"Self-motivation often springs from a desire to show that doubters are wrong.\"</a></em></p></li>\\r\\n  <li><p><em>An overflow of good converts to Awesome.</em> ~William Shakespeare</p></li>\\r\\n  <li><p><em>I skate to where the puck is going to be, not where it has been.</em> ~Gretzky</p></li>\\r\\n  <li><p><em>In mathematics the art of asking questions is more valuable than solving problems.</em> ~Cantor</p></li>\\r\\n  <li><p><em>Test fast, fail fast, adjust fast.</em> ~Tom Peters</p></li>\\r\\n  <li><p><a href=\"http://www.khoslaventures.com/\" rel=\"nofollow\"><em>\"Our willingness to fail gives us the ability and opportunity to succeed where others may fear to tread.\"</em></a></p></li>\\r\\n  <li><p><em>I\\'ve failed over and over and over again -- that is why I succeed.</em> —Michael Jordan</p></li>\\r\\n  <li><p><a href=\"http://chat.stackexchange.com/transcript/message/6016840#6016840\"><em>To keep pace with the growth of mathematics, one would have to read about fifteen papers a day</em></a>.</p></li>\\r\\n  <li><p><a href=\"http://apple.stackexchange.com/users/1346/jason-salaz\"><em>The best protagonists rarely say anything.</em></a>.</p></li>\\r\\n  <li><p><em>Don\\'t give advices and don\\'t listen advices.</em></p></li>\\r\\n  <li><p><em>The less you say the stronger the strength of your words is.</em></p></li>\\r\\n  </ul>\\r\\n  \\r\\n  <p><strong>3 Principles of Success</strong> (Harford)</p>\\r\\n  \\r\\n  <blockquote>\\r\\n    <ol>\\r\\n    <li><p>Seek out and try new things.</p></li>\\r\\n    <li><p>When trying something new, do it on a scale where failure is survivable.</p></li>\\r\\n    <li><p>Seek out feedback (to determine your level of success) and learn from your\\r\\n    mistakes as you go along.</p></li>\\r\\n    </ol>\\r\\n  </blockquote>\\r\\n</blockquote>\\r\\n\\r\\n<p><em>P.s. <a href=\"http://www.freerangefactory.org/site/uploads/Main/article1.pdf\" rel=\"nofollow\">6 ways to kill creativity</a>, contact in forename@surname.com.</em></p>\\r\\n',\n",
       " '<p>I am a biology student that have a lot to learn about statistics.</p>\\r\\n',\n",
       " \"<p>I'm a SCJP, currently studying CS. </p>\\r\\n\",\n",
       " 'third continent and counting...\\\\\\\\n<p>\\\\\\\\n<br>\\\\\\\\n<img src=\"http://unicornify.appspot.com/avatar/203842398470284375023465237405928345?s=128\"> ',\n",
       " '<p><a href=\"http://gis.stackexchange.com/users/1270/dimitris\">\\r\\n<img src=\"http://gis.stackexchange.com/users/flair/1270.png\" width=\"208\" height=\"58\" alt=\"profile for dimitris at GIS, Q&amp;A for cartographers, geographers and GIS professionals\" title=\"profile for dimitris at GIS, Q&amp;A for cartographers, geographers and GIS professionals\">\\r\\n</a></p>\\r\\n',\n",
       " '<ul>\\r\\n<li>Graduated in Economics at the University of Vienna</li>\\r\\n<li>Research Assistant at the Vienna University of Economics and Business</li>\\r\\n</ul>\\r\\n',\n",
       " '<p>Trying to adopt sane statistical strategies and use R to do it all.</p>\\r\\n',\n",
       " '<p>Economist working on reinforcement learning, CGE, agent-based modeling. networks and buisness cycles.</p>\\r\\n',\n",
       " '<p>Data Scientist</p>\\r\\n',\n",
       " '<p>I work at Google, engineering stuff.</p>\\r\\n\\r\\n<p>If you have a reasonably high profile on Stack Overflow, and would be interested in working for Google in the UK, US or elsewhere, please feel free to reach out to me on stackoverflow@pobox.com.</p>\\r\\n',\n",
       " '<p>Old-ish IT Geezer, young at heart, memoir fanboy</p>\\r\\n',\n",
       " '<p>MSc Biology with interest in bioinformatics, biostatistics, and cancer biology.</p>\\r\\n',\n",
       " '<p>Dr Paul Brewer</p>\\r\\n\\r\\n<p>Owner: Economic and Financial Technology Consulting LLC\\r\\n<a href=\"http://eaftc.com\" rel=\"nofollow\">eaftc.com</a></p>\\r\\n\\r\\n<p>Open Source Projects:</p>\\r\\n\\r\\n<p><a href=\"http://www.armdisarm.com\" rel=\"nofollow\">armdisarm.com</a> Unofficial HAMP Loan Modification Calculator</p>\\r\\n\\r\\n<p><a href=\"http://github.com/DrPaulBrewer/html5csv\" rel=\"nofollow\">html5csv.js</a> Javascript/JQuery lib for manipulating tabular data</p>\\r\\n\\r\\n<p>Hobbies:</p>\\r\\n\\r\\n<p>Amateur Radio Callsign <a href=\"http://www.qrz.com/db/KI6CQ\" rel=\"nofollow\">KI6CQ</a></p>\\r\\n',\n",
       " '<p>Statistician currently working for ValueOptions in Rocky Hill, CT.</p>\\r\\n',\n",
       " \"<p>I'm a lecturer of mathematics at the University of the South Pacific. My research interests are in algebraic topology and metric geometry. </p>\\r\\n\",\n",
       " '<p>Data visualization, C++, Forth</p>\\r\\n\\r\\n<p>I work for SAS Institute on <a href=\"http://www.jmp.com/\" rel=\"nofollow\">JMP</a>.</p>\\r\\n\\r\\n<p>Twitter: <a href=\"https://twitter.com/xangregg\" rel=\"nofollow\">@xangregg</a></p>\\r\\n\\r\\n<p><a href=\"http://stackexchange.com/users/37021\">\\r\\n<img src=\"http://stackexchange.com/users/flair/37021.png\" width=\"208\" height=\"58\" alt=\"profile for xan on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for xan on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\\r\\n</a></p>\\r\\n',\n",
       " '<p>I’m a <a href=\"http://stackoverflow.com/q/104355/1968\">bioinformatics</a> PhD student at <a href=\"http://www.ebi.ac.uk/\" rel=\"nofollow\">EMBL-EBI</a> and the <a href=\"http://www.cam.ac.uk/\" rel=\"nofollow\">University of Cambridge</a> but I’m originally from <a href=\"http://en.wikipedia.org/wiki/Berlin\" rel=\"nofollow\">Berlin</a>.</p>\\r\\n\\r\\n<p>I’m mainly working on genomics using next-generation sequencing data.\\r\\nMy current thesis project is about the regulation of tRNA expression in mammals.</p>\\r\\n\\r\\n<p>Here’s my …</p>\\r\\n\\r\\n<p><strong><a href=\"http://twitter.com/klmr\" rel=\"nofollow\"><img src=\"http://dl.dropbox.com/u/6101039/profiles/twitter-pic.png\" alt=\"twitter-pic\"> Twitter</a></strong> account<br>\\r\\n<strong><a href=\"http://gplus.to/klmr\" rel=\"nofollow\"><img src=\"http://dl.dropbox.com/u/6101039/profiles/g%2B-pic.png\" alt=\"g+-pic\"> Google+</a></strong> account<br>\\r\\n<strong><a href=\"https://github.com/klmr\" rel=\"nofollow\"><img src=\"http://www.google.com/profiles/c/favicons?domain=github.com\" alt=\"github-pic\"> Github</a></strong> account<br>\\r\\n<strong><a href=\"http://careers.stackoverflow.com/klmr\"><img src=\"http://www.google.com/profiles/c/favicons?domain=careers.stackoverflow.com\" alt=\"cv-pic\"> Resume</a></strong>  </p>\\r\\n',\n",
       " '<ul>\\r\\n<li>Consultant @ Veneficus</li>\\r\\n<li>Interested in the combination of Finance and Computer Science</li>\\r\\n</ul>\\r\\n',\n",
       " '<p>computational linguist</p>\\r\\n',\n",
       " '<p>See my <a href=\"http://www.slashhome.be\" rel=\"nofollow\">webpage</a> for more information about me.</p>\\r\\n',\n",
       " '<p>Post Doc at Glostrup University Hospital, Denmark.</p>\\r\\n',\n",
       " '<p>Finance, law, statistics, machine learning, and programming.  Somehow this tries to all live in one place. </p>\\r\\n',\n",
       " '<p>Ex-tropical biologist, current software tester, python enthusiast, aspiring data/stats guy.</p>\\r\\n',\n",
       " '<p>French R and linux user.</p>\\r\\n',\n",
       " '<p>Computational biologists at European Molecular Biology Laboratory (EMBL), previously at Carnegie Mellon University and at the Institute for Molecular Medicine (University of Lisbon).</p>\\r\\n',\n",
       " '<p>Statistician and R programmer at the faculty of Bio-Engineering, university of Ghent</p>\\r\\n\\r\\n<p>Co-author of \\'<a href=\"http://eu.wiley.com/WileyCDA/WileyTitle/productCd-1119962846.html\" rel=\"nofollow\">R for Dummies</a>\\' (out in july 2012 )</p>\\r\\n\\r\\n<p>contact : Joris - dot - Meys - at - Ugent - dot - be</p>\\r\\n',\n",
       " '<p>I teach philosophy at UCSB.</p>\\r\\n\\r\\n<p>In my spare time I <a href=\"https://bitbucket.org/kenko\" rel=\"nofollow\">tinker</a> with python and haskell.</p>\\r\\n',\n",
       " \"<p>I am currently an independent consultant located in Berkeley, Ca.  Formerly I was a statistician at the now-defunct SolFocus, Inc., a concentrating photovoltaic technology company.  Lots of work around estimating how much energy we'd get out of a particular site!  And reliability work too, of course.  Prior to that, I worked for a number of years in Hewlett-Packard's Strategic Planning and Modeling (SPaM) group, an industry award-winning internal, and occasionally external, consulting organization, and as an independent consultant.</p>\\r\\n\",\n",
       " '<p>Interested in NLP, ML.</p>\\r\\n',\n",
       " '<p><strong>Website:</strong> <a href=\"http://www.netconstructor.com\" rel=\"nofollow\" title=\"http://netconstructor.com\">http://www.netconstructor.com</a> <br />\\r\\n<strong>Linkedin:</strong> <a href=\"http://linkedin.com/in/netconstructor\" rel=\"nofollow\" title=\"Christian Hochfilzer\">Christian Hochfilzer</a>  <br />\\r\\n<strong>Dribbble:</strong> <a href=\"http://www.netconstructor.com\" rel=\"nofollow\" title=\"San Diego web development\">Christian Hochfilzer</a> <br />\\r\\n<strong>Twitter:</strong> <a href=\"http://linkedin.com/in/netconstructor\" rel=\"nofollow\" title=\"Christian Hochfilzer\">Christian Hochfilzer</a> <br /></p>\\r\\n\\r\\n<p><strong><a href=\"http://www.netconstructor.com\" rel=\"nofollow\" title=\"http://netconstructor.com\">NetConstructor.com</a></strong> is a <a href=\"http://www.netconstructor.com\" rel=\"nofollow\" title=\"San Diego web development\">San Diego web development</a> and marketing firm founded by <a href=\"http://linkedin.com/in/netconstructor\" rel=\"nofollow\" title=\"Christian Hochfilzer\">Christian Hochfilzer</a>, Leo Baghdassarian and Kris Fredrickson and is composed of a tightly-knit group of highly talented and experienced UX Designers, Usability Experts and BigData Development Pros.</p>\\r\\n\\r\\n<p>At the heart of NetConstructor is the simple belief that our results speak for themselves. We understand that regardless of how aesthetically pleasing any website, application or marketing campaign maybe, or how much theoretical sense the technique makes, unless one is able to deliver measurable results and meet customer expectations it\\'s failed.</p>\\r\\n\\r\\n<p>The majority of our efforts are spent supporting and delivering results for the clients of some high profile design agencies but whenever possible we enjoy working directly with firms. Generally speaking, we are called up on when clients are looking to either:</p>\\r\\n\\r\\n<ol>\\r\\n<li><strong>Increasing Sales, Business Intelligence &amp; Online Positioning</strong></li>\\r\\n<li><strong>Reducing Operational, Marketing &amp; Support Costs</strong></li>\\r\\n</ol>\\r\\n\\r\\n<p>Our teams experience includes such firms as ACE Parking, Arkeia.com, Bakbone.com, Breach.com, Gateway.com, Cooking.com, Kingston.com, SK-Sanctuary.com, Swarco.com.com and various others.</p>\\r\\n\\r\\n<p>What sets NetConstructor apart remains our high attention to detail, our tightly knit team of gurus, and our focused on delivering quantifiable results focused on current/future growth.</p>\\r\\n\\r\\n<p><strong>Behance:</strong> <a href=\"http://www.behance.net/netconstructor\" rel=\"nofollow\" title=\"Behance Chistian Hochfilzer\">Chistian Hochfilzer</a>  <br />\\r\\n<strong>CrunchBase:</strong> <a href=\"http://www.crunchbase.com/company/netconstructor-com\" rel=\"nofollow\" title=\"NetConstructor\">NetConstructor</a>  <br />\\r\\n<strong>CrunchBase:</strong> <a href=\"http://www.worky.com/christian-hochfilzer\" rel=\"nofollow\" title=\"Crunchbase Profile: Christian Hochfilzer\">Christian Hochfilzer</a> <br />\\r\\n<strong>Worky:</strong> <a href=\"http://www.worky.com/christian-hochfilzer\" rel=\"nofollow\" title=\"Christian Hochfilzer\">Christian Hochfilzer</a>  <br /></p>\\r\\n',\n",
       " '<p><img src=\"http://i.stack.imgur.com/CG296.png\" alt=\"\"></p>\\r\\n',\n",
       " '<p>Reasearcher in applications of mathematics and statistics in the field of renewable energy.<br>\\r\\nAlso on <a href=\"http://mathoverflow.net/users/6531/robin-girard\" rel=\"nofollow\">Mathoverflow</a> </p>\\r\\n',\n",
       " '<p>I like pie</p>\\r\\n',\n",
       " \"I'm a scientific researcher for an image processing firm in the Midlands, working mostly in C++. \",\n",
       " '<p>C++ Programmer with a strong background in audio processing and music information retrieval.</p>\\r\\n\\r\\n<p>Strong interest in functional programming and programming languages in general.</p>\\r\\n\\r\\n<p>Currently employed in the telecomunications industry.</p>\\r\\n',\n",
       " '<p>I\\'m Senior Researcher at the <a href=\"http://www.mzes.uni-mannheim.de\" rel=\"nofollow\" title=\"The Mannheim Centre for European Social Research\">MZES</a> and a member of the <a href=\"http://reforms.uni-mannheim.de\" rel=\"nofollow\" title=\"SFB 884: The Political Economy of Reforms\">SFB 884</a>, both at the <a href=\"http://www.uni-mannheim.de/1/\" rel=\"nofollow\">University of Mannheim</a> in Germany.</p>\\r\\n',\n",
       " '<p>Programmer, fanatical about scripting languages</p>\\r\\n',\n",
       " '<p>I\\'m a PhD student at the <a href=\"http://www.icss.soton.ac.uk\" rel=\"nofollow\">Institute for Complex Systems Simulation</a> at the <a href=\"http://www.soton.ac.uk\" rel=\"nofollow\">University of Southampton</a>, UK studying complexity in Remote Sensing.</p>\\r\\n\\r\\n<p>As part of my work I am heavily involved in Remote Sensing and GIS technologies - particularly ENVI/IDL programming and ArcGIS scripting using Python.</p>\\r\\n\\r\\n<p>My <a href=\"http://www.rtwilson.com/academic\" rel=\"nofollow\">academic website</a> shows some examples of my work, and links to some of the <a href=\"http://www.rtwilson.com/academic/software\" rel=\"nofollow\">software I have written</a>.</p>\\r\\n',\n",
       " '<p>Risk Manager at Raiffeisen Capital Management</p>\\r\\n\\r\\n<p>External Lecturer at Vienna University of Technology</p>\\r\\n',\n",
       " '<p>Java developer</p>\\r\\n\\r\\n<p><a href=\"https://github.com/isopov\" rel=\"nofollow\">On Github</a></p>\\r\\n',\n",
       " '<p>Student :)</p>\\r\\n',\n",
       " '<p>I am a mathematician from Finland. In maths, my main interest are in algebra and number theory. Nowadays I mostly read mathematics on my own and look for an interesting math projects.</p>\\r\\n',\n",
       " 'Code or model? Mu.',\n",
       " \"<p>I'm a software engineer and distributed intelligent specialist.</p>\\r\\n\",\n",
       " '<p>I have an undergraduate degree in Statistics, but these days I approach data analysis from a less traditional perspective.  I focus more on computation and thinking about things in an abstract way to \"hack\" solutions; although the statistical foundations do help.</p>\\r\\n\\r\\n<p>My tool of choice is R, but I used to rely on Excel, VBA, and sometimes SAS.  Beyond that, I know a little bit about many other languages and tools. Oh, and I do a little work in Hadoop as well.</p>\\r\\n\\r\\n<p>My company is Chicago Data Science, and (as the name implies) we do work in data analysis.  </p>\\r\\n\\r\\n<p>You can reach me here: <br>\\r\\n<strong>gleynes at gmail</strong> <br>\\r\\nor here: <br>\\r\\n<strong>gene at chicagodatascience.com</strong></p>\\r\\n',\n",
       " '<p>Former assistant professor teaching statistics and Ph.D. candidate in Hungary. Currently full-time founder of <a href=\"http://rapporter.net\" rel=\"nofollow\">rapporter.net</a>, a web application frontend to <a href=\"http://www.r-project.org/\" rel=\"nofollow\">R</a>-based data analysis and reports. Husband, father of two. <em>I will die before repeating any professional task: we code, then let the machines do the rest.</em></p>\\r\\n\\r\\n<p>My <strong><a href=\"http://www.r-project.org/\" rel=\"nofollow\">R</a> packages</strong>: <a href=\"http://rapporter.github.io/pander/\" rel=\"nofollow\">pander</a>, <a href=\"http://rapport-package.info/\" rel=\"nofollow\">rapport</a>, <a href=\"http://hackme.rapporter.net/\" rel=\"nofollow\">sandboxR</a>, <a href=\"http://cran.r-project.org/web/packages/migration.indices/\" rel=\"nofollow\">migration.indices</a>, <a href=\"http://cran.r-project.org/web/packages/saves/\" rel=\"nofollow\">saves</a></p>\\r\\n\\r\\n<p>Please feel free to <strong>get in touch</strong> (but please <em>do mention your SO username</em>):</p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://www.meetup.com/Budapest-Users-of-R-Network\" rel=\"nofollow\">Hungarian R User Group</a> and <a href=\"http://r-projekt.hu/welcome\" rel=\"nofollow\">r-projekt.hu</a></li>\\r\\n<li><a href=\"http://github.com/daroczig\" rel=\"nofollow\">GitHub</a></li>\\r\\n<li><a href=\"http://hu.linkedin.com/in/daroczig/\" rel=\"nofollow\">LinkedIn</a>, <a href=\"https://plus.google.com/104026536443441875944\" rel=\"nofollow\">Google Plus</a> and <a href=\"http://twitter.com/daroczig\" rel=\"nofollow\">Twitter</a></li>\\r\\n<li><a href=\"http://www.f6s.com/rapporter\" rel=\"nofollow\">F6S</a></li>\\r\\n</ul>\\r\\n',\n",
       " \"<p>I'm a Master's student in Computer Science. I majorly work on C,C++ and Python. \\r\\nI know a fair amount of SQL,HTML and CSS as well. I have nothing against JAVA but it's not my preferred language for coding.</p>\\r\\n\",\n",
       " '<p>PhD Candidate at the Department of Computer Science &amp; Engineering, Ohio State University.</p>\\r\\n',\n",
       " '<p>Studying economics at Copenhagen University...\\r\\nUsing R when possible.</p>\\r\\n',\n",
       " '<ul>\\r\\n<li>Teacher and graduate student at University of Jyväskylä, Department of Mathematical Information Technology, with special interest in matters pertaining to programming languages.</li>\\r\\n<li>Experience in real-world programming with at least C, C++, Java, and Haskell.</li>\\r\\n<li>Debian Developer since 1999</li>\\r\\n</ul>\\r\\n',\n",
       " \"<p>I'm currently working on a PhD in Public Policy at the Sanford School of Public Policy at Duke University, where I'm focusing primarily on the relationship between the nonprofit sector and governance development in the Middle East. Life rocks.</p>\\r\\n\",\n",
       " \"<p>roses are red</p>\\r\\n\\r\\n<p>violets are blue</p>\\r\\n\\r\\n<p>functions don't have any color.</p>\\r\\n\",\n",
       " '<p>Formally I am educated within business and economics and I work with marketing as my main profession. I have also worked with WordPress development on and off since january 2012. </p>\\r\\n\\r\\n<p>I believe continued education within a diverse range of skills is important. New ideas and technologies emerge from cross disciplinary people. Especially marketing (in the business world) needs to be more innovative.</p>\\r\\n\\r\\n<p>Please support the new Marketing Q&amp;A site:<br>\\r\\n<a href=\"http://area51.stackexchange.com/proposals/51786/marketing?referrer=ZPyQhV_KHaLjvaTn2LelVw2\"><img src=\"http://area51.stackexchange.com/ads/proposal/51786.png\" width=\"220\" height=\"250\" alt=\"Stack Exchange Q&amp;A site proposal: Marketing\" /></a><br></p>\\r\\n',\n",
       " '<p>Biochemist working in prenatal screening in UK trying to formalise and expand the statistics I use daily.</p>\\r\\n',\n",
       " '<p>I’m redefining myself. Please stand by…</p>\\r\\n',\n",
       " '<p>Computer Engineer ;-)</p>\\r\\n',\n",
       " '<p>I like programming, language, and collecting hobbies.</p>\\r\\n\\r\\n<ul>\\r\\n<li><p><a href=\"http://evincarofautumn.blogspot.com/\" rel=\"nofollow\">My blog</a> is a pretty good blog.</p></li>\\r\\n<li><p>I am working on <a href=\"https://github.com/evincarofautumn/kitten\" rel=\"nofollow\">a programming language called Kitten</a> which you may like.</p></li>\\r\\n</ul>\\r\\n',\n",
       " \"<p>Statistician / Mathematician learning to programming.</p>\\r\\n\\r\\n<p>Don't hate me because I'm lazy!</p>\\r\\n\",\n",
       " '<p>Ph.D. student in Bar Ilan University, computer science department and economics department. Research interests:</p>\\r\\n\\r\\n<ul>\\r\\n<li>Fair Division of Land.</li>\\r\\n<li>Compuatational Linguistics.</li>\\r\\n<li>Negotiation Agents.</li>\\r\\n</ul>\\r\\n\\r\\n<p>Recently I wrote a <a href=\"http://econpapers.repec.org/paper/biuwpaper/2014-01.htm\" rel=\"nofollow\">working paper</a> in which I cited several answers that I got from Stack Exchange members. Many thanks to everyone!</p>\\r\\n\\r\\n<p>Email: erelsgl@gmail.com</p>\\r\\n',\n",
       " \"<p>I'm an enthusiast programmer since I'm 13 :-) Lost that passion a bit in later years while being an entrepreneur. Right now, I'm working hard to create a symbiosis of these two passions and enjoying it a lot ;-)</p>\\r\\n\",\n",
       " '<p>PhD in Computer Science, CTO at Shiftforward, Professor of CS at FEUP.</p>\\r\\n',\n",
       " 'Mind the gap.',\n",
       " '<p>Just a regular Norwegian guy. Nothing more to see here, move on =)</p>\\r\\n',\n",
       " '<p>PhD candidate in Evolutionary Genetics of Aging</p>\\r\\n',\n",
       " '<p>Interested lurker, occasional contributer, looking to increase my knowledge of statistics.</p>\\r\\n',\n",
       " '<p><em>Programming is fun again with Perl</em>.</p>\\r\\n\\r\\n<p><strong>All original source snippets I post on Stackoverflow.com and other sites in the StackExchange network are dedicated to the public domain.</strong></p>\\r\\n\\r\\n<p>I also post my thoughts on <a href=\"http://blog.qtau.com/\" rel=\"nofollow\">economics, politics and strategy</a> on <a href=\"http://blog.qtau.com/\" rel=\"nofollow\">Q&tau;</a> and <a href=\"http://blog.nu42.com/\" rel=\"nofollow\">programming related articles</a> on <a href=\"http://blog.nu42.com/\" rel=\"nofollow\">&nu;42</a>.</p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://www.google.com/profiles/sinan.unur\" rel=\"nofollow\">+A. Sinan Unur</a></li>\\r\\n<li><a href=\"https://twitter.com/sinan_unur\" rel=\"nofollow\">@sinan_unur</a></li>\\r\\n<li><a href=\"http://www.unur.com/\" rel=\"nofollow\">Unur.com</a></li>\\r\\n<li>Pet project: <a href=\"http://www.howtosayinturkish.com/\" rel=\"nofollow\">HowtosayinTurkish.com</a></li>\\r\\n</ul>\\r\\n',\n",
       " \"<p>I am a LaTeX user for several years now but I never took it much further than user level. Sometimes I wish I'd be a better coder but I am able to solve most of my problems with existing packages.</p>\\r\\n\\r\\n<p>Currently I am in the last year of my PhD in Chemical Engineering in Manchester, England which means that I won't be able to dedicate much time to tex.stackexchange in the next couple of month. However, I am going through phases of substantial procrastination and if I see a question I can answer, I'll try to give a good answer.</p>\\r\\n\\r\\n<p>Once finished with my PhD, I'd like to return to my home sweet home in Germany.</p>\\r\\n\",\n",
       " 'Computer Science Student.',\n",
       " '<p>Combined PhD / Masters candidate based in Sydney, currently cast adrift on a sea of statistical misunderstanding. </p>\\r\\n',\n",
       " '<p>I am a graduate student at Oregon State University performing research on wireless high-definition video encoded with H.264 video compression technology and transmitted over 802.11 (WiFi) networks.</p>\\r\\n',\n",
       " '<p>Researcher at UC Berkeley D-Lab, Feldenkrais teacher, programmer, moderately competent statistician</p>\\r\\n',\n",
       " '<p>A geographer, demographer, and spatial statistician. I work on projects that help organizations understand policy impacts and demographic change.</p>\\r\\n',\n",
       " '<p>I am interested in examining face and object recognition as well as the functional organisation of the ventral temporal lobes with masked priming techniques and functional magnetic resonance imaging coupled with multivariate pattern analysis.</p>\\r\\n',\n",
       " '<p>I work in bioformatics research with a strong emphasis on crowdsourcing.  I dabble in machine learning..</p>\\r\\n',\n",
       " '<p>I am an ex-physician, have studied mechanical engineering for a while, and have a master degree in product design.</p>\\r\\n\\r\\n<p>Now I work designing diagnostic equipment (surface EMG, posturography, pedobarography), dealing with system requirements, data visualization, and GUI design, and the like.</p>\\r\\n\\r\\n<p>I am also a die-hard cyclist, be it trails (not much nowadays), off-road, commuting, touring or randonneuring. Besides, I have deep interests in bike design and mechanics.</p>\\r\\n',\n",
       " '<p>5th year computational biology PhD student studying chromatin &amp; DNA replication.</p>\\r\\n',\n",
       " '<p>Evolutionary Biologist</p>\\r\\n',\n",
       " '<p>I am PhD student at UC Berkeley where I develop distributed systems for large-scale intrusion detection, network forensics, and incident response.</p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"https://github.com/mavam\" rel=\"nofollow\">github</a></li>\\r\\n<li><a href=\"https://twitter.com/mavam\" rel=\"nofollow\">twitter</a></li>\\r\\n</ul>\\r\\n',\n",
       " '<p>Working at university on the borderline between mathematics and computer sciences.</p>\\r\\n',\n",
       " '<p>I am the left brain @ We Are Cube.³</p>\\r\\n\\r\\n<p>We create beautiful and intuitive interfaces for web and mobile.</p>\\r\\n\\r\\n<p><a href=\"http://www.wearecube.ch\" rel=\"nofollow\">http://www.wearecube.ch</a></p>\\r\\n',\n",
       " '<p>I am a nutritional epidemiologist, with additional experience in biostatistics and health geography. I have a BA in Biology from Hendrix College (2001), and a MHS in International Health from the Bloomberg School of Public Health at Johns Hopkins University (2003). I earned my PhD in Nutritional Epidemiology at the University of North Carolina (2009), where I was also a NSF IGERT trainee at the Carolina Population Center. </p>\\r\\n\\r\\n<p>I worked as a Lecturer in Epidemiology and Biostatistics at the University of Leeds, where I held a three-year MRC Population Health Scientist Fellowship to investigate infant and child growth patterns. I also taught masters-level classes in Nutritional Epidemiology, and Obesity and Public Health. </p>\\r\\n\\r\\n<p>I am now at UCC as a Senior Post-doctoral Researcher funded by an Interdisciplinary Capacity Enhancement award, investigating nutrition and health across the life course using a variety of Irish cohort studies. I also work internationally with a number of large cohort studies in Brazil, the Philippines, India, South Africa, and Guatemala (<a href=\"http://www.cohortsgroup.org/\" rel=\"nofollow\">COHORTS</a>). </p>\\r\\n\\r\\n<p>Methodologically, I use functional data analysis to look at longitudinal, anthropometric data; and latent variable methods to measure complex, multidimensional constructs such as dietary patterns and socioeconomic status.</p>\\r\\n\\r\\n<p><a href=\"http://dantalus.org\" rel=\"nofollow\">http://dantalus.org</a></p>\\r\\n\\r\\n<p>@statsepi</p>\\r\\n',\n",
       " '<blockquote>\\r\\n  <p><em>That is not dead which can eternal lie.</em></p>\\r\\n  \\r\\n  <p><em>And with strange aeons even death may die.</em></p>\\r\\n</blockquote>\\r\\n',\n",
       " \"<p>I'm a PhD student in computer science working on Search Engines, Information Retrieval and Ranking algorithms.</p>\\r\\n\",\n",
       " '<p><a href=\"http://www.xmwconsulting.co.uk\" rel=\"nofollow\">XMW Consulting</a> provides consultancy services in the areas of data mining and statistical analysis, specializing in the development of technical algorithms and applications with <a href=\"http://www.mathworks.com/products/matlab/\" rel=\"nofollow\">MATLAB®</a>.</p>\\r\\n\\r\\n<p>Its founder, Sam Roberts, has over fifteen years of experience (including seven as a customer-facing consultant at <a href=\"http://www.mathworks.com\" rel=\"nofollow\">MathWorks®</a>, makers of MATLAB) applying these methods to help a wide range of clients from the pharmaceutical, energy, finance, aerospace, Formula 1, food, consumer goods, petrochemical and retail sectors.</p>\\r\\n\\r\\n<p>Read my <a href=\"http://www.xmwconsulting.co.uk/backslashblog\" rel=\"nofollow\">MATLAB blog</a> — <a href=\"http://www.linkedin.com/in/samroberts\" rel=\"nofollow\">Connect</a> with me — <a href=\"http://www.twitter.com/sllroberts\" rel=\"nofollow\">Follow</a> me</p>\\r\\n',\n",
       " '<p>I work at Twilio.</p>\\r\\n\\r\\n<p>Here are some side projects I\\'ve been working on: <a href=\"http://kev.inburke.com/projects\" rel=\"nofollow\">http://kev.inburke.com/projects</a></p>\\r\\n',\n",
       " '<p>I am now a grad student at the University of Ottawa. I recently graduated with a Master\\'s in Computer science from the University of Ottawa, after earning an Honors Bachelor of Science with double majors in Computer Science and Logic and a minor in Math from the University of Toronto. Most of my Saturday nights are spent writing hobby code (right now, my fascination is with Genetic Algorithms) - which speaks to the lack of my social life.</p>\\r\\n\\r\\n<p>I love to learn new things. Paradoxically, don\\'t enjoy reading more than 10 pages per new topic (much to my disappointment). I have therefore developed an ability to speed read - something that the <a href=\"http://lab.arc90.com/experiments/readability/\" rel=\"nofollow\">readability</a> and <a href=\"http://www.eyercize.com/practice/bm_read/\" rel=\"nofollow\">speed read it</a> bookmarklets have helped with.</p>\\r\\n\\r\\n<p>Author of <a href=\"http://pypi.python.org/pypi?%3aaction=display&amp;name=Pyvolution\" rel=\"nofollow\">Pyvolytion in the python package index</a> (which I <a href=\"http://pyvideo.org/video/1561/a-pure-python-genetic-algorithms-framework\" rel=\"nofollow\">talked about at PyConCA, 2012</a> and <a href=\"http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=6557872\" rel=\"nofollow\">published it in the IEEE</a>), co-author of <a href=\"https://pypi.python.org/pypi/cfg/0.0.0\" rel=\"nofollow\">cfg in the python package index</a>.</p>\\r\\n\\r\\n<p>I can be found on <a href=\"http://twitter.com/inspectorG4dget\" rel=\"nofollow\">Twitter</a>, <a href=\"https://plus.google.com/104021670379494721122/posts\" rel=\"nofollow\">Google+</a>, <a href=\"http://careers.stackoverflow.com/cv/employer/39354\">Careers 2.0</a>, and <a href=\"http://www.linkedin.com/profile?viewProfile=&amp;key=51627923&amp;authToken=th64&amp;authType=NAME_SEARCH&amp;locale=en_US&amp;srchindex=1&amp;pvs=ps&amp;goback=.fps_ashwin+panchapakesan_%2a1_%2a1_%2a1_%2a1_%2a1_%2a1_%2a1_Y_%2a1_%2a1_%2a1_false_1_R_true_CC%2CN%2CI%2CG%2CPC%2CED%2CFG%2CL%2CDR_%2a2_%2a2_%2a2_%2a2_%2a2_%2a2_%2a2_%2a2_%2a2_%2a2_%2a2_%2a2_%2a2_%2a2_%2a2_%2a2\" rel=\"nofollow\">LinkedIn</a></p>\\r\\n',\n",
       " 'I am a programmer analyst at the Medical College of Wisconsin.',\n",
       " '<p>I write automated trading applications in Java using the TWS API from Interactive Brokers.  I use Python and C++ for data analysis and exploration.</p>\\r\\n',\n",
       " 'Science & Technology Aficionado',\n",
       " '<p>As <a href=\"http://algo.inria.fr/flajolet/\" rel=\"nofollow\">somebody</a> used to say: </p>\\r\\n\\r\\n<p><em>Does research. Smokes. Battles administration. Smokes. Wishes he could stop battling administration so that he could have more time to do research. Smokes some more.</em> </p>\\r\\n\\r\\n<p>The same. Except I do not smoke.</p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p>This paragraph is for my personal use but freely available:</p>\\r\\n\\r\\n<p>Welcome to Math.SE! Please, consider updating your question to include what you have tried and where you are getting stuck. That way, people on this site will know exactly what help you need.</p>\\r\\n',\n",
       " '<blockquote>\\r\\n  <p>\"I would never die for my beliefs\\r\\n  because I might be wrong.\"</p>\\r\\n</blockquote>\\r\\n\\r\\n<p>-- Bertrand Russell</p>\\r\\n',\n",
       " '<p>I am a graduate student working towards a Ph.D. in Statistics, specifically bioinformatics. I am also a regular R, GNU Emacs and ubuntu user. </p>\\r\\n',\n",
       " '<p>Statistics tragic, most definitely an objective Bayesian style statistician.  I find myself quite compelled by the \"probability as extended logic\" philosophy, particularly the work of Edwin Jaynes.  I have also derived on of the most important rules ever: BAYESIAN+JAYNES=JAYNESIAN. lol.  But seriously, I am in awe of this guy: he is like the \"Jesus\" of statistical thinking.</p>\\r\\n\\r\\n<p>I also quite dislike the use of the word \"prior beliefs\" which gets attach to prior probabilities in the Bayesian world.  It brings over the wrong connotations - I think \"prior assumptions\" or \"prior state of knowledge\" better describes what is actually encapsulated by \"the prior\" of Bayesian statistics.</p>\\r\\n\\r\\n<p>Which is worse: a frequentist who refuses to use prior\\'s (but is happy to assume a likelihood) or a subjective Bayesian who claims that they can be whatever you \"believe\"?</p>\\r\\n',\n",
       " '<p>I am an Assistant Professor of Biology at Longwood University.  My main interests are how biodiversity affects ecosystem function in aquatic sediments.  You can find out more about what my lab is doing here: <a href=\"http://longwood.edu/staff/fortinok\" rel=\"nofollow\">http://longwood.edu/staff/fortinok</a></p>\\r\\n',\n",
       " '<p>Gavin Chait is a Senior Risk Analyst and Strategist at consulting firm, Whythawk, which focuses on economic modelling, and market and investment risk analysis.</p>\\r\\n\\r\\n<p>Gavin\\'s core interests revolve around researching, analysing and writing about the frontiers of human development, particularly those of technology, and of wealth and poverty.</p>\\r\\n\\r\\n<p>Follow me at my <a href=\"http://www.whythawk.com/blog\" rel=\"nofollow\">blog</a> or <a href=\"http://twitter.com/#!/GavinChait/\" rel=\"nofollow\">@GavinChait</a></p>\\r\\n',\n",
       " '<p>Industrial researcher with expertise in biomedical signal processing and pattern recognition.</p>\\r\\n',\n",
       " '<p>Buhddist, Trader, Programmer, Data scientist.\\r\\nI love my job!</p>\\r\\n',\n",
       " '<p>UPDATE: My new book, \"Data Push Apps with HTML5 SSE\" is out and selling! Get it from O\\'Reilly here: <a href=\"http://shop.oreilly.com/product/0636920030928.do\" rel=\"nofollow\">http://shop.oreilly.com/product/0636920030928.do</a>   Or from Amazon here: <a href=\"http://rads.stackoverflow.com/amzn/click/1449371930\" rel=\"nofollow\">http://www.amazon.com/dp/1449371930</a> Or from any good bookseller.</p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p>I\\'m director at QQ Trends, a company that solves difficult data and software challenges for our clients. (We often have freelance projects, so get in touch if interested.)</p>\\r\\n\\r\\n<p>UPDATE: currently looking for a part-time CSS/JS programmer. 理想は東京所在、日本語が読める方。Interesting project(s), you will definitely learn something.</p>\\r\\n\\r\\n<p>(And, of course, please do get in touch if you have interesting challenges that you would like our world-class experts to work on!)</p>\\r\\n\\r\\n<p>Typical work: doing fun stuff with data (fixing, mining, etc.), web sites (front and back-ends), trading strategies. Research: trading strategies, computer go, machine translation, understanding context, AI search algorithms. Languages: C++, PHP, R, javascript, and many more. </p>\\r\\n\\r\\n<p>I\\'m British, living and working in Tokyo for almost 20 years. Human Languages: English, Japanese (fairly fluent, 1 kyu), some German, Chinese and Arabic.</p>\\r\\n\\r\\n<p>(Contact me at dc at qqtrend dot com: please mention you are coming from StackOverflow, so I know it is not spam.)</p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p>Easy ways to irritate me on StackExchange sites (whether my own question or someone else\\'s): 1. Downvote without a comment (N/A/ if someone already left a comment and you just agree with it, of course); 2. Answer in comments. Other than that I\\'m a really easy-going pragmatic guy :-)</p>\\r\\n',\n",
       " '<p>Studying Maths at Cambridge, worked at Reach.ly and Google.</p>\\r\\n\\r\\n<p><a href=\"http://twitter.com/p_e\" rel=\"nofollow\">http://twitter.com/p_e</a></p>\\r\\n',\n",
       " '<p>Web developer at babeliumproject.com and diariolinux.com. \\r\\nComputer Science teacher at Euskal Herriko Unibertsitatea</p>\\r\\n',\n",
       " '<p>I am not actually a rabbit I am a person.</p>\\r\\n',\n",
       " \"<p>I'm a psychology professor who mostly works with genetically informative samples, studying the ways that people differ. </p>\\r\\n\\r\\n<p>I've worked in the genetics of dyslexia and language, personality, intelligence, memory, social traits like altruism.</p>\\r\\n\\r\\n<p>My interests are Philosophy, Science in general, the arts. Fun is sailing and skiing.</p>\\r\\n\",\n",
       " '<p>about.me/sarmatangirala, user1150857</p>\\r\\n',\n",
       " '<p>Fanatic LabVIEW developer, interested in GUI design and multi-developer teams.</p>\\r\\n',\n",
       " \"<p>I'm an aspiring statistician.</p>\\r\\n\\r\\n<p>I also like R, Python, fitness, books, and a few other things.</p>\\r\\n\",\n",
       " '<p>Phd candidate in Industrial engineering</p>\\r\\n',\n",
       " \"<p>I'm a freelance web developer in Upstate New York. I love writing code in Ruby, but I'll write code in any language in order to get the job done.</p>\\r\\n\",\n",
       " '<p>I am a software architect by trade and a coder/hacker by compulsion.  I\\'ve worked on projects large and small, and have experience in more languages than I care to count (Java, ActionScript, Objective-C, JavaScript, C, C++, HTML, CSS, Velocity, SQL, Php, Bash, Perl, and probably others) and a number of different functional domains (concurrency/multiprogramming, build and test automation, server administration, team and project management, high-level design and architecture, etc.).  </p>\\r\\n\\r\\n<p>Notable open-source contributions include <a href=\"https://github.com/adam-roth/matchbook\" rel=\"nofollow\">matchbook</a>, a platform-agnostic matchmaking service, API, and SDK for mobile applications (think multi-platform real-time gaming), a <a href=\"https://github.com/adam-roth/coredata-threadsafe\" rel=\"nofollow\">thread-safe wrapper</a> for Apple\\'s Core Data framework, and a <a href=\"https://github.com/adam-roth/screen-cap-view\" rel=\"nofollow\">screen-capture utility</a> for iOS.</p>\\r\\n\\r\\n<p>I\\'ve also self-published a <a href=\"http://rads.stackoverflow.com/amzn/click/B003URRNCS\" rel=\"nofollow\">book</a> about self-publishing books, and a <a href=\"http://www.pieceable.com/view/bundle/bwx/80e4b/au.com.suncoastpc.Webcomix\" rel=\"nofollow\">comic reader app</a> for the iPhone.  I maintain and occasionally update a <a href=\"http://codethink.no-ip.org\" rel=\"nofollow\">blog</a> about various topics that I happen to find interesting or noteworthy.</p>\\r\\n',\n",
       " \"<p>I'm a teacher. My subjects are Mathematics and Economics and most of my teaching is at pre-university level (UK A levels). I also teach Management undergraduates courses in Economics and Mathematics.</p>\\r\\n\\r\\n<p>I studied Mathematics at university some twenty years ago and have forgotten much of it. Moreover, I didn't study Statistics which I have become very interested in and have taught. The trouble is, my knowledge is hapazard; I've picked it up as I've gone along.</p>\\r\\n\",\n",
       " '<p>Software Development Engineer, AWS Elastic Beanstalk</p>\\r\\n\\r\\n<p>MS CS from Georgia Tech. At Georgia Tech worked on Vein-to-Vein\\r\\n<a href=\"https://github.com/C4G/V2V\" rel=\"nofollow\">https://github.com/C4G/V2V</a>\\r\\n<a href=\"https://github.com/jembi/bsis\" rel=\"nofollow\">https://github.com/jembi/bsis</a>\\r\\nVideo: <a href=\"http://www.youtube.com/watch?v=O_zIIXepPHc\" rel=\"nofollow\">http://www.youtube.com/watch?v=O_zIIXepPHc</a></p>\\r\\n',\n",
       " '<p>Student of Computational Linguistics B.Sc. at the University of Potsdam, Germany</p>\\r\\n\\r\\n<p>Programmer at a small backup software company (Visual C++, Python, some VB6 and PHP)</p>\\r\\n',\n",
       " '<p><code>*</code> denotes convolution, $\\\\\\\\cdot$ denotes multiplication</p>\\r\\n\\r\\n<p>I am the developer of the midasr R package: </p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://mpiktas.github.io/midasr/\" rel=\"nofollow\">http://mpiktas.github.io/midasr/</a></li>\\r\\n</ul>\\r\\n\\r\\n<p>and you cand find me on </p>\\r\\n\\r\\n<ul>\\r\\n<li><p>Google+: <a href=\"https://plus.google.com/112784314334842289710/\" rel=\"nofollow\">https://plus.google.com/112784314334842289710/</a></p></li>\\r\\n<li><p>Github: <a href=\"https://github.com/mpiktas\" rel=\"nofollow\">https://github.com/mpiktas</a> and <a href=\"https://github.com/vzemlys\" rel=\"nofollow\">https://github.com/vzemlys</a></p></li>\\r\\n</ul>\\r\\n',\n",
       " '<p>Curious &amp; Evolving...</p>\\r\\n',\n",
       " '<p>Researcher working in the field of environmental science. My work focusses on the use of earth observation data for hydrological applications.</p>\\r\\n',\n",
       " '<p>Co-Founder &amp; CEO at learnDataAnalysis, PhD student (Stats-UCI), Data Scientist at MIND Research, Biostatistician, Sharif graduate, recently in-love with d3.js !</p>\\r\\n',\n",
       " \"<p>I have several years experience in business and analysis, using tools such as Excel, Access, SQL, as well as BI tools. I'm learning predictive modeling and how to use R. I begin about a year ago with both.  My goals are to more accurately predict who will retire or voluntarily resign from my organization. I am interested in both accuracy at the aggrregate level (total number of retirements or resignations by various demographic attrributes such as department and job type) and at the individual level (probability that a person will retire or resign).</p>\\r\\n\",\n",
       " '<p>Computer science student\\r\\nat the University of Warwick\\r\\nInterested in Machine Vision, Artificial Intelligence and Android.</p>\\r\\n',\n",
       " \"I've been a software engineer for a few years. I love what I do and I'm always trying to learn more. Advice and critical discussions are welcome!\\\\\\\\n<br />\\\\\\\\nCurrently studying discrete math, multivariable calculus, algorithm design and analysis, and data structures (mainly graphs). I am also researching what I think my future field will be. I'm leaning towards soft computing/machine learning currently. Right now it's academia, but most things start out that way. I think it will be huge when enough people completely grok it.\",\n",
       " '<p>Biostatistics/Psychometrics consulting, bohemian scientist. </p>\\r\\n\\r\\n<p>Also on <a href=\"http://www.linkedin.com/in/christophelalanne\" rel=\"nofollow\">linkedin</a> and <a href=\"http://twitter.com/chlalanne\" rel=\"nofollow\">twitter</a>, but check my <a href=\"http://www.aliquote.org\" rel=\"nofollow\">website</a> for more information.</p>\\r\\n',\n",
       " '<p>I work for Esri on open source tools for ocean science.\\r\\nI\\'m affiliated with the <a href=\"http://www.nceas.ucsb.edu\" rel=\"nofollow\">National Center for Ecological Analysis and Synthesis</a>, and recently graduated from at nearby <a href=\"http://www.ucsb.edu\" rel=\"nofollow\">UCSB</a>, where I studied the global shipping trade in an ecological context (<a href=\"http://4326.us/thesis/walbridge-masters-thesis.pdf\" rel=\"nofollow\">thesis</a>).</p>\\r\\n\\r\\n<p>Source snippets I post on GIS.SE are dedicated to the public domain.</p>\\r\\n',\n",
       " '<p>Experienced Web Developer<br/>\\r\\nCurrently playing with C# WPF</p>\\r\\n',\n",
       " \"<p>Just graduated with First Class Honours in the B.Sc. (Hons) ICT in Computer Science &amp; Artificial Intelligence course at the University of Malta.</p>\\r\\n\\r\\n<p>Currently working at CERN's IT department as an Associated Member of the personnel.</p>\\r\\n\\r\\n<p>Will start working as a full-time software developer in Malta once I finish my summer studentship at CERN.</p>\\r\\n\",\n",
       " '<p>phd student: Nuclear Organization (cell biology &amp; 3D-image analysis)</p>\\r\\n',\n",
       " '<p>M.Sc student in Tel-Aviv University.</p>\\r\\n',\n",
       " '<p>Machine Learning, Computer Vision, Graphical Models, Matlab, Python, JavaScript, JQuery</p>\\r\\n',\n",
       " '<p>Programmer. Designer. Photo-/Videographer. Musician.</p>\\r\\n',\n",
       " '<p>The majority of my research focuses on 3D data analysis. I also am very interested in engineering education.</p>\\r\\n',\n",
       " 'A python, matlab and django programmer in a bioinformatics PhD program',\n",
       " 'Physicist, interested in agent-based simulation and statistical physics of models inspired by economic and biological reasoning.\\\\\\\\n\\\\\\\\nProgramming in C and Python.',\n",
       " '<p>I\\'m an astrophysicist and heavy numpy / scipy user.</p>\\r\\n<p><a href=\"http://twitter.com/keflavich\">@keflavich</a></p>',\n",
       " '<p>I am a PhD student at the <a href=\"http://wwwagak.cs.uni-kl.de\" rel=\"nofollow\">Algorithms &amp; Complexity group</a> at University of Kaiserslautern, Germany. I research design and analysis of parallel algorithms and data structures.</p>\\r\\n\\r\\n<p>In my free time I read books, enjoy (and sometimes make) music, code, work out and roam the webs.</p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p>You can find sources for all self-created images I post on Stack Exchange <a href=\"http://akerbos.github.io/sesketches/\" rel=\"nofollow\">here</a>.</p>\\r\\n',\n",
       " '<p>web enthusiast</p>\\r\\n\\r\\n<p>loves algorithms</p>\\r\\n\\r\\n<p>loves Java</p>\\r\\n\\r\\n<p>loves Python</p>\\r\\n\\r\\n<p>loves Perl</p>\\r\\n',\n",
       " '<p>Rebel with rather too many causes.</p>\\r\\n',\n",
       " \"<p>I'm a fire chief looking to use some of the huge amounts of data we collect/report to help make better decisions for service delivery.</p>\\r\\n\",\n",
       " '<p>Author of <a href=\"https://github.com/mmurdoch/arduinounit\" rel=\"nofollow\">ArduinoUnit</a>, the <a href=\"http://arduino.cc\" rel=\"nofollow\">Arduino</a> unit testing framework.</p>\\r\\n\\r\\n<p>Curator of <a href=\"http://codeonipad.com/\" rel=\"nofollow\">Code on iPad</a>.</p>\\r\\n',\n",
       " '<p>i mess with computers as a distraction to doing real work. </p>\\r\\n',\n",
       " '<p>Graduate business management student, Faculty of Economics and Business, University of Padua</p>\\r\\n',\n",
       " '<p>Daniel Graziotin is a PhD student in Computer Science at the Free University of Bozen-Bolzano. His research interests include human aspects in empirical software engineering with psychological measurements, Web engineering, and Open Science. He is Editorial Associate at the <a href=\"http://openresearchsoftware.metajnl.com\" rel=\"nofollow\">Journal of Open Research Software</a> and the local coordinator of the <a href=\"http://okfn.org\" rel=\"nofollow\">OKF</a> <a href=\"http://openscience.it\" rel=\"nofollow\">Open science Italia</a> group. He is a member of the ACM, SIGSOFT, IEEE, and the IEEE Computer Society.</p>\\r\\n\\r\\n<p><a href=\"http://task3.cc\" rel=\"nofollow\">More info</a>.</p>\\r\\n',\n",
       " '<p>Contributing Editor and Columnist, SD Times. Editor of now-defunct magazines including Software Development, Computer Language, and AI Expert. Founding Editor of Game Developer, which still totally rocks.</p>\\r\\n\\r\\n<p>On Twitter at @lobrien</p>\\r\\n\\r\\n<p>Sr. Software Engineer at Gemini Observatories.</p>\\r\\n\\r\\n<p>Not a bad coder.</p>\\r\\n',\n",
       " '<p>you can trust</p>\\r\\n',\n",
       " '<p>a learner and teacher of statistics</p>\\r\\n',\n",
       " '<p>Coder. </p>\\r\\n',\n",
       " '<p>My hair has gone grey because of stats!</p>\\r\\n',\n",
       " '<p>Ruben Baetens is an eager practitioner in the domain of architecture and system engineering, interested in the holistic assessment of projects in the built environment. After an abroad study and research stay in Norway, he graduated in the summer of 2009 becoming Master of Applied Sciences and  Engineering in Architecture (ir.-arch.) and started as PhD candidate at the K.U.Leuven.</p>\\r\\n',\n",
       " '<p>father, software engineer, surfer</p>\\r\\n',\n",
       " \"<p>I'm a PhD student in Statistics at UCL, supervised by Mark Girolami and Alex Beskos.  I'm working on improving Monte Carlo methods using ideas from Differential Geometry.</p>\\r\\n\",\n",
       " '<p>I am in my third year of studies in mathematics at University of Montreal. My current interests are number theory, analysis, measure theory and abstract algebra.</p>\\r\\n',\n",
       " '<p>I am a medical resident. My research area is in the field of biomedical optics.</p>\\r\\n',\n",
       " \"<p>I'm a PhD student in the joint Statistics and Public Policy program at Carnegie Mellon University.</p>\\r\\n\",\n",
       " '<p>computer science guy.\\r\\nfascinated by machine learning.\\r\\nso much to learn still...</p>\\r\\n',\n",
       " '<p>Computer science student &amp; enthusiast</p>\\r\\n',\n",
       " '<p>www.vanheusden.com</p>\\r\\n',\n",
       " \"<p>I'm here to learn more and more about Statistics</p>\\r\\n\",\n",
       " 'I am currently a student of Computer Science at the University of Waterloo. ',\n",
       " '<p>Currently studing at University of Genoa, Italy</p>\\r\\n',\n",
       " \"<p>I'm a PhD student in the stats department at Stanford.</p>\\r\\n\",\n",
       " '<p>Blog: <a href=\"http://www.parasdoshi.com\" rel=\"nofollow\">http://www.parasdoshi.com</a></p>\\r\\n\\r\\n<p>MSDN profile: <a href=\"http://social.msdn.microsoft.com/profile/paras%20doshi/?type=forum\" rel=\"nofollow\">http://social.msdn.microsoft.com/profile/paras%20doshi/?type=forum</a> (Microsoft Community Contributor 2011 Award Recipient)</p>\\r\\n',\n",
       " '<p>PhD Student in Laboratory Neurogenetics and Behavior</p>\\r\\n',\n",
       " '<ul>\\\\\\\\n  <li>Maths & Computing</li>\\\\\\\\n</ul>\\\\\\\\n\\\\\\\\n<ul>\\\\\\\\n  <li>C</li>\\\\\\\\n  <li>Java and JVMs</li>\\\\\\\\n  <li>Ruby</li>\\\\\\\\n</ul>\\\\\\\\n\\\\\\\\n',\n",
       " '<p>I am an applied economics student at Montana State University and user of the following code: R, LaTeX, TikZ, Python, and Fortran.  I also dabble in GIS with QGIS, PostgreSQL, and PostGIS.</p>\\r\\n',\n",
       " '<p>I am a soil ecologist and R user.</p>\\r\\n',\n",
       " '<p>Applied mathematician</p>\\r\\n',\n",
       " '<p>One script for all!</p>\\r\\n\\r\\n<p><a href=\"http://stackexchange.com/users/49384\">\\r\\n<img src=\"http://stackexchange.com/users/flair/49384.png\" width=\"208\" height=\"58\" alt=\"profile for Barun on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Barun on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\\r\\n</a></p>\\r\\n',\n",
       " '<p>Work in analytical CRM / database marketing.</p>\\r\\n',\n",
       " 'I am an artist',\n",
       " '<p>I am a PhD student in Social and Decision Sciences at Carnegie Mellon University</p>\\r\\n',\n",
       " '<p>Its me :), \\r\\nMohamed cherif DANI, 25yo, technologies fan :) , ;) </p>\\r\\n',\n",
       " '<p>Web developer and Ph.D. candidate</p>\\r\\n',\n",
       " '<p>Doctoral Student, CISE, University of Florida. </p>\\r\\n',\n",
       " \"<p>I'm a political science graduate student at the University of Minnesota. My interests are American politics broadly, statistical methodology and computational methods for text extraction from other medias.</p>\\r\\n\",\n",
       " \"<p>I'm doing a PhD in Computer Science, University College London.</p>\\r\\n\",\n",
       " '<p>Contract developer, worked on everything from MIPS assembly to financial J2EE systems. </p>\\r\\n\\r\\n<p>I wish to go work on Mars. Or Australia.</p>\\r\\n',\n",
       " '<p>merge keep An architect of AB 32, the California Global Warming Solutions Act, first legislation capping greenhouse gas emissions in the U.S.</p>\\r\\n\\r\\n<p>Co-author (with Nia Robinson) of \"A Climate of Change: African Americans, Global Warming, and a Just Climate Policy for the US\"</p>\\r\\n\\r\\n<p>Co-designer (with Jim Barrett) of the first comprehensive climate plan for the U.S. to be endorsed by major U.S. labor unions</p>\\r\\n\\r\\n<p>Editor of the Natural Resources Tax Review</p>\\r\\n\\r\\n<p>Developer of a partial disability tolling theory for people sexually abused as children bringing actions against their abusers as adults, since adopted by courts in five states.</p>\\r\\n\\r\\n<p>Currently engaged in a career transition, recasting myself as an expert on the causes and consequences of income inequality in the United States.</p>\\r\\n',\n",
       " '<p>Health care analytics / data science: SAS for back-end data analytics; javascript, python, django for data visualization &amp; web applications; vba for little excel apps; sql for back-end data pipeworks.   </p>\\r\\n\\r\\n<p>My own projects &amp; development work has also involved a lot of C++, C, Objective-C / Cocoa.</p>\\r\\n\\r\\n<p>I completed the first two actuarial exams (Probability and Financial Mathematics). Also had done all the studying for the Construction of Actuarial Models exam when I decided to focus on computer science instead. </p>\\r\\n',\n",
       " '<p>Plain and simple, math is awesome!</p>\\r\\n',\n",
       " '<p>R enthusiast, ecological statistics, mainly self-taught, still (always...) learning.</p>\\r\\n',\n",
       " '<p>Student at Texas A &amp; M Univeristy</p>\\r\\n',\n",
       " 'Neuroscience PhD candidate.',\n",
       " 'PHP freelancer',\n",
       " '<p>healthIT\\r\\nprotein folding</p>\\r\\n',\n",
       " '<p>Data scientist at CQuotient</p>\\r\\n',\n",
       " '<p>see blog</p>\\r\\n',\n",
       " '<p><a href=\"http://www.mendeley.com/profiles/kay-cichini/\" rel=\"nofollow\">My Profile at Mendeley</a></p>\\r\\n',\n",
       " '<p>I work as a senior software engineer in a small, research oriented company. We program mostly in C#. </p>\\r\\n\\r\\n<p>Interest areas: object oriented design patterns, functional programming, data structures and algorithms. </p>\\r\\n',\n",
       " '<p>PhD student in econometric theory at UCSD</p>\\r\\n',\n",
       " '<p><strong><a href=\"http://blog.stackoverflow.com/2013/04/get-to-know-the-new-stack-employees/\">Stack Overflow Valued Associate</a></strong></p>\\r\\n\\r\\n<ul>\\r\\n<li>Contributor since September 15<sup>th</sup> 2008</li>\\r\\n<li>Skeptics Moderator from February 2011 to March 2013</li>\\r\\n<li>Core dev since March 2013</li>\\r\\n</ul>\\r\\n\\r\\n<p>You can find me on</p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://sklivvz.com\" rel=\"nofollow\">Personal site</a> (served out of my home broadband)</li>\\r\\n<li>Twitter <a href=\"http://twitter.com/sklivvz\" rel=\"nofollow\">@sklivvz</a></li>\\r\\n<li><a href=\"http://careers.stackoverflow.com/marco-cecconi\">Careers 2.0</a></li>\\r\\n</ul>\\r\\n\\r\\n<p>Some code of mine, mostly old :-)</p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"https://code.google.com/p/sixpack-library/\" rel=\"nofollow\">https://code.google.com/p/sixpack-library/</a></li>\\r\\n<li><a href=\"https://github.com/sklivvz\" rel=\"nofollow\">https://github.com/sklivvz</a></li>\\r\\n<li><a href=\"https://bitbucket.org/sklivvz\" rel=\"nofollow\">https://bitbucket.org/sklivvz</a></li>\\r\\n</ul>\\r\\n',\n",
       " '<p>I like programming, machine learning, statistics, all kinds of problem solving, and I play chess.</p>\\r\\n\\r\\n<p><a href=\"https://github.com/Akavall?tab=repositories\" rel=\"nofollow\">My github</a></p>\\r\\n',\n",
       " \"<p>I am a mathematics enthusiast, although I have no formal training. What I know is what I've been able to glean from searching Wikipedia.</p>\\r\\n\\r\\n<p>I enjoy writing computer programs, although I have never been employed as a programmer. I've used two-dozen different programming languages, but my current favourite is Haskell.</p>\\r\\n\\r\\n<p>I also have an interest in various areas of science.</p>\\r\\n\\r\\n<p>On top of all that, I'm also a bad musician, a mediocre dancer, and a very bad juggler. :-P</p>\\r\\n\",\n",
       " \"<p>data_stuff &lt;- paste('data', c('scientist', 'munger', 'learner', 'geek')</p>\\r\\n\\r\\n<p>r_stuff &lt;- paste('R', c('enthusiast', 'community builder')</p>\\r\\n\",\n",
       " \"<h1>Developer</h1>\\r\\n\\r\\n<pre>def Lover(Programming):\\r\\n    return ['Love']*len(Programming)\\r\\n\\r\\nprint Lover(['Python','Fortran','VB','Matlab',[]])</pre>\\r\\n\",\n",
       " '<p>building computing systems, tools, methods in bioinformatics and healthcare</p>\\r\\n',\n",
       " '<p><a href=\"http://openlibrary.org/books/OL20544334M/De_cuando_en_cuando_Saturnina\" rel=\"nofollow\"><em>«Nosotros no estudiamos historia, sólo programación y tejidos andinos.»</em></a></p>\\r\\n\\r\\n<p>I sometimes write language learning software.</p>\\r\\n\\r\\n<p>I used to make <a href=\"http://ianmackinnon.co.uk/films\" rel=\"nofollow\">films</a>.</p>\\r\\n\\r\\n<p>I like to <a href=\"http://www.ravelry.com/people/ianmackinnon\" rel=\"nofollow\">knit</a>.</p>\\r\\n',\n",
       " 'Web dev',\n",
       " \"<p>I'm a Post-Doctoral Associate in a plant evolutionary ecology and evolutionary genetics lab.\\r\\nI love kite flying, percussion World music (tabla), staying fit (swimming, running and kettlebells), and I think cats>dogs</p>\\r\\n\",\n",
       " '<p>Just another algorithm geek.</p>\\r\\n',\n",
       " '<p>I study psychology and work in a research group. My main interests are evolution, individual differences, personality development and intelligence.</p>\\r\\n\\r\\n<p>I developed an open-source <a href=\"https://formr.org\" rel=\"nofollow\">survey framework called formr</a> as a summer side gig.</p>\\r\\n',\n",
       " '<p>Hi! I work for Google as a software engineer in their Chicago office. I recently graduated from Northwestern University.</p>\\r\\n',\n",
       " '<p>Professional CMS and TYPO3 developer. Feel free to write me an <a href=\"http://www.google.com/recaptcha/mailhide/d?k=01t1kGaxWdHhM2HBITlzL1WA==&amp;c=m8H_TE-gIy28sjV9rrCiaBhZk2iT0yRncIbFTMLrxIk=\" rel=\"nofollow\">e-mail</a>.</p>\\r\\n\\r\\n<p><a href=\"http://www.phpclasses.org/browse/author/441663.html\" rel=\"nofollow\">phpclasses</a> \\r\\n <a href=\"http://sourceforge.net/u/nano-math/profile/\" rel=\"nofollow\">Sourceforge</a>\\r\\n <a href=\"http://www.phpdevpad.de/\" rel=\"nofollow\">Phpdevpad</a>\\r\\n <a href=\"http://www.phpdevpad.de/geofence\" rel=\"nofollow\">Geofence</a><br>\\r\\n <a href=\"http://www.ozone3d.net/benchmarks/furmark_192_score.php?id=119383\" rel=\"nofollow\">Furmark-ID</a>\\r\\n <a href=\"http://www.3dmark.com/3dm11/6357413\" rel=\"nofollow\">3DMark</a>\\r\\n <a href=\"http://www.3dmark.com/3dmv/4654305\" rel=\"nofollow\">3DMark Vantage</a>\\r\\n <a href=\"http://www.3dmark.com/3dmv/4813519\" rel=\"nofollow\">3DMark Vantage 2</a></p>\\r\\n',\n",
       " '<p>working as a Professor and Dean, Faculty of Commerce and Management</p>\\r\\n',\n",
       " '<p>A student in physics doing scientific computing.</p>\\r\\n',\n",
       " '<p>Interests: Politics, data analysis, ethics, programming, and psychology as it relates to personal optimization.</p>\\r\\n\\r\\n<p>Specifics: (inter)National politics, statistics and econometrics, the proper role of a state, some Python with a preference for functional programming and less Java, and behavioral economics.</p>\\r\\n\\r\\n<p>Goals: Learn new things, save the world, and spend time with interesting people.</p>\\r\\n\\r\\n<p>Quotes:\\r\\n\"There are no limits. There are only plateaus, and you must not stay there, you must go beyond them.\" --Bruce Lee \"Cui bono?\" --Cicero</p>\\r\\n',\n",
       " '<p><p><a href=\"http://twitter.com/ceptional\" rel=\"nofollow\">@ceptional</a></p><p><a href=\"http://au.linkedin.com/in/alexholcombe\" rel=\"nofollow\">linkedin</a></p></p>\\r\\n',\n",
       " '<p>Always learning.  Please let me know if you have any suggestions or advice.</p>\\r\\n',\n",
       " '<h1>Architect · Senior Developer · Scrum Master</h1>\\r\\n\\r\\n<ul>\\r\\n<li>Veteran developer working on all layers -- UI, Middleware, and Database/SQL</li>\\r\\n<li>Computer Science degree and strong language fundamentals.</li>\\r\\n<li>Expert and advocate for Agile software development.  </li>\\r\\n<li>Architect, Team Lead, formal trainer, and always a leader, an informal trainer, and a mentor.</li>\\r\\n</ul>\\r\\n',\n",
       " '<p>Developer working for Varyc, a process control consultancy firm.</p>\\r\\n',\n",
       " '<p>Economist and Professor at the Federal Institute of Education, Science and Technology Minas Gerais - Campus Formiga</p>\\r\\n',\n",
       " '<p>Ph.D student in Statistics and Math</p>\\r\\n',\n",
       " '<p>Contact:\\r\\n<a href=\"http://www.linkedin.com/pub/kristjan-laane/40/9b0/698\" rel=\"nofollow\">http://www.linkedin.com/pub/kristjan-laane/40/9b0/698</a></p>\\r\\n',\n",
       " \"<p>I am a biostatistician, in my 4th year of work (2011). I have worked largely on candidate gene studies, and am moving into large scale population epidemiology, as well as some clinical trial work in the area of pedatric diabetes.</p>\\r\\n\\r\\n<p>My blog is literally a dumping ground for 'notes to self' and code I plan to reuse. If even a single post gets found by someone in google and helps them then it will have been worth doing it as a blog rather than it's original form (.txt file).</p>\\r\\n\\r\\n<p>Always eager to learn and happy to help. Will happily chat, sports first stats second, I don't make the rules the alphamabet does.</p>\\r\\n\\r\\n<p>Matt</p>\\r\\n\",\n",
       " \"<p>Scientist, programmer, and a digger of all things tech nestled snugly in the Columbia River Gorge (i.e. Heaven). When not paying my mortgage, I'm building Hydrasi, an environmental data company that is trying its best to drag the environmental sciences kicking and screaming into the 21st century.</p>\\r\\n\",\n",
       " '<p>My name is Chew Kai Feng, currently working in a project, called <a href=\"http://cloudst.at\" rel=\"nofollow\">CloudStat</a>. CloudStat is a web-based statistical platform that leverage R Language to allow researchers to analyze data collaboratively in the cloud with high performance infrastructure. </p>\\r\\n',\n",
       " '<p>Consultant (environmental and spatial stats a specialty), expert witness, and teacher.  I can be reached through (outdated but still valid) links posted <a href=\"http://www.quantdec.com/quals/quals.htm\" rel=\"nofollow\">on my web site</a>.</p>\\r\\n\\r\\n<p>Twitter: @WilliamAHuber  // ASA-P website: <a href=\"http://amstatphilly.org/\" rel=\"nofollow\">http://amstatphilly.org/</a></p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<blockquote>\\r\\n  <p>Why waste time learning, when ignorance is instantaneous?</p>\\r\\n</blockquote>\\r\\n\\r\\n<p><em>--T(iger) Hobbes.</em></p>\\r\\n\\r\\n<blockquote>\\r\\n  <p>For any complex problem there is a simple solution.  And it\\'s always wrong.</p>\\r\\n</blockquote>\\r\\n\\r\\n<p>--[Mis?]attributed to H.L. Mencken by Dava Sobel, <em>Longitude</em>.</p>\\r\\n',\n",
       " '<p>I am statistician developer interested in R, Python, behavioral finance, and conveying complex statistical models through visualization. </p>\\r\\n\\r\\n<p><a href=\"http://kldavenport.com\" rel=\"nofollow\">http://kldavenport.com</a></p>\\r\\n',\n",
       " \"I'm a web developer/designer in Dallas, TX. I primarily work with PHP on the Symfony framework, along with MySQL, jQuery, Objective-C (for iPhone development), and of course XHTML/CSS.\",\n",
       " '<p><strong>Me</strong></p>\\r\\n\\r\\n<p>Web developer.</p>\\r\\n\\r\\n<p><strong>Website</strong></p>\\r\\n\\r\\n<p><a href=\"http://www.tomgullen.co.uk/\" rel=\"nofollow\">http://www.tomgullen.co.uk/</a></p>\\r\\n\\r\\n<p><strong>Contact Information</strong></p>\\r\\n\\r\\n<p>Please DON\\'T contact me for help regarding any answers I provided!  If it\\'s suitable for StackOverflow, please post it on StackOverflow.  For everything else I can be contacted on tomsotherone <strong>[insert the at sign here]</strong> googlemail <strong>()</strong> com</p>\\r\\n',\n",
       " '<p>I have a PhD in economics. I mostly use Python, Maple, Matlab, Stata, and more recently R.</p>\\r\\n',\n",
       " '<p><img src=\"http://i.imgur.com/xiIeR.jpg\" alt=\"C++ DaVinci Code\"></p>\\r\\n',\n",
       " '<p>I\\'m a <a href=\"http://www.johndcook.com/veryappliedmath.html\" rel=\"nofollow\">very applied mathematician</a> and software developer. </p>\\\\\\\\n\\\\\\\\n<p>I blog at <a href=\"http://www.johndcook.com/blog\" rel=\"nofollow\">The Endeavour</a> and I\\'m <a href=\"http://twitter.com/johndcook\" rel=\"nofollow\">@JohnDCook</a> on Twitter.</p> \\\\\\\\n\\\\\\\\nOther <a href=\"http://www.johndcook.com/contact.html\" rel=\"nofollow\">contact info</a>.',\n",
       " '<p>Each day I learn something interesting on this site. Most days I learn how little I know.</p>\\r\\n',\n",
       " \"David's dayjob consists of working as a consultant for an IT company in Geneva. After dark he works on his main interests, which include home and building automation.\",\n",
       " '<p>I am a PhD student in enegineering department and I am trying to improve my mathematical skills. I am always grateful to any of your comments.</p>\\r\\n',\n",
       " '<p>Discrete optimization.  Computational geometry.  Combinatorics.  Probabilistic analysis.  Bioinformatics.  Approximation algorithms.</p>\\r\\n',\n",
       " '<p>Pursuing my PhD in statistics.</p>\\r\\n',\n",
       " '<p>Geospatial Geek, Ubuntu and Android user who admires the power of Free and Open Source Software!</p>\\r\\n\\r\\n<p>Follow me on Twitter <a href=\"http://twitter.com/gischethans\" rel=\"nofollow\">@gischethans</a></p>\\r\\n',\n",
       " '<p>I like math.</p>\\r\\n',\n",
       " '<p>Just a slow learner. \\r\\nThanks for your enlightenment and patience!</p>\\r\\n',\n",
       " '<p>Currently an economics and applied math (statistics) undergraduate student. Hope to pursue a PhD in economics some day.</p>\\r\\n',\n",
       " ' - I love Algorithms, Programming, AI and machine learning\\\\\\\\n<br/>\\\\\\\\n - In my spare time, I like playing real time strategy games, chess, watching movies.',\n",
       " '<p>I am a PhD Student at INRIA Rennes Bretagne Atlantique research center,  France. My PhD topic is on privacy preservation in user-centric peer to peer search environments. The field includes cryptography, differential privacy, data mining, secure multi-party computation and large scale peer to peer systems. I am also a staff member at the Computer Science department, Helwan University, Egypt. I currently hold a masters degree in Computer Science from University Rennes I, France.</p>\\r\\n\\r\\n<p>My interests include computational complexity and computability theory, linguistics and natural language processing, cryptographic protocols and quantum secure multi-party computation</p>\\r\\n',\n",
       " '<ul>\\r\\n<li>Ruby </li>\\r\\n<li>jruby</li>\\r\\n<li>Ruby on Rails</li>\\r\\n<li>Celerity</li>\\r\\n</ul>\\r\\n',\n",
       " '<p>As an analyst and statistical consultant, Joshua focuses on biostatistics, and is interested in reproducible research and graphical displays of data and statistical models. Through his work at <a href=\"http://elkhartgroup.com\" rel=\"nofollow\">Elkhart Group Ltd.</a> and former work at UCLA Statistical Consulting Group, he has supported a wide array of clients ranging from graduate students, to experienced researchers, and biotechnology companies.</p>\\r\\n',\n",
       " '<p>I do research in behavioural change.</p>\\r\\n',\n",
       " '<p>@Test\\r\\n    public void me(){\\r\\n        assertEquals(me,PEOPLE.Programmer);\\r\\n    }</p>\\r\\n',\n",
       " '<p>PhD student at Antwerp University, Belgium. General FOSS-enthusiast, esp. R, Python &amp; GNU/Linux.</p>\\r\\n',\n",
       " \"<p>I'm a chemist researcher, working now for a french company making high-performance materials.</p>\\r\\n\",\n",
       " \"<p>I'm towards the end of a Neuroscience PhD, and occasionally enjoy helping other people make sense of <em>their</em> data instead of wondering what's going on in my own. </p>\\r\\n\\r\\n<p>My current research focuses on how the brain represents visual information and how those representations can be altered by behavioral demands. Before I got interested in brains, I was trained as a computational linguist.</p>\\r\\n\\r\\n<p>If you've got an interesting project involving biological data, pretty pictures, or lots of words, send me a message!</p>\\r\\n\",\n",
       " '<p>I am a student of psychology located in Heidelberg, Germany. I need statistics for psychological studies only on a very superficial level but I am very interested in the workings underneath. Unfortunately I struggle with mathmatical syntax a lot, which is why I will probably keep asking very basic questions for a while. Thank you for your patience.</p>\\r\\n',\n",
       " '<p>Data Scientist at <a href=\"http://frogtek.org\" rel=\"nofollow\">Frogtek</a></p>\\r\\n\\r\\n<p>Statistician/Data Scientist. Máster en Estadística Aplicada en la U. Granada. Diplomado en Estadística en la U. Zaragoza <a href=\"http://es.linkedin.com/in/calejero\" rel=\"nofollow\">http://es.linkedin.com/in/calejero</a></p>\\r\\n\\r\\n<p>I am a statistician, specializing in the application of data mining and multivariate analysis methods to various explanatory and predictive problems. I have produced models, reports, and visualizations for many purposes, including sovereign debt, identity resolution, reputation scoring and environmental analysis.</p>\\r\\n',\n",
       " \"<p>I'm a statistician and applied economist.\\r\\nI'm actually doing my PhD in climate change economics in West Africa.</p>\\r\\n\",\n",
       " '<p><a href=\"http://twitter.com/discob0t\" rel=\"nofollow\">@discob0t</a></p>\\r\\n',\n",
       " '<p>Educational background in innovation management. All theory is nice, but I need tools to do innovation, which became computers. My main focus is on manipulating data so we can create innovations on the basis of those new insights. </p>\\r\\n',\n",
       " '<p>Yet another physicist pretending to be a statistician. Mortgages, housing, economic forecasts, and trading strategies, oh my.</p>\\r\\n',\n",
       " '<p>Urban Planner / Population Demographer / GIS Analyst by training.</p>\\r\\n\\r\\n<p>Data Mining / Sales Statistics by trade.</p>\\r\\n\\r\\n<p>Loves anything to do with science, logic, math, and skepticism; and ALWAYS ready to learn somethign new.</p>\\r\\n',\n",
       " \"<p>I'm a software developer (currently focused on Python), living in Australia (currently focused on Sydney).</p>\\r\\n\\r\\n<p>I am an on-again/off-again moderator of Skeptics.SE. (I was Pro Tem Moderator, I handed in my diamond when the first elections were held, and then ran in the second elections about a year later.)</p>\\r\\n\",\n",
       " '<p>vegetation ecologist and remote sensing scientist from Hamburg, Germany</p>\\r\\n',\n",
       " '<p>Environmental scientist working for an engineering consultancy, with interests in GIS and programming.</p>\\r\\n',\n",
       " '<p>Algorithmic Artist</p>\\r\\n',\n",
       " '<p>BSc Statistics (Hons), First Class - 2007</p>\\r\\n\\r\\n<p>MSc Biostatistics &amp; Epidemiology - 2009</p>\\r\\n\\r\\n<p>I currently work for a Canadian university as a biostatistical consultant within the faculties of medicine and nursing. I very much enjoy all thinigs related to epidemiology and statistics (except power calculations).</p>\\r\\n',\n",
       " '<p><img src=\"http://i.stack.imgur.com/STOQ5.png\" alt=\"contact details\"></p>\\r\\n\\r\\n<p><a href=\"http://mathematica.stackexchange.com/\"><img src=\"http://i.stack.imgur.com/niiV3.png\" alt=\"Visit Mathematica.SE\"></a></p>\\r\\n',\n",
       " '<p>I work with <a href=\"http://www.wikisquare.de\" rel=\"nofollow\">wikisquare.de</a> and developed <a href=\"http://en.geopublishing.org\" rel=\"nofollow\">Geopublisher and AtlasStyler SLD Editor</a>. Recently I do DBA and GIS for <a href=\"http://www.empirica-systeme.de/Karten\" rel=\"nofollow\">www.empirica-systeme.de</a></p>\\r\\n',\n",
       " 'Ramnath Vaidyanathan is an Assistant Professor of Operations Management at the Desautels Faculty of Management, McGill University. He got his PhD from the Wharton School and worked at McKinsey & Co prior to that.',\n",
       " '<p>finance PhD student</p>\\r\\n',\n",
       " '<p><a href=\"http://www.linkedin.com/in/gaborborgulya\" rel=\"nofollow\">http://www.linkedin.com/in/gaborborgulya</a></p>\\r\\n\\r\\n<p>I welcome consulting requests.</p>\\r\\n',\n",
       " '<p>Mechanical Engineer currently trying to tap into the secrets of time-series analysis for business forecasting and S&amp;OP.</p>\\r\\n',\n",
       " '<p>I need probability and statistics every single day but no one around to help!</p>\\r\\n',\n",
       " '<p>Just ordinary imperfect man, trying to beat higher level everyday. </p>\\r\\n',\n",
       " '<p>I am a biology PhD student, undertaking research about Stem cells in Drosophila and their relationship with the somatic niche. I am focusing on Notch signalling pathway.</p>\\r\\n',\n",
       " '<p>I am currently a student for software engineering, in the Technion - Israel Institute of Technology. I am interested in automata, graph algorithms, data structures, theory of computation, artificial intelligence and OOP languages, especially Java.</p>\\r\\n\\r\\n<p>I am not a native English speaker and encourage everyone to fix any language mistakes I make.</p>\\r\\n\\r\\n<p>P.S. The puppy is my Golden-Retriever named Louis, and yes - I had to hold a candy in order to take this picture :)</p>\\r\\n',\n",
       " '<p>I teach honours statistics in a psychology department primarily using R and simulation.</p>\\r\\n',\n",
       " '<p>Data Scientist at BlackLocus in Austin, TX, USA.</p>\\r\\n',\n",
       " \"<p>Moved to ubuntu a while ago, and haven't regretted it! Its fantastic and goes along well with me wanting to help others when they in trouble :)</p>\\r\\n\\r\\n<p>Profession wise, mechanical engineer and i do my day to day work in ubuntu :) got most of the software i need!</p>\\r\\n\",\n",
       " '<p>MSc. in Political Economy Student, at BI Norwegian Business School.</p>\\r\\n',\n",
       " '<p>Graduated from Drexel with a major in economics and a minor in math. Actively pursue a career in data science.</p>\\r\\n',\n",
       " '<p>I use R.</p>\\r\\n',\n",
       " '<p>Statistics, Ph.D., UCLA</p>\\r\\n',\n",
       " '<p>Self-made sysadmin, specialized on linux systems and trying to grow my own business. Also Python coder out of passion and necessity, learning my way through OOP and frameworks and whatnot.</p>\\r\\n',\n",
       " '<p>I mainly use the statistical programming language <code>R</code> to solve problems, typically of a spatial, biological, and geographical nature.  I have been using R since it was S-plus (back in the late 90s).  Despite this I have an unfortunate tendency to resort to the <code>for</code> loop, which is not exactly favoured in R practice, probably because I learned to program using Turbo Pascal version 5.5!</p>\\r\\n',\n",
       " '<p>I am an oceanographer who enjoys studying floating body hydrodynamics and surface waves.  I am also interested in the design of electrical generators.  My day-to-day work includes programming in Matlab and Simulink and statistical analysis of SQL databases. Am seeking to learn Simscape and improve Simulink skills. Thanks Stack Overflow for so many helpful answers to programming dilemmas!</p>\\r\\n',\n",
       " '<p>\"A brute-force solution that works is better than an elegant solution that doesn’t work.\" - Steve McConnell</p>\\r\\n',\n",
       " \"<p>Ask if you'd like to know more, I don't bite</p>\\r\\n\",\n",
       " '<p>PhD candidate in <a href=\"http://www.med.upenn.edu/gcb/program.shtml\" rel=\"nofollow\">computational biology</a> at the University of Pennsylvania working on efficient approaches to demographic inference with <a href=\"http://topicpages.ploscompbiol.org/wiki/Approximate_Bayesian_computation\" rel=\"nofollow\">Approximate Bayesian Computation</a> using genome-wide single nucleotide polymorphism data. </p>\\r\\n\\r\\n<p>The smorgasbord of topics that capture my interest include <a href=\"http://p4mi.org/p4-medicine\" rel=\"nofollow\">personalized medicine</a>, model-based approaches to machine learning, and <a href=\"http://cran.r-project.org/\" rel=\"nofollow\">R</a> and the miraculously wonderful <a href=\"http://dirk.eddelbuettel.com/code/rcpp.html\" rel=\"nofollow\">Rcpp project</a>.</p>\\r\\n',\n",
       " '<p>PhD Candidate. Doing research in natural language processing, data mining and machine learning.</p>\\r\\n',\n",
       " '<p>Exploring Data to improve business</p>\\r\\n',\n",
       " '<p><a href=\"http://Aurametrix.com\" rel=\"nofollow\">Aurametrix</a> is designed for people who experience food sensitivities, allergies, metabolic disorders and those who want to improve their diet and exercise to maximize their health.</p>\\r\\n\\r\\n<p>Sites:</p>\\r\\n\\r\\n<p><li><a href=\"http://youtube.com/aurametrix\" rel=\"nofollow\">Aurametrix Youtube Channel</a></li>\\r\\n<li><a href=\"http://facebook.com/aurametrix\" rel=\"nofollow\">Aurametrix Facebook page</a></li>\\r\\n<li><a href=\"http://twitter.com/aurametrix\" rel=\"nofollow\">Aurametrix Twitter channel</a></li>\\r\\n<li><a href=\"https://plus.google.com/108995684691677708442/posts\" rel=\"nofollow\">Aurametrix G+ page</a></li>\\r\\n<li><a href=\"http://aurametrix.blogspot.com\" rel=\"nofollow\">Health Technologies Blog</a></li>\\r\\n<li><a href=\"http://ibs.aurametrix.com\" rel=\"nofollow\">IBS Blog</a></li>\\r\\n<li><a href=\"http://olfactics.aurametrix.com\" rel=\"nofollow\">Olfactics Blog</a></li>\\r\\n<li><a href=\"http://environment.aurametrix.com\" rel=\"nofollow\">Environmental Health Blog</a></li></p>\\r\\n',\n",
       " '<p><em>\"The three chief virtues of a programmer are: Laziness, Impatience and Hubris.\"</em> - Larry Wall</p>\\r\\n\\r\\n<p><strong>Laziness</strong>: I\\'m too lazy to do the same task repeatedly so write scripts to do that task for me. This makes people think I am intelligent.</p>\\r\\n\\r\\n<p><strong>Impatience</strong>: I\\'m too impatient to wait for my code to run so rewrite the code to improve performance. This makes people think I am a good programmer.</p>\\r\\n\\r\\n<p><strong>Hubris</strong>: When someone asks if I can do something I just say Yes, then go find out how to do it (Google!). This makes people think I can do anything.</p>\\r\\n\\r\\n<p>Ultimately, it means I can make a career out of being Lazy, Impatient, and Hubristic(?).</p>\\r\\n',\n",
       " '<p>I\\'m a data hacker!\\r\\n<a href=\"http://www.kaggle.com/users/6696/zach\" rel=\"nofollow\">http://www.kaggle.com/users/6696/zach</a></p>\\r\\n<p><a href=\"https://plus.google.com/108375494102265580837/abou\">google</a></p><p><a href=\"http://www.linkedin.com/in/zacharymayer\">linkedin</a></p>',\n",
       " '<p>I\\'m a policy analyst and strategic researcher.  I use Excel and Access for the simple stuff and SPSS for the \"real\" stat analysis.  My job is a bit like a combination of private investigator, statistician, reference librarian and policy wonk rolled into one.</p>\\r\\n',\n",
       " '<p>Interested in Mobile Dev, Cloud Computing, Data mining, Learning Machines and Neural Network.\\r\\nInterested in Business Intelligence and Knowledge Management Processes.\\r\\nStill Student.</p>\\r\\n',\n",
       " '<p>I am a software engineer from Télécom ParisTech, working in mobile/web apps and image processing. I speak Python, C, JavaScript and Scala, spiced up with a little bit of Vim for good measure!</p>\\r\\n',\n",
       " '<p>I am a PhD student in social psychology at the University of Colorado Boulder.</p>\\r\\n',\n",
       " '<p>Living somewhere, sometime, doing something.</p>\\r\\n',\n",
       " '<p>Master student in computational Neuroscience...</p>\\r\\n',\n",
       " \"<p>Anti-matlabist, pro-python grad student at Caltech. I'm here on SE to learn useful bits of programming and workflow design (e.g. version control) that can be applied to science, and also to help others when able.  </p>\\r\\n\\r\\n<p>I dabble in Python, R, latex/bibtex/tikz, elisp and scheme, and matlab (when forced).</p>\\r\\n\\r\\n<p>If you work in an academic setting, please help educate people about free software alternatives to the bloated behemoth that is MATLAB.</p>\\r\\n\",\n",
       " '<p>I design stuff for Stack Exchange. Also a professional bacon eater.</p>\\r\\n\\r\\n<p>blog: <a href=\"http://www.8164.org\" rel=\"nofollow\">8164.org</a></p>\\r\\n\\r\\n<p>twitter: <a href=\"http://twitter.com/jzy\" rel=\"nofollow\">@jzy</a></p>\\r\\n',\n",
       " '<p>Physicist, Photonics Engineer, cellist, R-evangelist, pinball fanatic, xkcd follower. And owner of \"Hells Doggies,\"  2 wonderful Papillons and one incredibly sweet motley mutt.</p>\\r\\n\\r\\n<p>New!  <a href=\"https://github.com/cellocgw\" rel=\"nofollow\">https://github.com/cellocgw</a>  for various R- code tools and toys.</p>\\r\\n',\n",
       " '<p>Working on combined BS/MS in computer science.</p>\\r\\n',\n",
       " '<p>I work at Quantopian, <a href=\"http://www.quantopian.com\" rel=\"nofollow\">www.quantopian.com</a>.  Quantopian is a community and a backtester for stock trading algorithms written in Python.</p>\\r\\n',\n",
       " \"<p>I'm an analyst with an interest in public health and civic data hackery.</p>\\r\\n\",\n",
       " '<p>Machine Learner!</p>\\r\\n',\n",
       " '<p>I am not a statistician but I am interested in learning Statistics.</p>\\r\\n',\n",
       " '<p>Undergraduate student of Industrial Engineering at IIT Kharagpur, India</p>\\r\\n',\n",
       " \"<p>I'm a student / programmer studying computer science at the University of Witwatersrand.\\r\\nI'm mostly interested in functional programming language, machine learning, and robotics.</p>\\r\\n\",\n",
       " '<p>PhD student in machine learning, reinfofcement learning in particular.</p>\\r\\n',\n",
       " \"<p>Passionate about programming, computer science, data-mining and new technologies in general. Always eager to learn, and happy to teach what I know.</p>\\r\\n\\r\\n<p>Software engineer with a background in CS and AI, I'm currently working in the big data space with technologies such as Hadoop. I've joined the StackExchange network in an effort to improve my knowledge and also help others get the answers they're looking for.</p>\\r\\n\",\n",
       " '<p>Former mathematics and statistics modeller in the biological and medicine sciences who currently works with computing engineering, but also have a dream to start working with football (soccer) analysis.</p>\\r\\n',\n",
       " '<p>Software Engineer at your service.</p>\\r\\n',\n",
       " '<p>grad student.\\r\\ninterested in arcgis, python, R and LaTeX.</p>\\r\\n',\n",
       " '<p><a href=\"http://twitter.com/omitevski\" rel=\"nofollow\">http://twitter.com/omitevski</a></p>\\r\\n',\n",
       " '<p>ผู้สอน (Instructor of) <a href=\"http://bit.ly/WannikAcademy\" rel=\"nofollow\">Wannik Academy</a></p>\\r\\n\\r\\n<p>ผู้สร้าง (Creator of) <a href=\"http://worasait.com/font.aspx\" rel=\"nofollow\">Worasait font</a></p>\\r\\n\\r\\n<p>ผู้เขียนหนังสือ (Author of) <a href=\"http://rads.stackoverflow.com/amzn/click/B004TYZ0N0\" rel=\"nofollow\">Java Keywords</a>, <a href=\"http://worasait.com/java.aspx\" rel=\"nofollow\">เขียนโปรแกรมจาวาเบื้องต้น (Introduction to Java)</a>, <a href=\"http://www.wannik.com/netbeans/2.0/\" rel=\"nofollow\">Java GUI using NetBeans</a>, <a href=\"http://www.wannik.com/jsp\" rel=\"nofollow\">JSP สำหรับงาน E-Commerce</a>, <a href=\"http://www.wannik.com/inw/\" rel=\"nofollow\">คู่มือเทพคอม (Inw Com\\'s Manual)</a>, <a href=\"http://wannik.com\" rel=\"nofollow\">ฯลฯ (etc.)</a></p>\\r\\n',\n",
       " '<p>Bob Webster from Pryor, OK</p>\\r\\n',\n",
       " '<p>\"I have the only working phaser ever built. It was fired only once to keep William Shatner from making another album.\\'\\'</p>\\r\\n',\n",
       " '<p>software engineer at it-motive AG based in Duisburg, Germany</p>\\r\\n\\r\\n<p>Mainly active in the Java/JEE world but doing some web development as well.</p>\\r\\n',\n",
       " 'Lecturer, programmer, data miner, machine learning',\n",
       " '<p>Data scientist in training at the USF MS in Analytics program.</p>\\r\\n',\n",
       " '<p>Bioinformatician</p>\\r\\n',\n",
       " '<p>aattard82 |at| gmail.com</p>\\r\\n',\n",
       " '<p><strong>Occupation:</strong> Scientist (Psychologist, with an affinity to technology), working in mobile media, critical thinking, and reflection</p>\\r\\n\\r\\n<p><strong>Interests:</strong> Writing, creating books, photography, work methods</p>\\r\\n',\n",
       " '<p>Learning Java....  :)</p>\\r\\n',\n",
       " '<p>I\\'m a reluctant SO user, truth be known. I\\'ve had 30 years programming experience, the last 20 of which have been almost exclusively C++. I prefer to spend my idle time anywhere than on internet forums, so I only come here when I have unusual or perplexing questions. But that leads to a low reputation, and perhaps people assume that means I\\'m a novice, or don\\'t understand the basics. The reality is I usually find myself asking questions from a viewpoint of \"this doesn\\'t make sense - something is missing - there is this factor and that factor and another factor... and yet those are the trees, and I can\\'t see the forest for them.\" And then I roll my eyes in despair when people give me answers about trees, and wonder whether SO is worth the pain of trying to pose questions about forests.</p>\\r\\n',\n",
       " '<p>I\\'m an astrophysicist working with the <a href=\"http://en.wikipedia.org/wiki/Pan-STARRS\" rel=\"nofollow\">PS1</a>, <a href=\"http://en.wikipedia.org/wiki/Gaia_%28spacecraft%29\" rel=\"nofollow\">GAIA</a>, and <a href=\"http://en.wikipedia.org/wiki/LAMOST\" rel=\"nofollow\">LAMOST</a> projects.</p>\\r\\n',\n",
       " '<p>Computational Biology / Biomedical Informatics </p>\\r\\n',\n",
       " '<p>I am an amateur statistics enthusiast: I enjoy using statistical tools &amp; thinking in attempting to investigate various questions, self-experiments, or data-sets. Examples of my efforts include:</p>\\r\\n\\r\\n<ul>\\r\\n<li><p><a href=\"http://www.gwern.net/tags/statistics\" rel=\"nofollow\">http://www.gwern.net/tags/statistics</a></p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://www.gwern.net/Google%20shutdowns\" rel=\"nofollow\">http://www.gwern.net/Google%20shutdowns</a></li>\\r\\n<li><a href=\"http://www.gwern.net/2012%20election%20predictions\" rel=\"nofollow\">http://www.gwern.net/2012%20election%20predictions</a></li>\\r\\n<li><a href=\"http://www.gwern.net/Death%20Note%20script\" rel=\"nofollow\">http://www.gwern.net/Death%20Note%20script</a></li>\\r\\n<li><a href=\"http://www.gwern.net/Nootropics\" rel=\"nofollow\">http://www.gwern.net/Nootropics</a></li>\\r\\n<li><a href=\"http://www.gwern.net/Zeo\" rel=\"nofollow\">http://www.gwern.net/Zeo</a></li>\\r\\n<li><a href=\"http://www.gwern.net/hpmor\" rel=\"nofollow\">http://www.gwern.net/hpmor</a></li>\\r\\n</ul></li>\\r\\n</ul>\\r\\n',\n",
       " '<p>Teaching statistics to biology students and working as data analyst.</p>\\r\\n\\r\\n<p>My professional information on <a href=\"http://www.linkedin.com/pub/didzis-elferts/67/624/aa8\" rel=\"nofollow\">LinkedIn</a></p>\\r\\n',\n",
       " \"<p>I am the greatest programmer guy ever.\\r\\nOne time I put 6 semicolons at then end of my code.\\r\\nBecause I'm just that good.</p>\\r\\n\",\n",
       " '<p>merge keep</p>\\r\\n',\n",
       " \"<p>I'm working on my PhD in aerospace engineering, specializing in computational turbulent combustion. My focus is primarily on massively parallel algorithms and computational methods for solving fluid and structural mechanics problems. Primary work is done in Fortran (90, 95, and 2003) but recent work has me branching into python, C and C++. </p>\\r\\n\\r\\n<p>I'm also interested in international affairs and law. </p>\\r\\n\\r\\n<p>Also interested in applying computational techniques to sports, in particular cycling aerodynamics and performance optimization. Particular emphasis on track cycling and time trialing.</p>\\r\\n\",\n",
       " 'Freelance .NET Developer\\\\\\\\n<br>\\\\\\\\n<br>\\\\\\\\n<ul>\\\\\\\\n<li>\\\\\\\\nCurrently working at C Tech Development Corporation\\\\\\\\n</li>\\\\\\\\n<li>\\\\\\\\nUniversity teacher at ESNE for OOP (C#) and Graphics Programming (XNA)\\\\\\\\n</li>\\\\\\\\n</ul>\\\\\\\\nMicrosoft MVP XNA/DirectX (2008+)',\n",
       " '<p>I am a final year student @ Imperial College London.</p>\\r\\n',\n",
       " '<p><br>\\r\\n<br>\\r\\n<br>\\r\\n<br>\\r\\n<br>\\r\\n<br>\\r\\n<br>\\r\\n<br>\\r\\n<br>\\r\\n<br>\\r\\n<br>\\r\\n<br>\\r\\n<br>\\r\\n<br>\\r\\n<br>\\r\\n<br></p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p><sub>13138 273 49363 55265 52151</sub></p>\\r\\n',\n",
       " '<p><strong>Experience</strong></p>\\r\\n\\r\\n<p>I work as a quantitative researcher on the buy side, finding new sources of alpha and designing relative value models and trading strategies around them.  I\\'ve dealt with practically all the major asset classes.\\r\\n<br>\\r\\nPresently at <a href=\"http://www.lordabbett.com/\" rel=\"nofollow\">Lord Abbett</a>, one of the oldest asset management firms to still actively innovate and push the envelope.\\r\\n<br>\\r\\nPreviously at Parkcentral Capital Management, the hedge fund division of Perot Investments.</p>\\r\\n\\r\\n<p><strong>Education</strong></p>\\r\\n\\r\\n<p>PhD in Economics from Princeton University.\\r\\n<br>\\r\\nSB in Economics from MIT.</p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p>I live in New York City with my wife and daughter.</p>\\r\\n\\r\\n<p>To stalk me further, you can check out my <a href=\"http://www.linkedin.com/in/tfishman\" rel=\"nofollow\">LinkedIn profile</a> and twitter: @TalQuant.</p>\\r\\n',\n",
       " '<p>Developer at IBM Toronto</p>\\r\\n',\n",
       " '<p>Rails Developer, Health IT consultant</p>\\r\\n',\n",
       " \"<p>M.D. in the United States with a bachelor's degree in biochemistry and a master's degree in public health. Slowly but steadily learning more about the basics of statistics with an emphasis on techniques with application to clinical research. R and Stata user.</p>\\r\\n\",\n",
       " '<p>Grad student.</p>\\r\\n',\n",
       " '<p>I am interested in Biology, Evolution, Genetics/Genomics, Machine Learning and Bayesian statistics.</p>\\r\\n',\n",
       " '<ul>\\r\\n<li>interests in open source software, Linux, GTK, Android, and more</li>\\r\\n<li>loves action films</li>\\r\\n</ul>\\r\\n',\n",
       " '<p>Graduate Student studying Machine Learning.</p>\\r\\n',\n",
       " '<p>Medical student in Halifax, with an interest in statistical inference and modeling.</p>\\r\\n',\n",
       " '<p>Here I am.</p>\\r\\n',\n",
       " '<p>Nothing.</p>\\r\\n',\n",
       " '<p>a lovely shell with a rotten core</p>\\r\\n',\n",
       " '<p>Algorithm Eigeneer</p>\\r\\n',\n",
       " \"<p>I'm a polymath into engineering, software design, simulation, search and rescue, building things, cooking....</p>\\r\\n\",\n",
       " 'Average Python / C / Scala datageek with a taste for artificial intelligence, machine learning, cloud computing, OpenCL, NLP, the semantic web and braaaaains!',\n",
       " '<p>Interested in learning all that is out there!!</p>\\r\\n',\n",
       " 'Todo',\n",
       " \"<p>I'm a student in a Medical class! So my programming skills are far from being at an acceptable level :D Whoops! I love coding and I usually write some script for fun.</p>\\r\\n\",\n",
       " '<p>Embedded software engineer</p>\\r\\n',\n",
       " '<p>I like all kinds of games but mostly strategy and RPGs \\r\\nHere is a list of some games I have played and my thoughts <a href=\"http://bronzebeard.wordpress.com/games/\" rel=\"nofollow\">http://bronzebeard.wordpress.com/games/</a></p>\\r\\n',\n",
       " '<p>R User, board game enthusiast.</p>\\r\\n',\n",
       " '<p>Interests: SQL, C#, ASP.NET, Java</p>\\r\\n\\r\\n<p>--- Trying to answer questions of others helps you learn as well ---</p>\\r\\n',\n",
       " '<p>Computational Linguist, focus on Computational/Algorithmic Semantics.</p>\\r\\n',\n",
       " '<p>Biologist, interested in R language, Latex, Ecology and Evolution</p>\\r\\n',\n",
       " '<pre><code>* Artificial Intelligence\\r\\n* Alan Turing\\r\\n* Math\\r\\n* physics\\r\\n* biology\\r\\n* Neuroscience\\r\\n* DNA\\r\\n* Literature\\r\\n* Visualization of information. Multivariate statistics and datamining Dada\\r\\n* Travel everywhere\\r\\n* etc.\\r\\n</code></pre>\\r\\n\\r\\n<p>Twitter:@marianasoffer</p>\\r\\n',\n",
       " '<p>merge keep - done on 21Jan</p>\\r\\n',\n",
       " '<p>After studying algebraic geometry, algebraic topology and category theory, \\r\\nafter teaching  mathematics and computer science \\r\\nin university and high school, \\r\\nafter studying bioinformatics and implementing image analysis algorithms in a biotech start-up, \\r\\nI have since been working in finance, in London, Tōkyō and Hong Kong.</p>\\r\\n\\r\\n<p>My preferred programming languages are R and Perl, \\r\\nbut I try to use the best language and tools \\r\\nto solve the problem at hand.\\r\\nI also like to learn more marginal languages, such as Haskell or Oz/Mozart. I am interested in human languages, too.</p>\\r\\n\\r\\n<p>Here is a selection of my answers.</p>\\r\\n\\r\\n<ul>\\r\\n<li><p>Computing a probability density function \\r\\nfrom its \\r\\n<a href=\"http://stackoverflow.com/questions/10029956/calculating-a-density-from-the-characteristic-function-using-fft-in-r/10038141#10038141\">characteristic function</a>\\r\\nor by \\r\\n<a href=\"http://stats.stackexchange.com/questions/26598/maximum-entropy-sampler/26605#26605\">maximizing its entropy</a>;</p></li>\\r\\n<li><p>Quadratic programming \\r\\n<a href=\"http://stackoverflow.com/questions/9817001/optimization-with-constraints/9817442#9817442\">to ensure a sequence is increasing</a>\\r\\nor to <a href=\"http://stackoverflow.com/questions/9971268/plot-an-item-map-based-on-difficulties/9971741#9971741\">position labels on a plot</a></p></li>\\r\\n<li><p>Reparametrizing \\r\\n<a href=\"http://stackoverflow.com/questions/9666120/constrain-optimisation-problems-in-r/9667097#9667097\">optimization problems</a>\\r\\nto make them \\r\\n<a href=\"http://stackoverflow.com/questions/9592369/in-r-how-do-i-find-the-optimal-variable-to-maximize-or-minimize-correlation-bet/9593809#9593809\">unconstrained</a>,\\r\\nfor instance \\r\\n<a href=\"http://stats.stackexchange.com/questions/25007/fitting-the-parameters-of-a-stable-distribution\">to fit stable distribution</a>;\\r\\npenalizing them to restrict them to a <a href=\"http://stackoverflow.com/questions/11110848/how-to-optimize-for-integer-parameters-and-other-discontinuous-parameter-space/11111069#11111069\">discrete search space</a></p></li>\\r\\n<li><p>Speeding up computations in R\\r\\n<a href=\"http://stackoverflow.com/questions/9526691/efficiently-compute-histogram-of-pairwise-differences-in-a-large-vector-in-r/9527046#9527046\">using C</a>\\r\\nor by \\r\\n<a href=\"http://stackoverflow.com/questions/10011425/vectorize-function-to-avoid-loop/10011954#10011954\">expanding</a>\\r\\nsome of the computations</p></li>\\r\\n<li><p><a href=\"http://stackoverflow.com/questions/9106401/select-rows-of-a-data-frame-based-on-column-properties/9106581#9106581\">Computing the skyline</a></p></li>\\r\\n<li><p><a href=\"http://stats.stackexchange.com/questions/16631/what-are-essential-rules-for-designing-and-producing-plots/22010#22010\">On the use of colour in statistical plots</a></p></li>\\r\\n<li><p><a href=\"http://stackoverflow.com/questions/9747426/how-can-i-produce-plots-like-this/9748213#9748213\">Plotting the result of a time series clustering</a></p></li>\\r\\n<li><p><a href=\"http://stackoverflow.com/questions/10150161/ordering-117-by-perfect-square-pairs/10150797#10150797\">Solving puzzles with graph theory</a></p></li>\\r\\n</ul>\\r\\n',\n",
       " '<p>PhD student in INRIA, Paris</p>\\r\\n',\n",
       " '<p>Professional Engineer</p>\\r\\n',\n",
       " '<p>Reality is finally beginning to resemble my dream world, where there is plenty of data, storage and number crunching capacity. Now there is no excuse for going with gut feeling instead of hard data.</p>\\r\\n',\n",
       " \"I'm a computer engineering student at sharif university in tehran. Not a great geek, but have some talents.\\\\\\\\nHope I can help the community here sometimes. \",\n",
       " '<ol>\\r\\n<li>Mainly Interested in Machine Learning, Data Mining, Image Processing, Signal Processing and some interesting mathematical problems. </li>\\r\\n<li>MAC and Linux User</li>\\r\\n<li>LaTeX and XeLaTeX user</li>\\r\\n<li>Also, I play FOOTBALL!!!</li>\\r\\n</ol>\\r\\n',\n",
       " \"<p>I love to learn more about Statistics! I'm an economist so my statistics background is not too well (though we have a LOT of statistics subjects back in the university) :)</p>\\r\\n\\r\\n<p>Nice to be here! </p>\\r\\n\",\n",
       " '<p>I am a Ph.D. student in the Electrical Engineering and Computer Science Department at MIT. </p>\\r\\n\\r\\n<p>My academic interests lie in the intersection between optimization and statistics. I am currently developing methods for data-driven decision making under uncertainty with applications in areas such as climate change, crime prediction, healthcare, revenue management and sports analytics.</p>\\r\\n\\r\\n<p>In my spare time, I play basketball, take pictures, and write reviews on Yelp.</p>\\r\\n',\n",
       " '<p>I taught myself programming on a recycled Compaq 9071GL running Mandrake at the tender age of 12 and have been debating compilers ever since.</p>\\r\\n',\n",
       " '<p>Paulo C. Marques F.</p>\\r\\n\\r\\n<p>BA in Physics. PhD in Mathematical Statistics.</p>\\r\\n\\r\\n<p>Interested in Bayesian Inference, Statistical Theory, and stochastic composition of string quartets.</p>\\r\\n',\n",
       " '<p>I spend my professional time working with Linux server technologies.</p>\\r\\n\\r\\n<p>I previously obtained an RHCE, and studied Computing and Statistics for an undergraduate degree and am currently working through a Medicinal Chemistry MSc.</p>\\r\\n\\r\\n<p>Have made some minor contributions to some open-source projects on github; <a href=\"https://github.com/tolland\" rel=\"nofollow\">https://github.com/tolland</a></p>\\r\\n\\r\\n<p>and maintain a terrible ranty, unresearched speculation blog on blogspot.co.uk; <a href=\"http://take-your-vitamins.blogspot.com/\" rel=\"nofollow\">http://take-your-vitamins.blogspot.com/</a></p>\\r\\n\\r\\n<p>None of this is fit for human consumption.</p>\\r\\n',\n",
       " '<p>PhD candidate at The University of Melbourne</p>\\r\\n',\n",
       " '<p>University of Chicago, James Franck Institute, Undergraduate.</p>\\r\\n\\r\\n<p>Imperial College London, Centre for Cold Matter, PhD Candidate.</p>\\r\\n\\r\\n<p>Interests in experimental atomic/molecular physics, statistical mechanics and mathematical physics.</p>\\r\\n\\r\\n<p><a href=\"http://www.linkedin.com/profile/view?id=233197957&amp;trk=tab_pro\" rel=\"nofollow\">http://www.linkedin.com/profile/view?id=233197957&amp;trk=tab_pro</a></p>\\r\\n\\r\\n<p><a href=\"https://www.researchgate.net/profile/Dylan_Sabulsky/\" rel=\"nofollow\">https://www.researchgate.net/profile/Dylan_Sabulsky/</a></p>\\r\\n',\n",
       " '<p>I like to learn things but to pay my way I will aim to teach 1 person something for every 117 things I learn by virtue of contributions of others</p>\\r\\n',\n",
       " \"<p>Currently, I'm a student and CV lurker getting up on my statistics.</p>\\r\\n\",\n",
       " \"<p>I'm a graduate student at Yale University studying visual cognition. I'm interested in what visual information we can acquire without conscious awareness, and how awareness contributes to representing information in the brain. I'm also interested in disambiguating how neural populations encode different types of information using methods like fMRI adaptation and multi-voxel pattern analysis.</p>\\r\\n\",\n",
       " \"<p>Software developer. I work primarily in C#, Silverlight, SQL, WPF, and ASP[.NET] at work and home, but am trying to find a new practical language to learn that isn't quite so C-based. Right now I'm refreshing my abilities in functional programming with some F#. Also somewhat experienced with C++, PHP, Java, LaTeX, HTML/CSS, and a touch of Scheme.</p>\\r\\n\",\n",
       " '<p>Merge keep</p>\\r\\n',\n",
       " \"<p>I'm interested in recommender engines and underlying theoretic base like statistics, probability and machine learning.<br>\\r\\nCurrently I'm working as a web developer.</p>\\r\\n\",\n",
       " '<p>CTO and Co-founder of mobile messaging startup <a href=\"http://touchgr.am\" rel=\"nofollow\">touchgr.am</a>.</p>\\r\\n\\r\\n<p>Graduate at the <a href=\"http://fi.co\" rel=\"nofollow\">Founder Institute</a>, from the 2014 semester in Perth Western Australia.</p>\\r\\n\\r\\n<p>Occasional freelance multi-platform software developer currently busy on iOS <a href=\"http://www.lingopal.com/\" rel=\"nofollow\">Lingopal</a>. </p>\\r\\n\\r\\n<p>Author of \"<a href=\"http://www.packtpub.com/getting-started-with-leveldb/book\" rel=\"nofollow\">Getting Started with LevelDB</a>\" from Packt.</p>\\r\\n\\r\\n<ul><li>non-computing interests: \\r\\n<ul><li>Chow Gar Kung Fu and Yang-style Tai Chi, \\r\\n<li>reading and (so-far unpublished) writing SF\\r\\n<li>snow skiing\\r\\n<li>scuba diving, \\r\\n<li>solar housing, \\r\\n<li>carpentry</ul>\\r\\n\\r\\n<li>computing interests: \\r\\n<ul><li>Functional programming and tools\\r\\n<li>OOD, OO languages and frameworks\\r\\n<li>Usability, \\r\\n<li>cross-platform development, \\r\\n<li>code generation</ul>\\r\\n</ul>\\r\\n',\n",
       " '<p>Top-50 in the Netflix Prize.</p>\\r\\n',\n",
       " '<p>MBA Duke University, 2011<br />\\r\\nStrategy and Finance<br />\\r\\n<br />\\r\\nMS Johns Hopkins University, 2007<br />\\r\\nComputer Science<br />\\r\\n<br />\\r\\nBS Rose-Hulman Institute of Technology, 2003<br />\\r\\nComputer Science and Mathematics<br /></p>\\r\\n',\n",
       " '<p>I am a software developer. I am particularly fond of the areas: <ul> <li> artificial intelligence, computer vision &amp; machine learning</li><li>high end graphics &amp; image processing</li><li>games</li></ul></p>\\r\\n\\r\\n<p>As this profile is now posted across multiple sites:</p>\\r\\n\\r\\n<p>I am a member of the Church of Jesus Christ of Latter-Day Saints, also known as a Mormon.  <br>\\r\\nI am also a mathematician.<br>\\r\\nI quite like interesting games and enjoy playing them with those of my many wonderful children who are old enough. We will often edit what we might find to be sleazy in a game without altering the game play, which quite widens the range of games we can play while staying within what we consider to be gospel standards.</p>\\r\\n',\n",
       " '<p>Hard to find a question that is not already asked here. </p>\\r\\n',\n",
       " '<p>Research officer in glaciology at Swansea University.</p>\\r\\n',\n",
       " '<p></p>\\r\\n',\n",
       " 'fields of interest: Open Source Software, Design Patterns, Real-Time and Embedded Systems, Software Architecture, Advanced Object-Oriented Design and Analysis, Modern C++ Design and Programming, Data Mining, Machine Learning, Safety Critical Systems, TDD, Agile Software Development, RIA Development\\\\\\\\n',\n",
       " '<ul>\\r\\n<li>va san diego hsr&amp;d</li>\\r\\n<li>head of research > statigrafix &lt;</li>\\r\\n</ul>\\r\\n',\n",
       " '<p>I am currently working towards a PhD in microbial genetics. Before that I spent a bit over a decade working as a programmer, with most of that time spent writing computer games. I have degrees in Mathematics (MMath), Life Sciences (BSc) and Molecular Genetics (MSc).</p>\\r\\n',\n",
       " \"<p>I am a Principal Data Scientist at LinkedIn. I lead a team of extraordinary data scientists and engineers who are developing data-driven monetization strategies for LinkedIn's product suite and contributing to our IP portfolio through research. Some of our projects include the development of reserve prices for ads sold through the auction, estimation of survival-based lifetime value of subscribing members, development of forecasting algorithms, and identification and development of improvements to job recommendations in key segments. Prior to LinkedIn, I spent 5+ years as a Quantitative Manager (Research) at Google. There, I led research in search advertising effectiveness and developed machine learning models to help improve the targeting of Google's ads products and sales offerings. Some of our work was featured in the letter to board of directors and our research on search advertising has been featured in Techcrunch, Search Engine Land, and various media outlets. I continue to be active in the research community by publishing regularly and by serving as referee / editorial committee member for several journals (Engineering Optimization, Journal of Advertising Research, International Journal of Production Research, etc.). I hold a PhD from Northwestern University, a M.S. from University of Illinois, and a B.E from National Institute Of Technology (Karnataka, India).</p>\\r\\n\",\n",
       " '<p>Twitter: @glenn_mcdonald</p>\\r\\n\\r\\n<p>Work: the.echonest.com</p>\\r\\n\\r\\n<p>Defunct previous works: Interchange, eRoom, Needle.</p>\\r\\n',\n",
       " '<p>I work on problems that blend software development, data, and decision making. My background is a mixture of the computer science, engineering, and mathematical theory useful for working on interesting problems. I enjoy building great user experiences and data visualizations, creating automated and repeatable workflows, and helping people make complex decisions based on data.</p>\\r\\n\\r\\n<p><a href=\"http://github.com/adamgreenhall\" rel=\"nofollow\">github</a> | <a href=\"http://twitter.com/adamgreenhall\" rel=\"nofollow\">twitter</a> | hello@adamgreenhall.com</p>\\r\\n',\n",
       " '<p>Graduate student in educational psychology. I study inference.</p>\\r\\n',\n",
       " '<p>Working with a wide range of languages, but mainly Oracle PL/SQL, Java and C.</p>\\r\\n',\n",
       " 'I just want a badge.',\n",
       " '<p><a href=\"https://twitter.com/SuperSachin\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/F1eEq.png\" alt=\"My Twitter Profile\"></a>\\r\\n<a href=\"http://www.facebook.com/SuperSachin\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/m6ane.png\" alt=\"My Facebook Profile\"></a>\\r\\n<a href=\"http://plus.google.com/+SachinShekhar\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/afTo0.png\" alt=\"My Facebook Profile\"></a><br>\\r\\n<a href=\"http://stackexchange.com/users/281536/sachin-shekhar\"><img src=\"http://stackexchange.com/users/flair/281536.png\" width=\"208\" height=\"58\" alt=\"profile for Sachin Shekhar on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Sachin Shekhar on Stack Exchange, a network of free, community-driven Q&amp;A sites\" /></a><br>\\r\\nGeek, Gamer, Student, Android Developer, Arduino Ninja, Computer Scientist &amp; Entrepreneur</p>\\r\\n\\r\\n<p>I mostly live in the Sci-Fi, Gaming &amp; Cartoon world, but -- at night -- I secretly masquerade as someone who pretends to know Android, Arduino, Linux, Java &amp; jQuery.</p>\\r\\n\\r\\n<p>Also, I am \"truely\" in love with a beautiful girl too...</p>\\r\\n\\r\\n<p><strong>My Fav TV Series:</strong> Doctor Who, Marvel\\'s Agents of S.H.I.E.L.D., The Originals, Star Trek: Enterprise, Star Trek: The Next Generation, Star Trek: Voyager, Star Wars: The Clone Wars, Justice League: Unlimited, The Vampire Diaries</p>\\r\\n\\r\\n<p><strong>My Fav MMO Games:</strong> Planetside 2, Warframe, Star Trek Online, Eve Online, AirMech, Boom Beach, Clash of Clans</p>\\r\\n',\n",
       " '<p>Oruko ni mi Dapo.</p>\\r\\n',\n",
       " '<p>I am a PhD student at BYU in CS.</p>\\r\\n',\n",
       " '<p>matlab/stats/linux nerd, learning R slowly; proud, sleepless, new parent; HCSSiM alum, CMU grad, postdoc survivor; faking it as a quantitative analyst at a small quant fund in San Francisco; wants to be a statistician when he grows up.</p>\\r\\n<p><a href=\"http://twitter.com/shabbychef\">@shabbychef</a></p>',\n",
       " '<p>Perpetually confused.</p>\\r\\n',\n",
       " '<p>PhD. &amp; Assistant professor at the University of Alicante, Spain. Research interests: knowledge discovery, business intelligence, data mining, statistical analysis</p>\\r\\n',\n",
       " '<p><sup><a href=\"http://web.archive.org/web/20080514205739/http://www.math.wustl.edu/~msh210/legal.html#sm\" rel=\"nofollow\"><em>msh210</em> is a service mark.</a></sup></p>\\r\\n',\n",
       " \"<p>I'm an undergrad who's so willing to learn statistics. Help me please. Especially in Logistic regression.</p>\\r\\n\",\n",
       " '<p>rogue econometrician. \\r\\ndata mining ninja. \\r\\nheuristics guru. \\r\\nai pirate. \\r\\nmaths/statistics lecturer at Nyenrode. \\r\\ninto python/R. \\r\\ntravelling developer. \\r\\nwant to learn more about ruby/d3/web</p>\\r\\n',\n",
       " 'unix sysadmin and general geek',\n",
       " 'College student in NYC looking to improve my skills in PHP, Java and develop some innovative web apps.',\n",
       " '<p>Interested on statistics, mathematics and machine learning :)</p>\\r\\n',\n",
       " '<p>I do bioinformatics and cancer research at University College London (UCL).</p>\\r\\n',\n",
       " '<p>Digital Technology Nerd, Music fanatic, Health Foodie, Political News Junkie &amp; Lover of ALL things Apple.</p>\\r\\n\\r\\n<p>University of Texas at Austin</p>\\r\\n\\r\\n<p>B.A. Economics</p>\\r\\n',\n",
       " '<p>Currently I work as a Computer Biologist at FUNDACIÓN INVESTIGACIÓN CLÍNICO DE VALENCIA-INCLIVA after being working at Genome Campus, Cambridge-UK (both Sanger and EBI) for more than 10 years. My current job deals with genomic analysis of complex diseases in humans using NGS.</p>\\r\\n\\r\\n<p>I have a Zoology degree, a master in genetics and evolution and a PhD in molecular genetics in complex diseases (Hypertension).</p>\\r\\n\\r\\n<p>I create software applications for genetic analysis and genomic visualization using Perl (bioperl API, ensEMBL API), R (bioconductor) and javascript (jquery and extjs)</p>\\r\\n',\n",
       " '<p>PhD student in Psychology, University of Essex</p>\\r\\n',\n",
       " '<p>I\\'m a software engineer at Universal Postal Union, with an interesting consultant background. I am very enthusiastic about science and technology and in this blog, I will try to report my introduction to Python expirience, starting from scratch. Please check my recent blog about python on <a href=\"http://insidepython.wordpress.com/\" rel=\"nofollow\">http://insidepython.wordpress.com/</a>, please give your feedback.</p>\\r\\n',\n",
       " '<p>PhD candidate at Vrije Universiteit Brussel</p>\\r\\n',\n",
       " '<p>[Tools I use]<br/></p>\\r\\n\\r\\n<ul><li>UnitTest++</li>\\r\\n<li>Boost</li>\\r\\n<li>LibBeecrypt</li>\\r\\n<li>SQLite3</li>\\r\\n<li>CppSQLite</li>\\r\\n<li>stxxl( learning )</li>\\r\\n</ul>\\r\\n\\r\\n<p>[Build Tools]</p>\\r\\n\\r\\n<ul>\\r\\n<li>cmake</li>\\r\\n<li>scons</li>\\r\\n<li>make</li>\\r\\n<li>nmake</li>\\r\\n</ul>\\r\\n\\r\\n<p>[Compilers]</p>\\r\\n\\r\\n<ul>\\r\\n<li>MSVC 2012</li>\\r\\n<li>MSVC 2010</li>\\r\\n<li>MSVC 2008</li>\\r\\n<li>MSVC .NET 2005</li>\\r\\n<li>MSVC .NET 2003</li>\\r\\n<li>MSVC 6.0</li>\\r\\n<li>GCC 4.x</li>\\r\\n<li>GCC 3.3</li>\\r\\n</ul>\\r\\n',\n",
       " \"<p>I'm a postgraduate student with an interest in social network analysis. Experienced in Python (and Django), PHP, SQL, Java, HTML, CSS and jQuery.</p>\\r\\n\",\n",
       " \"<p>I'm a graduate student at New York University with interest in Machine Learning, Natural Language Processing, Deep Learning and I use python to do all my stuff.</p>\\r\\n\",\n",
       " '<p>Yet another math enthusiast...</p>\\r\\n',\n",
       " \"<p>I'm an MS student in Statistics at the University of Minnesota.</p>\\r\\n\",\n",
       " '<p><a href=\"http://www.onehourtranslation.com/affiliate/gigili\" rel=\"nofollow\"><img src=\"http://static.onehourtranslation.com/images/banners/set2/translation-services-125x125.jpg\" alt=\"professional translation\" ></a></p>\\r\\n\\r\\n<p></p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p>I write to discover what I think. </p>\\r\\n',\n",
       " '<p>PhD Student in Statistics at the University of Bath. Working on statistical inference for dynamical processes. </p>\\r\\n',\n",
       " '<p>Just developing...</p>\\r\\n',\n",
       " '<p>Student at University of Washington\\r\\nApplied and Computational Mathematics; ECONOMICS(BS)</p>\\r\\n',\n",
       " '<p>I am interested in</p>\\r\\n\\r\\n<ul>\\r\\n<li>artificial intelligence</li>\\r\\n<li>machine learning</li>\\r\\n<li>neural networks</li>\\r\\n<li>reinforcement learning</li>\\r\\n<li>computer vision</li>\\r\\n</ul>\\r\\n\\r\\n<p>I am using</p>\\r\\n\\r\\n<ul>\\r\\n<li>C++ (Qt, OpenGL, OpenCV, Eigen)</li>\\r\\n<li>Ruby for most of my scripts</li>\\r\\n<li>Java</li>\\r\\n<li>Python</li>\\r\\n</ul>\\r\\n',\n",
       " '<p>My professional interest is developing visualization methods and software to help people to explorer and understand high dimensional data. I am especially interested in geometrical data modelling which is often also referred to as multidimensional scaling (MDS) or non-linear dimensionality reduction. </p>\\r\\n',\n",
       " '<p>PhD student.</p>\\r\\n',\n",
       " '<p>loves social media technologies, data analyst and researcher in Social Network Analysis!</p>\\r\\n',\n",
       " '<p>part-time statistician, software engineer</p>\\r\\n',\n",
       " '<p>Postgraduate student. University of Leeds UK. Statistical Epidemiology,</p>\\r\\n',\n",
       " '<p>I am a software developer... I am certified (MCP) in WindowsForms.NET, ASP.NET and ADO.NET (with C#) and I really like .NET (I specially love LINQ)... </p>\\r\\n\\r\\n<p>I sometimes <a href=\"http://www.javamexico.org/blogs/luxspes\" rel=\"nofollow\">blog at Javamexico</a> or at my <a href=\"http://luxspes.blogspot.mx/\" rel=\"nofollow\">personal blogspot</a>. I also <a href=\"https://twitter.com/lux_spes\" rel=\"nofollow\">tweet ocasionally</a></p>\\r\\n\\r\\n<p>I also know how to code in Java (I love Hibernate,and I love Spring-Mvc but I think for the web the future is in JSF with Seam... and for SmartClients XAML... but perhaps Flex will give a good fight) , I know a little of Delphi... and I have worked with Oracle and SQLServer...</p>\\r\\n\\r\\n<p>The best web technology that I have used is WebObjects (sadly IMHO Apple just doesn\\'t seem to know its value) I think SQL is full of flaws... I used to believe the only way to go is to use an ORM and I really disliked plain ADO.NET and JDBC, I used to think that to build the business rules of a software system.. there was nothing better than Domain Model pattern... built with Apple\\'s EOF... or Hibernate (or NHibernate or LINQ &amp; EntityFramework). But lately, after reading <a href=\"http://www.thethirdmanifesto.com/\" rel=\"nofollow\">TheThirdManifesto</a>, I have been thinking that relational is not limited, it is underexploited, and perhaps Alphora <a href=\"http://dataphor.org\" rel=\"nofollow\">Dataphor</a> or <a href=\"http://dbappbuilder.sourceforge.net/Rel.php\" rel=\"nofollow\">Rel</a> will finally exploit its power.</p>\\r\\n\\r\\n<p>I am also certified in TFS 2010, and I know my way around CVS, SVN, Mercurial, Bazaar and a little Git</p>\\r\\n',\n",
       " 'twitter.com/kjhealy',\n",
       " '<p>Intrested in Computers and Photography</p>\\r\\n',\n",
       " '<p>Hello,</p>\\r\\n\\r\\n<p>My name is Mike Stumpf and I am a post-postgraduate living in Canada.  My main academic focus was on early modern drama, particularly Shakespeare and his contemporary playwrights, but most of my research has also been involved with the digital in one way or another.  My research blog <a href=\"http://allistrue.org/\" rel=\"nofollow\">All Is True</a> is one example.  Another is my project website, the <a href=\"http://earlymodernsandbox.com/\" rel=\"nofollow\">Early Modern Sandbox</a>, whose aim is to create a working environment to explore literature using digital tools.</p>\\r\\n\\r\\n<p>Although both of my degrees are in literature, I am searching for a position in Toronto, ON, in web development.  My passion for the digital aspect of my scholarship has evolved into a passion of its own.  I taught myself HTML, CSS, JavaScript, PHP, and MySQL during the second half of my time at university and, since graduating, I have focused more on Content Management Systems, particularly WordPress, Drupal, and Omeka.  If I had to choose one, I would say that my specialty is WordPress.</p>\\r\\n\\r\\n<p>The purpose of my <a href=\"http://mikestumpf.com/\" rel=\"nofollow\">personal website</a> is to create a personal portfolio of my work and my experiences while I am looking for a job.  I am open to contract or short-term work but I am ultimately seeking a permanent position.  If you are interested in my work or in hiring me, feel free to contact me using the \"Contact\" link on the menu above or this <a href=\"http://mikestumpf.com/contact/\" rel=\"nofollow\">link</a>.</p>\\r\\n\\r\\n<p>Cheers,</p>\\r\\n\\r\\n<p>Mike</p>\\r\\n',\n",
       " '<p>I am a wildlife biologist who is desperately trying to improve in statistics while learning R.</p>\\r\\n',\n",
       " 'Academic nerd',\n",
       " '<p>I am a Computer Scientist and Open Scholar: Web, OLAP, Databases, Time Series, Collaborative Filtering, Information Retrieval, e-Learning.</p>\\r\\n',\n",
       " '<p>I am a financial and monetary economist.</p>\\r\\n',\n",
       " '<p>I am a PhD student from Queensland University of Technology (QUT) at the beautiful Brisbane City, Australia. My research interest is on social media analytics and text mining. My personal interest on programming span across C#, Python, Ruby and HTML5, CSS3 web programming. I am also a Microsoft Certified Application Developer for .NET and Microsoft Office Expert World Champion on Microsoft Excel and National Champion for Microsoft Word.</p>\\r\\n',\n",
       " '<p>Interest in application of statistics to Biology and Medical Science. My research focus on the bioinformatics in influenza virus, including its evolutionary dynamics and its impact on public health.</p>\\r\\n',\n",
       " '<p>Library geek.</p>\\r\\n',\n",
       " '<p>Originally from Nicaragua, educated in Edinburgh and the USA, and now living primarily in London.</p>\\r\\n\\r\\n<h2>Useful questions and answers</h2>\\r\\n\\r\\n<h3>Stack Overflow</h3>\\r\\n\\r\\n<ol>\\r\\n<li><a href=\"http://stackoverflow.com/a/11218752/869912\">Debugging <code>CREATE TABLE</code> statements</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/a/11581118/869912\">Logging in Python</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/a/11637448/869912\">Simple string formatting in Python</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/11456334/clearing-a-variable-after-its-used-in-a-object-initializer-clears-the-objects\">Reference types in C#</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/a/11196014/869912\">String compression in C# using Gzip</a></li>\\r\\n</ol>\\r\\n\\r\\n<h3>Meta Stack Overflow</h3>\\r\\n\\r\\n<ol>\\r\\n<li><a href=\"http://meta.stackexchange.com/q/142632/187647\">Book recommendation questions</a></li>\\r\\n<li><a href=\"http://meta.stackexchange.com/q/139075/187647\">Answering old questions with a solution in the comments</a></li>\\r\\n</ol>\\r\\n\\r\\n<h3>IT Security</h3>\\r\\n\\r\\n<ol>\\r\\n<li><a href=\"http://security.stackexchange.com/q/6205/4217\">Cryptographically secure random strings in PHP</a></li>\\r\\n</ol>\\r\\n\\r\\n<h3>LaTeX</h3>\\r\\n\\r\\n<ol>\\r\\n<li><a href=\"http://tex.stackexchange.com/q/37248/7783\">Fitting a table on a page through rotation</a></li>\\r\\n</ol>\\r\\n\\r\\n<h2>StackExchange Flair</h2>\\r\\n\\r\\n<p><a href=\"http://stackexchange.com/users/465669\">\\r\\n<img src=\"http://stackexchange.com/users/flair/465669.png\" width=\"208\" height=\"58\" alt=\"profile for Ricardo Altamirano on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Ricardo Altamirano on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\\r\\n</a></p>\\r\\n',\n",
       " \"<p>I'm new here.</p>\\r\\n\\r\\n<p>Some things I'm into (listed chronologically of when they entered my love-life) - math, music, my wife, bike polo.</p>\\r\\n\",\n",
       " '<p>Long-time R user, new Stata user, occasional Stan user, and currently learning Scala. Data Scientist at <a href=\"http://datamininglab.com/\" rel=\"nofollow\">ERI</a>, an incredible company where we do everything from fraud detection and visualization, to training and and text mining.</p>\\r\\n',\n",
       " '<ul>\\r\\n<li><p>Likes: MATLAB, R, C++, Regression models and Linear Algebra. Spatial statistics are pretty cool too.</p></li>\\r\\n<li><p>Dislikes: Perl, Prolog and Asymptotic Theory (probably cause I am bad in all three of them).</p></li>\\r\\n</ul>\\r\\n',\n",
       " '<p>Experience is just inference from a limited and biased sample.</p>\\r\\n',\n",
       " 'Bioinformatician\\\\\\\\nVirology\\\\\\\\nGenetics\\\\\\\\nBiology\\\\\\\\nScience\\\\\\\\nScience2.0\\\\\\\\nWeb2.0\\\\\\\\nBioinformatics\\\\\\\\nGenotyping\\\\\\\\nWikipedia',\n",
       " '<p>Love computers, technology and Apple. Wanna be entrepreneur.</p>\\r\\n',\n",
       " 'Renewable Energy Analyst',\n",
       " \"<p>I'm employed as an analyst with an equal split in job duties between SAS programming and statistical consulting. The latter is actually a little embarrassing as I'm still learning but don't need a very high level of sophistication for the bulk of my work.</p>\\r\\n\",\n",
       " '<p>I am a CS PhD student and a data geek. I have experience in data (and\\r\\nnetwork) mining, analysis, visualization and predictive modeling as it\\r\\nrelates to very large volumes of data (mostly whats on web and what\\r\\nrelates to social media). I also like to build systems to collect\\r\\ndata. I use various database technologies and distributed platforms in\\r\\nmy day to day research.\\r\\nI am currently studying Images on social media and my thesis is about\\r\\nunderstanding the role of multimedia content in information spread.\\r\\nI love to write code, drink good coffee and read good books.</p>\\r\\n',\n",
       " '<p>UK-based freelance Software Engineer operating in the aerospace industry.</p>\\r\\n\\r\\n<p>Main Skills:<br>\\r\\n-Python<br>\\r\\n-C/C++<br></p>\\r\\n',\n",
       " '<p>Into biology, information sharing, and education...</p>\\r\\n',\n",
       " '<p>I am a PhD student at Stanford University studying geometry processing and computer graphics.</p>\\r\\n',\n",
       " '<p>Currently working with:</p>\\r\\n\\r\\n<ul>\\r\\n<li>Groovy</li>\\r\\n<li>Grails</li>\\r\\n<li>Griffon</li>\\r\\n</ul>\\r\\n',\n",
       " '<p><img src=\"http://i.stack.imgur.com/aIWUa.gif\" alt=\"enter image description here\"> \\r\\nSome answers I like:</p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://stackoverflow.com/questions/2135116/how-can-i-determine-distance-from-an-object-in-a-video/2152097#2152097\">How can I determine distance from an object in a video?</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/2691021/problem-with-precision-floating-point-operation-in-c/2691079#2691079\">Problem with Precision floating point operation in C</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/1768197/bounding-ellipse/1768440#1768440\">Bounding ellipse</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/1689839/transforming-captured-co-ordinates-into-screen-co-ordinates/1689899#1689899\">Transforming captured co-ordinates into screen co-ordinates</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/1500498/how-to-use-sift-algorithm-to-compute-how-similiar-two-images-are/1501903#1501903\">How to use SIFT algorithm to compute how similiar two images are?</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/1521958/shortest-path-to-transform-one-word-into-another/1521973#1521973\">Shortest path to transform one word into another</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/2446494/skewing-an-image-using-perspective-transforms/2448069#2448069\">Skewing an image using Perspective Transforms</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/3365487/code-golf-build-me-an-arc/3369332#3369332\">Code Golf: Build Me an Arc</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/2770155/plotting-bessel-function-in-matlab/2770272#2770272\">Plotting Bessel function in MATLAB</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/a/12765147/71131\">Incrementing values in a sparse matrix takes very long</a></li>\\r\\n</ul>\\r\\n\\r\\n<p>NB: The first person to get a <a href=\"http://stackoverflow.com/help/badges/993/computer-vision\"><code>computer-vision</code></a> &amp; <a href=\"http://stackoverflow.com/help/badges/1247/opencv\"><code>opencv</code></a> badge :)</p>\\r\\n',\n",
       " '<p>I\\'m currently a CS masters student at Columbia University in  NYC.</p>\\r\\n\\r\\n<p>See more at <a href=\"http://rkuykendall.com\" rel=\"nofollow\">rkuykendall.com</a>. Especially since this profile is probably out of date.</p>\\r\\n',\n",
       " '<p>Statistician &amp; SAS Programmer</p>\\r\\n',\n",
       " '<p>Contractor specialising in charity/not-for-profit sector<br/>\\r\\nMCTS, Asp.Net 3.5<br/>\\r\\nOpen-source SSRS Front end: <a href=\"http://stackoverflow.com/questions/5272753/alternative-ssrs-front-ends/6779738#6779738\">CrissCross</a><br/>\\r\\nBlog: <a href=\"http://www.codeulike.com\" rel=\"nofollow\">www.codeulike.com</a></p>\\r\\n',\n",
       " 'Nothing about me for now',\n",
       " '<p>researcher on education, poverty and urbanisation in developing countries</p>\\r\\n',\n",
       " '<p>I am a Research Scientist at Intel Labs, studying text-mining, graph analytics, and neuroinformatics.</p>\\r\\n',\n",
       " '<p>Business analyst in Vienna, Austria.</p>\\r\\n\\r\\n<p>My interests include data mining, statistical modelling and visualisation with open source tools. I mainly use RapidMiner and R.</p>\\r\\n',\n",
       " '<p>Currently doing things with data at ScraperWiki, UK.</p>\\r\\n',\n",
       " '<p>I\\'m a former Stack Exchange employee. </p>\\r\\n\\r\\n<p>For all site support issues, please <a href=\"http://meta.stackoverflow.com/help\">contact the team</a>.</p>\\r\\n',\n",
       " '<p>Student of economics, econometrics, and statistics</p>\\r\\n',\n",
       " '<p>I am a PhD student from University of Science and Technology of China. My interests includes machine learning and its applications.</p>\\r\\n',\n",
       " '<b>MSc Student / Assistant Researcher</b><br>\\\\\\\\nData Mining & Quantitative Finance ',\n",
       " '<p>Recent Western Washington University graduate in computer science. I scooped up by Amazon and now am working at <a href=\"https://aws.amazon.com/marketplace\" rel=\"nofollow\">AWS Marketplace</a>.</p>\\r\\n\\r\\n<p>If you would like to befriend me on <a href=\"http://www.facebook.com/johnthecoolguy\" rel=\"nofollow\">the facebook</a>, feel free.</p>\\r\\n',\n",
       " \"<p>I am one of the archetypal grad students you will find on the pages of numerous comic-strips, only that I'm not famous yet. I took up Statistics when I was an undergrad and since then I eat data, sleep data, dream data and if you come close enough you'd know I even smell of them ... </p>\\r\\n\",\n",
       " '<p>Political scientist trying to learn python and R. My blog <a href=\"http://polstat.org/blog\" rel=\"nofollow\">polstat.org/blog</a></p>\\r\\n',\n",
       " \"<p>I'm a computer scientist working with lots of data! :)</p>\\r\\n\",\n",
       " '<p>I\\'m a PhD Student working within the intersection of systems biology, computational biology and bioinformatics.My background is in applied mathematics and engineering.</p>\\r\\n\\r\\n<p>The majority of my data analysis has been within the Java ecosystem, although I have worked quite a bit in MATLAB. Over the past 10 years I have fiddled around with several other programming languages/environments (e.g. C, C++, Perl, Bash, SQL, PHP etc), but never got around to \"master\" any one of them. Long time ambition is to overcome my deep hatred of R. </p>\\r\\n\\r\\n<p>I have a purely casual interest in programming (in particular concurrent programming and HPC), hash functions and random number generators, while my professional interests are within bioinformatics &amp; computational biology, optimization and data-analysis/mining.</p>\\r\\n',\n",
       " '<p>Statistician,Machine Learning,Data Mining,R,Perl.\\r\\nI maintain a blog listing out probability puzzles with the intention of spreading a simplistic understanding of the science of probability.</p>\\r\\n',\n",
       " \"<p>I'm a statistician.</p>\\r\\n\",\n",
       " '<p>My name is Kristofer Monisit and I ask a lot of questions, work for the answers, which bring about more questions.</p>\\r\\n\\r\\n<p>Support this Area 51 proposal to bring it to commit stage:<br>\\r\\n<a href=\"http://area51.stackexchange.com/proposals/16617/academia?referrer=dC8p1D-IBbcjazD_NAENyw2\">Academia</a></p>\\r\\n\\r\\n<p>Support this Area 51 proposal to bring it to beta:<br>\\r\\n<a href=\"http://area51.stackexchange.com/proposals/5022/3d-graphics-techniques-and-software?referrer=F29BJboyyA9CxT7Iq4t4JA2\">3D Graphics Techniques and Software</a></p>\\r\\n\\r\\n<p>I am an electrical engineer by profession and I want to become a scientist. I\\'m learning TeX/LaTeX so that I\\'ll have one less thing to worry about when I write. I\\'m well-versed in MATLAB, and getting to that level with Python. I dabble in robotics, instrumentation, distributed computing, computer graphics.</p>\\r\\n\\r\\n<p>Ultimately, I want to move into nanotechnology, photovoltaics, renewable energy. Then a tech startup...</p>\\r\\n\\r\\n<p>See my undergraduate thesis here:<br>\\r\\nControlling the Microbot TeachMover using MATLAB GUI<br>\\r\\n<a href=\"http://www.youtube.com/watch?v=8-L2rHOyPdw\" rel=\"nofollow\">Part 1</a><br>\\r\\n<a href=\"http://www.youtube.com/watch?v=3ilp4iQoF8I\" rel=\"nofollow\">Part 2</a></p>\\r\\n',\n",
       " '<p>Computer Science PhD student in Bilkent University / Turkey</p>\\r\\n',\n",
       " '<p>I\\'m <strong>an applied mathematician &amp; engineer</strong>, living &amp; working just outside the London metro area.</p>\\r\\n\\r\\n<p>Over the years and on both sides of the Atlantic, I\\'ve led a variety of quantitative &amp; operational challenges across intelligent systems &amp; software engineering, sonar, defense, environmental technology, robotics, and most recently the complex world of multi-channel retail.</p>\\r\\n\\r\\n<p>The common thread that links passion, profession, and play is applying technology, good design, and quantitative modelling and simulation, to build better products, enable better decisions, and optimise performance.</p>\\r\\n\\r\\n<p><em>I\\'m always experimenting and tinkering, so feel free to get in touch with ideas.</em></p>\\r\\n\\r\\n<p>My Google+ <strong><a href=\"https://plus.google.com/u/0/+AssadEbrahim/about\" rel=\"nofollow\">profile</a></strong>, Google+ <strong><a href=\"https://plus.google.com/u/0/+AssadEbrahim/posts\" rel=\"nofollow\">posts</a></strong> (active), and <strong><a href=\"http://www.mathscitech.org/articles\" rel=\"nofollow\">blog</a></strong> (older)</p>\\r\\n',\n",
       " '<p>Studied physics,</p>\\r\\n\\r\\n<p>Works at statistical data analysis,</p>\\r\\n\\r\\n<p>Plays at making music.</p>\\r\\n',\n",
       " '<p>Learning and developing skills in machine learning (Open University, Stanford online ML-class), and want to move into machine learning as a career. Particularly interested in biologically-inspired algorithms (GP, NN, etc.). Python, Javascript, C.</p>\\r\\n',\n",
       " '<p>SCAer, lifelong gamer.</p>\\r\\n\\r\\n<p>Administrator for the Citizens of the Imperium boards for the Traveller Science-Fiction RPG at <a href=\"http://TravellerRPG.com\" rel=\"nofollow\">http://TravellerRPG.com</a></p>\\r\\n',\n",
       " \"<p>I'm majoring in Computer Science at Rochester Institute of Technology.</p>\\r\\n\",\n",
       " '<p>Programmer</p>\\r\\n',\n",
       " '<p>22 Aug 2013: A Level results........\\r\\nMathematics: A*,\\r\\nFurther Mathematics: A*, \\r\\nPhysics: A*, \\r\\nBiology: A*</p>\\r\\n\\r\\n<p><a href=\"http://stackexchange.com/users/1445301\">\\r\\n<img src=\"http://stackexchange.com/users/flair/1445301.png?\" width=\"208\" height=\"58\" alt=\"profile for Joe King on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Joe King on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\\r\\n</a></p>\\r\\n',\n",
       " '<p>Java programmer ,Web developer<br></p>\\r\\n\\r\\n<p><br>http://www.delicious.com/pramodc84<br></p>\\r\\n\\r\\n<p><br>http://www.twitter.com/pramodc84</p>\\r\\n\\r\\n<p><br>http://www.facebook.com/pramodc84</p>\\r\\n',\n",
       " '<p>Nothing to see here. Move along.</p>\\r\\n',\n",
       " '<p>b.schwehn@gmx.net</p>\\r\\n',\n",
       " '<p>Cardiac anesthesiologist, programmer (R, SQL, perl, unix...)</p>\\r\\n',\n",
       " '<p>mad scientist</p>\\r\\n',\n",
       " '<p>A data geek and Python enthusiast.</p>\\r\\n',\n",
       " \"<p>Programmer with computer vision background (or other way around). I work in one of the personalization groups at Netflix.</p>\\r\\n\\r\\n<p>C++, Matlab, Java, Hive, and R mostly, but I don't consider myself stuck on any language. I have applied these to machine learning, computer vision, web services and where some of those things intersect.</p>\\r\\n\\r\\n<p>Passionate about geometry, machine learning, mathematics, jazz, guitar, dogs, daily builds and regression tests.</p>\\r\\n\",\n",
       " \"<p>I'm a Master of Public Health (MPH) graduate student at Brown University. My interests include emergency medical interventions and orthopedics. I also like long distance running. Cooking for people is one of my favorite activities, and if applying to medical schools this spring is unsuccessful, I'm going to take my chances and open a restaurant. If you have any questions related to public health or what to cook for dinner tonight, shoot me an email at gabriel_verzino@brown.edu. </p>\\r\\n\",\n",
       " '<p>Too old to learn.  Too crazy to care.</p>\\r\\n',\n",
       " '<p>Currently about to finish my master thesis focusing on ownership structute in corporate turnarounds.</p>\\r\\n',\n",
       " \"<p>I am a PhD candidate at the University of New South Wales. I love LaTeX because it makes my documents look fantastic and I can do all sorts of presentation and data-integration tricks that Word or OO users wouldn't be able to do in a million years, in particular, integrating R with LaTeX is a magical combination.</p>\\r\\n\\r\\n<p>I encourage my colleagues to use LaTeX, but not many people are willing to go through the (quick) learning curve despite recognizing and commenting on the obvious presentation differences. Perhaps when their theses' are approaching the 200pg+ mark, with many tables and figures causing their system to crash, then they may reconsider...</p>\\r\\n\",\n",
       " '<p>~~<br>\\r\\n<strong>Green For Good Answers</strong><br>\\r\\n<a href=\"http://pm.stackexchange.com/questions/9072/how-to-sequence-tasks-given-time-estimates\">How to sequence tasks given time estimates?</a></p>\\r\\n\\r\\n<p>~~<br>\\r\\n<strong>Experiences on this site.....</strong></p>\\r\\n\\r\\n<p>::AND WHAT THE HELL IS WRONG WITH &lt;1% OF PPL CONTINUOUSLY PUTTING FAKE \\'HOMEWORK\\' TAGS!?! do these ppl never <strong>learn????????????</strong></p>\\r\\n\\r\\n<p>~ <strong>Case Easy Questions</strong> ~<br>\\r\\n<a href=\"http://www.ironpython.net/\" rel=\"nofollow\">http://www.ironpython.net/</a> answers one of my very few questions, like once in a whole month, (which i deleted) that NOBODY could help with!!</p>\\r\\n\\r\\n<p>~ <strong>Case Better Google</strong> ~<br>\\r\\nif you know the RIGHT terms, just google -- to solve simple, easy problems in minutes -- <a href=\"http://windowssecrets.com/forums/showthread.php/107596-Word-macro-needed-multiple-search-and-replace-(Word-2003)\" rel=\"nofollow\">http://windowssecrets.com/forums/showthread.php/107596-Word-macro-needed-multiple-search-and-replace-(Word-2003)</a> -- SO MUCH trouble... even when you gave them a very nice, detailed question -- stackoverflow.com/questions/13223561/find-replace-macro-long-and-detailed</p>\\r\\n\\r\\n<p>so <strong>disappointed</strong> at this site. </p>\\r\\n\\r\\n<p>~ <strong>Case Bad GIS</strong> ~<br>\\r\\nfound the great answer: <a href=\"http://spatialanalysis.co.uk/2011/12/2011/\" rel=\"nofollow\">http://spatialanalysis.co.uk/2011/12/2011/</a> -- no thanks to the ppl at gis stack <em>sigh</em></p>\\r\\n\\r\\n<p><strong>bad gis person:</strong> \"why should we spend our time answering something we already know?\" (SO GET LOST :D<br>\\r\\n<strong>actually smart people:</strong> \" <a href=\"http://www.youtube.com/watch?v=r9LCwI5iErE\" rel=\"nofollow\">http://www.youtube.com/watch?v=r9LCwI5iErE</a> \"<br>\\r\\n<strong>then</strong> an incrediably bad mod closed a good question out of merely not liking <a href=\"http://gis.stackexchange.com/questions/36733/good-maps-showing-findings-using-spatial-analysis\">http://gis.stackexchange.com/questions/36733/good-maps-showing-findings-using-spatial-analysis</a><br>\\r\\n<strong>but knows</strong> she reasonably can\\'t given similarly phrased questions asked by others -- <a href=\"http://gis.stackexchange.com/questions/36747/what-is-the-most-useful-spatial-analysis-trick-in-meeting-any-real-world-needs\">http://gis.stackexchange.com/questions/36747/what-is-the-most-useful-spatial-analysis-trick-in-meeting-any-real-world-needs</a><br>\\r\\n<strong>outcome:</strong> but since that gis mod can do whatever she wants, she banned my gis account for a month -- woho! </p>\\r\\n\\r\\n<p>~ <strong>Case Math &amp; Stat\\'s Inadequacy in knowing how to Communicate</strong> ~<br>\\r\\nGo Learn Stuff -- <a href=\"https://www.edx.org/\" rel=\"nofollow\">https://www.edx.org/</a> -- <a href=\"http://www.www.udacity.com\" rel=\"nofollow\">http://www.www.udacity.com</a> -- <a href=\"http://www.coursera.org\" rel=\"nofollow\">http://www.coursera.org</a> -- and get with the times and make significant, nonmarginal progress. also math &amp; stat ppl specifically really need to improve their answering skills so look towards <strong>quora on that respect</strong></p>\\r\\n',\n",
       " '<p><a href=\"http://stackexchange.com/users/d941154e3fa54592b111a6b8350d64c0\">\\r\\n<img src=\"http://stackexchange.com/users/flair/d941154e3fa54592b111a6b8350d64c0.png?theme=clean\" width=\"208\" height=\"58\" alt=\"profile for Mehper C. Palavuzlar on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Mehper C. Palavuzlar on Stack Exchange, a network of free, community-driven Q&amp;A sites\"> \\r\\n</a>\\r\\n<a href=\"http://mehper.tr.gg\" rel=\"nofollow\"><b>mehper.tr.gg</b></a></p>\\r\\n\\r\\n<p><b>Industrial Engineer <sup>M.Sc.</sup> </b></p>\\r\\n\\r\\n<hr/>\\r\\n\\r\\n<ul>\\r\\n<li>Author of <em><a href=\"http://rads.stackoverflow.com/amzn/click/383834555X\" rel=\"nofollow\">Distribution Planning of Magazines: A Practical Approach.</a></em></li>\\r\\n<li>Author of <em><a href=\"http://rads.stackoverflow.com/amzn/click/383836208X\" rel=\"nofollow\">Random Variate Generation If the Density Is Not Known: Basics, Methods, Implementations.</a></em></li>\\r\\n<li>Dealing with data analysis, system development and optimization.</li>\\r\\n<li>Using R, SQL, VBA languages.</li>\\r\\n<li>Tech-savvy.</li>\\r\\n<li>XBox 360 fan.</li>\\r\\n<br>\\r\\n</ul>\\r\\n',\n",
       " '<p>computer science and molecular computing</p>\\r\\n',\n",
       " '<p>I am currently a PhD student in Statistics.</p>\\r\\n',\n",
       " '<p>PhD Assistant Professor in developmental psychology.</p>\\r\\n',\n",
       " \"<p>I'm here to learn from others.</p>\\r\\n\",\n",
       " '<p>Programming Student in Java. Writing type person. Lover of Math and CompSci. Hello there!</p>\\r\\n',\n",
       " '<p>Avid Emacs and Vim hacker (I use Emacs with <strong><a href=\"http://www.emacswiki.org/emacs/Evil\" rel=\"nofollow\">Evil</a></strong> to get the best of both worlds ;)</p>\\r\\n\\r\\n<p>I\\'m experienced with Python, C#, Java, and Javascript, and have dabbled in C++, C, objective-C, Lisp, Ruby, Ocaml, and Scala. While I have a keen interest for all things code, I\\'m especially interested in the design and creation of programming languages themselves. </p>\\r\\n\\r\\n<p>Currently a Computer Science Sophomore at Cornell University. </p>\\r\\n\\r\\n<p>Formerly known as @CrazyJugglerDrummer.</p>\\r\\n',\n",
       " \"<p>I'm Quantitative Environmental Scientist in the Institute of Environmental Change &amp; Society, at the University of Regina, Canada. I undertake research on environmental problems, including climate change and atmospheric pollution, affecting lakes. I use lake sediments to look back in time at the history of lakes to look at what organisms are present and how the species in the lake have changed through time and how lakes evolve and respond to pollution and perturbations.</p>\\r\\n\\r\\n<p>I'm also an Adjunct Professor in the Department of Biology at the University of Regina.</p>\\r\\n\",\n",
       " '<p>Consultant and researcher.\\r\\nPhD in econometrics.\\r\\nMy main fields are Labour and Financial Econometrics</p>\\r\\n',\n",
       " '<p>Bioinformatics student, interested in Python, .NET, Unix Systems, R, Heuristics, Machine Learning, Math, ...</p>\\r\\n',\n",
       " '<p>Machine learning researcher at the Technische Universitaet Muenchen.</p>\\r\\n',\n",
       " '<p>Spending time working in neuroscience with bash, python and C</p>\\r\\n',\n",
       " '<p>While I have degrees in mathematics and theoretical computer science, I have since become a quantitative social science researcher. My day-to-day programming experience is largely with applied statistical analyses and simulations, although I maintain an amateur interest in abstract theoretical CS issues.</p>\\r\\n\\r\\n<p>My first major programming experience was with Mathematica. I have at various times programmed in (or at least dabbled with) C++, SAS, SPSS, Javascript, R, and Haskell. On the whole, I tend to use multi-paradigm languages in a functional style. I enjoy Haskell the most, and am trying to shift most of my work there, but still consider myself a beginner.</p>\\r\\n',\n",
       " 'Swiss geek, interested in machine learning, and eager to learn more about Java, PHP, C#, Lua and Python !',\n",
       " \"<p>I'm an undegrad double majoring in quantitative biology and statistics. I write almost exclusively in R. I use R for school and for independent research projects. I've been coding for about a year now and I really like it. I'm on stack overflow and X validated to give and receive help with simple to intermediate problems and to avoid making the folks on the R help mailing list angry.</p>\\r\\n\",\n",
       " '<p>I am very interested in learning mysteries of nature and find all the underlying cause. I wish to learn technologies so as to make my study of nature easier and effective.</p>\\r\\n',\n",
       " '<ul>\\\\\\\\n<li><a href=\"http://mathiasbynens.be/\" rel=\"nofollow\">mathiasbynens.be</a></li>\\\\\\\\n<li><a href=\"http://twitter.com/mathias\" rel=\"nofollow\">twitter.com/mathias</a></li>\\\\\\\\n<li><a href=\"http://qiwi.be/\" rel=\"nofollow\">qiwi.be</a></li>\\\\\\\\n</ul>',\n",
       " \"<p>I'm an assistant professor in the Econ department at Iowa State; I went to UCSD for grad school before that and my research focuses on Econometric Theory (time series in particular) and forecast evaluation.</p>\\r\\n\",\n",
       " \"<p>I'm a PhD student at York University studying Quantitative Methods in Psychology, specifically psychometrics and latent variable modelling techniques.</p>\\r\\n\",\n",
       " '<p>Professional and enthusiast currently working at <a href=\"http://www.yelp.com\" rel=\"nofollow\">Yelp</a>. As a logician, I believe anything can be made possible, though not all roads lead to tractable solutions. </p>\\r\\n\\r\\n<p>As an eclectic programmer, I\\'m not always the best person to answer your question. But, I do my best.</p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://www.linkedin.com/in/trobinso\" rel=\"nofollow\">LinkedIn Profile</a></li>\\r\\n</ul>\\r\\n',\n",
       " 'Founder at <a href=\"http://www.lokad.com/\" rel=\"nofollow\">Lokad.com</a> (online forecasting services)',\n",
       " '<p>My name is Kamran Zahid. I am doing MBA with specialization in Finance. I am also doing research on Liquidity Risk of Banks. I want to become a good teacher as well as researcher. I like research, reading and teaching.</p>\\r\\n',\n",
       " '<p>Visit <a href=\"http://mortoray.com/\" rel=\"nofollow\">my blog</a> about programming, software development and language design.</p>\\r\\n\\r\\n<p>If it\\'s programming related, the I\\'ve done it. A plethora of languages. Many application domains. A lot of companies. Several countries.</p>\\r\\n\\r\\n<p>I now spend my time working on the Leaf programming language.</p>\\r\\n',\n",
       " '<p>Asc. Prof. of CS/algorithms at NTNU. Author of \"Practical Python\"/\"Beginning Python\" and \"Python Algorithms\".</p>\\r\\n',\n",
       " '<p>Associate Editor of the O.E.I.S.  Online Encyclopedia of Integer Sequences<br>\\r\\nInterested by statistical distributions used in actuarial computations<br>\\r\\nWorking on statistical learning theory<br></p>\\r\\n',\n",
       " '<p>I am a statistical consultant and data scientist living in Ottawa, Canada, with over 10 years experience. I have yet to see a data set I did not like. I work primarily with R and SPSS.</p>\\r\\n',\n",
       " '<p>Doctoral Candidate in Cognitive Psychology and Cognitive Science at Indiana University (Bloomington)</p>\\r\\n',\n",
       " '<p>Professional UX designer and research; Author of Salstat, open source software for friendlier statistical analysis (<a href=\"https://github.com/salmoni/Salstat\" rel=\"nofollow\">https://github.com/salmoni/Salstat</a>); psychologist (Ph.D. in human-computer interaction) and statistician; and freelance business owner. </p>\\r\\n\\r\\n<p>I enjoy research, coding in Python (particularly stats: I wrote Salstat before stats got trendy!) and designing incredible experiences.</p>\\r\\n\\r\\n<p>In terms of work, I\\'m ideally looking for remote work that combines research, programming and design. If anyone has a need for that sweet spot, let me know. </p>\\r\\n',\n",
       " '<p>Msc. in Electrical Engineering.</p>\\r\\n\\r\\n<p>Interests:\\r\\nImage processing, machine learning, linear algebra</p>\\r\\n',\n",
       " '<p>Doing PhD in Electrical Engineering</p>\\r\\n',\n",
       " 'Oracle Developer/DBA',\n",
       " '<p>Student in Quantitative Analysis in the Social Sciences</p>\\r\\n',\n",
       " '<p>PhD Student in Astrophysics from PUC, Chile, working in the area of astrostatistics applied to exoplanet detection and characterization and in the area of variable stars. More info about me in my personal webpage <a href=\"http://www.astro.puc.cl/~nespino\" rel=\"nofollow\">here</a>.</p>\\r\\n',\n",
       " '<p>Statistics student.</p>\\r\\n',\n",
       " '<p>marketing analyst</p>\\r\\n',\n",
       " '<p>Nothing to say, I have</p>\\r\\n',\n",
       " '<p>\\r\\n<a href=\"http://www.twitter.com/TheCellarRoom\" rel=\"nofollow\">@TheCellarRoom</a><br/>\\r\\n<a href=\"http://www.github.com/TheCellarRoom\" rel=\"nofollow\">Github</a><br/>\\r\\n<a href=\"http://thecellarroom.net\" rel=\"nofollow\">Www</a><br/>\\r\\n</p>\\r\\n',\n",
       " \"<p>I'm a programming monkey with lots of Matlab experience and I'm getting good with C# and .NET. My background is in electrical engineering with a focus on modeling &amp; simulation, signal processing, and image processing.</p>\\r\\n\",\n",
       " '<p>Half network, half mathematician, half programmer, half computer scientist, technicaly half of evrything .\\r\\nR, java fan, open source lover, soft computing hacker.</p>\\r\\n',\n",
       " '<p>@neuinfo</p>\\r\\n\\r\\n<p>@jcachat</p>\\r\\n',\n",
       " '<p>MSCS student, almost finished!</p>\\r\\n\\r\\n<p>I enjoy Java and algorithms.</p>\\r\\n',\n",
       " '<p>PhD Student in financial mathematics</p>\\r\\n',\n",
       " '<p>Embedded systems engineer specializing in instrumentation and data analysis.</p>\\r\\n',\n",
       " \"<p>Powers are for the weak, I have no powers. Unless you count the power to blow minds with my weapons grade philosophical insights. I'm a thoughtocaster, a conundrummer, in a band called life puzzler. Hell, I've flipped more lids than a monkey in a soup kitchen - of the mind.  </p>\\r\\n\",\n",
       " \"<p>I'm a software engineer based in Glasgow, Scotland mostly writing software in C# and a little Java.</p>\\\\\\\\n<br>\\\\\\\\n<p>I'm interested in a variety of topics from best practice and coding standards and UI to nitty gritty programming details.</p>\",\n",
       " \"<p>I'm a Senior Research Fellow and Consulting Biostatistician at the University of Otago, Wellington, in occasionally-sunny New Zealand. I work providing statistical consulting service for researchers at the Medical School here, as well as being a co-investigator on numerous health-related projects.</p>\\r\\n\\r\\n<p>My pre-biostats background (undergraduate and PhD) is in experimental psychology and cognitive neuroscience.</p>\\r\\n\\r\\n<p>When I'm not working, I'm doing other stuff :-)</p>\\r\\n\",\n",
       " '<p>Graduate Student at Osaka University</p>\\r\\n',\n",
       " '<p>(my about me is currently blank)</p>\\r\\n',\n",
       " '<p>Software developer at <a href=\"http://i.tv\" rel=\"nofollow\">i.tv</a>. Creating our future robot overlords in my free time.</p>\\r\\n',\n",
       " \"<p>I'm a systems developer located in Stockholm, Sweden. My main expertise is PHP and MySQL, but I like to meddle with any language unlucky enough to get in my way.</p>\\\\\\\\n\\\\\\\\n<p>The more languages I learn, the less I seem to like PHP, so lately I've began doing all my console scripts in python, which I love more and more each time. </p>\\\\\\\\n\\\\\\\\n<p>I do use Java a lot as well, and is quite routinated with large parts of J2SE and Swing. I also meddle around with C, though I would run any of my code as root, if you catch my drift. :)</p>\\\\\\\\n\\\\\\\\n<p>On occasion I've also done some work in C++ and C#, though not as much as I'd like.</p>\",\n",
       " \"<p>I am a Political Science PHd. I program in R since 2007, and have about one year or less of experience with C, SQL and Python. I'm a Data Scientist at Hyperativa, a Brazilian based company.</p>\\r\\n\",\n",
       " '<p>Graduate student of Urban Planning. </p>\\r\\n',\n",
       " '<p>University student</p>\\r\\n\\r\\n<p>One life, live it.</p>\\r\\n\\r\\n<p>Interested in Android programming and statistics.</p>\\r\\n',\n",
       " 'Wikichiki, Pythonista, linguistics geek, free culture and free software enthusiast. <s>Oh and somehow my job is to write Prolog!</s> Now I get to write Python. woot. :)',\n",
       " \"<p>I’ve got my Bs.C in Information Systems from the Technion at 2008 , while I was working as a student at Intel Research department in Israel, doing Machine learning oriented research .</p>\\r\\n\\r\\n<p>I then spent 2 amazing years working at Outbrain , a cool start up that changes the way you read on the web (you just don’t even know you've already used it), and now I’m working as an NLP software engineer at Clearforest , a Thomson Reuters company , where I get the change to work with leading expert in the NLP world  and learn a bunch of new stuff (Whoo!)</p>\\r\\n\\r\\n<p>I’ve always been fascinated by computer science – everything from machine learning algorithms via vision problems to the best way to manage concurrency in Java and debugging log files (that’s real programmers porn ). Currently I feel like StackOverflow is the best thing that happened since bread came sliced! </p>\\r\\n\\r\\n<p>As Plutarch said – “The mind is a fire to be kindled , not a vessel to be filled.” . So far , I’ve never been cold.</p>\\r\\n\",\n",
       " '<p>I am a cognitive neuroscientist primarily studying spoken language and language impairments.</p>\\r\\n',\n",
       " '<p>I just graduated with a B.A. in Psychology and will be applying to graduate school (clinical psychology) in the fall. I am currently analyzing data that I have collected on the relationship between spirituality and mental health and plan to publish the results soon. However, I do NOT have extensive knowledge of advanced statistics because I don\\'t have a graduate degree, so I am here to try to find answers to the questions that I have encountered while analyzing my data. Additionally, I will try to answer the questions that I see under \"New Questions\" if I know the answer to any of them!</p>\\r\\n',\n",
       " 'PhD Student straddling the line between molecular biology and bioinformatics - I manage to cope by perennially breaking for coffee',\n",
       " \"<p>In no particular order, I'm an:</p>\\r\\n\\r\\n<p>Engineer; Programmer; Networker; Occasional Sysadmin; Parent; Perpetual Student; Spiritual Being; Wannabe Hacker; Nerd; Political Junkie; Scientist; White Hat; Geek</p>\\r\\n\\r\\n<p>Also, a recovering, though often relapsing, n00b. :-)</p>\\r\\n\\r\\n<p>Let's make code, not war.</p>\\r\\n\",\n",
       " '<p>Android app developer, interested in machine learning and numerical optimization algorithms.</p>\\r\\n\\r\\n<p>I\\'m currently a sophomore at Carnegie Mellon University studying computer science, and have previously worked as an Analog Engineering Intern at Audience, Inc (2012), Software Engineering Intern on the CHIMP project at NREC (2013), and Full-stack Engineer (Intern) at BigML (2013).</p>\\r\\n\\r\\n<p>On the side, I work on the AWARE project at CMU: <a href=\"http://www.awareframework.com/\" rel=\"nofollow\">http://www.awareframework.com/</a>.</p>\\r\\n\\r\\n<p>You can find me on LinkedIn at <a href=\"http://www.linkedin.com/pub/joey-robinson/31/326/a95\" rel=\"nofollow\">http://www.linkedin.com/pub/joey-robinson/31/326/a95</a>.</p>\\r\\n',\n",
       " '<p><img src=\"http://i.stack.imgur.com/goQvn.png\" alt=\"enter image description here\"></p>\\r\\n',\n",
       " '<p>Mathematics major, Statistics minor</p>\\r\\n',\n",
       " '<p>In the golden age of ancient Chinese philosophy, Gung-sun Lung Tsu was the one of the only thinkers, and certainly the most important, to discuss topics pertaining to logic and epistemology.</p>\\r\\n',\n",
       " '<p>I am a creative enthusiast who always seeks new ideas and technologies and aims to enhance work efficiency and increase my knowledge, I am able to balance competing priorities and tight deadlines with quality, I am able to develop and deliver plans, create new ideas and find market niches. I am a fast learner who cannot get enough of learning; I have excellent analytical skills and keen to work under pressure. I am always keen to work in different projects especially those who allow me the chance to learn.</p>\\r\\n',\n",
       " \"<p>Currently I am a post-doctoral bioinformatics researcher in at Cardiff university. My area of research is genetic predisposition of complex, polygeneic, non-mendelian diseases, specifically, those classified as psychiatric disorders (schizophrenia, autism, bi-polar disorder, and alzheimers). I am currently working with Next Gen sequencing data to discover new variants. \\r\\nIn terms of programming, mostly it's R, sometimes Python, Perl only when I have to, and the occasional awk and sed.</p>\\r\\n\",\n",
       " '<p>Currently a PhD student at NC State University.</p>\\r\\n',\n",
       " 'Web developer.',\n",
       " '\\r\\n\\r\\n<blockquote>\\r\\n  <p>No, his mind is not for rent<br>\\r\\n    to any god or government.<br>\\r\\n    Always hopeful, yet discontent.<br>\\r\\n    He knows changes aren\\'t permanent,<br>\\r\\n    but change is.</p>\\r\\n</blockquote>\\r\\n\\r\\n<p>— Rush, <em>Tom Sawyer</em></p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p>Taking an externally-imposed and much-needed break from SE activities.</p>\\r\\n\\r\\n<p>E-mail (flipped ROT13): zqd˙ʎʌuzʇ@ʇɐʌǝɥʇʌssqɹǝɥɟuɹʎɔ<br>\\r\\nAny code I\\'ve posted here I place under the <a href=\"http://www.wtfpl.net/\" rel=\"nofollow\">WTFPL</a>.</p>\\r\\n',\n",
       " '<p>see <a href=\"http://kenmankoff.com/\" rel=\"nofollow\">http://kenmankoff.com/</a></p>\\r\\n',\n",
       " '<p>Biostatistician with diverse, multifaceted training in health research. Experienced with Phase I-III clinical trials, pharmaceuticals, health plan analytics, survey design and implementation, oncology, epidemiology, and genetics. Interested in positions in medical research. Sharp technical edge with all major statistical programming platforms. Eager and fast-paced learner and performer, flexible and seeking new dimensions of growth.</p>\\r\\n',\n",
       " '<p>Software Developer in Ireland, who is fond of asking questions...</p>\\r\\n',\n",
       " \"<p>I'm a graduate student interested in comparative physiology, animal behavior, biomechanics, and myrmecology.</p>\\r\\n\",\n",
       " '<p>Computational Biologist \\r\\nBroad Institute of Harvard and MIT</p>\\r\\n',\n",
       " '<p>Hello.</p>\\r\\n',\n",
       " '<p>Software engineer in Cape Town, South Africa. I have been building commercial web applications professionally since 2003.</p>\\r\\n\\r\\n<ul>\\r\\n<li>My startup is <a href=\"http://mynames.co.za/\" rel=\"nofollow\" title=\"MyNames: Painless CO.ZA Domain Management\">MyNames</a>.</li>\\r\\n<li>I build repair shop software for <a href=\"http://ifix.co.za/\" rel=\"nofollow\" title=\"iFix Apple Repairs\">iFix Apple® Repairs</a> that runs eight retail stores across South Africa and processes millions in revenue.</li>\\r\\n<li>I built and ran <a href=\"http://rhythmmusicstore.com/\" rel=\"nofollow\" title=\"Rhythm shut down end 2012\">Rhythm Music Store</a> from 2007-2012, the biggest online music store in SA, with a catalogue of 85,000 DRM-free MP3s.</li>\\r\\n<li>I was involved with <a href=\"http://www.rooihub.co.za/\" rel=\"nofollow\" title=\"RooiHub\">peer-to-peer DC++</a> on campus.</li>\\r\\n<li>I graduated with an honours degree in Electronic Engineering with Computer Science from the <a href=\"http://sun.ac.za/\" rel=\"nofollow\">University of Stellenbosch</a>, in 2010.</li>\\r\\n<li>I blog on my <a href=\"http://petrustheron.com/\" rel=\"nofollow\">personal website</a>.</li>\\r\\n<li>You can find me as <a href=\"https://github.com/pate\" rel=\"nofollow\">pate on GitHub</a>.</li>\\r\\n</ul>\\r\\n',\n",
       " 'A CS Masters student at Simon Fraser University in British Columbia, with a focus in Bioinformatics.',\n",
       " '<p>Corey Yanofsky, Ph.D.</p>\\r\\n\\r\\n<p>I\\'m a biostatistician with an interest in financial statistics located in Ottawa, Canada. Philosophically, I\\'m a Bayesian of the <a href=\"http://en.wikipedia.org/wiki/Cox%27s_theorem\" rel=\"nofollow\">Cox</a>-<a href=\"http://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes\" rel=\"nofollow\">Jaynes</a> school. In practice I\\'m a statistical ecumenist whose role models are <a href=\"http://www.cs.berkeley.edu/~jordan/\" rel=\"nofollow\">Michael I. Jordan</a> and <a href=\"http://www.stat.columbia.edu/~gelman/\" rel=\"nofollow\">Andrew Gelman</a>. </p>\\r\\n',\n",
       " '<p>Applied Mathematician<br/>\\r\\nAustrian Institute of Technology</p>\\r\\n\\r\\n<p>Interests:<br/></p>\\r\\n\\r\\n<ul>\\r\\n<li>Math, [Computational] Statistics, Machine Learning, HPC</li>\\r\\n<li>Data Visualization (ggplot2, D3.js, ...), GIS (grass, qgis, osm, ...)</li>\\r\\n<li>R, Python, C[++], js, LaTeX, Vim, zsh, mercurial, ...</li>\\r\\n</ul>\\r\\n\\r\\n<hr/>\\r\\n\\r\\n<p><a href=\"http://stackexchange.com/users/57496\">\\r\\n<img src=\"http://stackexchange.com/users/flair/57496.png\" width=\"208\" height=\"58\" alt=\"profile for rcs on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for rcs on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\\r\\n</a></p>\\r\\n',\n",
       " 'Quantitative modeling, simulation and statistical analysis.',\n",
       " '<p>Garam masala.</p>\\r\\n',\n",
       " '<p>Electrical and computer engineering student. Currently 2nd year.</p>\\r\\n',\n",
       " 'Long time Schemer.\\\\\\\\n\\\\\\\\n',\n",
       " '<p>I run a <a href=\"http://www.kirkouimet.com\" rel=\"nofollow\">personal</a> site, a <a href=\"http://www.socwall.com\" rel=\"nofollow\">desktop wallpaper</a> site, and a <a href=\"http://www.yougetsignal.com\" rel=\"nofollow\">network tools</a> site.</p>\\r\\n\\r\\n<p>Add me on <a href=\"http://www.facebook.com/kirkouimet/\" rel=\"nofollow\">Facebook</a> or <a href=\"http://twitter.com/kirkouimet/\" rel=\"nofollow\">Twitter</a></p>\\r\\n',\n",
       " \"<p>I've been doing software development for about 14 years in a variety of languages from C,C++, Java/J2EE, PHP, and more recently C# and ASP.NET.</p>\\r\\n\",\n",
       " '<p>I use this a lot, check it out:<br>\\r\\n  <a href=\"http://codecogs.com/latex/eqneditor.php\" rel=\"nofollow\">http://codecogs.com/latex/eqneditor.php</a><br>\\r\\n  <a href=\"http://www.equationsheet.com/textoimage.php\" rel=\"nofollow\">http://www.equationsheet.com/textoimage.php</a>\\r\\n  <a href=\"http://code.google.com/intl/es-MX/apis/chart/image/docs/gallery/formulas.html\" rel=\"nofollow\">http://code.google.com/intl/es-MX/apis/chart/image/docs/gallery/formulas.html</a></p>\\r\\n\\r\\n<p>for table:</p>\\r\\n\\r\\n<p><a href=\"http://truben.no/latex/table/\" rel=\"nofollow\">http://truben.no/latex/table/</a></p>\\r\\n',\n",
       " '<p>...I put on my robe and a wizard hat...</p>\\r\\n',\n",
       " '<p>Python / C++ programmer. working in Bioinformatics. see my <a href=\"http://github.com/tanghaibao\" rel=\"nofollow\">GITHUB</a>.</p>\\r\\n',\n",
       " '<p>Always learning, always growing.</p>\\r\\n',\n",
       " 'Good-fast-cheap: pick two.',\n",
       " '<p>PhD student on Electrical Engineering, Marine Department\\r\\nInterest: Machine Learning, Mathematics, Statistics, Robotics, Marine Technology</p>\\r\\n',\n",
       " '<p>Ubuntu user since the end of 2008.</p>\\r\\n',\n",
       " '<p>PhD Biostatistician</p>\\r\\n\\r\\n\\r\\n',\n",
       " '<p>IBM Software Engineer.</p>\\r\\n',\n",
       " '<p>SSL isn\\'t good enough.  Your website can be hacked. <br>\\r\\nHelp solve the problem by advocating these RFCs:<br>\\r\\n<br>\\r\\n<a href=\"http://tools.ietf.org/html/rfc6698\" rel=\"nofollow\">TLSA (formerly DANE for DNS)</a> Fixes the hackable CA problem<br></p>\\r\\n\\r\\n<p><a href=\"http://www.browserauth.net/\" rel=\"nofollow\">TLS-OBC:</a> Fixes TLS, and the Related Domain Cookie Attack<br> </p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p><strong>About me</strong><br>\\r\\nI have no relation to the above sites; I am just an advocate<br>\\r\\n<br>\\r\\nWhy \"makerofthings7\"?  It\\'s a challenge to <i>\"make seven things in my life of significant quality and value\"</i>.   Who knows if those things will take the form of software, art, or people.  (I\\'m not married, no kids yet)\\r\\n<br>\\r\\n<br>See <a href=\"http://linkedin.com/in/makerofthings\" rel=\"nofollow\">...my LinkedIn profile</a></p>\\r\\n',\n",
       " 'Always happy, never blue.',\n",
       " 'Stock trader',\n",
       " '<p>I am currently doing my Masters in Computer Science and I am very ambituous about learning new technologies and new concepts.I am well equipped with SQL programming,Dataware Housing fundamentals and looking forward for a best learning through this website.</p>\\r\\n',\n",
       " \"<p>I'm a developer from Takoma Park, MD.</p>\\r\\n\\r\\n<p>At the office, I work on MSSQL, C#, VB.NET, Javascript, ASP.NET.</p>\\r\\n\\r\\n<p>At home, I'm a vegan, a sci-fi fan, dabble in foreign languages and enjoy book clubs.</p>\\r\\n\\r\\n<p>I will try to up vote everyone who answers me and I thank everyone who has freely given me some of their time to answer my various questions.</p>\\r\\n\",\n",
       " '<p>My name is Chris Lasher. I use computers to study biology. I enjoy revision control, unit testing, and long coding sessions in Python.</p>\\r\\n',\n",
       " '<p>That\\'s \"Steven\" (with the \"n\" at the end)</p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p><em>\"The whole problem with the world is that fools and fanatics are always so certain of themselves, but wiser people so full of doubts.\"</em>  &mdash; Bertrand Russell  </p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p>Product designer, consumer electronics: audio (with Philips), home automation.<br>\\r\\nDone computer science in a previous life too.  </p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p>Belbin team roles: <a href=\"http://en.wikipedia.org/wiki/Team_Role_Inventories#Plant\" rel=\"nofollow\">Plant</a> and <a href=\"http://en.wikipedia.org/wiki/Team_Role_Inventories#Resource_Investigator\" rel=\"nofollow\">Resource Investigator</a></p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p>Personal values: <strong>respect</strong>, <strong>honesty</strong>, <strong>pride</strong>, <strong>modesty</strong>, <strong>fairness</strong></p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p>I yell because I care  </p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<p><a href=\"http://nenya1.net/img/ritter.jpg\" rel=\"nofollow\">favorite candy</a></p>\\r\\n',\n",
       " '<p>I play with <code>R</code> mostly because I like how it allows me to deal with problems I didn\\'t even know I had to begin with. Do I actually <em>use</em> R in my day-to-day work? As a writing teacher, not really.... </p>\\r\\n\\r\\n<p>I like <a href=\"/questions/tagged/r\" class=\"post-tag\" title=\"show questions tagged \\'r\\'\" rel=\"tag\">r</a> questions, particularly those involving <a href=\"/questions/tagged/aggregate\" class=\"post-tag\" title=\"show questions tagged \\'aggregate\\'\" rel=\"tag\">aggregate</a>, <a href=\"/questions/tagged/reshape\" class=\"post-tag\" title=\"show questions tagged \\'reshape\\'\" rel=\"tag\">reshape</a>, or <a href=\"/questions/tagged/regex\" class=\"post-tag\" title=\"show questions tagged \\'regex\\'\" rel=\"tag\">regex</a>, and recently, <a href=\"/questions/tagged/data.table\" class=\"post-tag\" title=\"show questions tagged \\'data.table\\'\" rel=\"tag\">data.table</a>.</p>\\r\\n\\r\\n<p>More places to find me online:</p>\\r\\n\\r\\n<ul>\\r\\n<li>My rarely updated <a href=\"http://news.mrdwab.com\" rel=\"nofollow\">blog</a>, <a href=\"http://mrdwab.com\" rel=\"nofollow\">website</a>, and <a href=\"https://plus.google.com/u/0/100292419927879848929/about\" rel=\"nofollow\">Google+ profile</a></li>\\r\\n<li>Github, mostly with my slowly growing set of <a href=\"https://github.com/mrdwab/2657-R-Functions\" rel=\"nofollow\">utility functions</a></li>\\r\\n<li>Connexions, sometimes working on <a href=\"http://cnx.org/content/col11219/latest/\" rel=\"nofollow\">Collaborative Statistics Using R</a> </li>\\r\\n<li>StackExchange network, particularly <a href=\"http://graphicdesign.stackexchange.com/users/4044/\">Graphic Design</a></li>\\r\\n</ul>\\r\\n',\n",
       " '<p>Orthodox Christian, g33k, ham radio operator, musician, etc. This profile does not reflect my real age ;)</p>\\r\\n',\n",
       " \"<p>PhD in statistics from King's College Cambridge. Expertise in risk-adjusted healthcare monitoring and statistical quality control. 5yrs+ experience in applied statistics.</p>\\r\\n\",\n",
       " \"<p>I am a software engineer and a computer scientist. I've been developing software professionally as a coder, team leader, development manager, project manager, researcher, chief architect and CTO. I live in Tel-Aviv, Israel. </p>\\r\\n\\r\\n<p>I'm especially interested in large-scale, robust, distributed, secured, low-latency architectures, algorithms (math, statistics, text, data, signal, graph and network, geodesic and spatiotemporal); security engineering, machine learning and data science, knowledge representation and ontological engineering.</p>\\r\\n\",\n",
       " '<p>Flash and web developer with experience in online advertising and marketing.</p>\\r\\n',\n",
       " '<p>I am a programmer.</p>\\r\\n',\n",
       " '<p>I am fond of thinking statistical and mathematical problems. More importantly, I suppose that communicating and discussing with others can help us gain a deeper understanding to concepts, theorems, etc in statistics and mathematics. Therefore, I would like to talk about and exchange ideas about all interesting issues in statistics and mathematics with anyone in the world.</p>\\r\\n',\n",
       " '<p><img src=\"http://i.imgur.com/lLSyf.png\" alt=\"enter image description here\"></p>\\r\\n',\n",
       " '<p>Computer analyst</p>\\r\\n',\n",
       " '<p>I am into building web application these days. I tend to work on the frontend where I can combine my love for great user experience with programming.</p>\\r\\n',\n",
       " '<p>Studying Computer Science.</p>\\r\\n',\n",
       " '<p>Agriculture engineer\\r\\nMSc. Human Nutrition for Health Promotion (candidate)\\r\\nMáster Food Science and Technology (candidate)</p>\\r\\n',\n",
       " '<p>Mechanical Engineering Student at Memorial University of Newfoundland and Labrador.</p>\\r\\n',\n",
       " '<p>Program mostly in C# at the moment...</p>\\r\\n',\n",
       " \"<p>I love statistics and programming.  My primary languages are S/R, C/C++, Visual Basic.  I'm not sure if AutoHotKey is quite a programming language but there is nothing better for automating things in windows.</p>\\r\\n\",\n",
       " '<p>Just another geek</p>\\r\\n\\r\\n<p><a href=\"https://www.google.com/+SalmanArshad2000\" rel=\"nofollow\">Google+ Profile</a><br>\\r\\n<a href=\"http://www.linkedin.com/in/salmanarshad2000\" rel=\"nofollow\">LinkedIn Profile</a><br>\\r\\n<a href=\"http://salman-w.blogspot.com\" rel=\"nofollow\">Web Development Blog</a></p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://salman-w.blogspot.com/2013/05/jquery-ui-dialog-examples.html\" rel=\"nofollow\">5 jQuery UI Dialog Examples</a></li>\\r\\n<li><a href=\"http://salman-w.blogspot.com/2013/12/jquery-ui-autocomplete-examples.html\" rel=\"nofollow\">5 jQuery UI Autocomplete Examples</a></li>\\r\\n<li><a href=\"http://salman-w.blogspot.com/2013/01/jquery-ui-datepicker-examples.html\" rel=\"nofollow\">5 jQuery UI Datepicker Examples</a></li>\\r\\n<li><a href=\"http://salman-w.blogspot.com/2014/06/google-adsense-responsive-ads-why-not.html\" rel=\"nofollow\">AdSense Responsive Ads - Why Not?</a></li>\\r\\n<li><a href=\"http://salman-w.blogspot.com/2012/11/fix-the-php-script-does-not-work-problem.html\" rel=\"nofollow\">Find and Fix PHP Problems Yourself</a></li>\\r\\n</ul>\\r\\n',\n",
       " '<p>Coding something...</p>\\r\\n',\n",
       " '<p>SAS and R programming and modelling</p>\\r\\n',\n",
       " '<p>Developer, designer from Cork, Ireland working for <a href=\"http://www.teamwork.com\" rel=\"nofollow\">Teamwork</a>.</p>\\r\\n\\r\\n<hr>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://adamlynch.com\" rel=\"nofollow\">adamlynch.com</a></li>\\r\\n<li><a href=\"http://github.com/adam-lynch\" rel=\"nofollow\">github.com/adam-lynch</a></li>\\r\\n<li><a href=\"http://twitter.com/lynchy010\" rel=\"nofollow\">@lynchy010</a></li>\\r\\n</ul>\\r\\n',\n",
       " '<p>MS UC Berkeley</p>\\r\\n',\n",
       " \"<p>Statistics Undergrad at UIUC. GO ILLINI, LET'S NOT LOSE SO OFTEN!</p>\\r\\n\",\n",
       " '<p>Interested in statistics, gis and programming.</p>\\r\\n',\n",
       " '<p>My research focuses on business intelligence and visual analytics, spectral map analysis (Spectramap), time series analysis (Eviews), dynamic modeling and simulation (Powersim, Vensim). Methods, tools and techniques for exploratory data analysis and visualization have his special interest (Stata, R, SPSS).</p>\\r\\n',\n",
       " \"<p>I'm a Computer Science PhD student at the University of Utah, working with Suresh Venkatasubramanian. Mostly I work on convex optimization and machine learning.</p>\\r\\n\",\n",
       " '<p>I am a German <strong>computer science student</strong>, who also works as a part time web-developer with PHP and Zend Framework.</p>\\r\\n\\r\\n<p>I also blog about <strong>algorithms</strong> and other <strong>programming stuff</strong> at <a href=\"http://eliteinformatiker.de/\" rel=\"nofollow\">eliteinformatiker.de</a>. It’s mostly in German, but I also have a section with <a href=\"http://eliteinformatiker.de/category/english/\" rel=\"nofollow\">English articles</a> (which are usually those I think are especially interesting to international audience and cannot be found via Google that easy).</p>\\r\\n\\r\\n<p>At the moment I’m diving into <strong>games programming</strong> by developing a 2D Mario clone. Actually, I’m hoping that I will have a cool own 2D idea someday, but at the moment there only is one 3D idea.</p>\\r\\n',\n",
       " '<p>Hello, World!</p>\\r\\n',\n",
       " \"<p>I'm currently doing my masters thesis in geography. Therefore I'm working with R to perform cluster analysis of diffrent land-use types. I#m new to R and I don't have former expirience in programming.</p>\\r\\n\",\n",
       " '<p><a href=\"http://stackexchange.com/users/216182/blunders\"><img src=\"http://stackexchange.com/users/flair/216182.png\" width=\"208\" height=\"58\" alt=\"profile for blunders on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for blunders on Stack Exchange, a network of free, community-driven Q&amp;A sites\" /></a></p>\\r\\n',\n",
       " '<p>Making history every day.</p>\\r\\n',\n",
       " '<h2>Working as a Software Systems Engineer.</h2>\\r\\n\\r\\n<p><a href=\"http://stackexchange.com/users/200902\">\\r\\n<img src=\"http://stackexchange.com/users/flair/200902.png\" width=\"208\" height=\"58\" alt=\"profile for KronoS on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for KronoS on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\\r\\n</a></p>\\r\\n\\r\\n<p>I am an editor for the <a href=\"http://blog.superuser.com\">Super User Blog</a>.  Feel free to leave any comments on the blog, ping me in <a href=\"http://chat.stackexchange.com/rooms/118/root-access\">Root Access</a> or the <a href=\"http://chat.stackexchange.com/rooms/356/super-user-blog-editor-room\">Super User Blog Editor</a> chat rooms, or email me at:</p>\\r\\n\\r\\n<p>superuser [dot] kronos [at] gmail [dot] com</p>\\r\\n\\r\\n<p>You can also follow me on twitter:\\r\\n<a href=\"http://twitter.com/#!/SuperKronos\" rel=\"nofollow\">@SuperKronos</a></p>\\r\\n\\r\\n<p>Or check out my blog:\\r\\n<a href=\"http://kronoskoders.logdown.com/\" rel=\"nofollow\">KronoSKoderS</a></p>\\r\\n\\r\\n<blockquote>\\r\\n  <p>There are no winners or losers in the race of life, only finishers and quitters.</p>\\r\\n</blockquote>\\r\\n',\n",
       " '<p>Co-founder of Annotary.</p>\\r\\n',\n",
       " '<p>I am a PhD student in computer science, working in machine learning and computer vision.\\r\\nMy languages of choice are Python and C++. I am also working with CUDA and sometimes I have to work with Matlab.</p>\\r\\n',\n",
       " '<p>Math professor by day, hacker by night.</p>\\r\\n\\r\\n<p>I teach calculus and try to implement teaching and new media technologies.  I use a lot of LaTeX and the occasional scripting language.</p>\\r\\n\\r\\n<p>So glad I found this site--now I can unsubscribe to some of those mailing lists I should be paying attention to but never have the time when the mails come.</p>\\r\\n',\n",
       " '<p>Soc grad student.</p>\\r\\n',\n",
       " '<p>please delete me</p>\\r\\n',\n",
       " '<p>I\\'m into pretty much anything creative, mainly software, art and writing. My traditional spaces are Python and the Cloud, but have been getting more and more into hacking browsers over the last few years.</p>\\r\\n\\r\\n<p>For food, I build web apps, mostly on App Engine.</p>\\r\\n\\r\\n<p>I also try and do <a href=\"https://cosh-demo.appspot.com/\" rel=\"nofollow\">my bit</a> for free software and the docs, trying to get more people into CoffeeScript at the moment.</p>\\r\\n',\n",
       " '<p>Researcher, broadly interested in probability theory, mathematical, computational and applied statistics. </p>\\r\\n',\n",
       " '<p>Designer/Developer combo - serial entrepreneur.</p>\\r\\n',\n",
       " '<p>-</p>\\r\\n',\n",
       " '<ul>\\r\\n<li>Director of Data Science, <a href=\"http://cloudera.com\" rel=\"nofollow\">Cloudera</a></li>\\r\\n<li><a href=\"http://github.com/cloudera/oryx\" rel=\"nofollow\">Oryx</a> project creator (formerly Myrrix)</li>\\r\\n<li>Erstwhile VP/PMC/committer for <a href=\"http://mahout.apache.org\" rel=\"nofollow\">Apache Mahout</a></li>\\r\\n<li>Primary author, <a href=\"http://github.com/zxing/zxing\" rel=\"nofollow\">ZXing</a></li>\\r\\n</ul>\\r\\n',\n",
       " '<p>Worked in consumer insights and innovation for the last 25 years - spent the first years out of school as a Petroleum Engineer.  Love working with consumer data and finding actionable insights.</p>\\r\\n\\r\\n<p>Functional in SPSS - but fall back to XLSTAT for most things.</p>\\r\\n\\r\\n<p>Interested in R and have tried a few things - still very much a novice..</p>\\r\\n',\n",
       " '<p>PhD student in Medical Physics, studying medical ultrasound.</p>\\r\\n',\n",
       " '<p>ruby on rails, ruby, css, powershell!</p>\\r\\n',\n",
       " '<p>Junior academic respiratory paediatrician</p>\\r\\n',\n",
       " '<p>PhD-student and anesthesiologist</p>\\r\\n',\n",
       " '<p>There\\'s no such thing as \"beautiful code\"</p>\\r\\n\\r\\n<p>Beauty is in the design, the concept, the algorithm.</p>\\r\\n\\r\\n<p>The closer you approach the design, the concept, the algorithm -- the more beautiful the code, but the code itself is never beautiful.</p>\\r\\n',\n",
       " '<p>I am an Assistant Professor in the Department of Geography at Hunter College, City University of New York. My research interests fall under the general banner of \"complexity in urban systems\" and encompass interdisciplinary research into topics such as emergence and the multi-scale nature of commuting, the dynamics of housing markets, transportation modelling, and local labor markets. I am particularly interested in movements and flows of individuals, information, and commodities within urban environments, and the development and implementation of novel spatial analysis methods and software aimed at characterizing these flows.</p>\\r\\n',\n",
       " \"<p>I'm a PhD student at the Vrije Universiteit in Amsterdam. \\r\\nMy research is on the genetics of mental disorders and population genetics. \\r\\nSometimes I use Perl, R or bash to program something, and usually when I get stuck, the friendly users of stackoverflow are able to help me out :-)</p>\\r\\n\",\n",
       " '<p>Independent consultant in statistical methods and programming.</p>\\r\\n',\n",
       " '<p>Python dev, linux sysadmin and computational biology M.Sc. \\r\\nPython/Django/Flask/Pandas/Scikit\\r\\nPostgresql/Mysql/Sqlite\\r\\nMongoDB\\r\\nC/C++/CUDA</p>\\r\\n',\n",
       " '<p>Brazilian PhD student in Automatic Control with topic in Diagnosis of Industrial Robots. I like the idea of stackexchange and would like to contribute some.</p>\\r\\n',\n",
       " '<p>Financial analyst</p>\\r\\n',\n",
       " '<p>\"The spiritual quest for elegance can turn the hacker into an artist.\"</p>\\r\\n',\n",
       " '<p>Dipl. agr. biol. Peter Deplewski\\r\\nUniversität Hohenheim (350d)\\r\\nInstitut für Pflanzenzüchtung, Saatgutforschung und Populationsgenetik\\r\\nFg. Saatgutwissenschaft und -technologie</p>\\r\\n',\n",
       " '<p>Currently, I am studying and researching the field of Machine Learning through exploring and developing advanced and efficient solutions for enhancing decision making that could serve the following fields:\\r\\n- Bio-informatics\\r\\n- Bio-medical\\r\\n- Robotics\\r\\n- Natural Language Processing</p>\\r\\n\\r\\n<p>Specifically, the following topics are of current focus:\\r\\n- Feature Selection\\r\\n- High-Dimensional Data Classification\\r\\n- Ensemble Learning\\r\\n- Performance Evaluation and Validation Techniques\\r\\n- Handling Large Data Sets</p>\\r\\n',\n",
       " '<p>Just a curious programmer...</p>\\r\\n',\n",
       " \"I'm a C#, VB6, ASP (Classic), and Java developer professionally.\\\\\\\\n\\\\\\\\nI do workflow systems, a little Activity Based Costing, and regular old business systems.\\\\\\\\n\\\\\\\\nIn my free time I play with php and java on linux.\",\n",
       " '<p>hundreds of unrelated projects, nothing in common.  Jack of all languages, master of none.</p>\\r\\n',\n",
       " '<p>Occasional sysadmin, occasional coder, full-time bioinformatician</p>\\r\\n',\n",
       " \"I'm a graduate student in sociology at the University of California, San Diego. I'm interested in statistics, social network analysis, and R.\",\n",
       " '<p>Ph.D. Candidate in Computer Science, Brown University.</p>\\r\\n',\n",
       " '<p>Quantitative developer, writer, instructor, private trader, so-so musician</p>\\r\\n',\n",
       " '<p>Ph.D. Statistician from Ohio State University. Soon to be Data Scientist at Upstart.com.</p>\\r\\n',\n",
       " '<p>Hi! I\\'m Joseph. </p>\\r\\n\\r\\n<p>I am a computer programmer (my <a href=\"https://github.com/jweissman\" rel=\"nofollow\">Github</a>) and thinker/writer (my working group\\'s <a href=\"http://ontology.io/\" rel=\"nofollow\">blog</a>.) </p>\\r\\n\\r\\n<p>I\\'m also one of the friendly neighborhood moderators <em>pro tempore</em> at <a href=\"http://philosophy.stackexchange.com/\">Philosophy StackExchange</a>. </p>\\r\\n\\r\\n<p><strong>Education:</strong> I graduated from Georgia College and State University in 2008 with a B.S. Degree in Computer Science. I took upper-level classes in both philosophy and mathematics, becoming so interested in the study of philosophy that I ended up taking a sufficient upper-level philosophy hours to satisfy the coursework requirements (if I had taken a bit more foreign languages and written up a thesis, I would have qualified for a B.A. in Philosophy also.)</p>\\r\\n\\r\\n<p><strong>Work:</strong> I\\'m a <a href=\"http://www.thoughtworks.com\" rel=\"nofollow\">ThoughtWorker</a>. I sometimes speak at <a href=\"http://atlanta.coderfaire.com/\" rel=\"nofollow\">conferences</a>. I\\'m also a reader for a philosophy publishing house, <a href=\"http://univocalpublishing.com/\" rel=\"nofollow\">Univocal</a>.</p>\\r\\n\\r\\n<p><strong>Play:</strong> I love moving -- walking, running, working out. I also love books and music (listening and writing -- I play guitar and am trying to learn piano.)</p>\\r\\n',\n",
       " '<p>School of Public Health - The Chinese University of Hong Kong</p>\\r\\n\\r\\n<p>If you are interested in Public Health and/or Epidemiology, please follow <a href=\"http://area51.stackexchange.com/proposals/34565/public-health-epidemiology\">this proposal</a>, submit some sample questions, and vote (or downvote) those that exist already...</p>\\r\\n',\n",
       " \"<p>I'm a phd student at UC Riverside.</p>\\r\\n\",\n",
       " '<p>In 2007 I got my bachelor degree in applied informatics in Belgium. Immediately after, I started working at the company where I did my internship, <a href=\"http://aimproductions.be/\" rel=\"nofollow\">AIM Productions</a>. I liked the work and colleagues at the company too much to give up entirely for further studies, so I decided to combine the two. In 2009 I started studying for my master in Game and Media Technology at the <a href=\"http://www.uu.nl/EN/Pages/default.aspx\" rel=\"nofollow\">University of Utrecht</a> in the Netherlands and I graduated in 2012. At the moment I work as a research assistant at the <a href=\"http://www.itu.dk/\" rel=\"nofollow\">IT University of Copenhagen</a>, at <a href=\"http://itu.dk/pit/\" rel=\"nofollow\">the PIT lab</a> working on <a href=\"http://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction\" rel=\"nofollow\">Human-Computer Interaction</a>.</p>\\r\\n\\r\\n<p>I mainly develop in C#, but occasionally do C++, PHP, Flash/actionscript 3 and Java projects. I care a lot about proper code design, and some people tend to say I overdo it.</p>\\r\\n',\n",
       " '<p>I am a 23 year old web developer living and working in the Cayman Islands.</p>\\r\\n\\r\\n<p>I work with a great team on a great product - DomainNameSales.com</p>\\r\\n\\r\\n<p>I have extensive knowledge of PHP and MySQL.\\r\\nI also have experience with Javascript, Java, Ruby, and Python.</p>\\r\\n\\r\\n<p>I like learning new things, and I like cool stuff.</p>\\r\\n',\n",
       " \"<p>Today is a nice day, let's have some fun!</p>\\r\\n\",\n",
       " '<p>Finished with the PhD. Doing a post-doc in graph theory applied to social networks. Got too many things that are too heavy being juggle in my brain!!! ouchy</p>\\r\\n',\n",
       " '<p>Financial professional with expertise in financial modeling, risk management, and related quantitative methods including Monte Carlo simulation, linear and logit regression, and statistical hypotheses testing.</p>\\r\\n',\n",
       " \"<p>one-text-fits-all-sites self-description:</p>\\r\\n\\r\\n<ul>\\r\\n<li>I'm a physicist working in cognitive neuroscience on statistical data analysis methods.</li>\\r\\n<li>I program mainly in Matlab, but also occasionally in C, bash/sed/awk, Perl, Python, and Java.</li>\\r\\n<li>I use Debian GNU/Linux for working, Windows for playing, Android when mobile.</li>\\r\\n<li>I write pandoc-flavored Markdown in vim which is rendered by LaTeX, though sometimes MS Word can't be avoided.</li>\\r\\n<li>I dabble in typography and information design.</li>\\r\\n<li>I'm a non-native speaker interested in writing and speaking decent English.</li>\\r\\n</ul>\\r\\n\",\n",
       " '<p><a href=\"http://meta.stackexchange.com/questions/2950/should-hi-thanks-and-taglines-and-salutations-be-removed-from-posts\">\"Hi\" and \"thanks\" are for the RW, not SO...</a></p>\\r\\n\\r\\n<p>Obligatory social media links:</p>\\r\\n\\r\\n<p>Twitter: <a href=\"http://twitter.com/casperOne\" rel=\"nofollow\">@casperOne</a></p>\\r\\n',\n",
       " '<p>I make software.</p>\\r\\n',\n",
       " '<p>MS in Applied Statistics. Currently working as a Data Scientist. Interested in statistics in general, but more specifically: mathematical statistics, high dimensional data analysis, and statistical learning.</p>\\r\\n',\n",
       " '<p>Currently Studying B.Sc. in Sociology at University of Copenhagen (2011-)\\r\\nPreviously I studied B.Sc. in Mathematics and Economics at University of Copenhagen (2010-2011)</p>\\r\\n\\r\\n<p>Current field of interest is Social Network Analysis and Causal Analysis</p>\\r\\n\\r\\n<p>Fair knowledge of the software packages Mathematica, R and STATA.</p>\\r\\n\\r\\n<p>Chess player with an international rating of 2034 </p>\\r\\n',\n",
       " '<p>PostDoc</p>\\r\\n',\n",
       " '<p>PhD student</p>\\r\\n',\n",
       " '<p>I am a postdoc in the Department of Molecular Medicine at the University of South Florida (Tampa) where I study gene regulation in Apicomplexa parasites (Toxoplasma and Plasmodium, the vector of malaria).\\r\\nI am a wet bench biologists trying to get more scripting experience. I love R but I am just at the baby steps level.</p>\\r\\n',\n",
       " '<p>CS grad student.</p>\\r\\n',\n",
       " '<p>Bioinformatics PhD student at Boston University</p>\\r\\n',\n",
       " \"<p>I'm a Ph.D. student, working on limit theorems of probability theory.</p>\\r\\n\",\n",
       " '<p>My general StackExchange profile</p>\\r\\n\\r\\n<p><a href=\"http://stackexchange.com/users/916955/martin?tab=accounts\"><img src=\"http://stackexchange.com/users/flair/916955.png\" width=\"208\" height=\"58\" alt=\"profile for Martin on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for Martin on Stack Exchange, a network of free, community-driven Q&amp;A sites\"></a></p>\\r\\n\\r\\n<p>Feel free to connect with me. <a href=\"https://plus.google.com/111471790388609536603/\" rel=\"nofollow\"><img src=\"http://www.hdicon.com/wp-content/uploads/2012/01/google_plus.png\" height=\"35\" /></a>\\r\\n<a href=\"http://il.linkedin.com/in/ramzimartinkahil\" rel=\"nofollow\">\\r\\n<img src=\"https://static.licdn.com/scds/common/u/img/webpromo/btn_viewmy_120x33.png\"></a></p>\\r\\n',\n",
       " '<p>♦ moderator for <a href=\"http://english.stackexchange.com/\">English Language and Usage</a> and <a href=\"http://ell.stackexchange.com/\">English Language Learners</a>. If you (wish to dispute) or (have questions on) a moderator decision, please post on <a href=\"http://meta.english.stackexchange.com\">meta.english</a> or <a href=\"http://meta.ell.stackexchange.com\">meta.ell</a>.</p>\\r\\n',\n",
       " '<p>Arthur Small is co-founder and CEO of Venti Risk Management, a consultancy and software development firm based in State College, Pennsylvania, USA. His work focuses on the management of risks arising due to variations in weather and climate. </p>\\r\\n\\r\\n<p>Before co-founding Venti, Small served as Associate Professor of Applied Economics and Finance in the Department of Meteorology at Penn State University; and as Assistant Professor in the Graduate School of Business and the School of International and Public Affairs at Columbia University. He holds a A.B. degree in mathematics from Columbia University, an M.S. in mathematics from Cornell University, and a Ph.D. in Agricultural and Resource Economics from the University of California at Berkeley.</p>\\r\\n',\n",
       " '<p>As one of my idols once did, I use the name Elvis Jagger Abdul-Jabbar when I am too shy to tell my real name.</p>\\r\\n\\r\\n<p>I am a former algebraist who made a strategic move to applied research (successful move, now I have a position). I’m a full autodidact in statistics, still learning and struggling to enhance my level.</p>\\r\\n\\r\\n<p>My portrait as Homer has been drawn in R by one of my students!</p>\\r\\n',\n",
       " '<p>Electrical Engineer</p>\\r\\n',\n",
       " '<p>Just your average everyday programmer.</p>\\r\\n',\n",
       " '<p>Lover of technology, passion for software, and programming language junkie, Bitcoin enthusiast.</p>\\r\\n\\r\\n<p>About Me:\\r\\n<a href=\"http://procbits.com/pages/about\" rel=\"nofollow\">http://procbits.com/pages/about</a></p>\\r\\n',\n",
       " '<p>I\\'m a former research assistant for social research, with a diploma in social assistance. After about 15 years of such research, mostly under the banner of \"applied social gerontology\",  I was engaged with a teaching contract for introduction of empirical research and statistics in the departement for social assistance (Batchelor and Master) in the Kassel University. I had always a strong favor for programming statistical software and so I\\'ve some knowledge of the deeper insides of statistics as far as connected with linear algebra. In the last years I\\'ve re-discovered my sympathy with number theory and so I\\'m more active on the stat\\'s sister forum at math.stackexchange currently.</p>\\r\\n',\n",
       " '<p>Hello, world!</p>\\r\\n',\n",
       " '<p>A fan of all things numerical...</p>\\r\\n',\n",
       " '<p>Bioinformatician wannabe at UofT</p>\\r\\n',\n",
       " \"<p>MBA and BA in Psychology.  Microsoft developer since 1997.  I took a year-long statistics course in college and fell in love with it.  I'm not remotely as good as the others here at statistics, but I want to learn and try to be helpful.  I also have a strong interest in data mining and artificial intelligence/machine learning algorithms.</p>\\r\\n\\r\\n<p>I wish I could find a job doing stats or even marketing research.</p>\\r\\n\",\n",
       " '<p>Thank you for visiting my profile page.</p>\\r\\n',\n",
       " '<p>Currently working on my PhD at the <a href=\"http://super-resolution.de\" rel=\"nofollow\">Sauer Lab for Biophysics</a> in Würzburg, mostly working on and maintaining the open source program rapidSTORM for super-resolution microscopy.</p>\\r\\n',\n",
       " '<p>interesting in: \\r\\nC++, C#, Perl, Statistics and Time Series theory.</p>\\r\\n',\n",
       " '<p>Student of Computer Science Engineering at the Tecnhical University of Denmark (DTU). Passionate about scripting and data analyisis, currently looking for a student job.</p>\\r\\n',\n",
       " '<p><em>Mathematician by training, Systems Analyst by vocation.</em></p>\\r\\n',\n",
       " '<p>I\\'m a ♦ moderator on Super User and research Quality of Experience for video and multimedia applications. I\\'m also a software developer with a focus on web stuff.</p>\\r\\n\\r\\n<p>If you have any questions about moderation, feel free to ping me in the <a href=\"http://chat.stackexchange.com/rooms/114/ask-a-super-user-moderator\">Ask a Super User moderator</a> chat room. For everything else, don\\'t hesitate to write me a mail to <code>slhck</code> at <code>me.com</code>.</p>\\r\\n\\r\\n<p><a href=\"http://stackexchange.com/users/f54b004d985f4ccbb6b82ad573dccb6d\">\\r\\n<img src=\"http://stackexchange.com/users/flair/f54b004d985f4ccbb6b82ad573dccb6d.png\" width=\"208\" height=\"58\" alt=\"profile for slhck on Stack Exchange, a network of free, community-driven Q&amp;A sites\" title=\"profile for slhck on Stack Exchange, a network of free, community-driven Q&amp;A sites\">\\r\\n</a></p>\\r\\n',\n",
       " '<p>Student at the Software Department of the Higher School of Economics.</p>\\r\\n\\r\\n<p>Prefer C#, Java and Objective-C, ordered descending.</p>\\r\\n\\r\\n<p>Know a little bit about C++.</p>\\r\\n\\r\\n<p>Trying to keep up with the community.</p>\\r\\n',\n",
       " '<p>Computer programmer, etc.  Interests: AI, game programming</p>\\r\\n',\n",
       " '<p>Statistics newbie</p>\\r\\n',\n",
       " '<p>Programmer; musician; PhD in philosophy of language.</p>\\r\\n',\n",
       " '<p>Uhm, what about me? Just me.</p>\\r\\n',\n",
       " '<p>Programmer and NLP researcher.</p>\\r\\n',\n",
       " \"<p>I'm an Industrial/Organizational Psychologist.</p>\\r\\n\",\n",
       " '<p>Postdoctoral researcher on Information Retrieval</p>\\r\\n',\n",
       " '<p>Contact:</p>\\r\\n\\r\\n<blockquote>\\r\\n  <p>Skype: naught101</p>\\r\\n  \\r\\n  <p>XMPP/Googletalk: naught101@jabber.org</p>\\r\\n</blockquote>\\r\\n',\n",
       " '<p>student</p>\\r\\n',\n",
       " \"<p>I'm an international economics student at Bocconi University.<br/> My passions are data crunching and quantitative finance.<br/> Former Google intern!</p>\\r\\n\",\n",
       " '<p>Student</p>\\r\\n',\n",
       " '<p>Nothing deterministic about me..</p>\\r\\n',\n",
       " '<p>PhD student in particle physics</p>\\r\\n',\n",
       " '<p>.NET developer, with a little knowledge in building java/php/rails apps.</p>\\r\\n\\r\\n<p>B.S. Comp Sci - University of Oregon</p>\\r\\n',\n",
       " '<p>When not fighting crime in riverine environments and avoiding having my head detached from my body by public health and animal control authorities, I am a mild-mannered economic geographer. I teach a mandatory introductory level statistics course to groups of 60 to 90 largely unwilling students, as well as other kinds of descriptive and explanatory quantitative techniques in applied regional analysis.</p>\\r\\n',\n",
       " '<ol>\\\\\\\\n  <li>code</li>\\\\\\\\n  <li>math</li>\\\\\\\\n  <li>art</li>\\\\\\\\n  <li>anime</li>\\\\\\\\n</ol>\\\\\\\\n',\n",
       " '<p>I am a Ph.G. candidate in Applied Sociology and Social Research Methods from the University of Milan Bicocca. I am graduated in Sociology from University of Turin in 2010. My research interests are in transition models in labour market, social exclusion and poverty risk. I am mainly interested in quantitative methods and policy evaluation techniques. </p>\\r\\n',\n",
       " '<p>Researcher in the biomedical field</p>\\r\\n',\n",
       " '<p>I have questions about programming.  Less often, I have answers about programming questions.</p>\\r\\n\\r\\n<p>I like bioinformatics and Python.</p>\\r\\n',\n",
       " \"<p>I'm a mathematics professor at New Mexico Tech.  </p>\\r\\n\",\n",
       " '<p>PhD student from italy</p>\\r\\n',\n",
       " '<p>I am a UC Berkeley Undergraduate majoring in applied mathematics.</p>\\r\\n',\n",
       " '<ol>\\r\\n<li>Quantitative Analyst/Developer for a financial software company in Vancouver, B.C. \\r\\n<ol>\\r\\n<li>Fluent in C++ and Python, but not a guru.</li>\\r\\n<li>Interested in using math to dominate my fantasy football league.</li>\\r\\n<li>I also have a PhD in physics.</li>\\r\\n</ol></li>\\r\\n</ol>\\r\\n',\n",
       " '<p>I do professionally program in Java. I do also study Physics and Philosophy. (But i dont do these three things at the same time). \\r\\n9Zq4bu42S3wr0K12</p>\\r\\n',\n",
       " '<ul>\\r\\n<li><a href=\"http://stackoverflow.com/questions/7540227/strategies-for-simplifying-math-expressions/7542438#7542438\">Strategies for simplifying math expressions</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/7478322/why-is-air-nativeprocess-not-supported/7478416#7478416\">Why is AIR NativeProcess not supported?</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/6671405/flex-documentation-what-do-the-experts-use/6671462#6671462\">Flex documentation: what do the experts use?</a></li>\\r\\n</ul>\\r\\n\\r\\n<p>Member of OWASP. Join in too! <a href=\"https://www.owasp.org/\" rel=\"nofollow\">https://www.owasp.org/</a></p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://de.slideshare.net/ApEts/20130212-phpugsession\" rel=\"nofollow\">Alles in die Session, nix im Browser</a></li>\\r\\n</ul>\\r\\n\\r\\n<p>I declare</p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://www.codinghorror.com/blog/2012/07/new-programming-jargon.html\" rel=\"nofollow\">YODA comparisons</a> aren\\'t evil!</li>\\r\\n<li>Use assertions to permanently check the integrity of your code<a href=\"http://php.net/manual/en/function.assert.php\" rel=\"nofollow\"> [PHP</a>, <a href=\"http://docs.oracle.com/javase/1.4.2/docs/guide/lang/assert.html\" rel=\"nofollow\">Java</a> and <a href=\"http://php.net/manual/en/function.assert.php\" rel=\"nofollow\">ObjC]</a>!</li>\\r\\n<li>ZCE PHP 5 since 2009</li>\\r\\n</ul>\\r\\n',\n",
       " '<p>I am a researcher studying communication in Twitter. \\r\\nSee <a href=\"https://sites.google.com/site/twitterresearch09/\" rel=\"nofollow\">https://sites.google.com/site/twitterresearch09/</a></p>\\r\\n',\n",
       " \"<p>Ph. D. in Computer Science, interested in combinatorics and algorithmics applied to computational biology. If you think we may share common research interests, don't be shy and drop me a line! I'm always happy to work with others, and am looking forward to increase the number of publications and coauthors I have.</p>\\r\\n\",\n",
       " '<p>My name is Chris, and I have general interests in some things.</p>\\r\\n',\n",
       " \"<p>I'm a researcher in neurosciences</p>\\r\\n\\r\\n<p>I'm interested in all that regards pattern recognition, time series analysis, rhythms etc.</p>\\r\\n\",\n",
       " '<p>Passionate about Machine Learning, Analytics, Information Extraction/Retrieval and Search.</p>\\r\\n',\n",
       " \"<p>I'm a PhD student at the Bauhaus-University Weimar, Germany and a Deputy Head of the Health Sec\\xadtion at the State Office for Envi\\xadron\\xadmeant, Health and Con\\xadsumer Pro\\xadtec\\xadtion in the Min\\xadistry of Envi\\xadron\\xadment, Health and Con\\xadsumer Pro\\xadtec\\xadtion of the Fed\\xaderal State of Brandenburg</p>\\r\\n\\r\\n<p>My thesis is about urban dynamics simulation and complex data analysis of locating subpopulations an urban areas.</p>\\r\\n\\r\\n<p>As a Deputy Head in the health section I do statistical analysis with epidemiological data for the govermental health report (Right now about mental health).</p>\\r\\n\\r\\n<p>My interests are in multi-agent simulation in epidemiology and urbanistic.</p>\\r\\n\",\n",
       " '<p>I am an Adjunct Professor of Biology at the University of New Mexico, in Albuquerque, where I sometimes teach a course in Biological Statistics. I also own a small data analysis company. My background includes basic biology and ecology, computer programming, and modeling. Research areas include forest ecology and management, invasive species, and ecological networks. Professional interests include environmental impact studies, ecosystem restoration, and spatial analysis, and visualization. I have taught courses in Biological Statistics, Ecological Modeling, and Field Methods in Biodiversity in the US and Canada.</p>\\r\\n',\n",
       " '<p><strong>I am currently looking for a Job in London</strong>. You can contact me at <code>bjoern dot pollex at googlemail dot com</code>.</p>\\r\\n\\r\\n<p>I believe that there is some answer to any question. It is not the task of the users here to lecture others or criticize them for laziness or stupidity. There is little educational value in sarcasm. </p>\\r\\n\\r\\n<p>If a question is badly formulated, one can give advice on how to do better. </p>\\r\\n\\r\\n<p>If important information is missing, one can kindly ask for it. </p>\\r\\n\\r\\n<p>Someone else being unfriendly or badly mannered is no justification for being so yourself.</p>\\r\\n\\r\\n<p>Before posting any question about a problem with your code, please read <a href=\"http://sscce.org/\" rel=\"nofollow\">this</a> (twice).</p>\\r\\n\\r\\n<h2>Favorite Answers</h2>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://stackoverflow.com/a/8614314/160206\">C++ - How to refer to recursive structs through pointers using vectors</a></li>\\r\\n</ul>\\r\\n',\n",
       " \"<p>Luckily I attended the University of Michigan before the invention of video games, so I did finish my PhD. I've been employed in the private sector my entire career, so I prefer to post under a pseudonym. I'm a data guy, rather than a math guy.</p>\\r\\n\\r\\n<p>I really like this site. Very helpful answers, very civilized tone. Thanks to those organize it and those who contribute to it.</p>\\r\\n\",\n",
       " '<p>1st year PhD student in Biomedical and Health Informatics at the University of Washington School of Medicine.</p>\\r\\n',\n",
       " '<p>Computer Science and Mathematics student from Poland.</p>\\r\\n',\n",
       " '<p>At work, I do a lot of python and shell scripting, and a bit of Fortran, usually involving file systems and numerical solvers.</p>\\r\\n\\r\\n<p>At home, I do a lot of C#. Usually some graphics are involved.</p>\\r\\n\\r\\n<p>I like working with python and C#, and I have more respect for Fortran than most people.</p>\\r\\n',\n",
       " '<p>Developing in C++. Getting started with Python.</p>\\r\\n',\n",
       " '<p>.</p>\\r\\n',\n",
       " '<p>PhD student (CS)  (machine learning, graph analysis, python, mysql, java) widh web development background (C#, Access, JavaScript, jQuery)</p>\\r\\n',\n",
       " '<p>I work in Los Angeles</p>\\r\\n',\n",
       " '<p>PSU Alumni!!  Woot.</p>\\r\\n',\n",
       " 'Professor of Radiology<br>\\\\\\\\nUniversity of Washington<br>\\\\\\\\nSeattle, WA',\n",
       " '<p><a href=\"http://burtonsys.com/email/\" rel=\"nofollow\">http://burtonsys.com/email/</a></p>\\r\\n',\n",
       " '<p>about.me/kamilc</p>\\r\\n',\n",
       " '<p>Software Architect in an Applied Research Institute</p>\\r\\n',\n",
       " '<p>PNGJ: <a href=\"https://code.google.com/p/pngj/\" rel=\"nofollow\">https://code.google.com/p/pngj/</a></p>\\r\\n\\r\\n<p>Stereograms: <a href=\"http://hjg.com.ar/st/\" rel=\"nofollow\">http://hjg.com.ar/st/</a></p>\\r\\n\\r\\n<p>Guitar: \\r\\n<a href=\"http://leonbloyguitar.blogspot.com.ar/search/label/anime\" rel=\"nofollow\">http://leonbloyguitar.blogspot.com.ar/search/label/anime</a> \\r\\n<a href=\"http://www.youtube.com/channel/UCW20bKZ9hapL5RNdE91ncwQ\" rel=\"nofollow\">http://www.youtube.com/channel/UCW20bKZ9hapL5RNdE91ncwQ</a> \\r\\n<a href=\"http://hjg.com.ar/ghibli/musica/\" rel=\"nofollow\">http://hjg.com.ar/ghibli/musica/</a></p>\\r\\n',\n",
       " '<p>I ♥ Ruby!</p>\\r\\n\\r\\n<p>\\r\\nI\\'m a committer on:\\r\\n<ul><li>Ruby (MRI)</li>\\r\\n<li>Rubyspec</li>\\r\\n<li>Rubinius</li>\\r\\n</ul>\\r\\n</p>\\r\\n\\r\\n<p>Don\\'t be surprised if I refer to my <a href=\"http://github.com/marcandre/backports\" rel=\"nofollow\">extensive list of backports</a> that makes it easier to write Ruby code for different versions of Ruby.</p>\\r\\n',\n",
       " '<p>holla everybody.</p>\\r\\n',\n",
       " '<p>MSc Quantitative Finance at ETH Zurich, BSc in Business, Economics and Finance in Switzerland and Hong Kong. Interests in computer science, programming, mathematical and computational finance. \\r\\n<strong>Languages</strong>: <em>(X)HTML, CSS, Javascript, PHP, Perl, LaTeX, C++ (notions)</em> // \\r\\n<strong>Servers</strong>: <em>Apache, MySQL</em> // \\r\\n<strong>OS</strong>: <em>Windows 95-7, Linux (Ubuntu)</em> // \\r\\n<strong>Frameworks</strong>: <em>Codeigniter</em></p>\\r\\n',\n",
       " '<p>PhD in Genetics</p>\\r\\n',\n",
       " '<p>App Spec</p>\\r\\n',\n",
       " '<p>My track is security, intrusion detection.</p>\\r\\n',\n",
       " '<p>See also <a href=\"http://www.linkedin.com/in/deepdevelopment\" rel=\"nofollow\">http://www.linkedin.com/in/deepdevelopment</a> as well as the Web site.</p>\\r\\n\\r\\n<p>Thanks.</p>\\r\\n',\n",
       " '<p>Mechanical engineer. Student of applied mathematics. Tinkering with GAE.</p>\\r\\n',\n",
       " '<p>Just a random...person.</p>\\r\\n',\n",
       " '<p>i build Systems &amp; Tools for Analysis, Prediction, Visualization, &amp; Simulation.</p>\\r\\n\\r\\n<p>i also design, code, and deploy complete Machine Learning-based applications (e.g., anti-fraud filter, recommendation engine, monitoring/anomaly detectors), usually designed as modular internal thrift services decoupled from the main app.</p>\\r\\n\\r\\n<p><b>Techniques:</b> </p>\\r\\n\\r\\n<ul>\\r\\n<li><p><strong><em>Machine Learning</em></strong>: in particular,\\r\\nrecursive descent parser (CART/C4.5), Multi-layer Perceptron,\\r\\nSVM/SVR, Kernel Machines, kNN/kdtree, Probabilistic Graphical Models (eg, Markov Random Field)</p></li>\\r\\n<li><p><strong><em>Dimension Reduction Techniques</em></strong>: spectral decomposition (PCA &amp; kPCA, kLDA), Kohonen Map (self-organizing map)</p></li>\\r\\n<li><p><strong><em>Social Network Analysis &amp; Visualization</em></strong>: using graph theoretic techniques for e.g., community detection, id of members essential for network health/growth; identify nascent sub-communities; (particular fluency <em>GraphViz</em>, the premiere tool for graph layout/visualization, <em>NetworkX</em>, the primary network analysis library for python, and <em>d3</em>). </p></li>\\r\\n<li><p><strong><em>Analysis &amp; Modeling of Time-Dependent Data</em></strong></p></li>\\r\\n<li><p><strong><em>Optimization</em></strong>: Combinatorial Optimization and Constraint-Satisfaction Programming</p></li>\\r\\n<li><p><strong><em>Numerical Methods</em></strong>: e.g., matrix decomposition, Monte Carlo techniques, Gaussian quadrature, finite difference methods </p></li>\\r\\n<li><p><strong><em>Data Modeling</em></strong> for \"Non-Relational\" systems (in particularly <strong>Redis</strong> and MongoDB) and for relational (ROLAP), multi-dimensional (MOLAP), and hybrid (HOLAP) Data Warehouse systems using conventional relational/SQL servers.</p></li>\\r\\n</ul>\\r\\n\\r\\n<hr/>\\r\\n\\r\\n<p><b>toolchain:</b> </p>\\r\\n\\r\\n<ul>\\r\\n<li>python</li>\\r\\n<li>C</li>\\r\\n<li>cython</li>\\r\\n<li>SciPy + NumPy + Matplotlib</li>\\r\\n<li>javascript</li>\\r\\n<li>R</li>\\r\\n<li>redis</li>\\r\\n<li>riak</li>\\r\\n<li>HDF5 (&amp; pytables, h5py)</li>\\r\\n<li>graphviz</li>\\r\\n<li>node.js</li>\\r\\n<li>flask/werkzeug (python web framework)</li>\\r\\n<li>storm (distributed processing)</li>\\r\\n<li>d3.js (svg template primitives for rendering plots in the browser)</li>\\r\\n<li>git (&amp; gitHub)</li>\\r\\n<li>vagrant</li>\\r\\n</ul>\\r\\n\\r\\n<p>No recruiters</p>\\r\\n',\n",
       " '<p><a href=\"http://www.luigip.com/?p=82\" rel=\"nofollow\">Scala need-to-knows for Java programmers</a></p>\\r\\n\\r\\n<p><a href=\"http://www.luigip.com/?p=133\" rel=\"nofollow\">Game of Life (in 304 characters)</a></p>\\r\\n',\n",
       " '<p>For current updates and a recent bio, please refer to my website.</p>\\r\\n\\r\\n<p><strong>Nominated for reopen:</strong></p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://stackoverflow.com/questions/6416201/how-to-draw-string-with-background-on-graphics/6416215\">How to draw String with background on Graphics?</a></li>\\r\\n</ul>\\r\\n\\r\\n<p><strong>In the spotlight:</strong></p>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://stackoverflow.com/questions/3883414/is-there-any-reason-that-java-uses-late-static-binding-for-overloaded-methods-in/10596976#10596976\">Is there any reason that Java uses late/static binding for overloaded methods in the same class?</a></li>\\r\\n<li><a href=\"http://stackoverflow.com/questions/6056124/do-interfaces-inherit-from-object-class-in-java/6227003#6227003\">Do interfaces inherit from Object class in java</a></li>\\r\\n</ul>\\r\\n\\r\\n\\r\\n',\n",
       " '<p>Professor of Financial Econometrics at Warwick Business School and advisor to London Select Fund and Old Mutual Asset Managers in London. Interested in all areas of Stats, Econometrics and Finance.</p>\\r\\n',\n",
       " '<p>Contract financial developer at a large bank in Charlotte, NC.</p>\\r\\n',\n",
       " '\"The objects with which it deals are absolute numbers and measurable quantities which, though themselves unknown, are related to “things” which are known, whereby the determination of the unknown quantities is possible.\"<br/>\\\\\\\\n<i>Omar ibn Ibrahim Al-Nisaburi Khayyami</i>',\n",
       " '<p>Spring, Groovy, and JavaScript web applications for science data. </p>\\r\\n\\r\\n<p>Interested in NoSQL, Scala, interactive data, and mobile development.</p>\\r\\n',\n",
       " '<p>Find me on Twitter @endy_tj</p>\\r\\n',\n",
       " '<p>I am a PhD student at the University of Sussex studying Computational Linguistics. My research topic is text classification in imbalanced data streams.</p>\\r\\n',\n",
       " '<p>Ph.D. student in Mathematics at the University of Chicago</p>\\r\\n',\n",
       " '<p>.Net Programmer in Mobile/Web/Desktop App.</p>\\r\\n\\r\\n<p>Dynamic person and strongly aimed in objectives achieving. \\r\\nExcellent priorities sensibility and time management. \\r\\nVery interested to increasing knowledge.</p>\\r\\n',\n",
       " '<p>I build robots.</p>\\r\\n',\n",
       " '<p>I like turtles. email == firstname@lastname.ca</p>\\r\\n',\n",
       " '<p>Software developer specialized on image processing with a strong interest for machine learning and statistics</p>\\r\\n',\n",
       " '<p>I\\'m a software developer at <a href=\"http://rentrak.com/\" rel=\"nofollow\">Rentrak</a> - a television viewership measurement company. At work, I mainly use Perl and C++. At home, languages I\\'ve enjoyed using include:</p>\\r\\n\\r\\n<ul>\\r\\n<li>Haskell</li>\\r\\n<li>A variety of .NET languages such as Boo and especially F#</li>\\r\\n<li>Coq</li>\\r\\n<li>Mathematica</li>\\r\\n</ul>\\r\\n',\n",
       " '<p>ayakalam@gmail.com</p>\\r\\n',\n",
       " '<p>I have a lot questions ...</p>\\r\\n',\n",
       " '<p>student of bioinformatics. </p>\\r\\n\\r\\n<p>working at leibniz institute for crop plant reasearch and plant genetics. doing assemblys all day. :D </p>\\r\\n\\r\\n<p>interessted in programming languages. specially in scala. </p>\\r\\n',\n",
       " '<p>I am a software engineer in Pittsburgh, PA with a focus on architecture, design and development of solutions in both Java and .NET technologies.</p>\\r\\n\\r\\n<p>I recently helped a friend build a website for a business he is starting. <a href=\"http://www.championkitchens.com\" rel=\"nofollow\">http://www.championkitchens.com</a></p>\\r\\n',\n",
       " '<p>University of Coruña, Spain</p>\\r\\n',\n",
       " '<p>let it be!</p>\\r\\n',\n",
       " '<p>I\\'m an entrepreneur and computer scientist, with a particular interest in Artificial Intelligence and Peer-to-Peer.  My two most notable projects are <a href=\"http://freenetproject.org/\" rel=\"nofollow\">Freenet</a> and <a href=\"http://revver.com/\" rel=\"nofollow\">Revver</a> (I\\'m founder and co-founder respectively).  My current projects are a predictive analytics system called <a href=\"http://sensearray.com\" rel=\"nofollow\">SenseArray</a>, and a new approach to distributed computation called <a href=\"http://code.google.com/p/swarm-dpl/\" rel=\"nofollow\">Swarm</a>.  You can find my personal blog <a href=\"http://blog.locut.us/\" rel=\"nofollow\"> here</a>.\\\\\\\\n</p>\\\\\\\\n<p>\\\\\\\\nWhile I\\'ve used C, C++, ML, Haskell, Prolog, Python, even Perl in the past, these days I do most of my programming in Java.\\\\\\\\n\\\\\\\\nI am gaining experience with Scala though and expect to become my primary language as it and its tools mature.  I was honored to be asked by Scala\\'s creator to be on the program committee for the first Scala workshop.  \\\\\\\\n</p>',\n",
       " '<p>Long time SPSS designer, developer, statistician and strategy person.  Python fan.  Serious recreational cyclist</p>\\r\\n',\n",
       " '<p>Yet another programmer...</p>\\r\\n',\n",
       " \"<p>I'm an assistant professor in computational biology with a machine learning slant.  See my lab website for more details.</p>\\r\\n\",\n",
       " '<p>rerum cognoscere causas</p>\\r\\n',\n",
       " '<p>I work with animals and like it that way.</p>\\r\\n',\n",
       " '<p>I am currently a Ph.D. candidate in the labs of Dr. Graham Bell and Dr. Gregor Fussmann, Biology Department, McGill University.  I am studying the evolutionary and ecological response of phytoplankton to rising atmospheric CO2.\\r\\nI have experience working with R, teaching R and using R to teach statistics. <a href=\"http://www.meetup.com/Montreal-R-User-Group/\" rel=\"nofollow\">Co founder of the Montreal R User group</a>.\\r\\n<a href=\"http://ca.linkedin.com/in/etiennedecarie\" rel=\"nofollow\">I can be found on LinkedIn.</a></p>\\r\\n',\n",
       " \"<p>I'm a developer and project administrator for the Maxima project. I have a lot of experience in programming and some in mathematics. I have a special interest in Bayesian inference, among many other topics.</p>\\r\\n\",\n",
       " '<blockquote>\\r\\n  <p><sup>The algorithm designer who does not run experiments risks becoming lost in abstraction. <em>—Sedgewick</em></sup></p>\\r\\n</blockquote>\\r\\n\\r\\n<ul>\\r\\n<li><a href=\"http://chat.stackexchange.com/rooms/9446/theory-salon\">cstheory salon</a> &amp; <a href=\"http://chat.stackexchange.com/rooms/2710/computer-science\">cs chat</a></li>\\r\\n<li><a href=\"http://vzn1.wordpress.com/2012/12/08/outline-for-a-np-vsppoly-proof-based-on-monotone-circuits-hypergraphs-and-factoring/\" rel=\"nofollow\">outline for a NP vs P/poly proof based on monotone circuits, hypergraphs, factoring, and slice functions</a></li>\\r\\n<li><a href=\"http://vzn1.wordpress.com/code/collatz-conjecture-experiments/\" rel=\"nofollow\">Collatz conjecture experiments</a></li>\\r\\n<li><a href=\"http://vzn1.wordpress.com/code/turing-machine-compiler/\" rel=\"nofollow\">Turing machine compiler in ruby</a></li>\\r\\n<li><a href=\"http://vzn1.wordpress.com/volunteer/\" rel=\"nofollow\">volunteer/collaborate in open science</a></li>\\r\\n</ul>\\r\\n',\n",
       " \"<p>Hi, I'm Tim Jurka. I'm a researcher at the University of California, Davis, where I use methods from computer science, statistics, and population ecology to understand political systems.</p>\\r\\n\",\n",
       " 'engineer / techie / and nerd\\\\\\\\n\\\\\\\\na pianist whose only ineptitude on the instrument is only superceded by his ineptitude on the guitar - and can someone please send me a dictionary - my spelling is horrible!',\n",
       " '<p>Post-doctoral scholar at UC Berkeley, currently visiting at Duke</p>\\r\\n',\n",
       " '<p>Astrophysics grad student</p>\\r\\n',\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " \"<p>We have a dataset resulting from an experiment with a genotype A and a genotype B, which underwent either treatment X or no treatment Y, for four total sample groups. Each sample group had only three replicates. The measurements are cell counts. In a two-way ANOVA, we found a significant difference between treatment and non-treatment, and did not find a significant difference between the genotypes. That's all good. </p>\\n\\n<p>The issue is that apparently we expected the non-treatment groups to have values of zero- and they're not. A t-test shows that the difference between groups (A,Y) and (B,Y) to be not significant. I've been requested to determine if the non-zero values of those groups are statistically significantly different from zero or not. While it doesn't seem to make a whole lot of sense to me to do this, I need to provide something, and I'm not sure of an appropriate test that can be done in this case. If I pool all the values for Y, I get a mean of 36, SD of 24 and 95% CI of 19, so the CI doesn't even overlap zero. Is there an acceptable way of answering this statistically, or should I let them know that it's not exactly an appropriate question?</p>\\n\",\n",
       " \"<p>Using Cohen's $d$, I am getting small and medium effect sizes for results that are statistically non-significant ($p&gt;.05$). Does this make sense?</p>\\n\",\n",
       " '<p>AIC is a measure of how well the data is explained by the model corrected for how complex the model is.</p>\\n',\n",
       " '<p>Yes, unless you use a parametric approach and are willing to extrapolate.\\nSee <a href=\"http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#lifereg_toc.htm\" rel=\"nofollow\">SAS Lifereg.</a></p>\\n',\n",
       " '<p>What are some good papers describing <em>applications</em> of statistics that would be fun and informative to read? Just to be clear, I\\'m not really looking for papers describing new statistical methods (e.g., a paper on least angle regression), but rather papers describing how to solve real-world problems.</p>\\n\\n<p>For example, one paper that would fit what I\\'m looking is the climate paper from the <a href=\"http://meta.stats.stackexchange.com/questions/685/second-cross-validated-journal-club\">second Cross-Validated Journal Club</a>. I\\'m kind of looking for more statistics-ish papers, rather than machine learning papers, but I guess it\\'s kind of a fuzzy distinction (I\\'d classify the Netflix Prize papers as a bit borderline, and a paper on sentiment analysis as something I\\'m <em>not</em> looking for).</p>\\n\\n<p>I\\'m asking because most of the applications of statistics I\\'ve seen are either the little snippets you seen in textbooks, or things related to my own work, so I\\'d like to branch out a bit.</p>\\n',\n",
       " '<p>Try the h=2*52 and see how that works vs the \\'present tense\" points you do have.  See how far (or not far) off the error is?  Might even try taking h=(however many weeks till your present data) then add in the present data then do h=(however long till the next present data point) so on and so forth till you have added all of the present data.  Then compare that vs just the h=2*52+weeks_ahead to find the better of the two models?</p>\\n',\n",
       " '<p>(I know it is not quite appropriate to quote it as \"part 2\", but since the question has been dormant for quite a while, I hope by doing this will rise peoples\\' attention again, you may have a look of part I <a href=\"http://stats.stackexchange.com/questions/1228/how-to-interpret-a-control-chart-containing-a-majority-of-zero-values\">here</a>.)</p>\\n\\n<p>I have come across an article online talking about the case similar to mine, that most of the time the case count is zero, make sometimes when the case number increases to one, it already shoots above the control level and consider the case as \"out of control\".</p>\\n\\n<p>Since c-chart will be easier for my bosses to read and interpret, I wonder if the method is sound, or did anyone have some more official reference on this method? (I have googled quite a while but I can find nothing)</p>\\n\\n<p>The article can be found <a href=\"http://www.spcforexcel.com/small-sample-case-for-c-and-u-control-charts\" rel=\"nofollow\">here</a>.</p>\\n\\n<p>An to further my question, I want to ask one more thing: for the assumption of c-chart that the case count needs to follow a Poisson distribution, is it applicable to all lambda (i.e. mean of case count)?</p>\\n\\n<p>Thanks again.</p>\\n',\n",
       " '<p>Norman Matloff has written a mathematical statistics <a href=\"http://heather.cs.ucdavis.edu/probstatbook\">textbook</a> for computer science students that\\'s free.  Kind of a niche market, I suppose.  For what it\\'s worth, I haven\\'t read it, but Matloff has a Ph.D. in mathematical statistics, works for a computer science department, and wrote a really good R book, that I recommend for people who want to go to the next stage of programming R better (as opposed to just fitting models with canned functions).</p>\\n',\n",
       " '<p>You asked \"If other methodologies are more appropriate I’m happy to here about them.\" A generalized ARIMA can be easily expressed as a lagged auto-regression ADL or PDL . THis model easily adpats to changes in levels, trends , parameters , seasonal pulses , variability. You are assuming a structure rather than allowing the data to suggest the structure.</p>\\n',\n",
       " '<p>I am doing a regression analysis which troubled me. </p>\\n\\n<p>My independent variable are 4 interplanetary condition components, and the dependent variable is the latitude of auroral oval boundary. \\nSo far, the specific relationship is still unknown in physical principle, what we want to do is to get a model (function expression) from the massive data which shows how these independent variables affect the dependent variable.</p>\\n\\n<p>I used the Matlab statistical toolbox to do the regression analysis, but the results were very bad. The p values of the F statistic and t statistic are very small, but the R² is also very low, about 20%. </p>\\n\\n<p>So how should I improve the R²? Are there good methods? I see that SVM (or LS-SVM) can do regression anaysis, is it a good way to manage the massive data, multiple independent variables regression anaysis?</p>\\n\\n<p>The following are the results:</p>\\n\\n<pre><code>mdl = \\nLinear regression model:\\n    y ~ 1 + x1*x2 + x1*x3 + x1*x4 + x2*x3 + x2*x4 + x1^2 + x2^2 + x3^2 + x4^2\\n\\nNumber of observations: 18471, Error degrees of freedom: 18457    \\nRoot Mean Squared Error: 2.44  \\nR-squared: 0.225,  Adjusted R-Squared 0.225  \\nF-statistic vs. constant model: 413, p-value = 0  \\n</code></pre>\\n\\n<p>when we add another predictor, i.e., the independent variables become 5, the resuts of the regression analysis are:</p>\\n\\n<p>Linear regression model:</p>\\n\\n<pre><code>y ~ 1 + x1*x2 + x1*x3 + x1*x4 + x1*x5 + x2*x3 + x2*x4 + x2*x5 + x3*x5 + x2^2 + x3^2 + x4^2 + x5^2\\n</code></pre>\\n\\n<p>Number of observations: 18457, Error degrees of freedom: 18439\\nRoot Mean Squared Error: 2.21\\nR-squared: 0.366,  Adjusted R-Squared 0.366\\nF-statistic vs. constant model: 627, p-value = 0</p>\\n',\n",
       " '<p>Take a look at the math section at <a href=\"http://econphd.econwiki.com/books.htm\" rel=\"nofollow\">http://econphd.econwiki.com/books.htm</a>. I would also add the two measure theory books from the econometrics section.</p>\\n',\n",
       " '<p>This question is motivated by <a href=\"http://stats.stackexchange.com/q/30294/183\">my question on meta-analysis</a>. But I imagine that it would also be useful in teaching contexts where you want to create a dataset that exactly mirrors an existing published dataset.</p>\\n\\n<p>I know how to generate random data from a given distribution. So for example, if I read about the results of a study that had:</p>\\n\\n<ul>\\n<li>a mean of 102,</li>\\n<li>a standard deviation of 5.2 , and </li>\\n<li>a sample size of 72.</li>\\n</ul>\\n\\n<p>I could generate similar data using <code>rnorm</code> in R. For example, </p>\\n\\n<pre><code>set.seed(1234)\\nx &lt;- rnorm(n=72, mean=102, sd=5.2)\\n</code></pre>\\n\\n<p>Of course the mean and SD would not be exactly equal to 102 and 5.2 respectively:</p>\\n\\n<pre><code>round(c(n=length(x), mean=mean(x), sd=sd(x)), 2)\\n##     n   mean     sd \\n## 72.00 100.58   5.25 \\n</code></pre>\\n\\n<p>In general I\\'m interested in how to simulate data that satisfies a set of constraints. In the above case, the constaints are sample size, mean, and standard deviation. In other cases, there might be additional constraints. For example, </p>\\n\\n<ul>\\n<li>a minimum and a maximum in either the data or the underlying variable might be known.</li>\\n<li>the variable might be known to take on only integer values or only non-negative values.</li>\\n<li>the data might include multiple variables with known inter-correlations.</li>\\n</ul>\\n\\n<h3>Questions</h3>\\n\\n<ul>\\n<li><strong>In general, how can I simulate data that exactly satisfies a set of constraints?</strong></li>\\n<li><strong>Are there articles written about this? Are there any programs in R that do this?</strong></li>\\n<li><strong>For the sake of example, how could and should I simulate a variable so that it has a specific mean and sd?</strong></li>\\n</ul>\\n',\n",
       " \"<p>I think you are assuming something like this. People use health care if they are sick. If they use health care, than they spend more money on it. But, sick people will see more value on insurance and will be more likey to be insured. So, you will find out that insured people spends more money on health care, but this may be so due to selection bias.</p>\\n\\n<p>And you seem to think that, if you use only the subsample of people who used health care, than you don't have this problem. However, think of this example. Assume you have a woman who wants to be pregnant and a man who doesn't want to have a child, both on their 30's. The woman than gets insured, but the man not. Both get sick and use health care, so they both are in your subsample. However, they have different probabilities of being insured and they will spend different amount of money if she is pregnant. Thus selection bias will occur anyway.</p>\\n\\n<p>So, what I'm saying is that restricting your subsample will not solve the selection bias problem. You still have to account for the different probabilities of people getting insurance.</p>\\n\",\n",
       " '<p>Sounds like you are looking for something like relative risk or first differences. If so, calculate the predicted probability of observing a 1 with everything but your variable of interest set at its mean or median and your treatment at a low (or high) value, giving something like this</p>\\n\\n<pre><code>RR_rs = Pr(Y1 =r,Y2 =s|x1) / Pr(Y1 =r,Y2 =s|x)\\n</code></pre>\\n\\n<p>Or first differences</p>\\n\\n<pre><code>FD_rs =Pr(Y1 =r,Y2 =s|x1)−Pr(Y1 =r,Y2 =s|x).\\n</code></pre>\\n\\n<p>I suppose looking at some type of information criteria might partially get you what you want as well. </p>\\n',\n",
       " '<p>Use Factor Analysis (FA) instead, <a href=\"http://onlinelibrary.wiley.com/doi/10.1111/1468-0262.00273/abstract\" rel=\"nofollow\">Bai and Ng</a>, Econometrica 2002 to start with. FA sorts out much fewer PCs.</p>\\n',\n",
       " '<pre><code> names(mydat)[c(name)]&lt;-c(\"newname\") \\n</code></pre>\\n\\n<p>From this, I know that the column/variable name \"name\" of the data frame mydat is replaced with \"newname\".</p>\\n\\n<p>My question is if, I want to do this by a loop so that I will have some thing like:</p>\\n\\n<p>newname1 newname2 newname3 newname4 and so on, how do I do it?</p>\\n\\n<p>This is what did and it did not work:</p>\\n\\n<pre><code>for(i in 1:4){\\nnames(mydat)[c(name)]&lt;-c(\"newname\"i)\\n}\\n</code></pre>\\n\\n<p>Is there a way to code this?\\nmany thanks to all who could be of help.\\nOwusu Isaac</p>\\n',\n",
       " '<p>I try to estimate survival curves based on a Kaplan-Meier estimator using <code>proc lifetest</code>. However, SAS outputs an error message which I do not manage to circumvent. Can you help me? The data set is available <a href=\"http://paste.pocoo.org/raw/565353/\" rel=\"nofollow\">here</a>. </p>\\n\\n<p><strong>My code</strong></p>\\n\\n<pre><code>filename work_di \"TO BE COMPLETED\";\\n  data data;\\n  infile work_di(\"data.txt\") dlm=\"09\"x firstobs=2;\\n  input id centre time event x $;\\nrun;\\n\\nproc lifetest data=data method=KM plots=none;\\n  time time * event(0);\\n  by centre x;\\n  ods output ProductLimitEstimates = surv;\\nrun;\\n</code></pre>\\n\\n<p><strong>Error message</strong></p>\\n\\n<pre><code>NOTE: The LOGLOG transform is used to compute the confidence limits for the quartiles  of the\\n  survivor distribution. To suppress using this transform, specify CONFTYPE=LINEAR in the\\n  PROC LIFETEST statement.\\n  ERROR: Floating Point Overflow.\\n  NOTE: The data set WORK.SURV has 77 observations and 10 variables.\\n  ERROR: Termination due to Floating Point Exception\\n  NOTE: The SAS System stopped processing this step because of errors.\\n  NOTE: PROCEDURE LIFETEST used (Total process time):\\n  real time           0.09 seconds\\n  cpu time            0.03 seconds\\n</code></pre>\\n',\n",
       " '<p>We have two time series: $X_t$ and $R_t$, and a model saying that $R_{t+1} = (\\\\\\\\mu(X_t) - \\\\\\\\frac{1}{2}\\\\\\\\sigma^2(X_t))\\\\\\\\Delta T + \\\\\\\\sigma(X_t) \\\\\\\\sqrt{\\\\\\\\Delta T} \\\\\\\\epsilon_t$,\\nwhere $\\\\\\\\Delta T$ is given constant and $\\\\\\\\epsilon_t$-s are independent normally distributed with zero mean and unit variance. Further we assume that the functions $\\\\\\\\mu(x)$ and $\\\\\\\\sigma(x)$ are linear for simplicity. I would like to use some standard method (MLE comes to my mind) to estimate parameters of functions $\\\\\\\\mu(x)$ and $\\\\\\\\sigma(x)$, but I am not sure how to do this. </p>\\n\\n<p>I would be grateful for detailed answers, because I am not really experienced with statistics.</p>\\n',\n",
       " '<p>What you are doing in your first code block is indeed equivalent to box-constrained optimisation.  Here\\'s some sample code, with some unnecessary output removed to save space:</p>\\n\\n<pre><code>&gt; foo.unconstr &lt;- function(par, x) -sum(dnorm(x, par[1], par[2], log=TRUE))\\n&gt; \\n&gt; foo.constr &lt;- function(par, x)\\n+ {\\n+   ll &lt;- NA\\n+   if (par[1] &gt; 0 &amp;&amp; par[1] &lt; 5 &amp;&amp; par[2] &gt; 0 &amp;&amp; par[2] &lt; 5)\\n+   {\\n+     ll &lt;- -sum(dnorm(x, par[1], par[2], log=TRUE))\\n+   }\\n+   ll\\n+ }\\n&gt; \\n&gt; x &lt;- rnorm(100,1,1)\\n&gt; par &lt;- c(1,1)\\n&gt; optim(par, foo.constr, x=x)\\n$par\\n[1] 1.147690 1.077712\\n\\n$value\\n[1] 149.3724\\n\\n&gt; \\n&gt; par &lt;- c(1,1)\\n&gt; optim(par, foo.unconstr, lower=c(0,0), upper=c(5,5), method=\"L-BFGS-B\", x=x)\\n$par\\n[1] 1.147652 1.077654\\n\\n$value\\n[1] 149.3724\\n</code></pre>\\n\\n<p>They won\\'t give quite the same answers, because they are different algorithms. </p>\\n\\n<p>I\\'ll answer your constrOptim question over there, so other people who might be interested will see it. </p>\\n',\n",
       " '<p>I have programmed this from scratch once a few years ago, and I have a Matlab file for doing piece-wise linear regression on my computer. About 1 to 4 breakpoints is computationally possible for about 20 measurements points or so. 5 or 7 break points starts to be really too much.</p>\\n\\n<p>The pure mathematical approach as I see it is to try all possible combinations as suggested by user mbq in the question linked to in the comment below your question.</p>\\n\\n<p>Since the fitted lines are all consecutive and adjacent (no overlaps) the combinatorics will follow Pascals triangle. If there were overlaps between used data points by the line segments I believe that the combinatorics would follow Stirling numbers of the second kind instead.</p>\\n\\n<p>The best solution in my mind is to choose the combination of fitted lines that has the lowest standard deviation of the R^2 correlation values of the fitted lines. I will try to explain with an example. Keep in mind though that asking how many break points one should find in the data, is similar to asking the question \"How long is the coast of Britain?\" as in one of Benoit Mandelbrots (a mathematician) papers about fractals. And there is a trade-off between number of break points and regression depth.</p>\\n\\n<p>Now to the example.</p>\\n\\n<p>Suppose we have the perfect data $y$ as a function of $x$ ($x$ and $y$ are integers):</p>\\n\\n<p>$$\\\\\\\\begin{array}{|c|c|c|c|c|c|}\\n\\\\\\\\hline &amp;x &amp;y &amp;R^2 line 1 &amp;R^2 line 2 &amp;sum of R^2 values &amp;standard deviation of R^2 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;1 &amp;1 &amp;1,000 &amp;0,0400 &amp;1,0400 &amp;0,6788 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;2 &amp;2 &amp;1,000 &amp;0,0118 &amp;1,0118 &amp;0,6987 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;3 &amp;3 &amp;1,000 &amp;0,0004 &amp;1,0004 &amp;0,7067 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;4 &amp;4 &amp;1,000 &amp;0,0031 &amp;1,0031 &amp;0,7048 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;5 &amp;5 &amp;1,000 &amp;0,0135 &amp;1,0135 &amp;0,6974 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;6 &amp;6 &amp;1,000 &amp;0,0238 &amp;1,0238 &amp;0,6902 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;7 &amp;7 &amp;1,000 &amp;0,0277 &amp;1,0277 &amp;0,6874 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;8 &amp;8 &amp;1,000 &amp;0,0222 &amp;1,0222 &amp;0,6913 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;9 &amp;9 &amp;1,000 &amp;0,0093 &amp;1,0093 &amp;0,7004 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;10 &amp;10 &amp;1,000 &amp;-1,978 &amp;1,000 &amp;0,7071 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;11 &amp;9 &amp;0,9709 &amp;0,0271 &amp;0,9980 &amp;0,6673 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;12 &amp;8 &amp;0,8951 &amp;0,1139 &amp;1,0090 &amp;0,5523 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;13 &amp;7 &amp;0,7734 &amp;0,2558 &amp;1,0292 &amp;0,3659 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;14 &amp;6 &amp;0,6134 &amp;0,4321 &amp;1,0455 &amp;0,1281 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;15 &amp;5 &amp;0,4321 &amp;0,6134 &amp;1,0455 &amp;0,1282 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;16 &amp;4 &amp;0,2558 &amp;0,7733 &amp;1,0291 &amp;0,3659 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;17 &amp;3 &amp;0,1139 &amp;0,8951 &amp;1,0090 &amp;0,5523 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;18 &amp;2 &amp;0,0272 &amp;0,9708 &amp;0,9980 &amp;0,6672 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;19 &amp;1 &amp;0 &amp;1,000 &amp;1,000 &amp;0,7071 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;20 &amp;2 &amp;0,0094 &amp;1,000 &amp;1,0094 &amp;0,7004 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;21 &amp;3 &amp;0,0222 &amp;1,000 &amp;1,0222 &amp;0,6914 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;22 &amp;4 &amp;0,0278 &amp;1,000 &amp;1,0278 &amp;0,6874 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;23 &amp;5 &amp;0,0239 &amp;1,000 &amp;1,0239 &amp;0,6902 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;24 &amp;6 &amp;0,0136 &amp;1,000 &amp;1,0136 &amp;0,6974 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;25 &amp;7 &amp;0,0032 &amp;1,000 &amp;1,0032 &amp;0,7048 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;26 &amp;8 &amp;0,0004 &amp;1,000 &amp;1,0004 &amp;0,7068 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;27 &amp;9 &amp;0,0118 &amp;1,000 &amp;1,0118 &amp;0,6987 \\\\\\\\\\\\\\\\\\n\\\\\\\\hline &amp;28 &amp;10 &amp;0,04 &amp;1,000 &amp;1,04 &amp;0,6788 \\\\\\\\\\\\\\\\\\n \\\\\\\\hline  \\\\\\\\end{array}$$</p>\\n\\n<p>These y values have the graph:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/dNTHb.png\" alt=\"idealized data\"></p>\\n\\n<p>Which clearly has two break points. For the sake of argument we will calculate the R^2 correlation values (with the Excel cell formulas (European dot-comma style)):</p>\\n\\n<pre><code>=INDEX(LINEST(B1:$B$1;A1:$A$1;TRUE;TRUE);3;1)\\n=INDEX(LINEST(B1:$B$28;A1:$A$28;TRUE;TRUE);3;1)\\n</code></pre>\\n\\n<p>for all possible non-overlapping combinations of <em>two</em> fitted lines. All the possible pairs of R^2 values have the graph:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/jCuqf.png\" alt=\"R^2 values\"></p>\\n\\n<p>The question is which pair of R^2 values should we choose, and how do we generalize to multiple break points as asked in the title? One choice is to pick the combination for which the sum of the R-square correlation is the highest. Plotting this we get the upper blue curve below:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/wU4tz.png\" alt=\"sum of R squared and standard deviation of R squared\"></p>\\n\\n<p>The blue curve, the sum of the R-squared values, is the highest in the middle. This is more clearly visible from the table with the value $1,0455$ as the highest value. \\nHowever it is my opinion that the minimum of the red curve is more accurate. That is, the minimum of the standard deviation of the R^2 values of the fitted regression lines should be the best choice.</p>\\n\\n<p><a href=\"http://mobiusfunction.wordpress.com/2012/06/26/piece-wise-linear-regression-from-two-dimensional-data-multiple-break-points/\" rel=\"nofollow\">Piece wise linear regression - Matlab - multiple break points</a></p>\\n',\n",
       " '<p>Try <a href=\"http://www.foolabs.com/xpdf/download.html\" rel=\"nofollow\">xpdf</a>. It does a good job of extracting text from a pdf document.</p>\\n',\n",
       " '<p>Yes, you should absolutely compare your predicted values with actual values.  This is good practice with any kind of statistical modeling, not just time series analysis.</p>\\n\\n<p>If certain months are consistently off, you should use a seasonal model.</p>\\n',\n",
       " '<p>I think you explained well why the probability of the given tree is 0.5 if its topology does not count. Looking at the formula (2) superficially, I find it hard to imagine the definition of isomorphism under which it would work (only leaves can be rearranged?), though perhaps the trick is in finding the right definition of the counting function f.</p>\\n\\n<p>For the general case I would try to write a recursive formula, and I think the binomial coefficient should pop uo in it.</p>\\n',\n",
       " '<p>I have some items described by 43 categories like this:</p>\\n\\n<pre><code>Dataset Item Category1 Category2...               Category43\\nD1      Item1 1        0              0    ...    1 ...\\nD1      Item3 1        0              0    ...    1 ...\\nD2      Item4 1        0              0    ...    1 ...\\n..\\n</code></pre>\\n\\n<p>What I did was to create a frequency table like this</p>\\n\\n<pre><code>Dataset Category1 Category2...               Category43\\nD1        617       388    ...                 827\\nD2        1234      7272   ...                 1237\\n</code></pre>\\n\\n<p>I am testing to see if there is a relationship between the dataset type and the category frequency counts.</p>\\n\\n<p>I have the following data as the output of <code>dput</code>:</p>\\n\\n<pre><code>structure(list(data.OldFrequency = c(617L, 388L, 6L, 9L, 1344L, \\n857L, 30L, 63L, 60L, 22L, 23L, 107L, 9L, 16L, 9L, 10L, 14L, 28L, \\n9L, 174L, 245L, 103L, 4096L, 121L, 6L, 48L, 189L, 33L, 1426L, \\n64L, 16L, 135L, 77L, 26L, 110L, 44L, 75L, 1610L, 1022L, 38L, \\n1578L, 242L, 67L), data.NewFrequency = c(1220L, 959L, 307L, 29L, \\n5093L, 771L, 65L, 125L, 120L, 41L, 187L, 203L, 11L, 87L, 20L, \\n159L, 45L, 68L, 60L, 11L, 644L, 51L, 7053L, 159L, 6L, 162L, 208L, \\n52L, 3277L, 27L, 594L, 79L, 95L, 119L, 96L, 84L, 180L, 2991L, \\n2227L, 34L, 2249L, 37L, 29L)), .Names = c(\"data.OldFrequency\", \\n\"data.NewFrequency\"), row.names = c(NA, -43L), class = \"data.frame\")\\n</code></pre>\\n\\n<p>Running <code>chisq.test</code> using this gives me the following:</p>\\n\\n<pre><code>Pearson\\'s Chi-squared test\\n\\ndata:  d \\nX-squared = 2551.405, df = 42, p-value &lt; 2.2e-16\\n\\nWarning message:\\nIn chisq.test(d) : Chi-squared approximation may be incorrect\\n</code></pre>\\n\\n<p>I am confused on what null hypothesis this is testing and what the implications of this are. Can someone please help me understand how to interpret this? I am not a statistician and would love if someone could explain this in simple words. And how would I fix the warning message?</p>\\n',\n",
       " '<p>Here are a few observations to add to the existing answers.\\nI think it\\'s important to think through conceptually why you are getting a group with zero variance.</p>\\n\\n<h3>Floor and ceiling effects</h3>\\n\\n<p>In my experience in psychology, this example comes up most often when there is a floor or ceiling on a scale, and you have some groups that fall in the middle of the scale and others who fall on the extreme. For example, If your dependent variable is proportion of items correct out of five questions, then you might find that your \"smart\" group gets 100% correct or that your \"clinical group\" gets 0% correct. </p>\\n\\n<p>In this case:</p>\\n\\n<ul>\\n<li>You might want to fall back on ordinal non-parametric tests if you have no  variance in one of your groups. </li>\\n<li>Although it may not help you after the fact, you may also want to think conceptually about whether a different measure that did not have floor or ceiling effects would have been better to use. In some cases it wont matter. For example, the point of the analysis may have been to show that one group could perform a task and another could not. In other cases, you may want to model individual differences in all groups, in which case you may need a scale that does not suffer from floor or ceiling effects.</li>\\n</ul>\\n\\n<h3>Very small group size</h3>\\n\\n<p>Another case where you can get no group variance is where you have a group with a really small sample size (e.g., $n\\\\\\\\lt5$), usually in combination with a dependent variable that is fairly discrete.</p>\\n\\n<p>In this case, you may be more inclined to put the lack of variance down to chance, and proceed with a standard t-test.</p>\\n',\n",
       " \"<p>Let $X_1, X_2...X_n$ be iid with $f(x,\\\\\\\\theta)=\\\\\\\\dfrac{2x}{\\\\\\\\theta^2}$ and $0&lt;x\\\\\\\\leq\\\\\\\\theta$. \\nFind $c$ such that $\\\\\\\\mathbb{E}(c\\\\\\\\hat{\\\\\\\\theta})=\\\\\\\\theta$ where $\\\\\\\\hat{\\\\\\\\theta}$ denotes MLE of $\\\\\\\\theta$.</p>\\n\\n<p>What I have tried:\\nI found the MLE of $f(x;\\\\\\\\theta)$ to be $\\\\\\\\max\\\\\\\\{X_1,X_2\\\\\\\\cdots X_n\\\\\\\\}$ (which aligns with the answer at the back) but now I am stuck at this question.\\nThe answer given is $\\\\\\\\dfrac{2n+1}{2n}$.</p>\\n\\n<p>I would have proceeded as:\\n$$\\\\\\\\begin{align*}\\n\\\\\\\\mathbb{E}(c\\\\\\\\hat{\\\\\\\\theta})&amp;=\\\\\\\\theta\\\\\\\\\\\\\\\\\\n\\\\\\\\int_0^\\\\\\\\theta c \\\\\\\\dfrac{2x}{y^2} &amp;=\\\\\\\\theta \\\\\\\\quad (y = \\\\\\\\max\\\\\\\\{x_1,x_2,\\\\\\\\cdots,x_n\\\\\\\\})\\\\\\\\\\\\\\\\\\n\\\\\\\\dfrac{1}{y^2}\\\\\\\\int_0^\\\\\\\\theta c .{2x}{} &amp;=\\\\\\\\theta\\n\\\\\\\\end{align*}$$\\nBut continuing this way gives me an answer far off from the one in the book (I don't have a term of n to begin with).</p>\\n\\n<p>Help! Hints only please, no complete solutions.</p>\\n\",\n",
       " '<p>I have an unbalanced data and I want to use LIBSVM from MATLAB. LIBSVM has two different parameters for cost: -c and -wi. What is the difference between them?</p>\\n',\n",
       " \"<p>I'm trying to minimize a custom function. It should accept five parameters and the data set and do all sorts of calculations, producing a single number as an output. I want to find a combination of five input parameters which yields smallest output of my function.</p>\\n\",\n",
       " '<p>Here\\'s some more info: <a href=\"http://doingbayesiandataanalysis.blogspot.com/2012/07/sampling-distributions-of-t-when.html\">http://doingbayesiandataanalysis.blogspot.com/2012/07/sampling-distributions-of-t-when.html</a> </p>\\n\\n<p>A more complete discussion is provided here: <a href=\"http://www.indiana.edu/~kruschke/BEST/\">http://www.indiana.edu/~kruschke/BEST/</a>  That article considers p values for stopping at threshold N, stopping at threshold duration, and stopping at threshold t value.</p>\\n',\n",
       " \"<p>Let's say we are repeatedly tossing a fair coin, and we know number of heads and tails should be roughly equal. When we see a result like 10 heads and 10 tails for a total of 20 tosses, we believe the results and are inclined to believe the coin is fair.</p>\\n\\n<p>Well when you see a result like 10000 heads and 10000 tails for a total of 20000 tosses, I actually would question the validity of the result (did the experimenter fake the data), as I know this is more unlikely than, say a result of 10093 heads and 9907 tails.</p>\\n\\n<p>What is the statistical argument behind my intuition?</p>\\n\",\n",
       " \"<p>Support for matrix algebra. The vast majority of practiced statistics is multivariate and involves matrices, and often simplifying matrix forms requires special rules that aren't easily translated from a univariate case, so good matrix support would be really helpful.</p>\\n\",\n",
       " '<p>I’m reading R. Kreps\\' paper <a href=\"http://www.casact.org/pubs/proceed/proceed97/97553.pdf\" rel=\"nofollow\"><em>Parameter uncertainty in (log)normal distributions</em></a> and trying to figure out how the simulations were done. In order to generate Figure 1, Eqn (2.41) was used. So this is what I did in R:</p>\\n\\n<p>This is the equation they used: $Z_{eff} = v + z\\\\\\\\sqrt{\\\\\\\\frac{n(1+v^2)}{w}}$ where</p>\\n\\n<p>Note: This code is for the case when $n=3$ </p>\\n\\n<pre><code>v &lt;- rt(10000000, 1)/sqrt(3-2)\\nw &lt;- rchisq(10000000,2)\\nz &lt;- rnorm(n=10000000, m=0, sd=1) \\nz_eff_3 &lt;- v + z * sqrt((3*(1+v*v))/w)\\nplot(density(z_eff_3),ylim=c(0,1))\\n</code></pre>\\n\\n<p>However, I got a completely different graph. I would truly appreciate it if someone can take a look and explain to me what I did wrong. </p>\\n',\n",
       " '<p>I am trying to develop a mode choice model (4 modes: hov, transit, bike, walk) and below are two approaches I am using. I am having problems in both</p>\\n\\n<ul>\\n<li><p><strong>Approach 1</strong></p>\\n\\n<p>Mode choice as a function of price (cost of using the mode)</p>\\n\\n<p><code>price</code>: generic variable<br>\\n<code>trans1</code>: choice variable (0,1)<br>\\nDataset: <code>work</code><br>\\nCommand:  </p>\\n\\n<pre><code>&gt; mode.choice &lt;- mlogit(trans1 ~ price, work)\\n\\nError: Error in solve.default(H, g[!fixed]) : \\n  Lapack routine dgesv: system is exactly singular\\n</code></pre>\\n\\n<p>P.S: Unlike previous posts, I don’t have NA’s in my dataset. </p></li>\\n<li><p><strong>Approach 2</strong></p>\\n\\n<p>Mode choice as a function of price and some alternative specific variables</p>\\n\\n<p><code>price</code>: Generic Variable<br>\\n<code>trans1</code>: choice variable (0,1)<br>\\n<code>hh1</code>, <code>hh2</code>, <code>hh3</code>: alternative specific variables<br>\\nDataset: <code>work</code><br>\\nCommand: </p>\\n\\n<pre><code>&gt; mode.choice &lt;- mlogit(trans1 ~ price | hh1 + hh2 + hh3, work)\\n\\nError: Error in solve.default(H, g[!fixed]) : \\n  Lapack routine dgesv: system is exactly singular\\n</code></pre></li>\\n</ul>\\n\\n<p>I have tried different variables in both approaches but the singularity issue persists</p>\\n\\n<p>Help on any of these approaches would be greatly appreciated. </p>\\n',\n",
       " '<p>Two thoughts:</p>\\n\\n<p>A. When I try to get at the essence of \"Hello World\", it\\'s the minimum that must be done in the programming language to generate a valid program that prints a single line of text. That suggests to me that your \"Hello World\" should be a univariate data set, the most basic thing you could plug into a statistical or graphics program.</p>\\n\\n<p>B. I\\'m unaware of any graphing \"Hello World\". The closest I can come is typical datasets that are included in various statistical packages, such as R\\'s AirPassengers. In R, a Hello World graphing statement would be:</p>\\n\\n<pre><code>plot (AirPassengers)  # Base graphics, prints line graph\\n</code></pre>\\n\\n<p>or</p>\\n\\n<pre><code>qplot (AirPassengers) # ggplot2, prints a bar chart\\n</code></pre>\\n\\n<p>or</p>\\n\\n<pre><code>xyplot (AirPassengers) # lattice, which doesn\\'t have a generic plot\\n</code></pre>\\n\\n<p>Personally, I think the simplest graph is a line graph where you have N items in Y and X ranges from 1:N. But that\\'s not a standard.</p>\\n',\n",
       " \"<p>I got completely different results from lmer() and lme()! Just look at the coefficients' std.errors. Completely different in both cases. Why is that and which model is correct?</p>\\n\\n<pre><code>&gt; mix1c = lmer(logInd ~ 0 + crit_i + Year:crit_i + (1 + Year|Taxon), data = datai)\\n&gt; mix1d = lme(logInd ~ 0 + crit_i + Year:crit_i, random = ~ 1 + Year|Taxon, data = datai)\\n&gt; \\n&gt; summary(mix1d)\\nLinear mixed-effects model fit by REML\\n Data: datai \\n       AIC      BIC    logLik\\n  4727.606 4799.598 -2351.803\\n\\nRandom effects:\\n Formula: ~1 + Year | Taxon\\n Structure: General positive-definite, Log-Cholesky parametrization\\n            StdDev       Corr  \\n(Intercept) 9.829727e-08 (Intr)\\nYear        3.248182e-04 0.619 \\nResidual    4.933979e-01       \\n\\nFixed effects: logInd ~ 0 + crit_i + Year:crit_i \\n                 Value Std.Error   DF   t-value p-value\\ncrit_iA      29.053940  4.660176   99  6.234515  0.0000\\ncrit_iF       0.184840  3.188341   99  0.057974  0.9539\\ncrit_iU      12.340580  5.464541   99  2.258301  0.0261\\ncrit_iW       5.324854  5.152019   99  1.033547  0.3039\\ncrit_iA:Year -0.012272  0.002336 2881 -5.253846  0.0000\\ncrit_iF:Year  0.002237  0.001598 2881  1.399542  0.1618\\ncrit_iU:Year -0.003870  0.002739 2881 -1.412988  0.1578\\ncrit_iW:Year -0.000305  0.002582 2881 -0.118278  0.9059\\n Correlation: \\n             crit_A crit_F crit_U crit_W cr_A:Y cr_F:Y cr_U:Y\\ncrit_iF       0                                              \\ncrit_iU       0      0                                       \\ncrit_iW       0      0      0                                \\ncrit_iA:Year -1      0      0      0                         \\ncrit_iF:Year  0     -1      0      0      0                  \\ncrit_iU:Year  0      0     -1      0      0      0           \\ncrit_iW:Year  0      0      0     -1      0      0      0    \\n\\nStandardized Within-Group Residuals:\\n        Min          Q1         Med          Q3         Max \\n-6.98370498 -0.39653580  0.02349353  0.43356564  5.15742550 \\n\\nNumber of Observations: 2987\\nNumber of Groups: 103 \\n&gt; summary(mix1c)\\nLinear mixed model fit by REML \\nFormula: logInd ~ 0 + crit_i + Year:crit_i + (1 + Year | Taxon) \\n   Data: datai \\n  AIC  BIC logLik deviance REMLdev\\n 2961 3033  -1469     2893    2937\\nRandom effects:\\n Groups   Name        Variance   Std.Dev. Corr   \\n Taxon    (Intercept) 6.9112e+03 83.13360        \\n          Year        1.7582e-03  0.04193 -1.000 \\n Residual             1.2239e-01  0.34985        \\nNumber of obs: 2987, groups: Taxon, 103\\n\\nFixed effects:\\n               Estimate Std. Error t value\\ncrit_iA      29.0539403 18.0295239   1.611\\ncrit_iF       0.1848404 12.3352135   0.015\\ncrit_iU      12.3405800 21.1414908   0.584\\ncrit_iW       5.3248537 19.9323887   0.267\\ncrit_iA:Year -0.0122717  0.0090916  -1.350\\ncrit_iF:Year  0.0022365  0.0062202   0.360\\ncrit_iU:Year -0.0038701  0.0106608  -0.363\\ncrit_iW:Year -0.0003054  0.0100511  -0.030\\n\\nCorrelation of Fixed Effects:\\n            crit_A crit_F crit_U crit_W cr_A:Y cr_F:Y cr_U:Y\\ncrit_iF      0.000                                          \\ncrit_iU      0.000  0.000                                   \\ncrit_iW      0.000  0.000  0.000                            \\ncrit_iA:Yer -1.000  0.000  0.000  0.000                     \\ncrit_iF:Yer  0.000 -1.000  0.000  0.000  0.000              \\ncrit_iU:Yer  0.000  0.000 -1.000  0.000  0.000  0.000       \\ncrit_iW:Yer  0.000  0.000  0.000 -1.000  0.000  0.000  0.000\\n&gt; \\n</code></pre>\\n\",\n",
       " '<p>First off, let me say that I\\'m extremely poorly versed in statistics, and this is a question purely about terminology.</p>\\n\\n<p>I have a distribution for some quantity (height of person, say) and the most likely 95% of outcomes are within some range $h_0$ to $h_1$. I want to know what to call (ideally a 2 or 3 word phrase) this range. It is <em>not</em> the case that the distribution is normal, though we can safely assume that it is unimodal.</p>\\n\\n<p><strong>Clarification:</strong></p>\\n\\n<p>Let\\'s say I have a coin, which I think is biased for heads with $p_h=0.6$. Now I toss it $n$ times and I get $a$ heads. I want to write a sentence which basically states that the result is in some 95% \"likelihood region\", (something like \"middle 95 percentile\" seems a little clunky, if not ambiguous --- I\\'m not taking a region about some median!). Now, it might be that my $p_h$ is actually a prediction based on some data, and I have some uncertainties in it, but I <em>don\\'t</em> want to include those uncertainties! I want a term that refers to the fact that because I didn\\'t toss the coin an infinite number of times, there is some variance in the ratio $a/n$.</p>\\n',\n",
       " '<p>Basically, with MDS you create a 2D map for your data with one icon for one data point. With clustering algorithm you color icons with different colors (or cluster labels). MDS and clustering algorithm should be used independently from each other. It does not make much sense to apply clustering algorithm on the X,Ys created with MDS.</p>\\n',\n",
       " '<p>Here is a variant on your suggestion:</p>\\n\\n<p>If at $0$, go right with probability $k$ and left with probability $1-k$.  If right of $0$, go right with probability $p$ and left with probability $1-p$; if left of $0$, go right with probability $1-p$ and left with probability $p$. </p>\\n\\n<p>Providing $p \\\\\\\\lt \\\\\\\\frac{1}{2}$, the expected time that the walk is right of $0$ as a proportion of the time it is not at $0$ is $k$ (which I think is really what you are asking for) and the closer $p$ is to $\\\\\\\\frac{1}{2}$ the smaller the proportion of the total time the walk will be at $0$.  </p>\\n\\n<p>For $p \\\\\\\\gt \\\\\\\\frac{1}{2}$, there is a positive probability the walk will never return to $0$ while for $p = \\\\\\\\frac{1}{2}$ the expected time for each return is infinite.      </p>\\n',\n",
       " '<p>Here is the example of the quick and dirty R code to illustrate what Michael suggested:</p>\\n\\n<p>Define quantiles available:</p>\\n\\n<pre><code>q&lt;-c(0.1,0.2,0.4,0.6,0.8,0.9)\\n</code></pre>\\n\\n<p>Create artificial data and add some noise</p>\\n\\n<pre><code>data &lt;-jitter(qlnorm(q))\\n</code></pre>\\n\\n<p>Create function to minimise</p>\\n\\n<pre><code>fitfun &lt;- function(p)sum(abs(data-qlnorm(q,p[1],p[2])))\\n</code></pre>\\n\\n<p>Run the optimiser with the initial guess of parameters of log-normal distribution:</p>\\n\\n<pre><code>opt &lt;- optim(c(0.1,1.1))\\n</code></pre>\\n\\n<p>The parameters fitted:</p>\\n\\n<p>Display the fit visually:</p>\\n\\n<pre><code>aa&lt;-seq(0,0.95,by=0.01)\\nplot(aa,qlnorm(aa,opt$par[1],opt$par[2]),type=\"l\")\\npoints(q,data)\\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/ZMCU2.png\" alt=\"enter image description here\"></p>\\n\\n<p>Note, I intentionally plotted only 95%-quantile, since the log-normal distribution is unbounded, i.e. the 100%-quantile is infinity. </p>\\n\\n<p>Usual caveats apply, real life example might look much uglier than this one, i.e. fit might be much worse. Also try <a href=\"http://reference.wolfram.com/mathematica/ref/SinghMaddalaDistribution.html\" rel=\"nofollow\">Singh-Maddala</a> distribution instead of log-normal, it works better for income distributions.</p>\\n',\n",
       " '<p>It is common in regression to see $R^2$ formulated as follows: $$R^2\\\\\\\\equiv 1 - {SS_{\\\\\\\\rm err}\\\\\\\\over SS_{\\\\\\\\rm tot}},$$ where $SS_\\\\\\\\text{err}=\\\\\\\\sum_i (y_i - f_i)^2$ and $SS_\\\\\\\\text{tot}=\\\\\\\\sum_i (y_i-\\\\\\\\bar{y})^2$.  This exact formulation seems to make the most sense if the goal of the regression is indeed to minimize the residual sum of squares.  However, sometimes other, more exotic loss functions are more appropriate for a variety of reasons, and some predictive modeling algorithms might be tried to minimize this loss function.</p>\\n\\n<p>I would like to define my $R^2$-like measure in the following way: $$R^2= 1 - \\\\\\\\frac{\\\\\\\\sum_il(y_i, f_i)}{\\\\\\\\sum_il(y_i, \\\\\\\\bar{y})},$$</p>\\n\\n<p>where $l(y_i, f_i)$ denotes the \"loss\" occurred in some sense if $f_i$ is predicted but the actual answer is $y_i$.  In the case of OLS regression, $l(y_i, f_i) = (y_i - f_i)^2$, but this allows for other predictive models based on least absolute deviations or even more exotic functions based upon the problem domain.</p>\\n\\n<p>Is this valid to do?  Is anything gained with this more general formulation of $R^2$?  Is it still valid to call it $R^2$?  Does this add any useful interpretations, or is it more likely to just be confusing?  Should the traditional $R^2$ be reported as well?</p>\\n',\n",
       " '<p>I have a database with the Chilean equivalent SAT for 2010. It shows the highest score achieved by one student that belongs to one school. The scores goes from 100 to 820, and it has been published as a 1-to-100 school ranking. Database also contains several qualitative information of schools (managerial issues, building facilities, staff experience, etc.)</p>\\n\\n<p>If I want to obtain the main determinants inherent of one school that allows students to be at top ranking, do I use general-to-specific estimation, an ordered response probit, or other?</p>\\n',\n",
       " '<p><strong>Practical situation</strong>: I’ve got 120 days of data collected during rainy season. On an average it rained on 52.5 of those days.</p>\\n\\n<ol>\\n<li>What is the probability of it raining at least once in 30 days?</li>\\n<li>If it rains at least once in 30 days, then what is the probability that it continues to rain for 4 days in a stretch in those 30 days?</li>\\n</ol>\\n\\n<p>My Answer for (1): I considered a Poisson process with $\\\\\\\\lambda = 52.5/120 = 0.4375$,\\ncalculated the non-occurrence of an event (in this case \"no rain\") in 30 days and subtracted that value from 1 to get .9999 as the probability of it raining in 30 days.</p>\\n\\n<p>Please advise if I\\'m on the right track and also how do we go about part (2) of the question. </p>\\n',\n",
       " \"<p>You should start by peforming an ANOVA if you are trying to determine if there are differences in any of the mean responss for each group.  You'll need to check the assumptions of the ANOVA model by creating graphs and plots and other diagnostic tests to determine the feasibilty of the assumptions (such as normality of the error terms).  If you find the data violate the assumptions, then you will need to either transform the data to satisfy the assumptions or (or use other remediation methods) or you can perform a nonparametric ANOVA Kruskal-Wallis test.</p>\\n\",\n",
       " \"<p>I don't think that repeated measures is appropriate here - as I understand your protocol, the five temperatures (either hot or cold) are not repeated measures as they occur at different temperatures.</p>\\n\\n<p>However, it does feel like you need some data reduction across the five levels in each temperature group.  You might be well-served to consider some exploratory analysis to decide what the general shape of the relationship of pain perception to ordinal temperature input is across the hot and cold groups; then reduce that to a linear slope or exponential function; then model that single variable as your dependent variable with gender and A/B as factors.</p>\\n\\n<p>Latent growth curve modeling might also be appropriate, but that is beyond me.  </p>\\n\",\n",
       " '<p><strong>Please see edit.</strong></p>\\n\\n<p>When you have data with heavy tails, doing a regression with student-t errors seems like an intuitive thing to do. While exploring this possibility, I ran into this paper: </p>\\n\\n<p>Breusch, T. S., Robertson, J. C., &amp; Welsh, A. H. (November 01, 1997). The emperor\\'s new clothes: a critique of the multivariate t regression model. Statistica Neerlandica, 51, 3.) (<a href=\"http://onlinelibrary.wiley.com/doi/10.1111/1467-9574.00055/abstract\" rel=\"nofollow\">link</a>, <a href=\"http://dl.dropbox.com/u/29304719/Papers/critique%20of%20the%20multivariate%20t%20regression%20model.pdf\" rel=\"nofollow\">pdf</a>)</p>\\n\\n<p>Which argues that the scale parameter and the degrees of freedom parameter are not identifiable with respect to each other in some sense and that because of this doing a regression with t errors doesn\\'t do anything beyond what a standard linear regression does. </p>\\n\\n<blockquote>\\n  <p>Zellner (1976) proposed a regression model in which the data vector\\n  (or the error vector) is represented as a realization from the\\n  multivariate Student t distribution. This model has attracted\\n  considerable attention because it seems to broaden the usual Gaussian\\n  assumption to allow for heavier-tailed error distributions. A number\\n  of results in the literature indicate that the standard inference\\n  procedures for the Gaussian model remain appropriate under the broader\\n  distributional assumption, leading to claims of robustness of the\\n  standard methods. We show that, although mathematically the two models\\n  are different, for purposes of statistical inference they are\\n  indistinguishable. The empirical implications of the multivariate t\\n  model are precisely the same as those of the Gaussian model. Hence the\\n  suggestion of a broader distributional representation of the data is\\n  spurious, and the claims of robustness are misleading. These\\n  conclusions are reached from both frequentist and Bayesian\\n  perspectives.</p>\\n</blockquote>\\n\\n<p>This surprises me.</p>\\n\\n<p>I don\\'t have the mathematical sophistication to evaluate their arguments well, so I have a couple of questions: Is it true that doing regressions with t-errors is not generally useful? If they are sometimes useful, have I missunderstood the paper or is it misleading? If they are not useful, is this a well known fact? Are there other ways to account for data with heavy tails?</p>\\n\\n<p><strong>Edit</strong>: Upon closer reading, of paragraph 3 and section 4, it looks like the paper below is not talking about what I was thinking of as a student-t regression (errors are independent univariate t distribuions). The errors are instead drawn from a single distribution and are not independent. If I understand correctly, this lack of independence is precisely what explains why you cannot estimate the scale and degrees of freedom independently.</p>\\n\\n<p>I guess this paper provides a list of papers to avoid reading.</p>\\n',\n",
       " \"<p>I have a large dataset consisting of many features for each record which includes either a GOOD or BAD outcome. Three metrics have been created for classification purposes using three respective Bayesian Belief Networks:</p>\\n\\n<ul>\\n<li>Classification Network including outcome variable trained on both GOOD and BAD records</li>\\n<li>GOOD Network trained on only GOOD records</li>\\n<li>BAD Network trained on only BAD records</li>\\n</ul>\\n\\n<p>For the GOOD and BAD Networks, a most likely state is predicted for each feature given information on all other features and compared with the actual value present. The GOOD and BAD metrics are then defined by the percent of features correctly predicted.  </p>\\n\\n<p><strong>QUESTION</strong></p>\\n\\n<p>Are there any standard methods that exist for defining optimal thresholds and rule sets for classification on separate models like this? I am most familiar with ROC analysis and it seems there should be some analogous deterministic process using multiple metrics. </p>\\n\\n<p>So far I've found Random Forests to be most similar as a modeling process but, from my understanding, each classifier is built on the same training set using different variables for decisions unlike my method. Would the validation methods still apply? Also, there is probably a name for what I am doing. Please forgive my lack of knowledge on this and feel free to inform me. </p>\\n\\n<p><strong>Further Reading</strong></p>\\n\\n<p>Currently, I'm applying an ad-hoc method but would like to have more insight for future applications:</p>\\n\\n<p>Since the Classification model performed the best (according to ROC analysis) </p>\\n\\n<ul>\\n<li>BAD_Posterior (from classification model) greater than <strong>B</strong>, flag as BAD\\n<ul>\\n<li><strong>B</strong> was determined by finding the first point with a significant change in the TRUE Positive Rate</li>\\n</ul></li>\\n<li>GOOD_Posterior (from classification model) greater than <strong>G</strong>, flag as GOOD\\n<ul>\\n<li><strong>G</strong> was determined by finding the first point with a significant change in the FALSE Negative rate</li>\\n</ul></li>\\n</ul>\\n\\n<p>Further ROC analysis was run on all of the remaining unflagged records using both the GOOD and BAD Network models. Only one model (GOOD) was predictive so I optimized and applied additional rules based on the results. </p>\\n\\n<p>All of this is good and well for what I am doing right now but it feels terribly unscientific and inefficient. There must be some methods I'm overlooking. </p>\\n\",\n",
       " '<p>Maximin (if it is the same as minimax?) involves creating an objective function based on possible payoffs, i.e. you have the objective of minimizing the maximum possible loss. </p>\\n\\n<p>I could imagine a classification scenario where there are payoffs associated with each actual target label/prediction label pair, and you use the maximin critieria to build an objective function that is minimized by the learning algorithm..., but I have not seen this before.  Usually the payoff structure is +1 for correct 0 for incorrect, which results in maximizing the probability that the predicted label is correct.  However, I am sure there are plenty of real world examples where a maximin type of objective would make more sense.</p>\\n\\n<p>However, I am not sure how many standard machine-learning algorithms are adaptable to correspond to this type of optimization criteria...</p>\\n',\n",
       " '<p>While there is something to this Bayesian line of reasoning (deconstructed very thoroughly by Erik!), and indeed this line of thought would explain why many medical findings cannot be reproduced, this particular argument applies that thinking like a sledgehammer. </p>\\n\\n<p>The author presupposes two things without providing evidence: that exposure to smoke was chosen at random, and that almost nothing in the world causes heart disease. <strong>Under these lax standards of reasoning, the author could reject ANY conclusion that something causes heart disease.</strong> All you would need to do is assert:</p>\\n\\n<ol>\\n<li>That the hypothesis was chosen at random, and</li>\\n<li>That heart disease has very close to zero causes.</li>\\n</ol>\\n\\n<p>Both of these assertions are debatable (and, based on my general knowledge, very likely false). But, with these assumptions in place, even observing that 100% of people exposed to secondhand smoke dropped dead of a heart attack within a year, you could assert that the connection is merely a coincidental correlation with the hidden, singular, \"true\" cause.</p>\\n',\n",
       " '<p>One partial answer <a href=\"http://faculty.washington.edu/ezivot/econ583/gmm.pdf\" rel=\"nofollow\">seems to be that</a>:</p>\\n\\n<p>\"In models for which there are more moment conditions than model parameters, GMM estimation provides a straightforward way to test the specification of the proposed model. This is an important feature that is unique to GMM estimation.\"</p>\\n\\n<p>This seems like it would be important but insufficient to wholly explain the popularity of GMM in metrics.</p>\\n',\n",
       " \"<p>You're looking for PROC FREQ. That will build a frequency table from your data, and from there you can calculate a number of frequency table-based statistics, including most Chi-squared statistics, Fisher's Exact tests, etc.</p>\\n\",\n",
       " '<p>Suppose I observe a sample $(y_i,x_i)$, $i=1,...,n$. Suppose that I know the following:</p>\\n\\n<p>$y_i=\\\\\\\\alpha_0+\\\\\\\\alpha_1x_i+\\\\\\\\varepsilon_i$, $i \\\\\\\\in J\\\\\\\\subset\\\\\\\\{1,...,n\\\\\\\\}$\\r</p>\\n\\n<p>$y_i=\\\\\\\\beta_0+\\\\\\\\beta_1x_i+\\\\\\\\varepsilon_i$, $i \\\\\\\\in J^c$\\r</p>\\n\\n<p>where $\\\\\\\\varepsilon_i$ are i. i. d. and $J$ is not known in advance. Is it possible to estimate $\\\\\\\\alpha_0,\\\\\\\\alpha_1,\\\\\\\\beta_0,\\\\\\\\beta_1$? Or at least test the hypothesis that $J=\\\\\\\\varnothing$?</p>\\n\\n<p>If $J$ is known the problem is very easy to solve. Going through all the subsets is not feasible, since we have $2^n$ possible combinations. If we assume $J=\\\\\\\\{1,...,k\\\\\\\\}$ with unknown $k=1,...,n$, it is the classical change-point problem, for which many tests are available. I suspect that this maybe ill-posed problem, so I wanted to check before trying to solve it. </p>\\n\\n<p>Here is a simple illustration of the problem:</p>\\n\\n<pre><code>N &lt;- 200\\ns1 &lt;- sample(1:N,N %/% 2)\\ns2 &lt;- (1:N)[!(1:N) %in% s1]\\n\\nx &lt;- rnorm(N)\\neps &lt;- rnorm(N)\\n\\nind &lt;- 1:N\\n\\ny &lt;- rep(NA,N*T)\\ny[ind %in% s1] &lt;- 2+0.5*x[ind %in% s1]+eps[ind %in% s1]/5\\ny[ind %in% s2] &lt;- 1+1*x[ind %in% s2]+eps[ind %in% s2]/5\\ny\\n\\nsal1 &lt;- ind %in% s1\\n\\nplot(x, y)\\npoints(x[sal1], y[sal1], col=2)\\nabline(2, 0.5, col=2)\\nabline(1, 1)\\n</code></pre>\\n\\n<p>Graphically it is more or less obvious that we have two different models. Maybe it is possible to use some classification or data-mining techniques for solving this problem?</p>\\n',\n",
       " '<p>Let us have some data (shown below), predictand Y and two factors X1 and X2. X1 has 2 groups, and X2 has 3 groups. (In this particular example, the design is incomplete though, because combination X1=2 &amp; X2=3 is absent.)</p>\\n\\n<p>Let us run GLM command (shown). The settings are default: full factorial model, SS III type of squares, intercept present. The command requests to print out <em>observed</em> means for all groups of factors as well as for their combinations and also to print out the corresponding <em>estimated</em> means. It also saves predicted values for Y (shown below as \"pre\").</p>\\n\\n<pre><code>UNIANOVA y BY x1 x2\\n  /METHOD=SSTYPE(3)\\n  /INTERCEPT=INCLUDE\\n  /SAVE=PRED\\n  /EMMEANS=TABLES(OVERALL) \\n  /EMMEANS=TABLES(x1) \\n  /EMMEANS=TABLES(x2) \\n  /EMMEANS=TABLES(x1*x2) \\n  /PRINT=DESCRIPTIVE\\n  /CRITERIA=ALPHA(.05)\\n  /DESIGN=x1 x2 x1*x2.\\n\\n     y     x1  x2      pre\\n  .725581   1   1   .725581\\n -.147728   1   2   .046662\\n  .496867   1   2   .046662\\n -.985803   1   2   .046662\\n -.139656   1   2   .046662\\n -.381405   1   2   .046662\\n 1.437696   1   2   .046662\\n  .039809   1   3  -.748909\\n-1.537626   1   3  -.748909\\n -.402714   2   1   .159152\\n 1.900394   2   1   .159152\\n  .883087   2   1   .159152\\n-1.744157   2   1   .159152\\n 1.009084   2   2   .288968\\n 1.169746   2   2   .288968\\n  .579917   2   2   .288968\\n-1.022533   2   2   .288968\\n -.587685   2   2   .288968\\n  .814123   2   2   .288968\\n  .003084   2   2   .288968\\n-1.068938   2   2   .288968\\n -.175502   2   2   .288968\\n 1.290405   2   2   .288968\\n 1.166946   2   2   .288968\\n -.645831   2   3  -.645831\\n 1.061533   3   1  1.061533\\n 1.143789   3   2   .676997\\n  .210205   3   2   .676997\\n -.643339   3   3  -.360148\\n -.076957   3   3  -.360148\\n</code></pre>\\n\\n<p>Let us compare observed and estimated means printed out (I don\\'t show these tables here). First, we can notice that on the lowest (the cell) level of the design, i.e. on the level of combinations of groups X1 * X2 estimated means equal observed means. This is because we had used saturated, full factorial model including all possible interactions between the factors.  Second, we can see that when it comes to means on the higher, <em>marginal</em>, level, estimated means do not (generally) equal observed means. For example, the observed marginal mean for X1=1 is -0.05470 and the corresponding estimated mean is 0.00778.</p>\\n\\n<p>Can we show where this difference stems from? Yes. The observed marginal mean corresponds to the simple mean of the predicted values. For X1=1, this is <code>mean(.725581,.046662,.046662,.046662,.046662,.046662,.046662,-.748909,-.748909) = -0.05470</code> which is the same as the simple mean of the observed values <code>mean(.725581,-.147728,.496867,-.985803,-.139656,-.381405,1.437696,.039809,-1.537626) = -0.05470</code>. On the other hand, the estimated marginal mean is given by averaging the predicted values with the collapsed <em>groups weighted equally</em>. That is, X2=1, X2=2, X2=3 are given equal weight despite their unequal frequencies, and so <code>0.00778 = mean(.725581,.046662,-.748909)</code>. You may conclude yourself that if the design had been balanced - cells contained equal frequencies - estimated and observed means would have been equal to each other.</p>\\n\\n<p>That was a simple explanation for the simple case (by \"simple case\" I mean defaults such as Type III SS, intercept, no covariates). You may consult \"SPSS Algorithms\" help document to read about how estimated, expected means are actually computed in the general case.</p>\\n',\n",
       " '<p>Almost surely in a well designed experiment.  (Designed, of course, to elicit such a <a href=\"http://18th.eserver.org/hume-enquiry.html#7\" rel=\"nofollow\">connexion</a>.)</p>\\n',\n",
       " '<p>I\\'ve had good experiences with Graphviz, and while I\\'ve never drawn graphs that large myself, I know that it\\'s supported. See <a href=\"http://www.graphviz.org/content/root\" rel=\"nofollow\">this page</a> and <a href=\"http://www2.research.att.com/~yifanhu/GALLERY/GRAPHS/index.html\" rel=\"nofollow\">this page</a>. </p>\\n',\n",
       " \"<p>I've just started playing with the R forecast package and found I must be doing something wrong because I can't get a decent prediction for a simple sinus.</p>\\n\\n<pre><code>weightData &lt;- data.frame(weight = sin(seq(1:100)), week=1:100)\\nweight &lt;- as.numeric(weightData$weight)\\npredicted &lt;- forecast(weight,h=3,level=95) \\n\\n# see the predicted values by forecast predicted\\n\\nmyplot &lt;- forecast(weight,h=10,level=95)\\nplot(myplot)\\n</code></pre>\\n\\n<p>And I get a flat prediction. \\nI understand the generic forecast methods selects the best method for my data. Isn't that true? Am I missing something?</p>\\n\\n<p>Thanks in advance!</p>\\n\",\n",
       " '<p><a href=\"http://archive.ics.uci.edu/ml/datasets/Arcene\" rel=\"nofollow\"><strong>Arcene</strong></a><br>\\nn=900<br>\\np=10000 (3k is artificially added noise)<br>\\nk=2 (~balanced)<br>\\nFrom <a href=\"http://www.nipsfsc.ecs.soton.ac.uk/papers/NIPS2003-Datasets.pdf\" rel=\"nofollow\">NIPS2003</a>.</p>\\n',\n",
       " '<p>I\\'m simulating data from a logistic regression model:</p>\\n\\n<pre><code>log(p/1-p)= 0 + X\\n</code></pre>\\n\\n<p>where $X \\\\\\\\sim N(0,\\\\\\\\sigma^2)$. After I simulate the data, I fit a logistic regression model to the data and compare the fitted regression coefficients to the actual regression coefficients. </p>\\n\\n<p>I\\'ve noticed that as I increase $\\\\\\\\sigma$ (i.e. the variance of the original $X$ data) the fitted regression coefficient for $X$ (i.e. $\\\\\\\\beta_1$) is consistently greater than 1 (however, the sd of the estimate also increased so 1 is still contained in the confidence interval for beta1)</p>\\n\\n<p>I was wondering why when you increase the variance, the fitted $\\\\\\\\beta_1$\\'s tend to be greater than the actual $\\\\\\\\beta_1$ (i.e. $\\\\\\\\beta_1 = 1$), not less than? Is there a statistical explanation for this?</p>\\n\\n<p>Thanks!</p>\\n\\n<pre><code>beta0 = 0\\nbeta1 = 1\\nsigma = 1\\nnumber_samples = 10000\\ngenLogit = function(pos_prop,sd){\\n    generated_data = c()\\n\\nxtest = rnorm(10000,0,sd)\\nlinpred = beta0 + (xtest * beta1)\\nprob = exp(linpred)/ (1+exp(linpred))\\n\\nrunis = runif(10000,0,1)\\nytest = ifelse(runis&lt;prob,1,0)\\n\\npos = sample(xtest[ytest ==1],floor(pos_prop*1000))\\nneg = sample(xtest[ytest == 0], floor((1-pos_prop)*1000))\\n\\ngenerated_data = rbind(cbind(pos,rep(1,floor(pos_prop*1000))),cbind(neg,rep(0,floor((1-pos_prop)*1000))))\\ncolnames(generated_data) = c(\\'X\\',\\'Y\\')\\ngenerated_data = data.frame(generated_data)\\n\\nfit = glm(Y~X,data =generated_data, family=binomial(link=\"logit\"))\\n\\nreturn(fit)\\n}\\n</code></pre>\\n\\n<p>If you run <code>genLogit(.5,1000)</code> this is generating balanced (50/50) data with X distributed normal(0,1000). Running it multiple times, I get a beta0 estimate much greater than 0.</p>\\n',\n",
       " '<p>I have six 5-point Likert items (1-Strongly Disagree, 2-Disagree, 3-Neither agree or disagree, 4-Agree, 5-Strongly Agree) which are related to my dependent variable. I sum the six variables which gives the frequency as follows:</p>\\n\\n<pre><code>    Frequency\\n6.00    1\\n9.00    1\\n12.00   6\\n13.00   3\\n14.00   3\\n15.00   18\\n16.00   11\\n17.00   4\\n18.00   17\\n19.00   11\\n20.00   14\\n21.00   22\\n22.00   15\\n23.00   9\\n24.00   11\\n25.00   2\\n26.00   3\\n30.00   2\\nTotal   153\\n</code></pre>\\n\\n<p>Now I want to recode the sum variable again into a 5-point scale (1-5) to get a composite score of my dependent variable. I will use this composite dependent variable in further analysis of ordinal logistic regression. My question is this:</p>\\n\\n<p>What is the best way to recode the sum again into 5-point scale?\\nOr, please let me know if there exists any better way to create a composite scale from Likert items?</p>\\n\\n<p>just to clarify my quesion: I have six variables that are related to my DV, all are on 5 point likert scale. and I could not find the best way to reduce these six variables into one. The composite score may be considered as one of the option.</p>\\n',\n",
       " '<p>I recently started working for a tuberculosis clinic.  We meet periodically to discuss the number of TB cases we\\'re currently treating, the number of tests administered, etc.  I\\'d like to start modeling these counts so that we\\'re not just guessing whether something is unusual or not.  Unfortunately, I\\'ve had very little training in time series, and most of my exposure has been to models for very continuous data (stock prices) or very large numbers of counts (influenza).  But we deal with 0-18 cases per month (mean 6.68, median 7, var 12.3), which are distributed like this:</p>\\n\\n<p><img src=\"http://img827.imageshack.us/img827/1927/activetbcases.png\" alt=\"alt text\" title=\"Cases by month\"></p>\\n\\n<p><img src=\"http://img827.imageshack.us/img827/4348/tbcasedistribution.png\" alt=\"alt text\" title=\"Distribution of counts\"></p>\\n\\n<p>I\\'ve found a few articles that address models like this, but I\\'d greatly appreciate hearing suggestions from you - both for approaches and for R packages that I could use to implement those approaches.</p>\\n\\n<p><strong>EDIT:</strong>  mbq\\'s answer has forced me to think more carefully about what I\\'m asking here; I got too hung-up on the monthly counts and lost the actual focus of the question.  What I\\'d like to know is: does the (fairly visible) decline from, say, 2008 onward reflect a downward trend in the overall number of cases?  It looks to me like the number of cases monthly from 2001-2007 reflects a stable process; maybe some seasonality, but overall stable.  From 2008 through the present, it looks like that process is changing: the overall number of cases is declining, even though the monthly counts might wobble up and down due to randomness and seasonality.  How can I test if there\\'s a real change in the process?  And if I can identify a decline, how could I use that trend and whatever seasonality there might be to estimate the number of cases we might see in the upcoming months?</p>\\n\\n<p>Whew.  Thanks for bearing with me.</p>\\n',\n",
       " '<p>$s_{x}^2=\\\\\\\\dfrac{\\\\\\\\sum_{i=1}^n (x_i-\\\\\\\\bar{x})^2}{n-1}$ = sample variance of the variable $X$.</p>\\n\\n<p>$s_{y}^2=\\\\\\\\dfrac{\\\\\\\\sum_{i=1}^n (y_i-\\\\\\\\bar{y})^2}{n-1}$ = sample variance of the variable $Y$.</p>\\n\\n<p>$s_{xy}=\\\\\\\\dfrac{\\\\\\\\sum_{i=1}^n (x_i-\\\\\\\\bar{x})(y_i-\\\\\\\\bar{y})}{n-1}$ = sample covariance of the variables $X$ and $Y$.</p>\\n\\n<p>$s_{x}$ is simply the square root of $s_{x}^2$ and $s_{y}$ is the square root of $s_{y}^2$. They are called the sample standard deviations of $X$ and $Y$ respectively.</p>\\n',\n",
       " '<p>This depends on what is meant by \"interaction\".  If the data have no noise - the plot is literally just two parallel lines, then there is certainly no interaction, we know this deductively, without any need for statistics.  Secondly if the lines are not parallel, then we know deductively that there is interaction.  So there is no counter example if there is no noise.</p>\\n\\n<p>But if there is noise (or error), then there is basically more than one possible place that the \"noiseless\" or \"true\" lines could be.  It is also possible for the true lines to be parallel but if the noise is big enough and you get an \"unlucky\" sample of noise, then the noisy lines will cross.  Just how unlucky depends on how \"non-parallel\" the two \"true lines\" are and how many units have been sampled.  Consider the OLS case, the lines are generated by:</p>\\n\\n<p>$$y_{i}=x_{i}^{T}\\\\\\\\beta_{true}+n_{i}$$</p>\\n\\n<p>Where $\\\\\\\\beta_{true}$ is a 4-D vector with the intercept for group 1, the offset for group 2, the slope for group 1 and the offset to the slope for group 2</p>\\n\\n<p>Now you fit an OLS to the observed data, and you get</p>\\n\\n<p>$$\\\\\\\\beta_{OLS}=(X^{T}X)^{-1}X^{T}Y=(X^{T}X)^{-1}X^{T}(X\\\\\\\\beta_{true}+n)=\\\\\\\\beta_{true}+(X^{T}X)^{-1}X^{T}n$$</p>\\n\\n<p>So by a careful choice of the noise we can make the OLS estimates be basically anything.  So I don\\'t have to invert a $4\\\\\\\\times 4$ matrix, I will specialise to the case where both intercepts are equal to zero, and we have</p>\\n\\n<p>$$y_{ij}=\\\\\\\\beta_{1}x_{ij}+\\\\\\\\beta_{2}x_{i2}I(j=2)$$</p>\\n\\n<p>And then \\n$$(X^{T}X)^{-1}=\\\\\\\\frac{1}{\\\\\\\\left(\\\\\\\\sum_{i}x_{i1}^{2}\\\\\\\\right)\\\\\\\\left(\\\\\\\\sum_{i}x_{i2}^{2}\\\\\\\\right)}\\\\\\\\begin{pmatrix} \\\\\\\\sum_{i}x_{i1}^{2}+\\\\\\\\sum_{i}x_{i2}^{2} &amp; -\\\\\\\\sum_{i}x_{i2}^{2} \\\\\\\\\\\\\\\\ -\\\\\\\\sum_{i}x_{i2}^{2} &amp; \\\\\\\\sum_{i}x_{i2}^{2}\\\\\\\\n\\\\\\\\end{pmatrix}$$\\n$$=\\\\\\\\frac{1}{\\\\\\\\sum_{i}x_{i1}^{2}}\\\\\\\\begin{pmatrix} 1 &amp; -1 \\\\\\\\\\\\\\\\ -1 &amp; 1\\\\\\\\end{pmatrix}\\\\\\\\n+\\\\\\\\frac{1}{\\\\\\\\sum_{i}x_{i2}^{2}}\\\\\\\\begin{pmatrix} 1 &amp; 0 \\\\\\\\\\\\\\\\ 0 &amp; 0\\\\\\\\end{pmatrix}$$</p>\\n\\n<p>Now for $X^{T}n$ we have:</p>\\n\\n<p>$$X^{T}n=\\\\\\\\sum_{i}x_{i2}n_{i2}\\\\\\\\begin{pmatrix} 1\\\\\\\\\\\\\\\\1\\\\\\\\end{pmatrix}\\\\\\\\n+\\\\\\\\sum_{i}x_{i1}n_{i1}\\\\\\\\begin{pmatrix} 1\\\\\\\\\\\\\\\\0\\\\\\\\end{pmatrix}$$</p>\\n\\n<p>And so the total error from the regression is:</p>\\n\\n<p>$$\\\\\\\\frac{\\\\\\\\sum_{i}x_{i2}n_{i2}+\\\\\\\\sum_{i}x_{i1}n_{i1}}{\\\\\\\\sum_{i}x_{i2}^{2}}\\\\\\\\begin{pmatrix} 1\\\\\\\\\\\\\\\\0\\\\\\\\end{pmatrix}\\\\\\\\n+\\\\\\\\frac{\\\\\\\\sum_{i}x_{i1}n_{i1}}{\\\\\\\\sum_{i}x_{i1}^{2}}\\\\\\\\begin{pmatrix} 1\\\\\\\\\\\\\\\\-1\\\\\\\\end{pmatrix}$$</p>\\n\\n<p>Now if the true slopes are parallel, so that $\\\\\\\\beta_{2,true}=0$, then the OLS estimates will be:</p>\\n\\n<p>$$\\\\\\\\hat{\\\\\\\\beta}_{1}=\\\\\\\\beta_{1,true}+\\\\\\\\frac{\\\\\\\\sum_{i}x_{i2}n_{i2}+\\\\\\\\sum_{i}x_{i1}n_{i1}}{\\\\\\\\sum_{i}x_{i2}^{2}}+\\\\\\\\frac{\\\\\\\\sum_{i}x_{i1}n_{i1}}{\\\\\\\\sum_{i}x_{i1}^{2}}$$\\n$$\\\\\\\\hat{\\\\\\\\beta}_{2}=-\\\\\\\\frac{\\\\\\\\sum_{i}x_{i1}n_{i1}}{\\\\\\\\sum_{i}x_{i1}^{2}}$$</p>\\n\\n<p>Now this shows that the OLS estimate can indeed lead to erroneous interactions, just choose the \"true\" noise such that it is highly correlated with $x_{i1}$ - essentially you need to violate one of the assumptions of OLS, non-heteroscedasticity of the noise.  So if you generate data according to:</p>\\n\\n<p>$$y_{i1}=x_{i1}(\\\\\\\\beta_{1,true}+n_{i1})$$\\n$$y_{i2}=x_{i2}\\\\\\\\beta_{1,true}+n_{i2}$$</p>\\n\\n<p>And then try to fit an interaction model using OLS of $y$ on $x$ with an interaction, you will find a significant result, even though the true betas are the same.  The plots will cross because of the fanning in the first group.</p>\\n\\n<p>One example data set (true beta is 2 and noise was generated from standard normal).  You get a t-statistic above 10 for the interaction effect:</p>\\n\\n<p>$$\\\\\\\\begin{array}{c|c}\\\\\\\\ngroup\\t&amp;\\ty\\t&amp;\\tx\\t\\\\\\\\\\\\\\\\\\\\\\\\n1\\t&amp;\\t1.282817715\\t&amp;\\t1\\t\\\\\\\\\\\\\\\\\\\\\\\\n1\\t&amp;\\t2.026032115\\t&amp;\\t2\\t\\\\\\\\\\\\\\\\\\\\\\\\n1\\t&amp;\\t5.9786882\\t&amp;\\t3\\t\\\\\\\\\\\\\\\\\\\\\\\\n1\\t&amp;\\t22.1588319\\t&amp;\\t7\\t\\\\\\\\\\\\\\\\\\\\\\\\n2\\t&amp;\\t16.28587668\\t&amp;\\t9\\t\\\\\\\\\\\\\\\\\\\\\\\\n2\\t&amp;\\t15.12007527\\t&amp;\\t6\\t\\\\\\\\\\\\\\\\\\\\\\\\n2\\t&amp;\\t9.566273403\\t&amp;\\t5\\t\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\end{array}$$</p>\\n',\n",
       " '<p>Aside from my practical statistical suggestion, I wanted to raise a slightly different issue: I realize that the cinema\\'s goal is to maximize revenues, and of course the analysis (and strategy) can be geared towards that goal. However, I would like to suggest a broader, holistic view that companies as well as analysts should consider: the overall benefit. In this case, we can consider the value of the gaming addition to cinema goers. Are they happier or more satisfied with the overall experience? (this can be evaluated, e.g., via a quick questionnaire). Or, if the gaming is educational, for instance, then perhaps there is added benefit to those playing? I recall that in several cinemas in the U.S. there are word games on the screen before a movie starts. These can be perceived as fun and educational and could therefore be value added. In fact, if movie goers perceive the gaming service as value added, then they will likely choose this cinema over others and perhaps even visit it more frequently.</p>\\n\\n<p>What I am trying to say is that it is useful to define \"success\" in a broad manner and to think big. In the end, success will depend also on the wellness of the \"customers\" and the impact of \"treatments\" on society, culture, the environment, etc.</p>\\n\\n<p>Sorry if this is too philosophical, but I have had so many MBA students maximizing short-term financial gains and too few thinking of issues that are not monetary. Yet, data mining and statistics can be used for broader causes.</p>\\n',\n",
       " 'Is a property of a hypothesis testing method: the probability of rejecting the null hypothesis given that it is false, i.e. the probability of not making a type II error. The power of a test depends on sample size, effect size, and the significance ($\\\\\\\\alpha$) level of the test.',\n",
       " '<p>I\\'m sure this is a pretty standard statistics question, but I\\'m no expert...  I\\'m running an A/B test on my website to see if a change results in users adding more content.  So there are 2 basic things I\\'m looking at; the # of users adding at least 1 piece of content and the total # of pieces of content added by all users.</p>\\n\\n<p>I really care much more about the total # of pieces of content added by all users.  I\\'ll make the change permanent if I know it\\'s at least not worse than the existing site.  So I need to know how many samples (users logging in) I need to have a 95% confidence level.  Normally I can use <a href=\"http://www.prconline.com/education/tools/statsignificance/index.asp\" rel=\"nofollow\">one of the many web A/B test calculators</a> that use chi-square and similar tests to figure out if my test is statistically significant or to figure out the sample size I need.  In the first case of seeing how many users added content, I can do this.  But to see the total pieces of content added among all users, I can\\'t use those tests as their isn\\'t a \"conversion\" event.</p>\\n\\n<p>So what\\'s the best way to see what sample size I need to be statistically significant to a 95% confidence level?  And how can I see if my test shows whether there was a difference?  Again I just want to make sure the new change isn\\'t worse (or isn\\'t \"much\" worse, do I need to and how should I define \"much\"?).</p>\\n',\n",
       " '<p>There are numerous examples of Bayesian analysis of census data in the book <a href=\"http://rads.stackoverflow.com/amzn/click/158488388X\" rel=\"nofollow\">Bayesian Data Analysis</a> by Gelman, Carlin, Rubin, and Stern, especially in chapters 5-8. </p>\\n\\n<p>In fact, whenever the census data collection mechanism is not ignorable, this can often be a crucial part of the analysis. Consider, for example, prior beliefs about demographics that might be under-reported due to unusual household living situations. If you were targeting only a few specific covariates from the census data to estimate something about migrant workers, say, taking this into account would be extremely important. </p>\\n\\n<p>That example might not be very realistic, but surely there are other examples with census data that highlight a similar point: Bayesian methods allow you to account for hierarchical aspects of data collection and to concentrate your assumptions into well-articulated priors. This would be important when seeking models for underrepresented demographics or non-ignorable designs.</p>\\n\\n<p>More directly to your three questions:</p>\\n\\n<p>(1) This seems too simplistic. I don\\'t think you just \"run the data through Bayes\\' rule\". Bayesian analysis is the process of articulating a prior distribution (it could come from the past census, but probably you also know pieces of information that lead you to have a current prior belief that\\'s somewhat different from just the most recent census (say, regarding job loss or something)), and articulating a likelihood model, and then using computational approaches to construct the posterior. You don\\'t \"run\" the current census data \"through\" Bayes\\' theorem, unless that is your prior and you have already articulated some likelihood model for the problem you\\'re trying to perform inference on.</p>\\n\\n<p>I guess to answer (1) for you, we\\'d need to know more about what you\\'re thinking of using as a prior, how you are splitting the data analysis out (are you setting up a hierarchical model, with hyperparameters sampled from some meta-prior, etc.?), what sorts of parameters / test statistics you are interested in estimating, and what your likelihood model is.</p>\\n\\n<p>(2) You should probably be modeling things that effect the response rate.</p>\\n\\n<p>(3) I think you are confused a little here. You seem to think that either last year\\'s census or some polling data should be the \"prior\" and that this year\\'s census should be some sort of check on the posterior. It\\'s like you want to specify a model that converts old census data to new census data, but this doesn\\'t make much sense to me. </p>\\n\\n<p>I could be misunderstanding you, but it seems like you\\'re thinking of Bayes\\' theorem as a black box that you drop data into... but it\\'s certainly not that. You use Bayes\\' theorem to test your likelihood model of the data, incorporated with prior beliefs on the parameters of the model. Usually this is done in hierarchical stages to sequester different parts of the model from each other, generally because some subset of the data collection can be treated as ignorable conditioned on knowing certain hyper-parameters.</p>\\n',\n",
       " \"<p>I'm working on a Bayesian Regression problem where I would like to estimate the beta coefficients subject to this constraint (penalty):</p>\\n\\n<p>$\\\\\\\\sum|\\\\\\\\beta_i|&lt;C$ or similarly $\\\\\\\\sum \\\\\\\\beta_i^2&lt;C$</p>\\n\\n<p>Which is basically a Lasso or L2 Penalty.</p>\\n\\n<p>Now, if I understand correctly, we constrain the coefficients through the prior in Bayesian analysis.  Therefore my question is what would an appropriate prior be for the Betas?  I should note, that for my case, the betas are restricted to be positive, they cannot be negative.</p>\\n\",\n",
       " \"<p>I am looking for a robust version of Hotelling's $T^2$ test for the mean of a vector. As data, I have a $m\\\\\\\\ \\\\\\\\times\\\\\\\\  n$ matrix, $X$, each row an i.i.d. sample of an $n$-dimensional RV, $x$. The null hypothesis I wish to test is $E[x] = \\\\\\\\mu$, where $\\\\\\\\mu$ is a fixed $n$-dimensional vector. The classical Hotelling test appears to be susceptible to non-normality in the distribution of $x$ (just as the 1-d analogue, the Student t-test is susceptible to skew and kurtosis). </p>\\n\\n<p>what is the state of the art robust version of this test? I am looking for something relatively fast and conceptually simple. There was a paper in COMPSTAT 2008 on the topic, but I do not have access to the proceedings. Any help? </p>\\n\",\n",
       " '<p>You are right with word \"median\" on your mind, albeit Kruskal-Wallis is <em>not</em> the test for medians. What you need is <a href=\"http://en.wikipedia.org/wiki/Median_test\" rel=\"nofollow\">median test</a>. It tests (asymptotically by chi-square or exactly by permutations) whether several groups are same in regard to ratio of observations falling above/not above <em>some value</em>. By default, median of the combined sample is taken for that value (and hence is the name of the test, which is then the test for equality of population medians). But you could specify another value than median. Any quantile will do. The test then will compare groups in regard to the proportion of cases that fall not above the quantile.</p>\\n',\n",
       " '<p>\"Bayesian\" really means \"approximate Bayesian\".</p>\\n\\n<p>\"Fully Bayesian\" also means \"approximate Bayesian\", but with less approximation.</p>\\n\\n<p><strong>Edit</strong>: Clarification.</p>\\n\\n<p>The fully Bayesian approach would be, for a given model and data, to calculate the posterior probability using the Bayes rule \\n$$\\np(\\\\\\\\theta \\\\\\\\mid \\\\\\\\text{Data}) \\\\\\\\propto p(\\\\\\\\text{Data} \\\\\\\\mid \\\\\\\\theta)p(\\\\\\\\theta) \\\\\\\\&gt;.$$ \\nExcept for very simple models, this has too large computational complexity, and approximations are necessary. More accurate approximations, such as using MCMC with Gibbs sampling for all parameters $\\\\\\\\theta$, are sometimes called \"Fully Bayesian\". Less accurate approximations, such as using point estimation for some parameters, cannot be called \"Fully Bayesian\". Some approximate inference methods are in-between, like Variational Bayes or Expectation Propagation, and they are sometimes (rarely) also called \"Fully Bayesian\".</p>\\n',\n",
       " '<p>I have read about <a href=\"http://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1225&amp;context=usgsnpwrc\">controversies regarding hypothesis testing</a>  with some commentators suggesting that hypothesis testing should not be used. Some commentators suggest that <em>confidence intervals</em> should be used instead. </p>\\n\\n<ul>\\n<li>What is the difference between confidence intervals and hypothesis testing? Explanation with reference and examples would be appreciated.</li>\\n</ul>\\n',\n",
       " '<p>Standard deviations aren\\'t \"good\" or \"bad\". They are indicators of how spread out your data is. Sometimes, in ratings scales, we want wide spread because it indicates that our questions/ratings cover the range of the group we are rating. Other times, we want a small sd because we want everyone to be \"high\".</p>\\n\\n<p>For example, if you were testing the math skills of students in a calculus course, you could get a very small sd by asking them questions of elementary arithmetic such as $3+2$. But suppose you gave a more serious placement test for calculus (that is, students who passed would go into Calculus I, those who did not would take lower level courses first). You might expect a lower sd (and a higher average) among freshman at MIT than at South Podunk State, given the same test.</p>\\n\\n<p>So. What is the purpose of your test? Who are in the sample? </p>\\n',\n",
       " '<p>You have a binomial response.  That is the important part of this.  The count status of your explanatory variable doesn\\'t matter.  As a result, you should be doing some form of logistic regression.  The part that makes this more difficult is that your data are clustered within just four participants.  That means you need to either use a GLiMeM, or the GEE.  This is a subtle decision, but I discuss it at some length here: <a href=\"http://stats.stackexchange.com/questions/32419//32421#32421\">difference-between-generalized-linear-models-generalized-linear-mixed-models</a>. Depending on the options that your software affords you, you may also have to un-group your data, so that you have a (<em>very long</em>) matrix where the response listed in each row is a 1 or a 0.</p>\\n',\n",
       " '<p>I have the following function $Y = \\\\\\\\alpha\\\\\\\\ln(x)$, $\\\\\\\\alpha$ is a constant.   Question: </p>\\n\\n<p>(a) What is the expression for the derivative of Y with respect to x?<br>\\n(b) what is the elasticity of Y wrt. x?</p>\\n',\n",
       " \"<p>I'm using SigmaPlot to run PCA on various measurements that should all correspond to size of an animal. Running PCA using a covariance matrix (instead of a correlation matrix, since all of the measurements are measured in the same units) yielded really, really small eigenvalues for the first three PCs, despite relatively high proportion of variance explained. I've read that proportion of variance explained should correspond to eigenvalue, but I'm not sure how (and I can't seem to find out).</p>\\n\\n<p>Given the proportion of variance explained, how do I calculate the eigenvalues (i.e., confirm that SigmaPlot is not making some sort of error). Thanks!</p>\\n\",\n",
       " \"<p>This may become very problematic in Java due to optimizations made by the JVM.  Because subsequent runs may become faster due to optimizations or slower due to garbage collection, this might get weird.</p>\\n\\n<p>Were you to go the statistical route, you could simply perform robust regression on the identifiers of the instructions called.  For instance, suppose you have a 4 instruction vocabulary, IN1, ..., IN4.  If you do many measurements (even better: well-designed experiments, but let's assume you're doing reasonable variation in the sampling), you can regress the total time on the counts for each of IN1, ..., IN4.  Assuming no latency in execution, you may be able to set the intercept to 0.  With <code>rlm</code> in the MASS package, you can do a robust regression on the predictors.</p>\\n\\n<p>If this is for a demo for, say, a science museum, then the above answer may be adequate.  If you are attempting to develop a new microprocessor, then I would really address the Java issues.  It would be better to message when a light has changed and have a listener use that to update the screen; even better is to have it update just that section of the screen.</p>\\n\",\n",
       " '<p>Not in Matlab, but <a href=\"http://elki.dbs.ifi.lmu.de/\" rel=\"nofollow\">ELKI</a> (Java) provides a dozen or so cluster quality measures for evaluation.</p>\\n',\n",
       " '<p>I want to build a linear model to predict a scalar output from a vector of noisy scalar variable measurements.</p>\\n\\n<p>I have two separate training data sets.  One has output data and corresponding exact variable measurements.  The other has exact variable measurements and corresponding noisy variable measurements.  The noisy measurements of some variables are noisier (higher error variance) than others, and the noisy measurements of some variables are biased.  I do <em>not</em> have a single data set with output data and corresponding noisy variable measurements.</p>\\n\\n<p>How should I build my linear model?  Should I use the exact variable measurements and ignore the fact that when the model is applied/used, noisy measurements of the variables will be input to the model?  Or is there some way I can make use of what I can figure out about the noisy measurements of each variable when building the model? Can R help me with this problem?</p>\\n',\n",
       " '<p>Seems like the distribution in Scipy for the lognormal is not the same as in R, or generally, not the same as the distribution I am familiar with. John D Cook has touched on this:\\n<a href=\"http://www.johndcook.com/blog/2010/02/03/statistical-distributions-in-scipy/\" rel=\"nofollow\">http://www.johndcook.com/blog/2010/02/03/statistical-distributions-in-scipy/</a>\\nhttp://www.johndcook.com/distributions_scipy.html</p>\\n\\n<p>However, I haven\\'t found anything conclusive on how to use a lognormal density function in Python. If anyone would like to add to this, please feel free. </p>\\n\\n<p>My solution so far is to use the lognormal pdf evaluated at 0 to 180 (exclusive), and used as a dictionary in the python script. </p>\\n',\n",
       " '<p>The title is my whole question.  </p>\\n',\n",
       " '<p><em>Does an average of the 100 calls now count as 1 measurement?</em></p>\\n\\n<p>Basically you can measure anything and define the measurements to be the values of an (unknown) variable. In your case this variable X:=\"average duration across 100 functions calls given parameter n\", so 100 calls count as one measurement.</p>\\n\\n<p><em>Does it have a standard error?</em></p>\\n\\n<p>Yes, given you have enough data (2 points), you can measure the standarddeviation/error of every variable. The question is whether the value is meaningful, which is only the case when the distribution is symmetric. If you are not sure about that you are better of considering the percentiles of the data or a robust scale measurement (see <a href=\"http://stats.stackexchange.com/a/1949/264\">On univariate outlier tests (or: Dixon Q versus Grubbs)</a>)</p>\\n\\n<p><em>Is this approach completely orthogonal to the earlier use of the standard error?</em></p>\\n\\n<p>No. Let\\'s assume a normal distribution $N(\\\\\\\\mu,\\\\\\\\sigma^2)$. When one repeatly draws a sample of 100 values from this distribution and calculates the average across this values, is this the same as estimating the mean from $N(\\\\\\\\mu,\\\\\\\\sigma^2)$ based on samples of size 100. The distribution of this estimation is t-distributed (approximately normal) with mean=$\\\\\\\\mu$ and variance=$(\\\\\\\\frac{\\\\\\\\sigma}{\\\\\\\\sqrt{n}})^2$</p>\\n\\n<p>In summary: I\\'d check that the average duration of function calls is approximately normal (or symmetrically) distributed and just report the error. If this is not true, I\\'d report the percentiles instead. In the latter case (or in general) a <a href=\"http://en.wikipedia.org/wiki/Boxplot\" rel=\"nofollow\">boxplot</a> provides a good visualization.</p>\\n',\n",
       " \"<p>From ten points you are going to have 45 (10*(10-1)/2) points in your variogram cloud from the distances between each pair of points. Once the system has binned that, or even without binning, its going to be dominated by noise, I reckon. Get a plot of the variogram cloud to see what I mean.</p>\\n\\n<p>If autokrige can't fit a nice smooth variogram then it will do what it did, and just go 'heck, I can't work out the correlation with distance with just 10 points, my best guess is just the mean'. It really can't do better.</p>\\n\\n<p>If you want something to look 'realistic', then you could feed it variogram parameters with a bigger range, that would over-smooth the output. But then you may as well just do inverse-distance weighting if all you want is a pretty picture. The advantage of kriging is that it <em>is</em> realistic. But it rejects your reality and replaces it with its own...</p>\\n\\n<p>SUggestions:</p>\\n\\n<ul>\\n<li>Get a plot of the variogram cloud</li>\\n<li>Get more data :)</li>\\n<li>Look into bivariate kriging for your case with the two different data sets. I think the theory exists, there may even be code for it...</li>\\n</ul>\\n\",\n",
       " \"<p>I collect a bunch of questions from Twitter's stream by using a regular expression to pick out any tweet that contains a text that starts with a question type: who, what, when, where etc and ends with a question mark.</p>\\n\\n<p>As such, I end up getting several non-useful questions in my database like: 'who cares?', 'what's this?' etc and some useful ones like: 'How often is there a basketball fight?', 'How much does a polar bear weigh?' etc</p>\\n\\n<p>However, I am only interested in useful questions.</p>\\n\\n<p>I have got about 3000 questions, ~2000 of them are not useful, ~1000 of them are useful that I have manually label them. I am attempting to use a naive Bayesian classifier (that comes with NLTK) to try to classify questions automatically so that I don't have to manually pick out the useful questions.</p>\\n\\n<p>As a start, I tried choosing the first three words of a question as a feature but this doesn't help very much. Out of 100 questions the classifier predicted only around 10%-15% as being correct for useful questions. It also failed to pick out the useful questions from the ones that it predicted not useful.</p>\\n\\n<p>I have tried other features such as: including all the words, including the length of the questions but the results did not change significantly.</p>\\n\\n<p>Any suggestions on how I should choose the features or carry on?</p>\\n\\n<p>Thanks.</p>\\n\",\n",
       " '<p>I am looking at the text, <em>A Practitioner\\'s Guide to Resampling for Data Analysis, Data Mining, and Modeling</em>, by Phillip Good (<a href=\"http://rads.stackoverflow.com/amzn/click/1439855501\" rel=\"nofollow\">Amazon</a>).  </p>\\n\\n<ul>\\n<li>I would like to see if anyone has read this book (or seen a review) and can offer objective advice on it (rating and level of material).</li>\\n</ul>\\n\\n<p>(I have a suspicion this question - being subjective - should be placed in a community location. Problem is, not sure how to do that. So, I will beg forgiveness in advance and look to a moderator to help direct me.)</p>\\n',\n",
       " \"<p>You can not use the 36-coefficient model, and not because it's going to be slow. Speed is the least of your worries here. </p>\\n\\n<p>The real trouble is that you've taken an already under-determined problem (because of the correlations), and converted it into a problem which is severely under-determined for <em>any</em> data, because of linear dependencies. Simply put, $x_1-x_2=(x_1-x_3)-(x_2-x_3)$, so you can only determine 2 out of the 3 coefficients for these terms in the best case. The only way to fix this will be to prescribe some artificial regularization condition, like having minimal $\\\\\\\\sum c_i^2$, or whatever may be right in your case.</p>\\n\\n<p>It seems to me that you might be better off if you start by analyzing the correlation matrix and first figuring out which terms of the form $c_i-c_j$ should really appear in your problem.</p>\\n\",\n",
       " '<p>I am trying to classify a dataset which has 20 features and a class (target) feature. The problem is that \"class-1\" is found in 25% of cases while the rest 75% is \"class-2\" examples.</p>\\n\\n<p>My question is : How can i under sample instances of class2 (or oversample class1 instances) in R?</p>\\n\\n<p>I already tried to look for some answers but i would not like to use weighting.</p>\\n\\n<p>Thank you </p>\\n',\n",
       " '<p>Already nice suggestions, I would like to add the following articles that describe HMMs from perspective of application in biology by Sean Eddy. </p>\\n\\n<ul>\\n<li><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/8804822\" rel=\"nofollow\">Hidden Markov Models</a> </li>\\n<li><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/9918945\" rel=\"nofollow\">Profile hidden Markov models</a> </li>\\n<li><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/15470472\" rel=\"nofollow\">What is a hidden Markov model?</a> </li>\\n</ul>\\n',\n",
       " '<p>To have just two disciplines represented,one must occur twice and the other three times. So you could have (1) history twice amd math 3 times, (2) math twice and history three times (3) history twice and geography three time (4) geography twice and history three times (5)math twice and geography three times and (6) geogrpahy twice and math three times.\\nSo the possibilities can be represented as {H H H M M}, {H H M M M} {H H H G G} {H H G G G} {M M M G G} and {M M G G G} each case involves 2 of 1 discipline and 3 of another.  In each case there are 10 ways for this to occur.  So for each sequence compute its probabuility, multiply by 10 and sum. Note a math occurrence in any place is 20/40 =1/2=0.5. History occurs with probability 15/40=3/8=0.375 on any occurrence and Geography occurs 5/40 =1/8 =0.125 on any occurrence.</p>\\n',\n",
       " '<p>I have code that generates a random permutation.  In my case, a permutation consists of N binary features, and each of the N features is set or unset randomly.  How many times must I generate a random permutation in order to be reasonably assured that I have covered all possible permutations?  I\\'m not sure how to define \"reasonably assured\" (50%, 95%?).</p>\\n\\n<p>For example, let\\'s say that N = 10, so there are 1024 possible permutations, how many times should I randomly generate a permutation to be 50% confident that I have generated all 1024 permutations?</p>\\n\\n<p>It seems to me that this is related to the 36.8% duplicate hit rate when drawing a sample with resampling, but I\\'m not a statistician.</p>\\n',\n",
       " \"<p>What's the best SVM implementation of SVMs in MATLAB? Right now I'm looking at PEGASOS and SVM-Perf, which both claims to have good performance with large datasets and classes. But there are so many implementation out there, and I definitely missed some. Can I get some other recommendations?</p>\\n\\n<p>My application is in bags of word object recognition that uses the histogram of the words in classification.</p>\\n\",\n",
       " '<p>Cute problem. This is the kind of stuff that probabilists do in their heads for fun.</p>\\n\\n<p>The technique is to assume that there is such a probability of extinction, call it $P$.  Then, looking at a one-deep decision tree for the possible outcomes we see--using the Law of Total Probability--that</p>\\n\\n<p>$P=\\\\\\\\frac{1}{4} + \\\\\\\\frac{1}{4}P + \\\\\\\\frac{1}{4}P^2 + \\\\\\\\frac{1}{4}P^3$\\r</p>\\n\\n<p>assuming that, in the cases of 2 or 3 \"offspring\" their extinction probabilities are IID.  This equation has two feasible roots, $1$ and $\\\\\\\\sqrt{2}-1$.  Someone smarter than me might be able to explain why the $1$ isn\\'t plausible.</p>\\n\\n<p>Jobs must be getting tight -- what kind of interviewer expects you to solve cubic equations in your head?</p>\\n',\n",
       " '<p>I saw a funny one in an article.</p>\\n\\n<p>Butter production in Bangladesh has one of the highest correlation with the S&amp;P 500 over a ten year period.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/QkQot.jpg\" alt=\"http://www.forbes.com/sites/davidleinweber/2012/07/24/stupid-data-miner-tricks-quants-fooling-themselves-the-economic-indicator-in-your-pants/\"></p>\\n',\n",
       " \"<p>I'm looking for some book recommendations, video lectures, tutorials or anything that can help me learn SAS.</p>\\n\",\n",
       " '<p>The argument for randomising the order of the questions is due to the fact that the answers to later questions can be biased by the presentation of earlier questions. Randomising the question order means that the influence is no longer subject to this ordering bias.</p>\\n\\n<p>With repeated administration of the same survey, the ordering effect is only removed for the first time the person completes the questionnaire. I would randomise for the first survey, and then not worry about it for the following surveys.</p>\\n\\n<p>This concern has been around for decades, I imagine the main paper came out in the 1950s or 1960s. But here are some references to look at:</p>\\n\\n<ul>\\n<li><a href=\"http://iimkbltn-dev.massey.ac.nz/V9/MB_V9_A3_Gendall.pdf\" rel=\"nofollow\">summary of questionnaire design considerations with references</a></li>\\n<li><a href=\"http://books.google.co.nz/books?hl=en&amp;lr=&amp;id=LZaJnQmNJ4UC&amp;oi=fnd&amp;pg=PA210&amp;dq=questionnaire%20order%20effects&amp;ots=n4Wp1J6pvc&amp;sig=QpmobDkVasLSkHh26tYdVodr1CQ#v=onepage&amp;q=questionnaire%20order%20effects&amp;f=false\" rel=\"nofollow\">chapter on questionnaire design, book</a></li>\\n<li><a href=\"http://books.google.co.nz/books?hl=en&amp;lr=&amp;id=AXRZbfHM_94C&amp;oi=fnd&amp;pg=PA5&amp;dq=questionnaire%20order%20effects&amp;ots=VNoGWYwn0A&amp;sig=rrBOKTjiyy9qmgdXHQuNVSUmGFQ#v=onepage&amp;q=questionnaire%20order%20effects&amp;f=false\" rel=\"nofollow\">another book, one of the Sage series</a></li>\\n<li><a href=\"http://onlinelibrary.wiley.com/doi/10.1002/9781118150382.ch2/summary\" rel=\"nofollow\">recent article on questionnaire design</a></li>\\n<li><a href=\"http://psycnet.apa.org/journals/cbs/10/1/68/\" rel=\"nofollow\">study looking at order effects on a particular questionnaire</a></li>\\n</ul>\\n\\n<p>Any good questionnaire design book will cover this off.</p>\\n',\n",
       " '<p>You can use <code>dput()</code> to get a <code>structure()</code> that can be used later.</p>\\n\\n<pre><code>&gt; #Build the original data frame\\n&gt; x &lt;- seq(1, 10, 1)  \\n&gt; y &lt;- seq(10, 100, 10)  \\n&gt; df &lt;- data.frame(x=x, y=y)   \\n&gt; df   \\n    x   y\\n1   1  10\\n2   2  20\\n3   3  30\\n4   4  40\\n5   5  50\\n6   6  60\\n7   7  70\\n8   8  80\\n9   9  90\\n10 10 100\\n\\n&gt; #Use the dput() statement to print out the structure of df\\n&gt; dput(df)   \\nstructure(list(x = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), y = c(10, \\n20, 30, 40, 50, 60, 70, 80, 90, 100)), .Names = c(\"x\", \"y\"), row.names = c(NA, \\n-10L), class = \"data.frame\")\\n</code></pre>\\n\\n<p>The above <code>structure</code> statement is the output of <code>dput(df)</code>.   If you copy/paste that into your R text file, you can use it later.   Here\\'s how.</p>\\n\\n<pre><code>&gt; #Build a new dataframe from the structure() statement\\n&gt; newdf &lt;- structure(list(x = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), y = c(10, \\n20, 30, 40, 50, 60, 70, 80, 90, 100)), .Names = c(\"x\", \"y\"), row.names = c(NA, \\n-10L), class = \"data.frame\")\\n&gt; newdf\\n    x   y\\n1   1  10\\n2   2  20\\n3   3  30\\n4   4  40\\n5   5  50\\n6   6  60\\n7   7  70\\n8   8  80\\n9   9  90\\n10 10 100\\n</code></pre>\\n',\n",
       " \"<p>Yes it will. Testing hypotheses &amp; calculating confidence intervals are different things.  There's a relation between them: if you obtain a 95% confidence interval of $\\\\\\\\left(\\\\\\\\mu_{\\\\\\\\textrm{low}},\\\\\\\\mu_{\\\\\\\\textrm{high}}\\\\\\\\right)$ it means that if you <em>were</em> to carry out a two-sided hypothesis test with a null hypothesis that $\\\\\\\\mu=\\\\\\\\mu_{\\\\\\\\textrm{low}}$ or $\\\\\\\\mu=\\\\\\\\mu_{\\\\\\\\textrm{high}}$ the p-value would be 5%.</p>\\n\",\n",
       " '<p>The answer to your first question is no. If the null hypothesis of unit root is rejected, the alternative in its most general form is stationary series with <a href=\"http://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test\">time trend</a>. Here is the example:</p>\\n\\n<pre><code>&gt; rr &lt;- 1+0.01*(1:100)+rnorm(100)\\n&gt; plot(rr)\\n&gt; adf.test(rr)\\n\\n    Augmented Dickey-Fuller Test\\n\\ndata:  rr \\nDickey-Fuller = -4.1521, Lag order = 4, p-value = 0.01\\nalternative hypothesis: stationary \\n\\nMessage d\\'avis :\\nIn adf.test(rr) : p-value smaller than printed p-value\\n</code></pre>\\n\\n<p>So your findings are consistent with ADF test: there is no unit root, but there is a time trend.</p>\\n',\n",
       " '<p>I am a second-year undergraduate student, studying Math, and I\\'ve been talking to one of my professors a good amount about the difference between mathematical ability and statistical ability. One of the key differences he brought up was \"data sense\" which he explained as a combination of technical ability while operating within a set of what I\\'ll informally call \"common sense restraints\" i.e. not losing sight of the reality of the problem amidst a lot of theory. This is an example of what I was talking about, which appeared on Gowers\\'s blog:</p>\\n\\n<blockquote>\\n  <p>In several parts of the UK the police gathered statistics on where road accidents took place, identified accident blackspots, put speed cameras there, and gathered more statistics. There was a definite tendency for the number of accidents at these blackspots to go down after the speed cameras had been installed. Does this show conclusively that speed cameras improve road safety?</p>\\n  \\n  <p>The same person who argued for the randomized strategy in the negotiation game basically knew the answer to this question already. He said no, since if you pick out the extreme cases then you would expect them to be less extreme if you run the experiment again. I decided to move on quickly from this question since there wasn’t a lot more to say. But I told people about a plan I had had, which was to do a bogus telepathy experiment. I would get them to guess the outcomes of 20 coin tosses, which I would attempt to beam to them telepathically. I would then pick the three best performers and the three worst, and would toss the coins again, this time asking the best ones to help me beam the answers to the worst ones. People could see easily that the performances would be expected to improve and that it would have nothing to do with telepathy.</p>\\n</blockquote>\\n\\n<p>What I\\'m asking is <strong>how to learn more about this \"data sense\"</strong>, through any publications on the subject, if they exist, or through what other users have found to be helpful in developing this skill.  I\\'m sorry if this question needs clarifying; if so, please post your questions! Thanks.</p>\\n',\n",
       " '<p>Assume a hypothetical scenario of two events. During event 1, I observe a set of values $[X_1, X_2,X_3,...,X_n]$. A physical phenomena occurs and this triggers event 2 for a short period of time and I end up observing another set of values $[Y_1, Y_2,Y_3,...,Y_n]$ during that time. I don\\'t know anything about the underlying distributions.</p>\\n\\n<p>I have a number of $X-Y$ pairs and I want a method to detect when the Y values are generally larger than the $X$ values for further investigation.  The pairs are not comparable - that is I cannot use the $X$ values from another subject to inform me about the distribution, however, in the null case I expect $X$ and $Y$ should be from the approximately the same distribution.</p>\\n\\n<p>My first attempt is to define event 2 to have some importance if the median of the observed values from event 2 is higher than that of the observed values in event 1.</p>\\n\\n<p>I have a few questions:</p>\\n\\n<ul>\\n<li>Because median is known to be robust to outliers, what are the pitfalls of such a comparison i.e., when will this fail? </li>\\n<li>If this is not good, then can I make use of something like <a href=\"http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\" rel=\"nofollow\">KL-Divergence</a> or <a href=\"http://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\" rel=\"nofollow\">JS-Divergence</a> to understand the extent of divergence (and if it is beyond a threshold, conclude that event 2 is important)?</li>\\n<li>Does comparing divergence have any advantages over the median?</li>\\n</ul>\\n\\n<p>I think I want to detect significant change as an event. Any clarification will be greatly appreciated.</p>\\n',\n",
       " '<p>I would say no for several reasons:\\n1. the individual scores are not independent because of sample reuse\\n2. the distribution is confined to [0,1] so it is truncated and not normal (could be approximately normal though if truncation is not too great)\\n3,  Saying \"some sort of hypothesis test\" does tell what you want to do.  How many methods are you comparing ?  If it is more than 2 are you comparing them pairwise? If one method has a higher average score what does that tell you? Maybe a nonparametric ANOVA is really what you need.</p>\\n',\n",
       " '<p>These are not PDFs, but there are quite a few good videos at the <a href=\"http://www.khanacademy.org/math/statistics\" rel=\"nofollow\">Khan Academy</a>.</p>\\n',\n",
       " \"<p>I have a time series. I want to model it using ARMA, which will be used for forcasting. </p>\\n\\n<p>In R I am using <code>arima()</code> function to get the coefficients. But <code>arima()</code> requires order(p,d,q) as input. What is the simplest way in R to arrive at a good value for p and q (with d = 0) so that I don't overfit?</p>\\n\",\n",
       " \"<p>If you're to use SPSS, I'd recommend this book: Data Analysis for the Behavioral Sciences Using SPSS by Weinberg &amp; Abramowitz. It is very well written and accessible. Note that it doesn't cover time-series, though.</p>\\n\",\n",
       " '<p>I just came across this webpage hosted by ICPSR on <a href=\"http://www.icpsr.umich.edu/icpsrweb/ICPSR/dmp/index.jsp\" rel=\"nofollow\">data management plans</a>. Although I think the goals of ICPSR will be somewhat different than your business (e.g. they are heavily interested in making the data readily able to be disseminated without violating confidentiality), I imagine they have useful information to businesses. Particularly advice on creating metadata seems to me to be universal. </p>\\n',\n",
       " '<p>One way is to use the properties of the <a href=\"http://en.wikipedia.org/wiki/KL_divergence\">Kullback-Leibler divergence</a>.</p>\\n\\n<p>Let $\\\\\\\\mathfrak{P}$ be the family of distributions with the given margins, and let $Q$ be the product distribution (and obviously $Q \\\\\\\\in \\\\\\\\mathfrak{P}$).</p>\\n\\n<p>Now, for any $P \\\\\\\\in \\\\\\\\mathfrak{P}$, the cross entropy is:</p>\\n\\n<p>$H(P,Q) = E_P [\\\\\\\\log q(X)] = E_P \\\\\\\\left[ \\\\\\\\log \\\\\\\\prod_i q_i(X) \\\\\\\\right] = \\\\\\\\sum_i E_P [\\\\\\\\log q_i(X)] = \\\\\\\\sum_i H(P_i,Q_i)$</p>\\n\\n<p>that is, the sum of the cross entropy of the margins. Since the margins are all fixed, this term itself must be fixed.</p>\\n\\n<p>Now we can write the KL divergence as:</p>\\n\\n<p>$D_{KL}(P \\\\\\\\| Q) = H(P,Q) - H(P)$</p>\\n\\n<p>and hence:</p>\\n\\n<p>$\\\\\\\\operatorname*{arg\\\\\\\\,min}_{P \\\\\\\\in \\\\\\\\mathfrak{P}} \\\\\\\\ D_{KL}(P \\\\\\\\| Q) = \\\\\\\\operatorname*{arg\\\\\\\\,max}_{P \\\\\\\\in \\\\\\\\mathfrak{P}} \\\\\\\\ H(P) $</p>\\n\\n<p>that is, the distribution $P$ which maximises the entropy is the one which minimises the KL divergence with $Q$, which by <a href=\"http://en.wikipedia.org/wiki/KL_divergence#Properties\">the properties of the KL divergence</a>, we know is $Q$ itself.</p>\\n',\n",
       " '<p>This a job for Bayes Theorem, not for null-hypothesis testing. You\\'ve given us some information, so we should now simply determine how much more or less consistent that information is with the hypothesis than with its negation &amp; adjust our priors accordingly. Based on what you\\'ve told us I conclude that the likelihood \"Bill is skilled\" is low--somewhere in the neighborhood of 10% (that is, I\\'ll <em>bet</em> he is \"lucky\" unless you offer me odds better than 9:1 against \"skilled\"). Here\\'s why:</p>\\n\\n<ol>\\n<li><p>Neither the failure to make hypotheses in advance of the 2 games, the lack of information about how many games have been played or will be played after the 2 in question, nor the possibility that someone is cheating, etc. really matter. Having been supplied no information bearing on the situation, we should assume an 8-player poker game drawn at random from the universe of such games being played. That deals w/ w/ all the \"what ifs\" &amp; \"could bes\" that Whuber draws our attention to: b/c the proportion of games that are fixed, or involve 1 avg player competing w/ 7 imbeciles, or are played w/ unshuffled decks etc., is tiny in comparison to the proportion of games that are fair &amp; involve \"run of the mill players,\" you will just be making your life complicated if you assume this game involves anything other than a \"normal game\" (if you have a different sense of how the universe of 8-person games is populated, then modify this part of the analysis accordingly; I\\'m just trying to demonstrate how to think about this problem!).</p></li>\\n<li><p>Based on billions of games &amp; meticulous record-keeping, I put the likelihood at about 0.10 that any randomly selected player in a normal game of poker is a \"skilled\" (again, if you have a different sense of what the talent distribution looks like, substitute your own estimate here).</p></li>\\n<li><p>In a normal game of poker, the likelihood that a skilled poker player will win any 2 consecutive hands against 7 randomly selected players is only a scintilla higher than the likelihood that an unskilled one will. Poker is a game of skill, yes, but variance is super high (if you think otherwise, fine, but in that case you definitely are not an experienced poker player). If you had said \"Bill made 2 consecutive 40-foot jump shots,\" or \"won 2 consecutive olympic marathons,\" in contrast, then my priors about how much better he is than the average baseketball player or the average marathon runner would shift much much more dramatically-- those are lower-variance indicators of those types of skill.</p></li>\\n<li><p>Because the likelihood ratio for the hypothesis and the negation of the hypothesis is thus very close to 1, you won\\'t do much better here than going with your priors. Again, mine is that there is 1/10 chance that Bill is skilled.</p></li>\\n</ol>\\n',\n",
       " '<p>Like Karl Broman said in his answer, a Bayesian approach would likely be a lot better than using confidence intervals.</p>\\n\\n<h3>The Problem With Confidence Intervals</h3>\\n\\n<p>Why might using confidence intervals not work too well? One reason is that if you don\\'t have many ratings for an item, then your confidence interval is going to be very wide, so the lower bound of the confidence interval will be small. Thus, items without many ratings will end up at the bottom of your list.</p>\\n\\n<p>Intuitively, however, you probably want items without many ratings to be near the average item, so you want to wiggle your estimated rating of the item toward the mean rating over all items (i.e., you want to push your estimated rating toward a <em>prior</em>). This is exactly what a Bayesian approach does.</p>\\n\\n<h3>Bayesian Approach I: Normal Distribution over Ratings</h3>\\n\\n<p>One way of moving the estimated rating toward a prior is, as in Karl\\'s answer, to use an estimate of the form $w*R + (1-w)*C$:</p>\\n\\n<ul>\\n<li>$R$ is the mean over the ratings for the items.</li>\\n<li>$C$ is the mean over all items (or whatever prior you want to shrink your rating to).</li>\\n<li>Note that the formula is just a weighted combination of $R$ and $C$.</li>\\n<li>$w = \\\\\\\\frac{v}{v+m}$ is the weight assigned to $R$, where $v$ is the number of reviews for the beer and $m$ is some kind of constant \"threshold\" parameter. </li>\\n<li>Note that when $v$ is very large, i.e., when we have a lot of ratings for the current item, then $w$ is very close to 1, so our estimated rating is very close to $R$ and we pay little attention to the prior $C$. When $v$ is small, however, $w$ is very close to 0, so the estimated rating places a lot of weight on the prior $C$.</li>\\n</ul>\\n\\n<p>This estimate can, in fact, be given a Bayesian interpretation as the posterior estimate of the item\\'s mean rating when individual ratings comes from a <em>normal</em> distribution centered around that mean.</p>\\n\\n<p>However, assuming that ratings come from a normal distribution has two problems:</p>\\n\\n<ul>\\n<li>A normal distribution is <em>continuous</em>, but ratings are <em>discrete</em>.</li>\\n<li>Ratings for an item don\\'t necessarily follow a unimodal Gaussian shape. For example, maybe your item is very polarizing, so people tend to either give it a very high rating or give it a very low rating.</li>\\n</ul>\\n\\n<h3>Bayesian Approach II: Multinomial Distribution over Ratings</h3>\\n\\n<p>So instead of assuming a normal distribution for ratings, let\\'s assume a <em>multinomial</em> distribution. That is, given some specific item, there\\'s a probability $p_1$ that a random user will give it 1 star, a probability $p_2$ that a random user will give it 2 stars, and so on.</p>\\n\\n<p>Of course, we have no idea what these probabilities are. As we get more and more ratings for this item, we can guess that $p_1$ is close to $\\\\\\\\frac{n_1}{n}$, where $n_1$ is the number of users who gave it 1 star and $n$ is the total number of users who rated the item, but when we first start out, we have nothing. So we place a <em>Dirichlet prior</em> $Dir(\\\\\\\\alpha_1, \\\\\\\\ldots, \\\\\\\\alpha_k)$ on these probabilities.</p>\\n\\n<p>What is this Dirichlet prior? We can think of each $\\\\\\\\alpha_i$ parameter as being a \"virtual count\" of the number of times some virtual person gave the item $i$ stars. For example, if $\\\\\\\\alpha_1 = 2$, $\\\\\\\\alpha_2 = 1$, and all the other $\\\\\\\\alpha_i$ are equal to 0, then we can think of this as saying that two virtual people gave the item 1 star and one virtual person gave the item 2 stars. So before we even get any actual users, we can use this virtual distribution to provide an estimate of the item\\'s rating.</p>\\n\\n<p>[One way of choosing the $\\\\\\\\alpha_i$ parameters would be to set $\\\\\\\\alpha_i$ equal to the overall proportion of votes of $i$ stars. (Note that the $\\\\\\\\alpha_i$ parameters aren\\'t necessarily integers.)]</p>\\n\\n<p>Then, once actual ratings come in, simply add their counts to the virtual counts of your Dirichlet prior. Whenever you want to estimate the rating of your item, simply take the mean over all of the item\\'s ratings (both its virtual ratings and its actual ratings).</p>\\n',\n",
       " \"<p>In both cases, the statistic will have the same asymptotic distribution since it's the tabular <em>proportions</em> which determine the level of association in the 2 by 2 tables. Fixing the number of samples drawn in one of the margins of the table is often done to improve the power of statistical tests, such as case-control sampling where the distribution of disease in the population is rare and the exposure is expensive to determine. You would conceivably test 100,000s of cancer negative individuals before obtaining a cancer positive individual using simple random sampling.</p>\\n\",\n",
       " \"<p>Consistency of an estimator means that as the sample size gets large the estimate gets closer and closer to the true value of the parameter.  Unbiasedness is a finite sample property that is not affected by increasing sample size.  An estimate is unbiased if its expected value equals the true parameter value.  This will be true for all sample sizes and is exact whereas consistency is asymptotic and only is approximately equal and not exact.  To say that an estimator is unbiased means that if you took many samples of size n and computed the estimate each time the average of all these estimates would be close to the true parameter value and will get closer as the number times you do this increases.  The sample mean is both consistent and unbiased.  The sample estimate of standard deviation is biased but consistent. As described below there are apparently pathological cases where the variance does not have to go to 0 for the estimator to be strongly consistent and the bias doesn't even have to go to 0 either.</p>\\n\",\n",
       " \"<p>First of all, if your classifier doesn't do better than a random choice, there is a risk that there simply is no connection between features and class. A good question to ask yourself in such a position, is weather you or a domain expert could infer the class (with an accuracy greater than a random classifier) based on given features. If no, then getting more data rows or changing the classifier won't help. What you need to do is get more data using different features.</p>\\n\\n<p>IF on the other hand you think the information needed to infer the class is already in the labels, you should check whether your classifier suffers from a high bias or high variance problem.</p>\\n\\n<p>To do this, graph the validation error and training set error, as a function of training examples. </p>\\n\\n<p>If the lines seem to converge to the same value and are close at the end, then your classifier has high bias and adding more data won't help. A good idea in this case is to either change the classifier for a one that has higher variance, or simply lower the regularization parameter of your current one.</p>\\n\\n<p>If on the other hand the lines are quite far apart, and you have a low training set error but high validation error, then your classifier has too high variance. In this case getting more data is very likely to help. If after getting more data the variance will still be too high, you can increase the regularization parameter.</p>\\n\\n<p>This are the general rules I would use when faced with a problem like yours.</p>\\n\\n<p>Cheers.</p>\\n\",\n",
       " '<p>I have three GLM models (poisson distribution) that each have different factors but the same response variable. I plan to compare the fit of the models using AIC, but first I want to see if any of the models are significant according to a p value. Is is possible to get a p value for a whole model w/glm? Each model has 1 significant factor and one or two non-significant factors, but I was told to leave these non-significant factors in the models instead of removing them with sinful backwards stepwise regression. </p>\\n',\n",
       " '<p>They are different because of <a href=\"http://en.wikipedia.org/wiki/Jensen_inequality\" rel=\"nofollow\">Jensen\\'s inequality</a>: an expected value of a nonlinear function is not equal to the nonlinear function of expected values.</p>\\n\\n<p>I am not sure we can answer the question of what\\'s better for you. If <code>x1</code> is close to zero, then the first method would generate Cauchy normal variates, and if the population mean is zero, so would the second method.  If both population means are zeroes, then whatever you do with them in terms of their ratios will end up as 0/0.</p>\\n',\n",
       " '<p>This book on <a href=\"http://www.springer.com/mathematics/quantitative+finance/book/978-3-540-88232-9\" rel=\"nofollow\">Non-Life Insurance Mathematics</a> might actually work for you. It is oriented towards applications, though not engineering applications, but it is completely theoretically sound. If I am not mistaken, there is something in the book on Bayes estimation too.</p>\\n\\n<p>Note that you should look for the Cramèr-Lundberg model in the context of the book to find the compound Poisson process.</p>\\n',\n",
       " '<p>The Box Cox transformation is</p>\\n\\n<p>$y^{\\\\\\\\lambda}_i = \\\\\\\\frac{y_i^{\\\\\\\\lambda} - 1}{\\\\\\\\lambda} \\\\\\\\mbox{ if }  \\\\\\\\lambda\\\\\\\\ne 0 $\\nand $ \\\\\\\\ln(y_i) \\\\\\\\mbox{ if } \\\\\\\\lambda= 0 $</p>\\n',\n",
       " \"<p>I have my own implementation of a Wilcoxon signed rank test.  In my code, I of course calculate W- (sum of ranks of negative numbers) and W+ (sum of ranks of positive numbers);  the function provides a parameter for the user to select the alternate hypothesis, just as I have seen done in R:</p>\\n\\n<ul>\\n<li>lesser - the true location is less than zero</li>\\n<li>two-sided - the true location is not zero</li>\\n<li>greater - the true location is greater than zero</li>\\n</ul>\\n\\n<p>I also calculate the distributions of ranks as follows:\\n  if we have n ranked (non-zero) values, let a[i] (i from 0 to n(n-1)/2) be the outcomes for n-1.  Then the outcomes for n are b[i+n] = a[i] + a[i+n] (with a[j] = 0 for j out of bounds)</p>\\n\\n<p>Now for lesser I calculate the pvalue as sum of b[i=0..W-], for greater b[i=0..W+], and two sided as 2 b[i=0..min(W-,W+)].  But it turns out that my results are opposite of R -- i.e. my 'greater' p-value is R's 'lesser' p-value, and vice versa.  Is there some aspect I am not understanding with how to calculate the p-value?</p>\\n\",\n",
       " '<p>Thirty people are invited to a holiday party, where a gift exchange will take place. Each person brings a gift to put into the pot, and at the end of the night each person selects&mdash;at random&mdash;one gift from the pot to take home.</p>\\n\\n<p>The probability of one (1) person accidentally choosing their own gift from the pot is equal to $\\\\\\\\frac{1}{30} \\\\\\\\approx 3.33\\\\\\\\%$, obviously.</p>\\n\\n<p>What is the probability that <strong>at least</strong> one person will accidentally choose their own gift? In other words, how unlikely is it that all 30 guests will take home a new present?</p>\\n',\n",
       " '<p>Are there any free statistical textbooks available? </p>\\n',\n",
       " '<p>data cleaning of surveys takes longer than analysis and report write-up, so you\\'re not alone. :)</p>\\n\\n<p>Normally in a survey, we path the questions for respondents. So, for example, in computer-assisted telephone interviewing (or online interviews, face-to-face interviewing with a laptop), the survey programmers code the survey to literally skip questions that should not be presented if the respondent answers a particular way.</p>\\n\\n<p>It appears that a question skip pattern was missing from this survey, for whatever reason. If a skip pattern should have been implemented, then yes you can post-hoc introduce it for questions 2 and 3 and change the \"should not have answered\" responses to system-missing (or other missing code you\\'re using).</p>\\n\\n<p>There are a lot of survey books out there, and the ones for you will really depend on your particular need as they all have various strengths and weaknesses. Have a look at the range of books by David De Vaus, such as <a href=\"http://rads.stackoverflow.com/amzn/click/0761959386\">Analysing Social Science Data</a> - this looks particularly good for your situation. David De Vaus has written a number of other social science survey books, and they all come recommended. The <a href=\"http://rads.stackoverflow.com/amzn/click/0471698687\">Dillman et al book</a> also came highly recommended to me, although I have not used it myself. </p>\\n\\n<p>I also recommend <a href=\"http://www.unescap.org/stat/disability/pre-pilot-training/background-note-on-cognitive-testing.pdf\">cognitive testing</a> followed by <a href=\"http://wwwn.cdc.gov/qbank/QEM/Esposito_FieldStudies_QEM_Primary_Paper.pdf\">field testing</a> of a questionnnaire before going live with the survey. This type of testing is designed to show up question sequencing issues, while also showing how respondents interpret questions (this is sometimes not the same way as intended by the questionnaire designer!). While this process is too late for your current survey, you can implement it for future surveys.</p>\\n\\n<p>Best wishes with your survey analysis.</p>\\n',\n",
       " '<p>I have a textbook entitled Introduction to Econometrics, 3rd ed. by Stock and Watson that reads, \"if the errors are heteroskedastic, then the t-statistic computed using the homoskedasticity-only standard error does not have a standard normal distribution, even in large samples.\" I believe you cannot do proper inference/hypothesis testing without being able to assume your t-statistic is distributed as standard normal. I have a LOT of respect for Wooldridge (in fact, my graduate-level class also used his book) so I believe what he says about the t-stats using robust SEs require large samples to be appropriate is definitely correct, but I think we often have to deal with the large-sample requirement, and we accept that. However, the fact that using non-robust SEs won\\'t give a t-stat with the proper standard normal distribution <em>even if you DO have a large sample</em> creates a much bigger challenge to overcome. </p>\\n',\n",
       " '<p>The sample skewness $$\\\\\\\\gamma=\\\\\\\\frac{\\\\\\\\sum_{i=1}^n(x_i-\\\\\\\\bar{x})^3}{\\\\\\\\Big(\\\\\\\\sum_{i=1}^n(x_i-\\\\\\\\bar{x})^2\\\\\\\\Big)^{3/2}}$$ and the sample (excess) kurtosis $$\\\\\\\\kappa=\\\\\\\\frac{\\\\\\\\sum_{i=1}^n(x_i-\\\\\\\\bar{x})^4}{\\\\\\\\Big(\\\\\\\\sum_{i=1}^n(x_i-\\\\\\\\bar{x})^2\\\\\\\\Big)^{2}}-3$$ are often used as measures of non-normality.</p>\\n\\n<p>The sample skewness measures the asymmetry of the empirical distribution. If it is far from $0$, the distribution is not very symmetric. Since the normal distribution is symmetric, a sample from the normal distribution should be close to $0$.</p>\\n\\n<p>The sample kurtosis measures the \"peakedness\" of the distribution. If it is much greater than $0$, then the distribution is more peaked than the normal distribution, which typically means that it has heavier tails. If it is less than $0$ it is less peaked, which typically means that the distribution is bimodal. The sample kurtosis is bounded from below by $-2$ (a value that is obtained for a two-point distribution, which of course is extremely bimodal).!</p>\\n\\n<p>Here are two examples (normal distribution in grey, other distributions in red):</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/OyLw8.png\" alt=\"enter image description here\"></p>\\n\\n<p>The skew distribution has theoretical skewness $1.6$ whereas the kurtotic distributions has theoretical (excess) kurtosis $1.5$. As you can see, the kurtotic distribution has heavier tails than the normal distribution.</p>\\n\\n<p>So, why use skewness and kurtosis as quantifications of non-normality? The main reason is that they affect the asymptotics of the <a href=\"http://en.wikipedia.org/wiki/Central_limit_theorem\" rel=\"nofollow\">central limit theorem</a>, which as you may know often can be used to motivate the use of a statistical procedure (that is based on normality) even if the data does not come from a normal distribution, given that you have a \"large enough\" sample. If either the skewness or the kurtosis is high, larger sample sizes are needed for such motivations to be valid.</p>\\n\\n<p>For some inferential procedures you need to worry more about skewness, and for some you need to worry about heavy tails (kurtosis). I\\'ve written <a href=\"http://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless/30053#30053\">more about that elsewhere on this site</a>.</p>\\n',\n",
       " '<p>The adaboost method gives the predictions on logit scale. \\nYou can convert it to the 0-1 output:</p>\\n\\n<pre><code>gbm_predicted&lt;-plogis(2*gbm_predicted)\\n</code></pre>\\n\\n<p>note the 2* inside the logis</p>\\n',\n",
       " '<p>I have studied this topic in my thesis, “Cornish-Fisher Expansion and Value-at-Risk method in application to risk management of large portfolios” and it is currently available as pdf, you will need to download it. For time scaling, after modifying the variance formula you will just need to multiply by the time factor square root of estimation time, since time decay is independent of the parameters: mean, variance, skewness, etc.</p>\\n\\n<p>I have the formula in the thesis, hope it will help. Just search by the thesis name, you will find the pdf in diva portal.</p>\\n',\n",
       " '<p>One of the problems of a national census is that it takes a long time to get all the data together so it may be a while until it appears but if you want census data try\\n<a href=\"https://www.census.ac.uk\" rel=\"nofollow\">https://www.census.ac.uk</a></p>\\n',\n",
       " '<p>Homework?</p>\\n\\n<p>Hint:</p>\\n\\n<p>Remember the binomial theorem:</p>\\n\\n<p>$$\\\\\\\\n\\\\\\\\n(x+y)^n = \\\\\\\\sum_{k=0}^{n}\\\\\\\\binom{n}{k}x^ky^{n-k}\\\\\\\\n\\\\\\\\n$$</p>\\n\\n<p>Now, if you could just find x and y so that $x^ky^{n-k}$ is constant... </p>\\n',\n",
       " \"<p>I like what @whuber said, but one additional note - if your scores are percentages, they aren't really ranks but ratings.  Ranks have integer values, except for ties (which are handled in various ways).</p>\\n\",\n",
       " '<p>This turns out to be a two-part answer</p>\\n\\n<h3>1. Log-Determinant</h3>\\n\\n<p>The <code>spdep</code> package does contain tools to efficiently calculate the log-determinant of a sparse weights matrix. The following lines of code drop the calculation time from 11 seconds to about 2. The code is not very clean because the methods are not intended for use by typical statistical consumers.</p>\\n\\n<pre><code># Calculate log-determinant of (I- rho*W) - using methods in the spdep package\\nenv &lt;- new.env(parent=globalenv())\\nassign(\"listw\", Wlist, envir=env)\\nassign(\"can.sim\", can.sim, envir=env)\\nassign(\"similar\", FALSE, envir=env)\\nassign(\"family\", \"SAR\", envir=env)\\nassign(\"n\", n, envir=env)\\nMatrix_setup(env, Imult=2, super=FALSE)\\nget(\"similar\", envir=env)\\nlogdet &lt;- do_ldet(rho, env)\\n</code></pre>\\n\\n<h3>2. Maximum Likelihood Radius</h3>\\n\\n<p>Roger Bivand, the package author, explained to me that such a method is not really valid, and I discovered this myself when I couldn\\'t find any traction on the likelihood function. What he instead recommended was to simply estimate the same model specification over a grid of radii and select the radius that provided the maximum point. The figure below shows how the likelihood value changes for higher sample sizes and different radii on my dataset.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/xjPsp.png\" alt=\"Likelihood versus radius as sample size increases\"></p>\\n',\n",
       " \"<p>Although you wrote the question as though it was addressed to me, I think others are equally qualified to give their advice and their comments and answers should be welcomed.  My feeling is that since you want to do inference concentrate on the appropriate tests based on what you have done thus far. You want to compare 3 groups, apply a test that compares three groups.  The small sample size is not a reason to exclude group C.  If there is a clear difference between groups 1 and 2 I don't see how group 3 would hurt the analysis.  You will atill reject equality among the groups. When doing the pairwise tests you probably then will be able to detect a difference between A and B.  It may be that C differs from at least one of the other two groups but the small sample size for group C may mean that the power to detect the difference is too low.  So you probably won't be able to reject equality between A and C and B and C.  But this is to be expected and if the difference is large enough it might be detected even though c has a sample size of 5.</p>\\n\\n<p>Others may have different opinions and if so they should comment.</p>\\n\",\n",
       " \"<p>I'm a journalist, and I am trying to work out whether hospitals that are in political districts with low majorities (i.e. where the political representative is fighting hard for his or her seat) are more likely to get extra funding. </p>\\n\\n<p>In other words, do hospitals that have received extra funding tend to be in districts with relatively low majorities? I'm not sure which significance test is most appropriate here. </p>\\n\\n<p>Here are my numbers: </p>\\n\\n<ul>\\n<li>There are 600 districts overall. The average political majority is 18.5%, with a s.d. of 12.1%. </li>\\n<li>There are 1588 hospitals overall. Considering each hospital as one member of the population, and looking at the district it is in, the average majority is 18.6% with a s.d. of 12.6%. </li>\\n<li>There are 203 hospitals that have received extra funding. Considering each hospital as one element in the population, and looking at the district it is in, the average majority is 16.6% with a s.d. of 11.5%.</li>\\n</ul>\\n\\n<p>So I an see that the hospitals are in districts with lower majorities on average, but I'm not sure whether this counts as significant.</p>\\n\\n<p>(I'm pretty sure there is something going on! I also have the stats for each individual hospital, if that helps. It's a long time since I did statistics at college and I have forgotten whether I should be looking at means or at something else.)</p>\\n\\n<p>What complicates this for me is that the distribution of majorities probably isn't Normal, because I'm looking at the modulo majority, and am not concerned with which party has the majority. </p>\\n\\n<p>Any thoughts on how to assess the significance of this finding?</p>\\n\",\n",
       " \"<p>There's an argument to be made that there isn't any amount of subjects you can add to reject the null assuming you want to maintain the same alpha level you started with.  You've already done your test on your experiment.  Assuming you used an alpha of 0.05 to select your cutoff, another test of additional data in this experiment will have an alpha > 0.05.  There are no numbers of subjects that change that.</p>\\n\\n<p>What you can do is get the effect size and variance in this experiment and use a program like G*Power or the command <code>power.t.test</code> in R to work out how many subjects you would need for a subsequent experiment in which you tested the effect again.</p>\\n\\n<p>You really need to examine your effect size and confidence interval of your effect at this point, both reported in many t-test programs.  Is the range of effects captured by your confidence interval narrow enough?  If so, then you stop and say that there really isn't any meaningful difference between your conditions.  If the confidence interval is very wide then you may collect data to make it narrower to make a stronger statement about what the effect actually is.  But you cannot ever get alpha back down to 0.05 without a new experiment.</p>\\n\",\n",
       " '<p>I am trying to calculate inter-rater reliability scores for 10 survey questions (on which there were 2 raters)--seven questions are binary (yes/no), and 3 are Likert-scale questions.</p>\\n\\n<ol>\\n<li><p>Should inter-rater reliability be tested on EACH of the 10 questions, or is there an overall inter-relater reliability test that tests reliability of all questions at one time? If so, what is it?</p></li>\\n<li><p>For the binary questions, the agreement level between the two raters is 70-90% on nearly every question, however the Kappa score is often very poor (0.2- 0.4).  Can this be right?  (And if so, is there a more appropriate test?)</p></li>\\n<li><p>Finally, can you use a Kappa-based test on Likert scale questions? If not, what is the correct test for inter-rater reliability?</p></li>\\n</ol>\\n',\n",
       " '<p>Give a better justification for outlier removal than 3 SD.  Items happen at 3 SD by chance, at a pretty high probability, when you have this many samples.  There are also many other issues (see Miller (1991) for an example).  There may be improbable values based on theory (e.g. impossibly fast, or ridiculously slow).  Remove outliers because of that.  Don\\'t use an arbitrary (which is what this is) statistical procedure over your own judgment.  What if an RT of 80 ms is not 3SD away?  Do you keep it?  It\\'s not physically possible to be actually based on perception of a stimulus for button presses or vocal responses.  With a choice task values like the 190 ms in your plot would not be believable (and that looks like it\\'s post outlier removal).  You mention several seconds must be outliers, then remove them for that reason. They must reflect processes other than a response to the stimulus.  If you have an accuracy measure you can use those to guide outlier removal.  Perhaps accuracy is very low under a certain RT or drops off past a different RT.</p>\\n\\n<p>You are <em>not</em> allowed to use all of the degrees of freedom in all of your measurements in single level ANOVA because this is a repeated measures design.  You must aggregate your numbers so that each subject produces 4 numbers, 1 value in each condition.</p>\\n\\n<pre><code>items.agg &lt;- aggregate(TextDisplay9.RT ~ Conflicting + ContextPresent + Subject, itemsitems.cropped, mean)\\n</code></pre>\\n\\n<p>If you examine <em>these</em> aggregate data you may find your skew problem is solved through the central limit theorem (although the n is a bit low for that).  It would almost definitely be solved for the reciprocal of the RT in seconds (rate).  RT is an arbitrary representation of performance.  It is the time it took to complete the task.  The rate would be how many tasks can be completed per second.  They are both easily interpretable numbers but the latter has much better statistical properties. (do not forget that the meaning of rate is opposite of RT in that higher numbers = better performance)</p>\\n\\n<p>And then you have to stratify your results in the repeated measures ANOVA</p>\\n\\n<pre><code>m &lt;- aov(TextDisplay9.RT ~ Conflicting * ContextPresent + Error(Subject / (Conflicting * ContextPresent)), data = items.agg)\\nsummary(m)\\n</code></pre>\\n\\n<p>You\\'ll have much less power in your study now.</p>\\n\\n<p>You really should look at <a href=\"http://www.ualberta.ca/~baayen/publications/baayenCUPstats.pdf\" rel=\"nofollow\">Baayen\\'s (2008)</a> book on studying linguistic RT data.  It\\'s very specific to your field and avoids much of the messy statistical theory while being very practically helpful.</p>\\n\\n<p>Miller, J. (1991). Reaction time analysis with outlier exclusion: Bias varies with sample size. The Quarterly Journal of Experimental Psychology, 43A(4):907–912.</p>\\n',\n",
       " '<p>I’m studying the Restricted Boltzmann Machine (RBM) and having some issues understanding log likelihood calculations w.r.t. parameters of the RBM. \\nEven though a lot of research papers on RBM have been published, there are no detailed steps of the derivatives. After searching online I was able to find the following document.</p>\\n\\n<p><a href=\"http://image.diku.dk/igel/paper/AItRBM-proof.pdf\" rel=\"nofollow\">http://image.diku.dk/igel/paper/AItRBM-proof.pdf</a></p>\\n\\n<p>However, the details of this document are too advanced for me. Can somebody point out me to a good tutorial/lecture note about RBM?</p>\\n\\n<p>Edit: @ David  Confusing sections are shown below ( equation 29 in page 26).</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/xaIcq.png\" alt=\"log likelihood calculations\"></p>\\n',\n",
       " '<p>There is no best forecasting package in R. For one thing, R is open source so there are often multiple packages that do similar things and you can choose which one seems to be the most up-to-date and works to your taste.</p>\\n\\n<p>More importantly, though, is: how you forecast depends on how you model. Do you have a univariate time series and nothing but that data? Or do you have a univariate time series with several predictor variables? Do you have a multivariate time series? Are you even using a time series at all? (\"Forecasting\" implies time series, but that\\'s not necessarily the case.)</p>\\n\\n<p>So the first question would probably be: what data do you have, and what kind of model are you trying to make? Modeling options and forecasting options will then be easier.</p>\\n',\n",
       " \"<p>I've computed a scale with several theoretically defined factors. Now I am doing a factorial extraction to see if EFA discovers the same dimensions as in theory. My question is:  </p>\\n\\n<ol>\\n<li><p>Do I need to check internal reliabilities of the theoretical scales before doing the factorial analysis, that is take the items that are supposed to belong to a given scale and check the internal consistency of this scale? Then based on reliability coefficients, do I eliminate certain items that lower scale reliability and with the left items conduct the factorial analysis?  </p></li>\\n<li><p>Do I first extract the factors, try to determine the scales, whether they are consistent with the theoretical defined ones or not,  and then take the items found to comprise a scale with factor analysis and try to  determine their internal consistency? \\nIn other words, do I first check scales reliability and then conduct EFA, or first conduct EFA then check scale reliablity, after having eliminated some items?</p></li>\\n</ol>\\n\",\n",
       " \"<p>The question implies that the <em>standard deviation</em> (SD) is somehow normalized so can be used to compare the variability of two different populations. Not so. As Peter and John said, this normalization is done as when calculating the <em>coefficient of variation</em> (CV), which equals SD/Mean. The SD is in in the same units as the original data. In contrast, the CV is a unitless ratio. </p>\\n\\n<p>Your choice 1 (IQR/Median) is analogous to the CV. Like the CV, it would only make sense when the data are ratio data. This means that zero is really zero. A weight of zero is no weight. A length of zero is no length. As a counter example, it would not make sense for temperature in C or F, as zero degrees temperature (C or F) does not mean there is no temperature. Simply switching between using C or F scale would give you a different value for the CV or for the ratio of IQR/Median, which makes both those ratios meaningless.</p>\\n\\n<p>I agree with Peter and John that your second idea (Range/IQR) would not be very robust to outliers, so probably wouldn't be useful.</p>\\n\",\n",
       " '<p>Suppose we have constructed a model of some stochastic system; we are also able to perform Monte Carlo simulations of this system. Now, we have two sets of samples: one from our model and one from the MC-based approach, and we would like to assess the accuracy of our technique. </p>\\n\\n<p>The first two accuracy measures, which come to my mind, are the differences of expectations and variances. Then, one can try to construct the empirical PDFs/CDFs and compare them at certain points, e.g., find the RMSE. </p>\\n\\n<p>What are the most preferable and exhaustive ways to draw this comparison?  Which metrics are the first ones to look at, which are a must? </p>\\n',\n",
       " \"<p>I've designed a 4 classifiers which perform pretty decently (all of them are above 90% in accuracy).</p>\\n\\n<p>However, they don't have similar AUC for their respective ROC curves (obviously, it doesn't have to be).</p>\\n\\n<p>If I were to use these classifiers in real-time data, which one do I choose based on the following result</p>\\n\\n<p><strong>Classifier A</strong>: \\nAccuracy: 100%,  AUC: 84%</p>\\n\\n<p><strong>Classifier B</strong>:\\nAccuracy: 95%,  AUC: 83%</p>\\n\\n<p><strong>Classifier C</strong>:\\nAccuracy: 100%,  AUC: 69%</p>\\n\\n<p><strong>Classifier D</strong>:\\nAccuracy: 100%,  AUC: 77%</p>\\n\",\n",
       " '<p>The R <a href=\"http://cran.r-project.org/web/packages/psych/index.html\" rel=\"nofollow\">psych</a> package includes various routines to apply Factor Analysis (whether it be PCA-, ML- or FA-based), but see my short review on <a href=\"http://crantastic.org/packages/psych\" rel=\"nofollow\">crantastic</a>. Most of the usual rotation techniques are available, as well as algorithm relying on simple structure criteria; you might want to have a look at W. Revelle\\'s paper on this topic, <a href=\"http://personality-project.org/revelle/publications/vss.pdf\" rel=\"nofollow\">Very Simple Structure: An Alternative Procedure For Estimating The Optimal Number Of Interpretable Factors</a> (MBR 1979 (14)) and the <code>VSS()</code> function. </p>\\n\\n<p>Many authors are using orthogonal rotation (VARIMAX), considering loadings higher than, say 0.3 or 0.4 (which amounts to 9 or 16% of variance explained by the factor), as it provides simpler structures for interpretation and scoring purpose (e.g., in quality of life research); others (e.g. Cattell, 1978; Kline, 1979) would recommend oblique rotations since \"in the real world, it is not unreasonable to think that factors, as important determiners of behavior, would be correlated\" (I\\'m quoting Kline, <em>Intelligence. The Psychometric View</em>, 1991, p. 19). </p>\\n\\n<p>To my knowledge, researchers generally start with FA (or PCA), using a scree-plot together with simulated data (parallel analysis) to help choosing the right number of factors. I often found that item cluster analysis and VSS nicely complement such an approach. When one is interested in second-order factors, or to carry on with SEM-based methods, then obviously you need to use oblique rotation and factor out the resulting correlation matrix.</p>\\n\\n<p>Other packages/software:</p>\\n\\n<ul>\\n<li><a href=\"http://lavaan.ugent.be/\" rel=\"nofollow\">lavaan</a>, for latent variable analysis in R;</li>\\n<li><a href=\"http://openmx.psyc.virginia.edu/\" rel=\"nofollow\">OpenMx</a> based on <a href=\"http://www.vcu.edu/mx/\" rel=\"nofollow\">Mx</a>, a general purpose software including a matrix algebra interpreter and numerical optimizer for structural equation modeling.</li>\\n</ul>\\n\\n<p><strong>References</strong><br>\\n 1. Cattell, R.B. (1978). The scientific use of factor analysis in behavioural and life sciences. New York, Plenum.<br>\\n 2. Kline, P. (1979). Psychometrics and Psychology. London, Academic Press.</p>\\n',\n",
       " '<p>Suppose I roll a 4-side dice 25 time to get a final sum between 25 and 100. How I calculated the distribution of probability for each sum between 25 and 100?</p>\\n\\n<p>Thank you</p>\\n',\n",
       " '<p>Not knowing about the type of data you are using, but having been stuck in similar situations in the past: </p>\\n\\n<p>1) If your groups are sub-members (as would be mesh blocks in a census), then it may be cheap to define a hierarchy on larger groups. To use the analogy of a census, this will depend on whether you think there are sufficiently informative differences in the mesh blocks within the same local government area, or whether the differences between local government areas would be sufficient. </p>\\n\\n<p>2) Your problem may not be all that bad (apart from the computation), so long as each group has enough observations. Chapter 18 of <a href=\"http://rads.stackoverflow.com/amzn/click/052168689X\" rel=\"nofollow\">Gelman and Hill\\'s book</a> goes through this quite intuitively. Essentially, if each of your groups has only a few (or zero) observations, then you just get pooled estimate; the more observations the group has, the further from the pooled estimate its parameter estimate could possibly be. </p>\\n\\n<p>3) Recall you need a weighting vector as well, to account for the possible differences in the number of observations in each group.</p>\\n\\n<p>4) I would consider cutting down the number of varying slopes, as this adds a lot of columns. Though, again, this depends on your research question. </p>\\n\\n<p>Good luck!</p>\\n',\n",
       " '<p>+1 for writing a clear and detailed question.  I hope that my answer isn\\'t too frustrating.  I believe that hypothesis testing is not an appropriate approach in your case.  Null hypothesis significance testing is a reasonable thing to do when the answer <em>could</em> be yes or no, <em>but you don\\'t know which</em>.  (Unfortunately, it doesn\\'t actually tell you which, but this is a different issue.)  In your case, I gather, you want to know if your algorithm is good.  However, it is known (with certainty), that no computer program can generate truly random data from any probability distribution.  This is true firstly, because all computers are <a href=\"http://en.wikipedia.org/wiki/Finite-state_machine\">finite state machines</a>, and thus can only produce <a href=\"http://en.wikipedia.org/wiki/Pseudorandom_number_generator\">pseudorandom numbers</a>.  Furthermore (setting the lack of true randomness aside), it is not possible that the generated values perfectly follow any continuous distribution.  There are several ways to understand this, but perhaps the easiest is that there will be \\'gaps\\' in the number line, which is not true of any continuous random variable.  Moreover, these gaps are not all perfectly equally wide or perfectly equally spaced.  Among computer scientists who work on pseudorandom number generation, the name of the game is to improve the algorithms such that the gaps are smaller, more even, with longer periods (and also that can generate more values faster).  At any rate, these facts establish that hypothesis testing is the wrong approach for determining if your algorithm is properly following \"a specific, known probability distribution\", because it isn\\'t.  (Sorry.)  </p>\\n\\n<p>Instead, a more appropriate framework is to determine <em>how close</em> your data are to the theoretical distribution.  For this, I would recommend reconsidering plots, specifically <a href=\"http://en.wikipedia.org/wiki/Q-Q_plot\">qq-plots</a> and <a href=\"http://en.wikipedia.org/wiki/P-P_plot\">pp-plots</a>.  (Again, I recognize that this must be frustrating, and I apologize for that.)  However, you don\\'t have to actually make the plots or look at them, as weird as that sounds.  Instead, having converted your data appropriately for plotting, and having calculated the corresponding values from the theoretical distribution in question, you can correlate them.  This gives you a number, specifically an r-score, just like you want.  Moreover, the number gives you an appropriate measure of how good your algorithm is.  For this process, you can generate as much data as you would like; more data will give you more precision with respect to the measurement.  That is, we have shifted our conception of power from $1-\\\\\\\\beta$, the probability of rejecting a truly false null (which is guaranteed), to the <a href=\"http://nd.edu/~kkelley/publications/articles/Maxwell_Kelley_Rausch_2008.pdf\">accuracy in parameter estimation</a> perspective.  Obviously, your goal here is to produce an algorithm that gets you as close to $r=1$ as possible.  It may well be worthwhile to do this for both types of plots as they have different strengths and weaknesses (specifically, qq-plots give you better resolution in the tails of the distribution, whereas pp-plots afford better resolution in the center).  </p>\\n\\n<p>On one other note, with regard to evaluating the quality of your algorithm, you may want to time it relative to other standard pRNG\\'s.  </p>\\n\\n<p>Hope this helps.</p>\\n',\n",
       " '<p>Use <a href=\"http://elki.dbs.ifi.lmu.de/\" rel=\"nofollow\">ELKI</a>. It not only has <a href=\"http://elki.dbs.ifi.lmu.de/wiki/Algorithms\" rel=\"nofollow\">tons of anomaly detection algorithms</a> (they call them \"outlier detection\" though), but it also is significantly faster than the others, in particular when you used indexes.</p>\\n',\n",
       " \"<p>After doing some reading on stochastic processes for work I've found that a proof that the specific process <em>exists</em> is often one of the first things presented. </p>\\n\\n<p>Could someone please explain, in layman's terms, the purpose/necessity of this proof? </p>\\n\",\n",
       " '<p><em>It is possible that this question is homework but I felt this classical elementary probability question was still lacking a complete answer after several months, so I\\'ll give one here.</em> </p>\\n\\n<p>From the problem statement, we want the distribution of</p>\\n\\n<p>$$Y = \\\\\\\\max \\\\\\\\{ X_1, ..., X_n \\\\\\\\}$$</p>\\n\\n<p>where $X_1, ..., X_n$ are iid ${\\\\\\\\rm Uniform}(a,b)$. We know that $Y &lt; x$ if and only if every element of the sample is less than $x$. Then this, as indicated in @varty\\'s hint, combined with the fact that the $X_i$\\'s are independent, allows us to deduce</p>\\n\\n<p>$$ P(Y \\\\\\\\leq x) = P(X_1 \\\\\\\\leq x, ..., X_n \\\\\\\\leq x) = \\\\\\\\prod_{i=1}^{n} P(X_i \\\\\\\\leq x) = F_{X}(x)^n$$</p>\\n\\n<p>where $F_{X}(x)$ is the <a href=\"http://en.wikipedia.org/wiki/Uniform_distribution_%28continuous%29#Cumulative_distribution_function\">CDF of the uniform distribution</a>. Therefore the CDF of $Y$ is \\n$$F_{Y}(y) = P(Y \\\\\\\\leq y) = \\\\\\\\begin{cases} \\n0 &amp; y \\\\\\\\leq a \\\\\\\\\\\\\\\\ \\n\\\\\\\\phantom{} \\\\\\\\left[ (y-a)/(b-a) \\\\\\\\right]^n &amp; y\\\\\\\\in(a,b) \\\\\\\\\\\\\\\\\\n1 &amp; y \\\\\\\\geq b \\\\\\\\\\\\\\\\ \\n\\\\\\\\end{cases}$$</p>\\n\\n<p>Since $Y$ has an absolutely continuous distribution <a href=\"http://en.wikipedia.org/wiki/Probability_density_function#Absolutely_continuous_univariate_distributions\">we can derive its density by differentiating the CDF</a>. Therefore the density of $Y$ is </p>\\n\\n<p>$$ p_{Y}(y) = \\\\\\\\frac{n(y-a)^{n-1}}{(b-a)^{n}}$$</p>\\n\\n<p>In the special case where $a=0,b=1$, we have that $p_{Y}(y)=ny^{n-1}$, which is the density of a <a href=\"http://en.wikipedia.org/wiki/Beta_distribution\">Beta distribution</a> with $\\\\\\\\alpha=n$ and $\\\\\\\\beta=1$, since ${\\\\\\\\rm Beta}(n,1) = \\\\\\\\frac{\\\\\\\\Gamma(n+1)}{\\\\\\\\Gamma(n)\\\\\\\\Gamma(1)}=\\\\\\\\frac{n!}{(n-1)!} = n$. </p>\\n\\n<p>As a note, the sequence you get if you were to sort your sample in increasing order - $X_{(1)}, ..., X_{(n)}$ - are called the <a href=\"http://en.wikipedia.org/wiki/Order_statistic\">order statistics</a>. A generalization of this answer is that <a href=\"http://en.wikipedia.org/wiki/Order_statistic#The_order_statistics_of_the_uniform_distribution\">all order statistics of a ${\\\\\\\\rm Uniform}(0,1)$ distributed sample have a Beta distribution</a>, as noted in @bnaul\\'s answer.  </p>\\n',\n",
       " '<p>Among stratified random sampling schemes this is the one that uses <em>proportional allocation</em> to strata.</p>\\n\\n<p>Here is a <a href=\"http://www.math.umt.edu/patterson/549/Stratified.pdf\" rel=\"nofollow\">good brief summary</a> of the usual possibilities, including the selection of 100 as the sample size, with some R code at the end.</p>\\n',\n",
       " '<p><strong>Reliability Analysis</strong>: Yes, you should reverse score the reversed items.</p>\\n\\n<p><strong>Factor Analysis</strong>: It does not matter so much. Eigenvalues and associated indices (e.g., variance explained by factors, rules of thumb regarding number of factors to extract, etc.) should be the same. The sign of factor loadings will flip based on whether you reverse reversed items.</p>\\n',\n",
       " '<p>This is in part motivated by the following <a href=\"http://stats.stackexchange.com/questions/37569/lognormal-distribution-from-world-bank-quintiles-ppp-data\">question</a> and the discussion following it. </p>\\n\\n<p>Suppose the iid sample is observed, $X_i\\\\\\\\sim F(x,\\\\\\\\theta)$. The goal is to estimate $\\\\\\\\theta$. But original sample is not available. What we have instead are some statistics of the sample $T_1,...,T_k$. Suppose $k$ is fixed. How do we estimate $\\\\\\\\theta$? What would be maximum likelihood estimator in this case?</p>\\n',\n",
       " '<p>Your problem sound similar to one I\\'m looking at and <a href=\"http://stats.stackexchange.com/questions/12362/what-is-sequential-change-point-detection\">this</a> question, which is similar, but less well explained.</p>\\n\\n<p>Their answer links to a good summary on Change Detection.  For possible solutions, a quick google search found found a <a href=\"https://sites.google.com/site/changepointanalysis/\" rel=\"nofollow\">Change Point Analysis</a> package on Google code.  R also has some tools for doing this.  The <code>bcp</code> package is pretty powerful and really easy to use. If you want to do it on the fly as data comes in, the paper \"On-line changepoint detection and parameter estimation with application to genomic data\" describes a really sophisticated approach, though be warned that it\\'s slightly challenging.  There\\'s also the <code>strucchange</code> package, but this has worked less well for me.</p>\\n',\n",
       " '<p>Why not follow the principle \"look at plots of the data first\". One thing you can do is a 2 D scatterplot of the two class conditional densities for two covariates.  If you look at these and see practically no separation that could indicate lack of predictability and you can do this with all the covariates.  That gives you some ideas about the ability to use these covariates to predict. If you see some hope that these variables can separate a little then start thinking about linear discriminants, quadratic discriminants, kernel discrimination, regularization, tree classification, SVM etc.</p>\\n',\n",
       " '<p>If your hyper parameters are fixed (without any random distribution), you can use the MCMC method to simulate and update your parameters. MCMC does not need exact distribution function of the posteriors (with normalization constant). It only needs some candidate distributions for your parameters that you can define based on your opinion about each posteriors.</p>\\n',\n",
       " '<p>How can I transform a variable (non linear transformation) such that its values are more evenly spread, that is reduce the peak in the middle of the histogram and move more into tails?</p>\\n',\n",
       " '<p>First, I would generate a matrix $M$ such that $M_{ij}$ is exponentially distributed with mean $\\\\\\\\mu$ for all $i \\\\\\\\ne j$ or $0$ otherwise.</p>\\n\\n<p>Then, calculate your matrix\\n\\\\\\\\begin{align}\\nX_{ij} &amp;= \\\\\\\\frac{e^{-M_{ij}}}{\\\\\\\\sum_je^{-M_{ij}}}\\n\\\\\\\\end{align}</p>\\n\\n<p>To efficiently sample from a categorical distribution (a list of probabilities totalling 1), just build a Huffman tree using the probabilities of each outcome keeping track of the total probability in descendants to the left of each node.  Sample from a uniform distribution, and find the leaf node whose cdf is the smallest one larger than your sampled value.  (This has an amortized cost of the entropy of your categorical distribution.)</p>\\n',\n",
       " '<p>I am looking for a freely available, mathematical description of the standard analysis of variance (several factors, one dependent variable). It should be self-contained and be readable by a person without any knowledge of statistics but a good background in mathematics and probability theory.</p>\\n\\n<p>Almost all texts I find are either step-by-step instructions on example data-sets, heuristic outlines written for scientists from applied areas or they use a lot of statistical lingo to explain the concept (with only crippled math).</p>\\n\\n<p>Ideally, the text would include a derivation of the F-distribution (why does it emerge in this setting?) and the F-test as well as the relation to regression.</p>\\n',\n",
       " '<p>I am constructing Cox models that predict survival in a clinical trials cohort. </p>\\n\\n<p>After speaking to our statistician (who is away at the moment, hence this post), I was advised to take a forward likelihood ratio-test approach to building Cox survival models, starting with a base model and adding the term that improved the model, by computing a p value from subtracting the likelihood ratio statistic from the extended model from the likelihood ratio from the base model, as outlined in the R code below.</p>\\n\\n<p>I realise that Stata is probably a better fit for this sort of analysis, but I i) I don\\'t have easy access to Stata and ii) am familiar with R (also have access to SPSS), so with that caveat, here is the general format of the code I am using:</p>\\n\\n<p>Conceptually, this makes sense to me if I add binary covariates to the model, but I was wondering whether this approach is appropriate for adding a continuous variable, as outlined below? I\\'m not sure that a degree of freedom equal to one is correct for this comparison?</p>\\n\\n<pre><code>y&lt;-0:1\\ndata&lt;-data.frame(cbind(sample(y,100,replace=TRUE),runif(100,min=0,max=10),sample(y,100,replace=TRUE),runif(100,min=0,max=1)))\\ncolnames(data)&lt;-c(\"EFS_Status\",\"EFS_Time\",\"var1\",\"Contvar2\")   \\nlibrary(survival)  \\nbase &lt;- coxph(Surv(EFS_Time,EFS_Status) ~ var1, data=data) # Create base model  \\nlr1 &lt;- -2*base$loglik[2] # Likelihood ratio of base model\\n\\nextend &lt;- coxph(Surv(EFS_Time,EFS_Status) ~ var1 + Contvar2, data=data) # Extended model  \\nlr2&lt;- -2*extend$loglik[2] # Likelihood ratio of extended model \\n\\npchisq(q=lr1-lr2,df=1,lower.tail=FALSE) # 1 df correct for continuous variables?\\n</code></pre>\\n\\n<p>Any guidance is most appreciated. Obviously, I could binarize the continuous variable, but I suppose you\\'re losing information by taking that approach.</p>\\n\\n<p>Thanks for reading,\\nEd</p>\\n',\n",
       " '<p>I would also suggest you check out <a href=\"http://en.wikipedia.org/wiki/N-gram\" rel=\"nofollow\">N-grams</a>  and the <a href=\"http://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance\" rel=\"nofollow\">Damerau–Levenshtein</a> distance besides the other suggestions of Kwak.</p>\\n\\n<p>This <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.15.178&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">paper</a> compares the accuracy of a few different edit distances mentioned here (and is highly cited according to google scholar).</p>\\n\\n<p>As you can see there are many different ways to approach this, and you can even combine different metrics (the paper I linked to talks about this alittle bit). I think the Levenshtein and related based metrics make the most intuitive sense, especially if errors occur because of human typing. N-grams are also simple and make sense for data that is not names or words per say. </p>\\n\\n<p>While soundex is an option, the little bit of work I have seen (which is admittedly a very small amount) soundex does not perform as well as Levenshstein or other edit distances for matching names. And the Soundex is limited to phonetic phrases likely inputted by human typers, where as Levenshtein and N-grams have a potentially broader scope (especially N-gram, but I would expect the Levenshtein distance to perform better for non-words as well).</p>\\n\\n<p>I can\\'t help as far as packages, but the concept of N-grams is pretty simple (I did make an SPSS macro to do N-grams recently, but for such a small project I would just go with the already made packages in R the other posters have suggested). <a href=\"http://hetland.org/coding/python/levenshtein.py\" rel=\"nofollow\">Here</a> is an example of calculating the Levenshtein distance in python.</p>\\n',\n",
       " '<p>One explanation is that the standard deviation of your data is much less than one, and the histogram is giving something like the probability density.</p>\\n\\n<p>For example, see how the density on the histogram changes when I divide a uniform random variable with range (0, 1) by 1000:</p>\\n\\n<pre><code>set.seed(4444)\\nx &lt;- runif(100)\\ny &lt;- x / 1000\\n\\npar(mfrow=c(2,1))\\nhist(x, prob=TRUE)\\nhist(y, prob=TRUE)\\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/jwih8.png\" alt=\"enter image description here\"></p>\\n\\n<p>If you want more intuitive looking density values, you could possibly change the units of the variable.</p>\\n',\n",
       " '<p>I am trying to test if one algorithm is statistically significantly faster than another using the <a href=\"http://en.wikipedia.org/wiki/Student_t_test#One-sample_t-test\" rel=\"nofollow\">One-sample t-test</a></p>\\n\\n<p>These are the results I have and I am trying to prove Algo2 is significantly faster than Algo1</p>\\n\\n<pre><code>        Mean (ms)  Variance\\nAlgo1     5171       782 \\nAlgo2     3753       1920\\n</code></pre>\\n\\n<p>with a sample size of 78,869</p>\\n\\n<p>I\\'ve tried using the equation:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/wtIEI.png\" alt=\"one-sample t-test\"></p>\\n\\n<p>but I don\\'t seem to be getting a value that makes sense.</p>\\n\\n<p>How do I work this out?</p>\\n',\n",
       " '<p>I do experiments with a certain parameter x. The result is y. I assume y is linearly related to x.</p>\\n\\n<p>Suppose I can do 1000 experiments, which method will give me a better estimation of the linear relation?</p>\\n\\n<ul>\\n<li>Select 1000 different values of x, get a single y for each x, and do linear regression?</li>\\n<li>Select 100 different values of x, run 10 experiments for each x, average the y values for each x, and then do linear regression on the 100 averages?</li>\\n<li>Select 100 different values of x, run 10 experiments for each x, and do linear regression without averaging first?</li>\\n</ul>\\n\\n<p>What if I am not sure that the relation is linear?</p>\\n',\n",
       " '<p>I\\'ll just add some additional comments about causality as viewed from an <em>epidemiological perspective</em>. Most of these arguments are taken from <a href=\"http://occmed.oxfordjournals.org/content/56/6/434.2.full\">Practical Psychiatric Epidemiology</a>, by Prince et al. (2003).</p>\\n\\n<p>Causation, or <em>causality interpretation</em>, are by far the most difficult aspects of epidemiological research. <a href=\"http://en.wikipedia.org/wiki/Cohort_study\">Cohort</a> and <a href=\"http://en.wikipedia.org/wiki/Cross-sectional_study\">cross-sectional</a> studies might both lead to confoundig effects for example. Quoting S. Menard (<em>Longitudinal Research</em>, Sage University Paper 76, 1991), H.B. Asher in <em>Causal Modeling</em> (Sage, 1976) initially proposed the following set of criteria to be fulfilled:</p>\\n\\n<ul>\\n<li>The phenomena or variables in question must covary, as indicated for example by differences between experimental and control groups or by nonzero correlation between the two variables.</li>\\n<li>The relationship must not be attributable to any other variable or set of variables, i.e., it must not be spurious, but must persist even when other variables are controlled, as indicated for example by successful randomization in an experimental design (no difference between experimental and control groups prior to treatment) or by a nonzero partial correlation between two variables with other variable held constant.</li>\\n<li>The supposed cause must precede or be simultnaeous with the supposed effect in time, as indicated by the change in the cause occuring no later than the associated change in the effect.</li>\\n</ul>\\n\\n<p>While the first two criteria can easily be checked using a cross-sectional or time-ordered cross-sectional study, the latter can only be assessed with longitudinal data, except for biological or genetic characteristics for which temporal order can be assume without longitudinal data. Of course, the situation becomes more complex in case of a non-recursive causal relationship.</p>\\n\\n<p>I also like the following illustration (Chapter 13, in the aforementioned reference) which summarizes the approach promulgated by Hill (1965) which includes 9 different criteria related to causation effect, as also cited by @James. The original article was indeed entitled \"The environment and disease: association or causation?\" (<a href=\"http://epiville.ccnmtl.columbia.edu/assets/pdfs/Hill_1965.pdf\">PDF version</a>).</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/BNQEa.png\" alt=\"Hill1965\"></p>\\n\\n<p>Finally, Chapter 2 of Rothman\\'s most famous book, <em>Modern Epidemiology</em> (1998, Lippincott Williams &amp; Wilkins, 2nd Edition), offers a very complete discussion around causation and causal inference, both from a statistical and philosophical perspective.</p>\\n\\n<p>I\\'d like to add the following references (roughly taken from an online course in epidemiology) are also very interesting:</p>\\n\\n<ul>\\n<li>Swaen, G and van Amelsvoort, L (2009). <a href=\"http://www.uc.pt/en/fmuc/phdhs/Courses/Epidemiology/A_weight_of_evidence_approach_to_causal_inference.pdf\">A weight of evidence approach to causal inference</a>. <em>Journal of Clinical Epidemiology</em>, <em>62</em>, 270-277.</li>\\n<li>Botti, C, Comba, P, Forastiere, F, and Settimi, L (1996). <a href=\"http://www.uc.pt/en/fmuc/phdhs/Courses/Epidemiology/Causal_inference_in_environmental_epidemiology._the_role_of_implicit_values.pdf\">Causal inference in environmental epidemiology. the role of implicit values</a>. <em>The Science of the Total Environment</em>, <em>184</em>, 97-101.</li>\\n<li>Weed, DL (2002). <a href=\"http://www.uc.pt/en/fmuc/phdhs/Courses/Epidemiology/Environmental_epidemiology._Basics_and_proof_of_cause.effect.pdf\">Environmental epidemiology. Basics and proof of cause effect</a>. <em>Toxicology</em>, <em>181-182</em>, 399-403.</li>\\n<li>Franco, EL, Correa, P, Santella, RM, Wu, X, Goodman, SN, and Petersen, GM (2004). <a href=\"http://www.uc.pt/en/fmuc/phdhs/Courses/Epidemiology/Role_and_limitations_of_epidemiology_in_establishing_a_causal_association.pdf\">Role and limitations of epidemiology in establishing a causal association</a>. <em>Seminars in Cancer Biology</em>, <em>14</em>, 413–426.</li>\\n</ul>\\n\\n<p>Finally, this review offers a larger perspective on causal modeling, <a href=\"http://www.i-journals.org/ss/viewarticle.php?id=57\">Causal inference in statistics: An overview</a> (J Pearl, SS 2009 (3)).</p>\\n',\n",
       " \"<p>In fact you can't, unless you make some simple significance test to check if it is better than random guessing. </p>\\n\\n<p>Anyway, you can still use stochastic or N-fold CV to get more continuous error scale; or, supposing that your algorithm is stochastic, just aggregate several LOO results with different random seeds. </p>\\n\",\n",
       " '<p>The maximum of the cross-correlation function can be used for that.</p>\\n\\n<p><a href=\"http://en.wikipedia.org/wiki/Cross-correlation\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Cross-correlation</a></p>\\n\\n<p>(at least I know that it was used successfully in EEG time-series analysis, paper by Woody).</p>\\n\\n<p>This is an example, I hacked together:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/dR9Yp.jpg\" alt=\"enter image description here\"></p>\\n\\n<p>The top plot shows two noisy chirp-signals, the red one is offset by some 80 sampling points. The cross-correlation plot shows a clear peak at the correct offset.\\nIn MATLAB you can get the tau-estimate with</p>\\n\\n<pre><code>[xc,lags]=xcorr(y1,y2);\\n[m,i]=max(xc);\\ntau=lags(i);\\n</code></pre>\\n\\n<p>where y1 and y2 are the two input signals.</p>\\n\\n<p>This is the MATLAB code for generating the figure:</p>\\n\\n<pre>\\n<code>\\n% generate 2 signals\\nt = 0:0.001:0.5;            \\ny = chirp(t,0,1,150);\\ny2= [cos(0:0.1:2*pi) y];\\ny2= y2(1:size(y,2));\\n\\n% add noise\\nyr =y +0.9*rand([1 size(y,2)]);\\ny2r=y2+0.9*rand([1 size(y,2)]);\\n\\n% plot signals\\nsubplot( 2,1,1)\\nplot(yr);\\nhold on\\nplot( y2r, \\'r\\');\\nhold off\\n\\n% plot cross-correlation\\nsubplot( 2, 1,2)\\n[z, lags]=xcorr(yr, y2r);\\nplot( lags, z(1:end));\\n</code>\\n</pre>\\n',\n",
       " '<p>The problem with your data is not that it is extremely detailed: you have no values at weekends, that\\'s why it is plotted with gaps. There are two ways to deal with it:</p>\\n\\n<ol>\\n<li>Either try to guess approximate values in weekends with some smoothing methods (<code>smooth.spline</code>, <code>loess</code>, etc.). Code of simple interpolation is below. But in this case you will introduce something \"unnatural\" and artificial to the data. That\\'s why I prefer second option.</li>\\n</ol>\\n\\n<blockquote>\\n<pre><code>currentDate &lt;- min(as.Date(oracle$Date))\\ndates &lt;- c(currentDate)\\nopenValues &lt;- c(oracle$Open[5045])\\ni &lt;- 5044\\nwhile (i &gt; 0) {\\n  currentDate &lt;- currentDate + 1;\\n  dates &lt;- c(dates, currentDate)\\n  if (currentDate == as.Date(oracle$Date[i])) {\\n        # just copy value and move\\n        openValues &lt;- c(openValues, oracle$Open[i])\\n        i &lt;- i-1\\n      } else {\\n        # interpolate value\\n        openValues &lt;- c(openValues, mean(oracle$Open[i:i-1]))\\n  }\\n}\\nplot(dates, openValues, type=\"l\")\\n</code></pre>\\n</blockquote>\\n\\n<ol>\\n<li>You can go from daily basis to a weekly basis, just averaging (for example) five sequential points that belog to one week (in this case you are \"killing\" some information). Just a quick example of how to do that would be</li>\\n</ol>\\n\\n<blockquote>\\n<pre><code>openValues = c(mean(oracle$Open[1:5]));\\ndates = c(as.Date(oracle$Date[1]));\\nfor (i in seq(6,5045,5)) {\\n  openValues = c(openValues, mean(oracle$Open[i:i+5]));\\n      dates = c(dates, as.Date(oracle$Date[i]));\\n}\\nplot(dates, openValues, type=\"l\")\\n</code></pre>\\n</blockquote>\\n\\n<p>Hope it will help.</p>\\n',\n",
       " '<p>Recently I am conducting a research on the relationship between motivation/attitude variables (Gardner\\'s model) and English language proficiency in the Philippines. I encountered a problem: missing values.  I used a 160-item scale in my study, consisting of around 10 subscales, where each item has a 7-point Likert-type response set, with values from 1 to 7.  Some respondents failed to answer some items.</p>\\n\\n<p>I\\'d like to try \"Multiple Imputation\" using SPSS 18. But I have some questions, hope you can help out:</p>\\n\\n<ol>\\n<li><p>For example, the variable \"Interest in foreign languages\" is measured by a 10-item (Q1-Q10) scale, but some respondents left a few items unanswered. And again, \"Attitudes toward English-speaking people\" is measured by 8-item (e.g., Q11-Q18) scale.  I wonder if I can impute missing values on a dataset with variable names such as, \"ID, sex, age, Q1, Q2, Q3, Q4,...Q18, Final grade\"? Or do I really have to add up the items first to get a subscale score before \"Multiple Imputation\"? </p></li>\\n<li><p>Do I have to recode those negatively worded items before \"Multiple Imputation\"? For example, if Q1, Q3, Q5, Q7, Q9 are negatively worded, do I have to recode them first?</p></li>\\n<li><p>It seems AMOS 18 cannot do \"Calculate Estimates\" on those imputed data. Do you think I should just average the five imputed values for each missing data to get a new value, from which I can build a new dataset so that AMOS 18 will have to handle only one complete dataset, rather than the five imputed datasets plus the original?  Is averaging the five imputed values the right way of \"POOLING\"?</p></li>\\n</ol>\\n',\n",
       " '<p>Well, RBM is an energy based model, and as such it has undirected edges, and thus you could say \"symmetric weights\".</p>\\n\\n<p>The probability distribution over visible and hidden units, defined by the RBM is based on the Energy function:</p>\\n\\n<p>$$E = -\\\\\\\\sum_{i,j} w_{ij} \\\\\\\\, v_i \\\\\\\\, h_j -\\\\\\\\sum_i \\\\\\\\alpha_i \\\\\\\\, v_i - \\\\\\\\sum_i \\\\\\\\beta_i \\\\\\\\, h_i$$</p>\\n\\n<p>As you can see, even if you wanted to somehow introduce asymmetric weights, they would average out.</p>\\n\\n<p>In short, usage of asymmetric weights simply makes no sense in case of RBM, since it is an energy-based model defined by an undirected graph.</p>\\n\\n<p>Now, you wanted to know \"What is the intuition behind this design decision\". I guess you could ask this question here, \"Why make RBM\\'s energy based models defined by an undirected graph? Why not use a directed graph?\". And it would be a damn deep question.</p>\\n\\n<p>The short answer is: you can. A model similar to RBM with directed egdes is called sigmoid belief net. They are directed graphs, and not energy based. They are different in how they are trained, and in where the problems with training arises. Since it\\'s not directly connected to your original question, and I just thought you might be interested, I\\'ll drop you great learning material for both RBM and sigmoid belief nets:</p>\\n\\n<p><a href=\"https://class.coursera.org/neuralnets-2012-001/lecture/index\" rel=\"nofollow\">https://class.coursera.org/neuralnets-2012-001/lecture/index</a></p>\\n\\n<p>The class is taught by Geoffrey Hinton himself. I highly recommend it if you are interested in neural networks in general. Also, it might be a good idea to download the videos now, since class closes in few weeks, and then they won\\'t be available anymore. The lectures most relevant to your question, that will also really make your understanding of RBM much deeper are 11, 12, 13, 14.</p>\\n',\n",
       " '<p>Consider the following example long data.frame with two dependent measures, \"Score\" and \"NewVariable\", 1 between subjects variable \"Prep\" (3 levels), 2 within subjects variables \"Day\" (3 levels) and \"Experiment\" (2 levels), and a subject identifier \"SID\".</p>\\n\\n<pre><code>example &lt;- structure(list(SID = structure(c(1L, 8L, 12L, 13L, 5L, 6L, 1L, \\n8L, 12L, 13L, 5L, 6L, 1L, 8L, 12L, 13L, 5L, 6L, 1L, 8L, 12L, \\n13L, 5L, 6L, 1L, 8L, 12L, 13L, 5L, 6L, 1L, 8L, 12L, 13L, 5L, \\n6L), .Label = c(\"S1\", \"S10\", \"S11\", \"S12\", \"S13\", \"S14\", \"S15\", \\n\"S2\", \"S3\", \"S4\", \"S5\", \"S6\", \"S7\", \"S8\", \"S9\"), class = \"factor\"), \\n    Prep = structure(c(2L, 2L, 1L, 1L, 3L, 3L, 2L, 2L, 1L, 1L, \\n    3L, 3L, 2L, 2L, 1L, 1L, 3L, 3L, 2L, 2L, 1L, 1L, 3L, 3L, 2L, \\n    2L, 1L, 1L, 3L, 3L, 2L, 2L, 1L, 1L, 3L, 3L), .Label = c(\"Group work only\", \\n    \"Lecture only\", \"No instruction\"), class = \"factor\"), Day = structure(c(1L, \\n    1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, \\n    3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, \\n    3L, 3L, 3L, 3L, 3L), .Label = c(\"Day1\", \"Day2\", \"Day3\"), class = \"factor\"), \\n    Score = c(14, 14, 16, 18, 11, 12, 13, 15, 17, 15, 12, 11, \\n    18, 17, 18, 17, 10, 12, 15, 15, 17, 19, 12, 13, 14, 16, 18, \\n    16, 13, 12, 19, 18, 19, 18, 11, 13), NewVariable = c(-0.887056411864653, \\n    -0.480360621343027, -0.490415963314823, 1.3654758915317, \\n    -1.90913204292831, 0.0300532242614742, -1.84822348735206, \\n    -0.3813757992351, -1.70572162896999, 1.00321046322335, -0.758813794873949, \\n    -0.966033445643038, 0.11876111343571, -1.2312333727132, -0.836123526615442, \\n    0.137868615951057, -1.05143917652043, -0.556162009526374, \\n    0.112943588135347, 0.519639378656973, 0.509584036685177, \\n    2.3654758915317, -0.909132042928306, 1.03005322426147, -0.848223487352064, \\n    0.6186242007649, -0.705721628969986, 2.00321046322335, 0.241186205126052, \\n    0.0339665543569619, 1.11876111343571, -0.231233372713198, \\n    0.163876473384558, 1.13786861595106, -0.0514391765204339, \\n    0.443837990473626), Experiment = c(1, 1, 1, 1, 1, 1, 1, 1, \\n    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, \\n    2, 2, 2, 2, 2, 2, 2, 2, 2)), .Names = c(\"SID\", \"Prep\", \"Day\", \\n\"Score\", \"NewVariable\", \"Experiment\"), row.names = c(\"1\", \"2\", \\n\"6\", \"7\", \"13\", \"14\", \"16\", \"17\", \"21\", \"22\", \"28\", \"29\", \"31\", \\n\"32\", \"36\", \"37\", \"43\", \"44\", \"11\", \"23\", \"61\", \"71\", \"131\", \\n\"141\", \"161\", \"171\", \"211\", \"221\", \"281\", \"291\", \"311\", \"321\", \\n\"361\", \"371\", \"431\", \"441\"), class = \"data.frame\")\\n</code></pre>\\n\\n<p>My best attempt so far would involve creating the two corresponding wide datasets, e.g.</p>\\n\\n<pre><code>library(reshape2)\\ndcast(example,SID+Prep~Day+Experiment,value.var=\"Score\")\\ndcast(example,SID+Prep~Day+Experiment,value.var=\"NewVariable\")\\n</code></pre>\\n\\n<p>... then manually gluing them back together.</p>\\n\\n<p>Is there a better way?</p>\\n',\n",
       " '<p>I am regressing firm characteristics on some stock trading-related measures in a panel dataset. Firm size is a highly significant control variable, independent of the estimation method etc. My focus variables are related to firm size though, either by construction (e.g. $focus variable = x / firmsize$) or because of an economic relationship.</p>\\n\\n<p>As a consequence, I am finding myself in a classic multicollinearity situation: If firm size is put in as a control variable, my focus variables become insignificant. If firm size is left out, the focus variables are highly significant.</p>\\n\\n<p>Any of the usual advice (e.g. <a href=\"http://en.wikipedia.org/wiki/Multicollinearity\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Multicollinearity</a>) is not helping: I cannot obtain more data, I cannot run my regression on principal components because I need interpretable coefficients etc.</p>\\n\\n<p>I have little experience with this kind of problem but with some imagination, I came up with the following two ideas:</p>\\n\\n<ol>\\n<li><p>Running the regression with firm size as a control variables and additionally including interaction terms between each focus variable and firm size.</p></li>\\n<li><p>Trying to strip away the firm size effect from both the dependent variable and the focus variables, e.g. by first regressing firm size on the dependent/focus variable and then using the residuals as the dependent/focus variable in the actual regression.</p></li>\\n</ol>\\n\\n<p>Would either or both idea make any sense? Any comment or alternative ideas would be very welcome!</p>\\n',\n",
       " '<p>I have recently been reading a lot on this site (@Aniko, @Dikran Marsupial, @Erik) and elsewhere about the problem of overfitting occuring with cross validation - (Smialowski et al 2010 Bioinformatics, Hastie, Elements of statistical learning). \\nThe suggestion is that <em>any</em> supervised feature selection (using correlation with class labels) performed outside of the model performance estimation using cross validation (or other model estimating method such as bootstrapping) may result in overfitting. </p>\\n\\n<p>This seems unintuitive to me - surely if you select a feature set and then evaluate your model using <em>only</em> the selected features using cross validation, then you are getting an unbiased estimate of generalized model performance on those features (this assumes the sample under study are representive of the populatation)?</p>\\n\\n<p>With this procedure one cannot of course claim an optimal feature set but can one report the performance of the selected feature set on unseen data as valid?</p>\\n\\n<p>I accept that selecting features based on the entire data set may resuts in some data leakage between test and train sets. But if the feature set is static after initial selection, and no other tuning is being done, surely it is valid to report the cross-validated performance metrics? </p>\\n\\n<p>In my case I have 56 features and 259 cases and so #cases > #features. The features are derived from sensor data.</p>\\n\\n<p>Apologies if my question seems derivative but this seems an important point to clarify.</p>\\n\\n<p><strong>Edit:</strong>\\nOn implementing feature selection within cross validation on the data set detailed above (thanks to the answers below), I can confirm that selecting features prior to cross-validation in this data set introduced a <em>significant</em> bias. This bias/overfitting was greatest when doing so for a 3-class formulation, compared to as 2-class formulation. \\nI think the fact that I used stepwise regression for feature selection increased this overfitting; for comparison purposes, on a different but related data set I compared a sequential forward feature selection routine performed prior to cross-validation against results I had previously obtained with feature selection within CV. The results between both methods did not differ dramatically. This may mean that stepwise regression is more prone to overfitting than sequential FS or may be a quirk of this data set.</p>\\n',\n",
       " '<p>To supplement andrea\\'s response by extending it a bit to hazard ratios:</p>\\n\\n<p>The hazard of an event is the instantaneous probability of an event occurring at time t, conditional on it not having previously occurred.</p>\\n\\n<p>Your problem should be clear instantly - with no events, the probability is zero. Borrowing from andrea\\'s example, the incident rate is equivalent to a constant hazard - in your case, a constant hazard of zero.</p>\\n\\n<p>Dividing by zero tends to make software angry.</p>\\n\\n<p>You need to switch your reference category. My suggestion is to use \"Quartile 4\" or the other high value of the category, and step down, rather than using Quartile 1 and stepping up. If you were hoping to, for example, show an increase in the HR as you moved up a category, you\\'re now showing the equivalent protective effect from moving down one.</p>\\n\\n<p>I would also suggest taking a moment to consider <em>why</em> you have no events.</p>\\n\\n<p>It\\'s possible you\\'re simply having a run of \"bad luck\", at which point there\\'s nothing you can do but increase the study size or follow the population for longer in hopes of accumulating more events. But you should make sure there\\'s no reason that the probability of having an outcome in your population isn\\'t zero <em>for a reason</em>. For cardiac events I can imagine one, but it is always worth stopping to consider when you have zero events in some level of a covariate.</p>\\n',\n",
       " \"<p>Use <code>type='smooth'</code> instead of <code>type='l'</code>:</p>\\n\\n<pre><code>xyplot(y1+y2+y3~time|subject,dat,type='smooth')\\n</code></pre>\\n\",\n",
       " '<p>@JeromyAnglim, thanks for your answer to <a href=\"http://stats.stackexchange.com/a/1823/919\">\"How to determine the sample size needed for repeated measurement ANOVA\"</a>. My question: Is there a formula to estimate the minimum number of time points required for repeated measurements when we do a clinical trial?  I know that there\\'s a formula to estimate the sample size when we plan to do a clinical trial; I\\'m hoping for something like that.  </p>\\n\\n<p>I am now planning to do a clinical trial to evaluate the effect of a new device compared to the old one. The patients will be randomized 2:1 to a study arm or controlled arm. My outcome variable is continuous. If the study is conducted over 1 year, I want to know what is the minimum number of time points I should get? Could you give me some reference?</p>\\n',\n",
       " '<p>I recommend focusing on things like confidence intervals and model-checking.  Andrew Gelman has done great work on this.  I recommend his textbooks but also check out the stuff he\\'s put online, e.g. <a href=\"http://andrewgelman.com/2011/06/the_holes_in_my/\" rel=\"nofollow\">http://andrewgelman.com/2011/06/the_holes_in_my/</a></p>\\n',\n",
       " '<p>I have read from <a href=\"http://www.r-tutor.com/elementary-statistics/logistic-regression/estimated-logistic-regression-equation\" rel=\"nofollow\">here</a> and understand how to calculate the estimated logit from a fitted logistic regression model, but how to work on the confidence interval? As it involved a variance-covariance matrix and I think it is better to have a program to do the calculation, rather then doing it by myself.</p>\\n\\n<p>Thanks.</p>\\n\\n<h3>Edit 01</h3>\\n\\n<p>I have added a script here:</p>\\n\\n<pre><code>chdage.dummy &lt;- data.frame(chd=c(rep(1,50),rep(0,50)),\\n                           race=c(rep(\"white\",5),rep(\"black\",20),rep(\"hispanic\",15),rep(\"other\",10),\\n                                  rep(\"white\",20),rep(\"black\",10),rep(\"hispanic\",10),rep(\"other\",10)),\\n                           stringsAsFactors=FALSE)\\nchdage.dummy[,\"race\"] &lt;- factor(chdage.dummy[,\"race\"],levels=c(\"white\",\"black\",\"hispanic\",\"other\"))\\nchdage.lr.02 &lt;- glm(chd~race,data=chdage.dummy,family=\"binomial\")\\npredict(chdage.lr.02,newdata=data.frame(race=\"white\"))\\n</code></pre>\\n\\n<p><code>predict</code> function can give me an estimate, but I can\\'t use <code>confint</code> outside <code>predict</code>, so what can I do?</p>\\n',\n",
       " \"<p>I am looking for advice on circular statistics. In particular, I'd like to know if any one had any advice/ references that deal with regression models for circular variables and whether it is possible to include random effects as well. </p>\\n\\n<p>At the moment I can fit very simple models in WinBUGS using wrapped cauchy distributions, but I don't know how to go on to the next step and add either fixed or random effects. Below is the WinBUGS code I have been using so far. I have tested it with simulated data and so far it has performed well, but trying to add in fixed/ random effects has, so far, not worked.</p>\\n\\n<pre><code>model{\\n for (t in 1:N) {\\n\\n # likelihood for angles. We use the “ones” trick to sample from the\\n # Wrapped Cauchy distribution (see WinBUGS manual)\\n\\n ones[t]&lt;- 1\\n ones[t] ~ dbern(wc[t])\\n wc[t] &lt;- (1/(2*Pi)*(1-rho[t]*rho[t])/(1+rho[t]*rho[t]-2*rho[t]*cos(theta[t]-\\n mu.t[t])))/ 300 # Density function for Wrapped Cauchy distribution\\n\\n rho[t] &lt;- lambda.t    # mean cosine for the circular distribution\\n mu.t[t]&lt;- nu.t# mean direction for turns\\n }\\n\\n\\n\\n ###### priors for mean direction of angles\\n nu.t ~ dunif(-3.14159265359, 3.14159265359)\\nlambda.t ~ dunif(0,1) # prior for mean cosine of circular distribution\\n Pi &lt;- 3.14159265359 # define Pi\\n }\\n\\n## Simulated Data ##\\nlist(theta=c(1.57086666107637,0.624281203067249,4.83586153543422,5.52517105399153,0.250167755691792,5.24413183188724,0.175711907822086,0.503670499719972,0.00587906094477884,0.290131613934322,0.759047889069672,0.57973291007534,3.03128168541491,0.497790655905849,6.24730873150114,2.61159637947433,6.19811892339656,2.21476872674273,0.163464826891718,5.79300356573004,5.65352466175931,-0.0100726021401003,0.00574503925995024,0.260777171784755,5.8545805891331,6.09628602098184,6.07018161953988,5.90921466125829,0.0387070377090986,5.96019978900552,0.270388591408335,0.539775794451919,6.16303548945592,5.54317029065067,1.09867887761604,0.546155012914554,5.73154203573232,6.04837644493341,.242217723020124,0.201937287826239,6.19111529531002,0.602897213838987,5.53590129760264,0.304328180646957,6.12364810518025,0.0781317192586082,2.12148311222615,5.41742779164167,0.109722984863423,0.546244633029087,1.72483899231817,5.81142848191977,5.77431670621736,5.94852063016486,1.21880980868771,0.761391412364464,6.13385885651117,2.3278212791841,-0.00886837423371834,0.0509442654103693,0.919346146608449,0.22243489212092,0.0605109486858312,6.26215798187548,3.35930515203348,4.49262316826849,0.393662386151002,0.408276217352091,5.48604197934124,1.2319358669625,0.290890698266516,0.0356807866706245,5.01603150661483,2.13110569190685,5.58637984768018,0.705401496640296,0.474940761772081,5.58728776070886,6.12311166642116,0.00848809261322299,3.35074107197193,5.82089972193407,0.0531213061461832,5.97904289602246,4.31610462188531,5.61206825679503,0.184081838885041,0.288450927211418,0.594322121025956,1.07062485671203,0.400068367390392,5.08834932305335,4.35542895067301,6.08614182924595,6.14530696852739,5.25070254271081,5.91716602109256,1.78589020077607,6.23955405139402,6.09356179129423), N =100)\\n</code></pre>\\n\",\n",
       " \"<p>My Nonparametrics: Statistical Methods based on Ranks (Lehmann, 1975 edition) gives the formula as:</p>\\n\\n<p>$p(r=2k) = 2\\\\\\\\frac{{n-1\\\\\\\\choose{k-1}}^2}{2n\\\\\\\\choose{n}}$</p>\\n\\n<p>where the number of runs is counted as both the runs up and down.  Asymptotically, </p>\\n\\n<p>$ \\\\\\\\frac{r - n}{\\\\\\\\sqrt{n}} \\\\\\\\sim \\\\\\\\text{N}(0,1)$</p>\\n\\n<p>An alternative procedure is of course to do a permutation test; generate 10,000 or so random sequences by randomly permuting the collection of data points, and calculate the test statistic for each sequence.  You can then compare your calculated test statistic on the original sequence with the collection of randomly-generated test values to get a p-value.</p>\\n\\n<p>Note that this does not account for the possibility of ties, which is a definite possibility in the OP's case, as each byte can only take on one of 256 possible values.</p>\\n\",\n",
       " '<p>Because more then 10 days passed and there is no answer to my question, I\\'ll simply describe my own findings.</p>\\n\\n<p>I found only one framework with Symbolic Classification out of the box:</p>\\n\\n<p><a href=\"http://dev.heuristiclab.com/trac/hl/core\" rel=\"nofollow\">HeuristicLab</a> - simply great software, written on .Net.  Very easy to use. Actually it took me about 15 minutes to download it, install and find out how to run my problem with it. </p>\\n\\n<p>Other options which I found most interesting, but which require additional coding to do symbolic classification instead of symbolic regression were:</p>\\n\\n<p><a href=\"http://cs.gmu.edu/~eclab/projects/ecj/\" rel=\"nofollow\">ECJ</a> - one of most oftenly used Java frameworks for GP. Provides very good documentation and a rich set of features.</p>\\n\\n<p><a href=\"http://www.moeaframework.org/\" rel=\"nofollow\">MOEA</a> - another Java framework. Provides best set of algorithms for multiobjective problems.</p>\\n\\n<p><a href=\"http://cran.r-project.org/web/packages/rgp/index.html\" rel=\"nofollow\">rgp</a> - R package for genetic programming. Supports symbolic regression. As always in case of R, much easier to use then Java frameworks, but has less functionality and requires additional coding for custom tasks.</p>\\n\\n<p><a href=\"http://pystep.sourceforge.net/\" rel=\"nofollow\">pySTEP</a> - small and old Python framework for GE, but for me it turned out to be the most suitable among other Python options.</p>\\n\\n<p><a href=\"http://www.epochx.org/\" rel=\"nofollow\">epochX</a> - another Java framework. It is worse then the other two Java frameworks in functionality options it provides. But it is the simplest to use.</p>\\n',\n",
       " '<blockquote>\\n  <p><strong>Possible Duplicate:</strong><br>\\n  <a href=\"http://stats.stackexchange.com/questions/28942/computing-and-plotting-a-correlogram\">Computing and plotting a correlogram</a>  </p>\\n</blockquote>\\n\\n\\n\\n<p>I have spatial data for a grid of size 20x20. I wanted to know how I can plot a correlogram to see how much the data are correlated spatially.</p>\\n',\n",
       " '<p><a href=\"http://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm\" rel=\"nofollow\">MH sampling</a> is used when it\\'s difficult to sample from the target distribution (e.g., when the prior isn\\'t <a href=\"http://en.wikipedia.org/wiki/Conjugate_prior\" rel=\"nofollow\">conjugate</a> to the likelihood). So you use a proposal distribution to generate samples and accept/reject them based on the acceptance probability. The <a href=\"http://en.wikipedia.org/wiki/Gibbs_sampling\" rel=\"nofollow\">Gibbs sampling</a> algorithm is a particular instance of MH where the proposals are <em>always</em> accepted. Gibbs sampling is one of the most commonly used algorithm due to its simplicity but it may not always to possible to apply, in which case one resorts to MH based on accept/reject proposals.</p>\\n',\n",
       " '<p>The Kappa ($\\\\\\\\kappa$) test is a Z-test kind of test. If I am not very wrong, to compute the $\\\\\\\\kappa$ test, we can just estimate the appropriate variance $\\\\\\\\hat {var}(\\\\\\\\hat\\\\\\\\kappa)$ for the kappa statistic $\\\\\\\\hat\\\\\\\\kappa$  and then feed it to a z-test by taking $\\\\\\\\mu$ = $\\\\\\\\hat\\\\\\\\kappa$ and $\\\\\\\\sigma^2$ = $var(\\\\\\\\hat\\\\\\\\kappa)$.</p>\\n\\n<p>To compute the power of a z-test one can use the relation $1 - \\\\\\\\beta = \\\\\\\\phi(Z_{a} - \\\\\\\\sqrt n * (\\\\\\\\mu-\\\\\\\\mu_0)/\\\\\\\\sigma)$ </p>\\n\\n<p>Would the power of this underlying z-test be also the power of the original $\\\\\\\\kappa$ test? If not, why?</p>\\n',\n",
       " '<p>How about a <a href=\"http://usd.improvelectronics.com/boogie-board-lcd-writing-tablet/boogie-board-rip-tablet.html\" rel=\"nofollow\">Boogie Board?</a> You can write your notes on a slate and record them as \\'s the same idea as the LiveScribe pen but you can save them as PDF files...It isn\\'t out yet, but will be in less than a month. </p>\\n',\n",
       " '<p>I lack formal knowledge about statistics and got lost in the Wikipedia articles about the subject.</p>\\n\\n<p>I have an algorithm that produces a solution for a problem. There might be several solutions possible. Not all solutions are equally good, and I am able to give score for each solution. </p>\\n\\n<p>I want to be able to tell if the algorithm is good or not.</p>\\n\\n<p>For each problem I can calculate bounds on the best and worst possible solution (only bounds, not exact values).\\nI can also generate a random solutions for a problem, but not every random solution is a valid one (high chances for getting a valid one, though).</p>\\n\\n<p>The data set I am going to use have problems of approximately the same difficulty.</p>\\n\\n<p>Clearly I should use some statistical test, but which one? It seems that they require knowledge about the distribution and are more suitable for comparing algorithms. </p>\\n\\n<p><strong>Edit:</strong></p>\\n\\n<p>I do not want to add too much irrelevant specifics, so I will try to simplify.</p>\\n\\n<p>I have an algorithm that solves a problem. for example, it finds a path between two nodes in a graph. The input is a graph. The output is a path.</p>\\n\\n<p>Obviously, shorter paths are better than longer ones.</p>\\n\\n<p>I want to know if my algorithm is good. I do not know exactly how to define \"good\", but that is where statistics can help me, is not it?</p>\\n\\n<p>What I though about so far was to do the following:</p>\\n\\n<ol>\\n<li>Run the algorithm on a set of problems (problem is a graph). The sizes of all these  graphs are of the same order of magnitude.</li>\\n<li>Find a random path (in my specific problem it is possible to do) for each graph, and do this many times.</li>\\n<li>Compare the results of 1 and 2 to see if my algorithm is better than choosing a path at random.</li>\\n</ol>\\n\\n<p>Am I doing it right? How should I perform step 3?</p>\\n',\n",
       " '<p>I have made a very simple questionnaire that asks questions that are independent of each other. Every question can be answered with a rating between 1 and 5 where 1 means I strongly disagree and 5 I strongly agree.</p>\\n\\n<p>Now I was wondering with statistical methods I could use to evaluate the results. I know maybe I should have thought about that before performing the evaluation but now its too late and I have to get the most out of it.</p>\\n\\n<p>Currently i just calculate the following values for each question:</p>\\n\\n<ul>\\n<li>arithmetic mean</li>\\n<li>median</li>\\n<li>max value</li>\\n<li>min value</li>\\n<li>standard deviation</li>\\n</ul>\\n\\n<p>But are there any other good indicators I could use to analyze the answers?</p>\\n',\n",
       " '<p>I\\'m running into a problem where my coefficients are not estimable when using Bioconductor/limma on a two-color factorial design for a microarray analysis.</p>\\n\\n<p>I have microarray data I downloaded from Array Express using the ArrayExpress function in the ArrayExpress package. I managed to convert the NChannelSet object into an RGList object that I can use in limma using a tip I found in the archives (https://stat.ethz.ch/pipermail/bioconductor/2009-September/029705.html). After some bg subtraction and normalization, I have an MAList object with 32 arrays (red and green) by 34,944 probes.</p>\\n\\n<p>The experiment consists of two cell types (WT or KO), by 4 treatments (control, CD70, CD80, CD70+CD80), at four different time points (2, 4, 8, 14 hours), with a dye-swap. I\\'m mostly interested in genes that are differentially expressed when WT cells are hit with CD70+CD80 versus CD80 alone.</p>\\n\\n<p>I\\'m trying to create a targets file so I can use modelMatrix to create my design matrix. But I have a multifactorial design using a two color dye-swap design, and I\\'m not sure how to specify this in the targets file. The limma manual has information about factorial designs, but no examples for two-color experiments. I have the following factors:</p>\\n\\n<p>1: Celltype: WT or KO\\n2: Treatment: control (x), CD70, CD80, or CD70+CD80 (CD7080).\\n3: Timepoints: 2, 4, 8, and 14 hours.</p>\\n\\n<p>I tried collapsing all of factors and levels into a single string in my targets file:</p>\\n\\n<pre><code>&gt; targets\\n       array          Cy3            Cy5\\n1  array3329    WT.CD70.2         WT.x.2\\n2  array2675  KO.CD7080.8    WT.CD7080.8\\n3  array2242  WT.CD7080.2    KO.CD7080.2\\n4  array3328       WT.x.2      WT.CD70.2\\n5  array3310       WT.x.8      WT.CD70.8\\n6  array2246  KO.CD7080.2    WT.CD7080.2\\n7  array3337  WT.CD7080.4      WT.CD80.4\\n8  array3323 WT.CD7080.14     WT.CD80.14\\n9  array2673    KO.CD70.8      WT.CD70.8\\n10 array1938 WT.CD7080.14   KO.CD7080.14\\n11 array2240    WT.CD70.2      KO.CD70.2\\n12 array3336    WT.CD80.4    WT.CD7080.4\\n13 array3322   WT.CD80.14 WT.CD7080.14.2\\n14 array2674  WT.CD7080.8    KO.CD7080.8\\n15 array3321   WT.CD70.14        WT.x.14\\n16 array2241    KO.CD70.2      WT.CD70.2\\n17 array2597  KO.CD7080.4    WT.CD7080.4\\n18 array3313  WT.CD7080.8      WT.CD80.8\\n19 array1939 KO.CD7080.14 WT.CD7080.14.2\\n20 array3335    WT.CD70.4         WT.x.4\\n21 array2672    WT.CD70.8      KO.CD70.8\\n22 array3320      WT.x.14     WT.CD70.14\\n23 array3334       WT.x.4      WT.CD70.4\\n24 array3311    WT.CD70.8         WT.x.8\\n25 array3331  WT.CD7080.2      WT.CD80.2\\n26 array1941   KO.CD70.14   WT.CD70.14.2\\n27 array2588    WT.CD70.4      KO.CD70.4\\n28 array2596  WT.CD7080.4    KO.CD7080.4\\n29 array3330    WT.CD80.2    WT.CD7080.2\\n30 array3312    WT.CD80.8    WT.CD7080.8\\n31 array1940 WT.CD70.14.2     KO.CD70.14\\n32 array2593    KO.CD70.4      WT.CD70.4\\n</code></pre>\\n\\n<p>I then created a design matrix, where the reference group is WT cells, untreated, 2-hour timepoint.</p>\\n\\n<pre><code>design &lt;- modelMatrix(targets, ref=\"WT.x.2\")\\n</code></pre>\\n\\n<p>This creates a 32x25 design matrix, but when I fit the model, I get a \"Coefficients not estimable\" error/warning:</p>\\n\\n<pre><code>&gt; fit &lt;- lmFit(d, design)\\nCoefficients not estimable: WT.CD70.14.2 WT.CD80.14 WT.CD80.2 WT.CD80.4 WT.CD80.8 WT.x.14 WT.x.4 WT.x.8 \\nWarning message:\\nPartial NA coefficients for 34944 probe(s) \\n</code></pre>\\n\\n<p>Any help with how I can specify this multifactorial time-course design in a two-channel dye-swap experiment would be greatly appreciated. As I said, I\\'m most interested in the conditions on arrays 8-9, 13-14, 19, 26, and 30-31, where I\\'m looking at WT cells treated with CD70 and CD80 versus CD80 alone.</p>\\n\\n<p>Thanks in advance for any help!</p>\\n',\n",
       " '<p>There is a variant of boosting called <a href=\"http://dx.doi.org/10.1214/aos/1016218223\">gentleboost</a>.  How does gentle boosting differ from the better-known <a href=\"http://en.wikipedia.org/wiki/AdaBoost\">AdaBoost</a>?</p>\\n',\n",
       " '<p>I run the cross-validation experiment for a given data set, and tried two different approaches: one is based on SVM, another is based on SVM plus Adaboost. But the confusion matrix for two experiments are exactly the same. I am confused on how to explain this kind of result. Adaboost is supposed to start with a weak classifier, but how to determine whether a classifier is weak?</p>\\n',\n",
       " \"<p>I am trying to write a Monte Carlo simulation in R and I am really stuck! I want to know the probability distribution of a random person in the UK becoming ill from eating a cooked 100g piece of chicken. </p>\\n\\n<p>I have the following information: out of 1000 pieces of chicken tested 20 had bacteria in question and I have data for the $\\\\\\\\log_{10}$ counts of these 20 pieces, I also have min and max $\\\\\\\\log_{10}$ counts (0.1 and 3.0). I also know the average person in UK eats 2 x 100g portions of chicken a week. The model for risk of illness <em>given an ingested number</em> of the bacteria is predicted by $R=1-\\\\\\\\exp(-aD)$ where $D$ is the ingested number of organisms and I have a value for $a$.</p>\\n\\n<p>I can write basic Monte Carlo simulations but I am struggling with the start of this one as I can't get my head around the model being ingested bacteria and the question being risk from eating a 100g portion.</p>\\n\\n<ul>\\n<li>Is my first step here to obtain the CDF? </li>\\n<li>And what is the distribution I should use?</li>\\n</ul>\\n\",\n",
       " '<p>Conover (1999:202) suggested that the expected values can be \"as small as 0.5, as long as most are greater than 1.0, without endangering the validity of the test.\"</p>\\n\\n<p>He also provides a \"rule of thumb\" from Cochran (1952) which suggested that if expected values are less than 1 or if more than 20% are less than 5, the test may perform poorly.  However, Conover (1999) provides some evidence that Cochran\\'s \"rule of thumb\" is overly conservative.</p>\\n\\n<p><strong>References</strong></p>\\n\\n<p>Cochran, W. G. 1952. The $\\\\\\\\chi^2$ test of goodness of fit. Annals of Mathematical Statistics 23:315-345.</p>\\n\\n<p>Conover, W. J. 1999. Practical nonparametric statistics. Third Edition. John Wiley &amp; Sons, Inc., New York, New York, USA.</p>\\n',\n",
       " '<p>I need help figuring out the correct way to calculate winners at our science fair. I don\\'t want my ignorance of statistics &amp; math to get in the way of a kid\\'s chances of winning. (lots of scholarship &amp; advancement benefits at stake). <em>Thanks in advance for your help.</em></p>\\n\\n<p><strong>First a little background of how we have things set up:</strong></p>\\n\\n<p>Our fair typically has around 600 student projects. These projects are completed and presented by individual students or a team of students. A team can consist of 2 or 3 kids.</p>\\n\\n<p>The students are divided into two divisions: Elementary (grades 6-8) and Secondary (grades 9-12). Each division has different categories: 9 categories for Elementary projects and 17 categories for the Secondary division projects. </p>\\n\\n<p>Awards are given for first, second and third place for each category in each division. Honorable mention awards are also given for placements beyond third place.</p>\\n\\n<p>For each project, we assign between 4 to 6 judges. We make our assignments based on the judges\\' qualifications, their category preference and their past judging experience. (more experienced are assigned to the senior division projects).</p>\\n\\n<p><strong>How the judges score a project:</strong></p>\\n\\n<p>For each project there are 5 criteria that are assigned points. Each criteria can be awarded between 1 and 20 points. General criteria are:</p>\\n\\n<ul>\\n<li>Overall objective + hypothesis + use of resources (<em>1..20</em>)</li>\\n<li>Design + procedures (<em>1..20</em>)</li>\\n<li>Data collection + results (<em>1..20</em>)</li>\\n<li>Discussion + conclusion (<em>1..20</em>)</li>\\n<li>Interview (<em>1..20</em>)</li>\\n</ul>\\n\\n<p>For team projects a sixth criteria is assessed called \"team deduction\", where a judge can deduct points (<em>up to 15</em>) for teammates who didn\\'t participate or didn\\'t show up.</p>\\n\\n<ul>\\n<li>Team deduction (<em>0..-15</em>)</li>\\n</ul>\\n\\n<p>So a judge can score every project between 5 and 100 points. If the project is a team project, the score can be reduced by 15 points.</p>\\n\\n<p><strong>Raw data:</strong></p>\\n\\n<p>During the course of a few hours we collect up to 3,600 scores from judges. These scores are entered into a database where I can do all kinds of sorting, averaging, standard deviation calculations, etc. I just don\\'t know exactly what I should do with these raw scores. Right now, I\\'m doing a simple average for each project, but I worry that I\\'m not adjusting for judge biases, team deductions, or any number of other things that I\\'m not considering.</p>\\n\\n<p><strong>Desired result:</strong></p>\\n\\n<p>In the end, I\\'d like to process the scores so that I can award first, second and third place projects for each category, and then honorable mention awards for the subsequent places. I would like to be confident that the positions were calculated correctly and the kids who win are deserving of the recognition (and prizes).</p>\\n\\n<p>Thanks a lot for reading my long question and for your help figuring this out. I\\'ll be happy to answer any follow-up questions you may have.</p>\\n',\n",
       " '<p>I have been using the fPortfolio package and have found some inconsistency in the usage of terminology in the package. </p>\\n\\n<p>What does a \"covariance risk budget\" in the result returned by mean variance portfolio optimization- mean?</p>\\n',\n",
       " \"<p>It's not so much the percentage of participants as the number that is important. The rule of thumb is that you want 10 people in the smallest class per independent variable. So, with 4 IVs, you want at least 40 people in cluster 1 and 40 in cluster 2.</p>\\n\\n<p>Whether you should combine clusters is really a substantive matter. Does it make sense to combine them?  Whether you should omit clusters is also a substantive issue: Do you want to distinguish between clusters 3 and 4? Would that be interesting? </p>\\n\",\n",
       " \"<p>Is there a term that describes what I'm trying to do below?\\nAlso, how would you do this using something like JMP or Excel? (or do I need to code this in something like perl?)</p>\\n\\n<p>Given this sort of data:</p>\\n\\n<pre><code>ID| opened     | closed     | quantity\\n--------------------------------------\\n1 | 2010-01-01 | 2010-01-03 | 1\\n2 | 2010-01-02 |            | 2\\n3 | 2010-01-02 | 2010-10-05 | 3\\n</code></pre>\\n\\n<p>I'd like to get this data and then graph with x being a time line and Y being total quantity open:</p>\\n\\n<pre><code>on date    | total quantity open\\n---------------------------------\\n2010-01-01 | 1\\n2010-01-02 | 6\\n2010-01-03 | 5\\n2010-01-04 | 5\\n2010-01-05 | 2 \\n</code></pre>\\n\",\n",
       " '<p>One simple adjustment to your current graphic production would be, instead of producing all of the elements of the box-plot (even the minimalist Tufte style one), would be to produce a line chart connecting the summary statistics that the box plot is displaying (median, quartiles, mean, outer hinges, whatever). Below is an example displaying the 90th and 99th percentile of a simulated distribution of 50 observations over 100 weeks.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/ykJxK.png\" alt=\"enter image description here\"></p>\\n\\n<p>This connecting of the lines allows you to make the temporal connections between the summary statistics between weeks much easier, and reduces the data to ink ratio of the plot. Even Tufte in the <em>Visual Display of Quantitative Information</em> has an example where connecting the lines in a display allows one to discern periodicity in a temporal series that would be very difficult to see in a scatterplot display (and I assume the same problem would extend to the box-plot display).</p>\\n\\n<p>What exactly you should display in the lines I believe would take more insight in the nature of your data and what you are interested in (and could change as the nature of your data changes over time as well). To get a broad sense of the distribution I believe different quantiles (including the median) can be informative. Although it may take some experimentation to see where informative quantiles lie from week to week (in this display the 99th percentile is quite noisy). </p>\\n\\n<p>Line charts like this could also be extended to different statistical summaries (such as the skew of the distribution), although I believe the quantiles as a first run are the most informative. Also if you are interested in identifying outliers you may want to consider including the dots of outliers (defined in whatever way suits your fancy) in these same line plots. There was an interesting discussion of outliers for skewed data in this question on the site, <a href=\"http://stats.stackexchange.com/q/13086/1036\">Is there a boxplot variant for Poisson distributed data?</a>, and the questions tagged with <a href=\"/questions/tagged/control-chart\" class=\"post-tag\" title=\"show questions tagged \\'control-chart\\'\" rel=\"tag\">control-chart</a> I believe would be applicable.</p>\\n\\n<p>Even though from just this you can probably imagine generating a multitude of different lines on one plot, it is fairly easy to plot too much information in one graphic. A rule-of-thumb I try to abide by is that a plot should not have any more than 4-5 data elements (where here a data element would be a line). Even that is frequently too many. To attempt to get around this problem, try to make a consistent template, in which the axis of the plots are consistent, so you can make accurate comparisons between plots. Or if your software allows it, make a series of small multiple plots (again using the same axis for all plots). Then even if you say have the outer-hinges and outliers on one plot and the median and quartiles on another you may be able to discern patterns between those two plots. And then you can combine the outliers and the quartiles into one plot for more scrutiny if you think you see a pattern between them.</p>\\n\\n<p>EDIT: As an example of the smoothing that @whuber is talking about here is a similar plot to that above (generated by a simulated process in the exact same manner), except that a loess smoother is applied to the lines. </p>\\n\\n<p><img src=\"http://i.stack.imgur.com/Yu8Mv.png\" alt=\"enter image description here\"></p>\\n\\n<p>I couldn\\'t bring myself to not plot the original data, but I just made the smoothed lines thicker (and gave them color) to bring them to the foreground of the image, and left the original lines thinner and a light grey color so they are merely in the background of the image (and hence are not as distracting). The smoother allows one to assess general trends that can be obfuscated by the variance of the series.</p>\\n\\n<p>Tukey has other suggestions I did not display here, such as plotting the alpha hull of all the observations (and labeling those observations that make up the vertices). Some more food for thought. </p>\\n',\n",
       " \"<p>I am dealing with handling overdispersed count data(Poisson distributions fails to fit).</p>\\n\\n<p>I need to compare three different mixtures (Gamma, Log-Normal and Inverse Gaussian) of Poisson rate parameter distributions using a simulation study. </p>\\n\\n<p>I would like to know the available methods(apart from Likelihood ration and Pearson's Chi Square test) to compare three completely different distributions to fit overdispersed count data.</p>\\n\\n<p>Any reference also highly appreciated!!</p>\\n\\n<p>Thank you for your kindness.</p>\\n\",\n",
       " \"<p>I am looking for a method that may be used to classify and 'cluster' some multi-dimensional data gathered from drill holes, to impute some (only partially measured) physical properties of the rocks within the drill holes. The data are (1) categorical qualitative: broadly classified lithology and stratigraphy, and (2) continuous or semi-continuous quantitative: where a set of instruments records a set of measurements of things like natural gamma response, electrical resistivity, acoustic velocity, and so on. A brief snippet of a set of data would be like this:</p>\\n\\n<pre><code>DEPTH   LITH    K   CALI    DT  GR  LAT\\n474.1173    12  1.290   240.1594    392.5612    54.2610 9.0883\\n474.2697    12  1.290   241.1248    389.7305    53.9297 9.2322\\n474.4221    12  1.290   242.9032    382.1031    53.5055 9.6901\\n474.5745    12  1.290   254.1457    388.4041    53.0907 9.7129\\n474.7269    12  1.290   248.7685    395.1500    53.8861 9.5200\\n474.8793    12  1.290   241.3833    393.8504    53.7665 9.8039\\n476.0985    12  1.194   236.8224    397.4059    55.1024 9.6624\\n476.2509    12  1.194   237.8029    397.2021    53.5209 9.5717\\n</code></pre>\\n\\n<p>These geophysical well-logs have a response based on the physical properties of the rocks, ultimately on the mineralogy and physical state. I want to find the best set of well-log data that may be used to impute another physical property that is much harder and more expensive to obtain. </p>\\n\\n<p>For this problem it was suggested to me to use a SOM method implemented in some software. The primary benefit of the suggested implementation is its ease of use, and that it has been developed to cater to geoscientists.</p>\\n\\n<p>In my child-like naivete the SOM method seems a fine one to use. However I'm sure there are pros and cons I'm unaware of, and there might be better techniques. Its primary drawback seems to me to be that it is a little 'black-box'. Are there other methods suitable and appropriate that you can suggest? </p>\\n\",\n",
       " '<p>I was just about to ask a question concerning a certain 3D data-set (each point in space has a value associated), but couldn\\'t figure out how to visualize it for you. I can understand it myself by rotating the point set in a Matlab figure, but it would be difficult to grasp from just seeing a screenshot:</p>\\n\\n<p><a href=\"http://i.stack.imgur.com/nPzeD.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/nPzeD.png\" alt=\" \"></a></p>\\n\\n<p>(marker size and color are both proportional to the value at that point in space)</p>\\n\\n<p>What\\'s the best way of solving this problem?</p>\\n',\n",
       " '<p>I have a question on a survey that asks students how useful a feature of the software was that they used for learning. I then have three questions that assess in what way the feature was useful. The measures use a 7-point Likert scale from Very Strongly Disagree to Very Strongly Agree:</p>\\n\\n<pre><code>The [[feature]]\\n...was useful for learning overall                          \\n...helped me to mentally organize the structure of the content.                         \\n...enabled me to control my learning pace                           \\n...made it easy for me to find the content I needed\\n</code></pre>\\n\\n<p>The question I\\'m trying to answer is \"If the feature was useful, in what way was it useful?\" </p>\\n\\n<p>I\\'m wondering what kind of statistical tools can be used to do this. Could a covariance matrix determine this sufficiently? Could I compute Cronbach\\'s alpha for each question paired with the first question? I have also come across Confirmatory Factor Analysis, but I don\\'t yet understand it well enough to know if it is an applicable tool.</p>\\n\\n<p>I\\'m new to stats so I\\'m just looking for a pointer in the right direction.</p>\\n\\n<p>Edit: If it makes a difference, it is a within-subjects experiment where the student has used the software both with and without the feature.</p>\\n',\n",
       " '<p>I\\'m going through the videos in Andrew Ng\\'s free <a href=\"http://www.ml-class.org/\">online machine learning course</a> at Stanford.  He discusses Gradient Descent as an algorithm to solve linear regression and writing functions in Octave to perform it.  Presumably I could rewrite those functions in R, but my question is doesn\\'t the lm() function already give me the output of linear regression?  Why would I want to write my own gradient descent function?  Is there some advantage or is it purely as a learning exercise?  Does lm() do gradient descent?</p>\\n',\n",
       " '<p>I\\'d like to know how to calculate the sample size needed for comparing the \"change from baseline\" scores between two groups?</p>\\n\\n<p>Thank you in advance for your consideration</p>\\n',\n",
       " '<p>Regarding \"how to do it in R\", the prefmod  package <a href=\"http://cran.r-project.org/web/packages/prefmod/index.html\">http://cran.r-project.org/web/packages/prefmod/index.html</a>  is meant for preference analysis with paired comparisons, rankings and ratings. It fits Bradley-Terry models and pattern models with object and subject covariates. See my answer here <a href=\"http://stats.stackexchange.com/questions/26994/how-to-fit-bradleyterryluce-model-in-r-without-complicated-formula\">How to fit Bradley–Terry–Luce model in R, without complicated formula?</a> for a short intro, or this paper <a href=\"http://www.jstatsoft.org/v48/i10\">http://www.jstatsoft.org/v48/i10</a> for more info. </p>\\n',\n",
       " \"<p>As @gung said, your question is vague. However, in general, there are some things you could do:\\n1) Increase sample size. A larger sample with the same effect size will be more significant.</p>\\n\\n<p>2) Measure things more accurately. Inaccurate measures add to error which makes it harder to find a relationship.</p>\\n\\n<p>3) Get a better model. Maybe the reason your relationships are not significant is that your model is wrong. But without knowing anything about your model, it's hard to say. </p>\\n\",\n",
       " '<p>I\\'m trying to find out how log-likelihood function works for linear regression. I found the formula <a href=\"http://amath.colorado.edu/courses/7400/2010Spr/lecture4.pdf\">here</a> and <a href=\"http://www.xycoon.com/lsrloglikelihood.htm\">here</a>. Making some experiments with it (see code below), I was quite surprised that the likelihood uses <code>SSE/n</code> instead of <code>MSE</code> (<code>SSE/df</code>). <code>MSE</code> was used everywhere up to now! I thought MSE is much better estimator of $\\\\\\\\sigma^2$ mentioned in the formula in the <a href=\"http://amath.colorado.edu/courses/7400/2010Spr/lecture4.pdf\">1st resource (page 6)</a> - the actual residual variance. But the <a href=\"http://www.xycoon.com/lsrloglikelihood.htm\">2nd resource</a> and my experiment clearly states that $\\\\\\\\sigma^2$ is defined as <code>SSE/n</code> (where n is length of the outcome variable vector).</p>\\n\\n<p>Here is the code to play with:</p>\\n\\n<pre><code>set.seed(128)\\ny = c(rnorm(200, 20, 4), rnorm(300, 30, 4), rnorm(400, 40, 4), rnorm(500, 50, 4))\\ncat1 = as.factor(c(rep(1, 200), rep(2, 300), rep(3, 400), rep(4, 500)))\\nrand_order = sample(1:length(cat1))\\ncat2 = cat1[rand_order]\\ncat2y = c(rep(1, 200), rep(-2, 300), rep(3, 400), rep(-4, 500))\\ny = y + cat2y[rand_order]\\nm1 = lm(y ~ 0 + cat1 + cat2)\\n\\n# logLik using residual degrees of freedom (-3941.94):\\n-length(m1$model$y)/2*log(2*pi) - length(m1$model$y)/2*log(sum((m1$residual)^2)/m1$df.residual) - 1/2*m1$df.residual\\n\\n# logLik using N (-3941.931)\\n-length(m1$model$y)/2*log(2*pi) - length(m1$model$y)/2*log(sum((m1$residual)^2)/length(m1$model$y)) - 1/2*length(m1$model$y)\\n\\n# real logLik (-3941.931)\\nlogLik(m1)\\n</code></pre>\\n',\n",
       " '<p><strong>BACKGROUND FIRST:</strong></p>\\n\\n<p>I\\'m in the process of putting together one data file that includes various employee demograhic information for continuing employees and employees that have left the company in the last several years for the purpose of building a predictive model. I plan to using Random Forest and the target will be a binary classifier (\"stayer\"/\"departer\"). I am hoping I end up with a reliable model that I can apply to beginning (new fiscal year) employee population and use to supply useful information to management about potential flight risk employees (propensity-to-leave scores). I will also predict number of terminations with this model.</p>\\n\\n<p>I want to add some sort of economic indicator(s) to my data file. </p>\\n\\n<p><strong>QUESTIONS:</strong></p>\\n\\n<ol>\\n<li>What are some good leading and lagging indicators that are freely available and that I can easily incorporate into my data file?</li>\\n<li>How do you incorporate this into a data file? For example we were playing with the idea of using unemployment rate but didn\\'t know if to use monthly or annual. For example a small part of our file includes people who left the company in different months. I am assuming its best to use the unemployment rate that was calculated for the month they left the company. But I am not sure if that\\'s worth the work.  Meanwhile the rest of the file is made up of continuing employees. I don\\'t know what I would use for them. They all would have the same thing, right?</li>\\n</ol>\\n',\n",
       " \"<p>I know that linear regression is based on the assumption that the errors are normally distributed (from both bayesian and classical views). I'm just trying to verify this assumption based on the final model.</p>\\n\\n<p>Assume I've got 3 normal random variables x1, x2, x3. I can regress (linearly) x1 on x2, x3 and get a linear regression model of the form: </p>\\n\\n<p>x1 = b0 + b1*x2 + b2*x3 + e. </p>\\n\\n<p>Reorganizing, e = x1 - b0 - b1*x2 - b2*x3.</p>\\n\\n<p>Here, if I estimate bi's using least squares method, I can say 'e' is a linear combination of normally distributed variables, so it's normally distributed.</p>\\n\\n<p>But if I estimate bi's using a Bayesian method and assume bi's also follow normal distribution, then 'e' is no longer a linear combination of normal random variables. Effectively, it's sum of normally (b0) and product-normally (b1*x2, b2*x3) distributed variables, and product-normal distribution is not normal in general. (http://math.stackexchange.com/questions/101062/is-the-product-of-two-gaussian-random-variables-also-a-gaussian) </p>\\n\\n<p>Is there anything improper in above reasoning? How else can I try to validate the normal assumption for the error term?</p>\\n\",\n",
       " '<p>I\\'m trying to analyze effect of Year on variable logInd for particular group of individuals (I have 3 groups). <strong>The simplest model:</strong></p>\\n\\n<pre><code>&gt; fix1 = lm(logInd ~ 0 + Group + Year:Group, data = mydata)\\n&gt; summary(fix1)\\n\\nCall:\\nlm(formula = logInd ~ 0 + Group + Year:Group, data = mydata)\\n\\nResiduals:\\n    Min      1Q  Median      3Q     Max \\n-5.5835 -0.3543 -0.0024  0.3944  4.7294 \\n\\nCoefficients:\\n              Estimate Std. Error t value Pr(&gt;|t|)    \\nGroup1       4.6395740  0.0466217  99.515  &lt; 2e-16 ***\\nGroup2       4.8094268  0.0534118  90.044  &lt; 2e-16 ***\\nGroup3       4.5607287  0.0561066  81.287  &lt; 2e-16 ***\\nGroup1:Year -0.0084165  0.0027144  -3.101  0.00195 ** \\nGroup2:Year  0.0032369  0.0031098   1.041  0.29802    \\nGroup3:Year  0.0006081  0.0032666   0.186  0.85235    \\n---\\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 \\n\\nResidual standard error: 0.7926 on 2981 degrees of freedom\\nMultiple R-squared: 0.9717,     Adjusted R-squared: 0.9716 \\nF-statistic: 1.705e+04 on 6 and 2981 DF,  p-value: &lt; 2.2e-16 \\n</code></pre>\\n\\n<p>We can see the Group1 is significantly declining, the Groups2 and 3 increasing but not significantly so.</p>\\n\\n<p><strong>Clearly the individual should be random effect, so I introduce random intercept effect for each individual:</strong></p>\\n\\n<pre><code>&gt; mix1a = lmer(logInd ~ 0 + Group + Year:Group + (1|Individual), data = mydata)\\n&gt; summary(mix1a)\\nLinear mixed model fit by REML \\nFormula: logInd ~ 0 + Group + Year:Group + (1 | Individual) \\n   Data: mydata \\n  AIC  BIC logLik deviance REMLdev\\n 4727 4775  -2356     4671    4711\\nRandom effects:\\n Groups     Name        Variance Std.Dev.\\n Individual (Intercept) 0.39357  0.62735 \\n Residual               0.24532  0.49530 \\nNumber of obs: 2987, groups: Individual, 103\\n\\nFixed effects:\\n              Estimate Std. Error t value\\nGroup1       4.6395740  0.1010868   45.90\\nGroup2       4.8094268  0.1158095   41.53\\nGroup3       4.5607287  0.1216522   37.49\\nGroup1:Year -0.0084165  0.0016963   -4.96\\nGroup2:Year  0.0032369  0.0019433    1.67\\nGroup3:Year  0.0006081  0.0020414    0.30\\n\\nCorrelation of Fixed Effects:\\n            Group1 Group2 Group3 Grp1:Y Grp2:Y\\nGroup2       0.000                            \\nGroup3       0.000  0.000                     \\nGroup1:Year -0.252  0.000  0.000              \\nGroup2:Year  0.000 -0.252  0.000  0.000       \\nGroup3:Year  0.000  0.000 -0.252  0.000  0.000\\n</code></pre>\\n\\n<p>It had an expected effect - the SE of slopes (coefficients Group1-3:Year) are now lower and the residual SE is also lower.</p>\\n\\n<p><strong>The individuals are also different in slope so I also introduced the random slope effect:</strong></p>\\n\\n<pre><code>&gt; mix1c = lmer(logInd ~ 0 + Group + Year:Group + (1 + Year|Individual), data = mydata)\\n&gt; summary(mix1c)\\nLinear mixed model fit by REML \\nFormula: logInd ~ 0 + Group + Year:Group + (1 + Year | Individual) \\n   Data: mydata \\n  AIC  BIC logLik deviance REMLdev\\n 2941 3001  -1461     2885    2921\\nRandom effects:\\n Groups     Name        Variance  Std.Dev. Corr   \\n Individual (Intercept) 0.1054790 0.324775        \\n            Year        0.0017447 0.041769 -0.246 \\n Residual               0.1223920 0.349846        \\nNumber of obs: 2987, groups: Individual, 103\\n\\nFixed effects:\\n              Estimate Std. Error t value\\nGroup1       4.6395740  0.0541746   85.64\\nGroup2       4.8094268  0.0620648   77.49\\nGroup3       4.5607287  0.0651960   69.95\\nGroup1:Year -0.0084165  0.0065557   -1.28\\nGroup2:Year  0.0032369  0.0075105    0.43\\nGroup3:Year  0.0006081  0.0078894    0.08\\n\\nCorrelation of Fixed Effects:\\n            Group1 Group2 Group3 Grp1:Y Grp2:Y\\nGroup2       0.000                            \\nGroup3       0.000  0.000                     \\nGroup1:Year -0.285  0.000  0.000              \\nGroup2:Year  0.000 -0.285  0.000  0.000       \\nGroup3:Year  0.000  0.000 -0.285  0.000  0.000\\n</code></pre>\\n\\n<h3><strong>But now, contrary to the expectation, the SE of slopes (coefficients Group1-3:Year) are now much higher, even higher than with no random effect at all!</strong></h3>\\n\\n<p>How is this possible? I would expect that the random effect will \"eat\" the unexplained variability and increase \"sureness\" of the estimate!</p>\\n\\n<p>However, the residual SE behaves as expected - it is lower than in the random intercept model.</p>\\n\\n<p><a href=\"http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R\" rel=\"nofollow\">Here is the data</a> if needed.</p>\\n\\n<h2>Edit</h2>\\n\\n<p>Now I realized astonishing fact. If I do the linear regression for each individual separately and then run ANOVA on the resultant slopes, <strong>I get exactly the same result as the random slope model!</strong> Would you know why?</p>\\n\\n<pre><code>indivSlope = c()\\nfor (indiv in 1:103) {\\n    mod1 = lm(logInd ~ Year, data = mydata[mydata$Individual == indiv,])\\n    indivSlope[indiv] = coef(mod1)[\\'Year\\']\\n}\\n\\nindivGroup = unique(mydata[,c(\"Individual\", \"Group\")])[,\"Group\"]\\n\\n\\nanova1 = lm(indivSlope ~ 0 + indivGroup)\\nsummary(anova1)\\n\\nCall:\\nlm(formula = indivSlope ~ 0 + indivGroup)\\n\\nResiduals:\\n      Min        1Q    Median        3Q       Max \\n-0.176288 -0.016502  0.004692  0.020316  0.153086 \\n\\nCoefficients:\\n              Estimate Std. Error t value Pr(&gt;|t|)\\nindivGroup1 -0.0084165  0.0065555  -1.284    0.202\\nindivGroup2  0.0032369  0.0075103   0.431    0.667\\nindivGroup3  0.0006081  0.0078892   0.077    0.939\\n\\nResidual standard error: 0.04248 on 100 degrees of freedom\\nMultiple R-squared: 0.01807,    Adjusted R-squared: -0.01139 \\nF-statistic: 0.6133 on 3 and 100 DF,  p-value: 0.6079 \\n</code></pre>\\n\\n<p><a href=\"http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R\" rel=\"nofollow\">Here is the data</a> if needed.</p>\\n',\n",
       " '<p>There are two time sequences in my system, one of them represents IN events, and the other OUT events. Each IN event would be released by the nearest following OUT event or reaching the deadline. I want to get the pdf of the interval between the time the IN event is triggered until the time it is released (OUT event occurs or censored at deadline).</p>\\n\\n<p>As the following figure shows, $t_a$, $t_r$, $t_r^\\'$ represent the IN event time, the nearest OUT event time and last OUT event time prior to $t_r$. The nearest OUT event can be represented as $t_r&gt;t_a$ and $t_r^\\' &lt; t_a$. I extend the situation to statistics, so that $t_a$ is a sample of random variable $T_a \\\\\\\\sim U(0,T)$. $t_r$ obey $T_r \\\\\\\\sim U(0,2T)$. The interval of two OUT event, $t_r-t_r^\\'$, should be treated as $T_i \\\\\\\\sim Exp(\\\\\\\\lambda)$.  </p>\\n\\n<p><img src=\"http://i.stack.imgur.com/RfuzT.png\" alt=\"enter image description here\"></p>\\n\\n<p>How to model this complex multivariate random variable and get its pdf? </p>\\n\\n<p>Furthermore, deadline of event $t_a$ is $2T$, which means that if there is no $t_r$ between $(t_a, 2T)$, it would be released at time $2T$. This condition could introduce more difficulty of modeling.</p>\\n',\n",
       " '<p>I am facing the evaluation of two text classifiers. I have a <strong>large training dataset</strong> (to be used for training only), <strong>and a separated small test set</strong> (to be used for testing only), <strong>both being balanced</strong>. Which one of the following methods should be most appropiated one and why?</p>\\n\\n<p>1) Stratified Repeated held-out evaluation (repeated subsampling):\\nSample k times without repetition from the training dataset, each sample being balanced. For every sample, classifier is trained with the sample, and accuracy is tested with the full test dataset. Results are averaged. </p>\\n\\n<p>2) Stratified Cross-validation:\\nDivide the full training dataset in k slices with equal size, each slice being balanced. For every slice, classifier is trained with the slice, and accuracy is tested with the full test dataset. Results are averaged. </p>\\n\\n<p>3) Stratified Bootstrapping:\\nSample k times with repetition from the large training dataset, each sample being balanced. For every sample, classifier is trained with the sample, and accuracy is tested with the full test dataset. Results are averaged.</p>\\n',\n",
       " '<p>I\\'ve been trying to remember my High School teachings and are falling short.</p>\\n\\n<p>I\\'m working on a project where I need to give a % of correctness for an integer (How close a given number is to the actual number within a 300% difference).  For example if the number we want is 50, any number from -100 to 150 will return > 0% correctness.</p>\\n\\n<p>The problem is that we need a curve (log, or bell curve, or something similar) to return a non-linear % correctness (i.e. 100 is 50% correct in linear terms, but we would want maybe 66%? ... and 125 is 33%?) and I dont have a formula to get this response.  </p>\\n\\n<p>Something like this (sorry, used mspaint quickly to try to explain)\\n(please go to <a href=\"http://stackoverflow.com/questions/7759062/math-statistics-bell-curve-computing-correct-given-2-numbers-c\">http://stackoverflow.com/questions/7759062/math-statistics-bell-curve-computing-correct-given-2-numbers-c</a> to see the image, cant post it here due to me being a new user)</p>\\n\\n<p>Am I explaining this properly?  Make sense?  Any help?  ;)</p>\\n\\n<p>I ran into standard deviation too, just its a bit complicated for me to process right now. If you understand it, can you throw me a quick formula?</p>\\n',\n",
       " '<p>For R, I understand that the package lme4 and the function glmer roughly corresponds to glimmix in SAS.  What is the default covariance structure when fit and can it be changed?  If so how?</p>\\n',\n",
       " '<p>Hello after struggling with using R for the last couple of days I was hoping someone could help me with a statistical analysis I am completing for an environmental science honours project. Using R statistics is not something we have been taught and I am worried that I may have bitten of more then I can chew, however my whole project is based around the <strong>hierarchical partitioning method and the exhaustive search multiple regression analysis method.</strong></p>\\n\\n<p>The <a href=\"http://cran.r-project.org/web/packages/hier.part/index.html\" rel=\"nofollow\">hier.part</a> package was installed along with <a href=\"http://cran.r-project.org/web/packages/gtools/index.html\" rel=\"nofollow\">gtools</a>.</p>\\n\\n<p>I have converted my dataset to a .csv file with seven independent variables and one dependant variable with around 400 replicates (my intention is to do this analysis on eight datasets in total with different amounts of replicates and another dependant variable, but I am starting with this one). The dependant variable is GPP, the independent variables are, NDVI, Temperature, Precipitation, Solar Radiation, Nutrient Availability and Soil Available Water Capacity.</p>\\n\\n<p>Secondly I imported the .csv file into R using the script</p>\\n\\n<pre><code>GPPANDDRIVER &lt;- read.table(\"C:\\\\\\\\\\\\\\\\etc, header=T, sep=\",\")\\n</code></pre>\\n\\n<p>This works fine and I can edit the table using </p>\\n\\n<pre><code>edit(GPPANDDRIVER)\\n</code></pre>\\n\\n<p>After looking at the <code>hier.part</code> package documentation available <a href=\"http://cran.r-project.org/web/packages/hier.part/hier.part.pdf\" rel=\"nofollow\">here</a> it seems like I need to define Y which in the script below is the dependent variable and define <code>scan</code> which is the independent variables (mentioned before).</p>\\n\\n<pre><code>hier.part(y, xcan, family = \"gaussian\", gof = \"RMSPE\", barplot = TRUE)\\n</code></pre>\\n\\n<p>I was defining the dependant <code>y</code> vector as </p>\\n\\n<pre><code>y &lt;- as.vector(GPPANDDRIVER[\"GPP\"])\\n</code></pre>\\n\\n<p>This also works fine and I have my y vector. However I am not sure how to load independent variables onto the xcan dataframe part of the script. I have tried typing in two scripts but they have not worked.</p>\\n\\n<pre><code>xcan &lt;- as.vector(GPPANDDRIVER[-GPP])\\n## AND\\nxcan &lt;- data.frame(GPPANDDRIVER[-GPP])\\n</code></pre>\\n\\n<p>If anyone could help me find the right script for representing my independant variables as xcan that would be greatly appreciated. Also once defined if I entered in the hier.part script mentioned above would R then show me results of the analysis after processing? I will be moving onto to the regression analysis after this if anyone can shed some light on this first problem.</p>\\n\\n<pre><code>*information on hier.part arguments.*\\n\\n**Arguments**\\n\\ny a vector containing the dependent variables\\n\\nxcan a dataframe containing the n independent variables\\n\\nfamily family argument of glm\\n\\ngof Goodness-of-fit measure. Currently \"RMSPE\", Root-mean-square ’prediction’\\n\\nerror, \"logLik\", Log-Likelihood or \"Rsqu\", R-squared\\n\\nprint.vars if FALSE, the function returns a vector of goodness-of-fit measures. If TRUE, a data frame is returned with first column listing variable combinations and the\\nsecond column listing goodness-of-fit measures.\\n</code></pre>\\n',\n",
       " '<p>There are already some good answers on project management (eg. <a href=\"http://stats.stackexchange.com/questions/2910\">How to efficiently manage a statistical analysis project?</a>). These are great, and make life a lot easier. In particular, the workflow that runs along the lines of </p>\\n\\n<ol>\\n<li>Load raw data.</li>\\n<li>Manipulate raw data into useful forms. (Unload raw data to save space).</li>\\n<li>Perform analysis and store results.</li>\\n<li>Make and store figures.</li>\\n</ol>\\n\\n<p>One thing that\\'s still hard is to get from raw data to final results and figures, without having to run the whole project from scratch every time something changes. What I would like to be able to do is something like this:</p>\\n\\n<ul>\\n<li>Attempt to create figure. Is manipulated data is available in memory?\\n<ul>\\n<li>Yes: create figure.</li>\\n<li>No: Does manipulated data exist on disk?\\n<ul>\\n<li>Yes: Load manipulated data. </li>\\n<li>No: Load raw data, manipulate, and save, then unload raw data objects.</li>\\n</ul></li>\\n</ul></li>\\n</ul>\\n\\n<p>Also, in each step it would be nice to have some trigger to force a full re-load, if the raw data has been updated.</p>\\n\\n<p>Is there an existing framework in R for doing something like this? Or are there any recommended ways of doing this? As it stands, I often have to run everything from scratch, which can take ages (large data files, complex manipulations), and is a waste of resources</p>\\n',\n",
       " '<p>I am not very good in statistics (ok, I\\'m really bad), I guess this is a very simple question but I dont understand much of the literature.</p>\\n\\n<p>I have a dataset that is arranged in 2 columns (<code>var1</code> is my response variable, <code>var2</code> my predictor). Based on Pearson correlation coefficient, I found that my data is correlated (with a significant p-value &lt; 0.01). I also fitted a regression model in R as <code>lm(var1 ~ var2)</code>, and made a nice graph.</p>\\n\\n<p>If my two variables are correlated, does this mean that I can use the results from my linear regression to make a prediction, or do I have to rely on another technique? What I am trying to predict is the number of failures by time X in the same software.\\nThe regression equation I obtained reads $0.002282\\\\\\\\cdot x +0.545751$.</p>\\n\\n<p>The data sample is not very big, it is the cumulative number of failures experienced in a software program through time.</p>\\n\\n<pre><code>&gt; summary(FAILURES,TOTALTIME)\\n  Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \\n  1.00    4.25    7.50    7.50   10.75   14.00\\n&gt;cor.test(TOTALTIME,FAILURES)\\nPearson\\'s product-moment correlation\\ndata:  TOTALTIME and FAILURES \\nt = 54.1572, df = 12, p-value = 8.882e-16\\nalternative hypothesis: true correlation is not equal to 0 \\n95 percent confidence interval:\\n0.9933656 0.9993741 \\nsample estimates:\\ncor \\n0.9979606 \\n</code></pre>\\n\\n<p>Here is a graph of my data:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/gNZze.jpg\" alt=\"scatterplot with fit\"></p>\\n',\n",
       " '<p>I have an set of input ranges <code>{[a1, b1], [a2, b2], ...}</code>. Each <code>a</code> and <code>b</code> represent integer values.</p>\\n\\n<p>I have a constant \"segment length\" <code>i</code> that is always less than <code>b-a</code> for all <code>a</code> and <code>b</code>.</p>\\n\\n<p>I would like to uniformly sample (either with or without replacement) from the input ranges to get a set of subranges that fall within those inputs.</p>\\n\\n<p>My naive approach is to expand the inputs to a larger set:</p>\\n\\n<pre><code>{\\n  [a1, a1 + i],\\n  [a1 + 1, a1 + i + 1],\\n  ...\\n  [b1 - i, b1],\\n  [a2, a2 + i],\\n  ...\\n  [b2 - i, b2],\\n  ...\\n}\\n</code></pre>\\n\\n<p>and then sample from this. This would be a pretty memory-exhaustive approach. I also need to deal with duplicate elements. Is there a smarter way to subsample?</p>\\n',\n",
       " '<p>One simple way would be to model your data as beta distributed. The beta is by definition between 0 and 1:</p>\\n\\n<pre><code>xx &lt;- seq(.01,.99,by=.01)\\nplot(xx,dbeta(xx,shape1=12,shape2=2),type=\"l\",xlab=\"\",ylab=\"\")\\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/eb0t2.png\" alt=\"beta density\"></p>\\n\\n<p>For an added twist, you could model the little bump at zero by mixing the beta with a point mass there. This is commonly done to add \"additional\" zeros to the Poisson distribution (\"zero-inflated Poisson\"), but it looks like it could be helpful in your case, too.</p>\\n',\n",
       " '<p>I was a bit confused with the meaning of $\\\\\\\\beta$, and thought its usage was rather loose. In fact, it seems that $\\\\\\\\beta$ is used to express two distinct concepts:</p>\\n\\n<ol>\\n<li>The generalisation of the sample \"b coefficient\" to the population concerned.</li>\\n<li>The standardized regression coefficients (regression coefficients obtained when all variables are standardized with a sd of 1).</li>\\n</ol>\\n\\n<p>Would there be an alternative symbol to any one of the two significations above, to avoid this confusion?</p>\\n',\n",
       " '<p>The first thing you need to do is to figure out is the source of your generalization error. Is it \"bias\" or is it \"variance\"?  (or perhaps something else?) If it is variance, your training set might be small for what you are trying to accomplish, and you might need more training data.  If it is bias, then changing to a different model, or changing the parameters of your SVM might help you get a better result. </p>\\n',\n",
       " '<p>I am doing Cox regression models and KM plots for a data set where the end point is death.  In addition there is information about whether the death was cancer-specific or not.  So I have three categories:</p>\\n\\n<ol>\\n<li>No death, last seen is the date of censor</li>\\n<li>Death - cancer-specific</li>\\n<li>Death - other cause</li>\\n</ol>\\n\\n<p>What I would like to know is what to do with category 3 data when I am looking at cancer-specific survival as my endpoint.  Do I censor that data at the date of death OR do I just remove that data from the dataset? </p>\\n',\n",
       " '<p>A good book to read/reference for the history is \"The Theory That Would Not Die\".</p>\\n\\n<p>For the example I would start with something that can be done either way, a common dataset that a frequentist would do a simple t-test and confidence interval on, then show how you could analyze the same data/question using a Bayesian approach.  Show that for diffuse priors the 2 methods give pretty much the same results.  Then talk about when you might use each one, for the simple case the frequentist approach is quick and easy, but the Bayesian case would allow the use of prior information.  Then maybe show some other examples that could be done either way, but the Bayesian approach has advantages, maybe think about linear regression and show (or briefly point out) that the slopes can be estimated either way and you can constuct intervals either way, but what about if you know the slopes are all non-negative (or want to constrain that anyways).  That can be done using frequentist methods, but it takes more work and it is less clear how to construct the confidence intervals.  A Bayes approach is to just use priors on the slopes that are 0 for negative values.  Then maybe end with a more complicated example where a Bayesian approach really shines.</p>\\n',\n",
       " '<p>This problem is identical with the problem of \"learning to rank (LTR)\" in the field of Machine Learning and IR. LTR focuses on how to rank the web pages according a given query. So, your problem is same as LTR\\'s. Now, many approaches have been applied to address this problem. These approaches can be categorized into three directions: 1. point-wise approach, that is treat the ranking problem as a regression or classification problem (similar with the approaches proposed as linear transformation.). 2. pair-wise approach, as described by @GaBorgulya, the state-of-art model is <strong>RankSVM</strong>, which works well for this task. 3. List-wise approach, treating the list as whole, performing permutation (as I guss), then find a best ranking.</p>\\n\\n<p>I\\'m lazy man, you can easily ask for google to search \"Learning to Rank\", and also the Yahoo conducts a context about this field last year. You will find more technique and theory papers for this task.  </p>\\n',\n",
       " '<p><a href=\"http://scholar.google.com/scholar?q=%22Optimal+Cluster+Preserving+Embedding+of+Nonmetric+Proximity+Data%22&amp;btnG=&amp;hl=en&amp;as_sdt=0%2C5&amp;as_vis=1\" rel=\"nofollow\">Optimal Cluster Preserving Embedding of Nonmetric Proximity Data</a> should fit your case. The paper shows how you can obtain a metric vector representation of your objects given only a matrix of pairwise dissimilarity function such that the cluster assignments will be preserved for a range of clustering algorithms, including $k$-means.</p>\\n\\n<p>In your case, what you basically need to do is:</p>\\n\\n<ol>\\n<li>Have your dissimilarity matrix $D$ with zero self-dissimilarity.</li>\\n<li>In case it is not symmetric already, symmetrize by averaging $D_{ij}$ and $D_{ji}$.</li>\\n<li>center it (i.e. subtract row and column mean) to obtain $D^c$</li>\\n<li>Calculate $S^c = -\\\\\\\\frac{1}{2}D^c$</li>\\n<li>Perform a spectral shift: Subtract the $S^c$\\'s smallest eigenvalue from $S^c$\\'s spectrum to ensure it becomes positive-semidefinite. Do this to obtain $\\\\\\\\tilde S^c$.</li>\\n<li>Calculate the eigenvector decomposition of $\\\\\\\\tilde S^c = V \\\\\\\\Lambda V^\\\\\\\\top$.</li>\\n<li>Restore a vector representation in a $n-1$-dimensional metric space of your data: $X = V\\\\\\\\Lambda^{1/2}$.</li>\\n</ol>\\n\\n<p>This assumes that $n$ is not too large. If it is, additionally doing PCAwill give you a more meaningful representation of the data. (The paper describes how to do this, too).</p>\\n',\n",
       " '<p>I have longitudinal data (one measurement being continous and the other ordinal) and I want to fit a model which takes into account the correlation between these two measures. I am using stata. Can anyone help me?</p>\\n\\n<p>Thanks in advance</p>\\n',\n",
       " '<p>Perhaps the poor teaching of statistics to end consumers. The fact is that most courses have given a medieval menu, not including new theoretical developments, computational and best practices, insufficient teaching of modern and complete analysis of real data sets, at least in poor and developing countries, what is the situation in developed countries?</p>\\n',\n",
       " \"<p>I'm currently analysing some EEG data using a repeated measures general linear model, and I've run into a problem. I have three within-subject factors; Scalp area (3), condition (2) and word (2). When I run a three-way analysis I get mean values for Word (separated by condition) which don't match up to the means I've calculated for myself, for condition 1 (those for condition 2 seem fine). When I take the relevant columns of data and enter them into a two-way analysis for just one condition (so Scalp area (3) x Word (2)), I get the correct means, and those for condition 1 don't match the ones from the three-way analysis.</p>\\n\\n<p>I've tried this in SPSS and Statistica with identical results; I've checked the data, and I'm pretty sure it's not an input error. I've checked the order in which I'm entering the variables (slowest-changing first), and that seems fine. So either I'm doing something wrong in the analysis, or I've misunderstood the output, or... something else? \\nI may well be staring straight past an obvious error, but I've spent all afternoon tearing my hair out, so I'm hoping someone might be able to offer some insight!</p>\\n\\n<p>Thanks in advance!</p>\\n\",\n",
       " '<p>Who of you in this forum uses \">R with the <a href=\"http://cran.r-project.org/web/packages/multicore/index.html\">multicore</a>, <a href=\"http://cran.r-project.org/web/packages/snow/index.html\">snow</a> packages, or <a href=\"http://www.nvidia.com/object/cuda_home_new.html\">CUDA</a>, so for advanced calculations that need more power than a workstation CPU? On which hardware do you compute these scripts? At home/work or do you have data center access somewhere?</p>\\n\\n<p>The background of these questions is the following: I am currently writing my M.Sc. thesis about R and High Performance Computing and need a strong knowledge about who actually uses R. I read that R had 1 million users in 2008, but that\\'s more or less the only user statistics I could find on this topic - so I hope for your answers!</p>\\n\\n<p>Sincerely Heinrich</p>\\n',\n",
       " '<p>There are several approaches:</p>\\n\\n<ol>\\n<li>Use the normal distribution as an approximation as described in whuber\\'s link to the <a href=\"http://stats.stackexchange.com/questions/3614/how-to-easily-determine-the-results-distribution-for-multiple-dice\">earlier question</a>, or use some of the other suggestions there.  The mean will be 62.5 while the variance will be 31.25.</li>\\n<li>Do a recurrence starting with $f(i,0)=0$ except when $i=0$ in which case $f(0,0)=1$, and then use $f(i,j+1)=f(i-4,j)/4+f(i-3,j)/4+f(i-2,j)/4+f(i-1,j)/4$ to find $f(i,25)$</li>\\n<li>Expand $(x/4+x^2/4+x^3/4+x^4/4)^{25}$, for example by putting <code>Expand[(x/4+x^2/4+x^3/4+x^4/4)^25]</code> into <a href=\"http://www.wolframalpha.com\" rel=\"nofollow\">Wolfram Alpha</a> and press \"=\" then \"show more terms\" several times</li>\\n<li>Hope somebody has already done some recurrence calculations for you, which they have in rows 925-1000 of an <a href=\"http://oeis.org/A008287/b008287.txt\" rel=\"nofollow\">OEIS table</a> and then divide the results by $4^{25} = 1125899906842624$ </li>\\n</ol>\\n',\n",
       " \"<p>I'm playing the iPhone/Steam game Hero Academy. It's a bit like Chess, except there's some randomness, and at the beginning of a game players choose what team they'll use, each with different strengths and weaknesses. Both players can pick the same team.</p>\\n\\n<p>About 400 of us play in an unofficial league, where we record our games online for Elo rankings. I'm not sure how many are active players, though.</p>\\n\\n<p>I have the win/loss record for every matchup between the four teams. Draws are very rare. The rarest matchup has 143 games; the most-common matchup has 260. There are 3,234 games in the database.</p>\\n\\n<p><strong>What is the appropriate test for determining whether the teams are of equal strength?</strong></p>\\n\\n<p>Let's assume that rock-paper-scissors imbalances are undesirable. That is, if Team A beats Team B which beats Team C which beats Team A, we have an imbalanced game. Every matchup should be fair.</p>\\n\\n<p>A complication is that many players have preferred teams, so that if Team A is very popular among top players, it'll appear stronger. I suspect we'll have to ignore this effect for now.</p>\\n\\n<p>Another complication is that there's a slight advantage to going first, despite our efforts to ameliorate this with a handicap. Let's ignore this effect as well.</p>\\n\",\n",
       " '<p>I don\\'t know what you mean by folded normal distribution.  The distribution of $|X|$ where $X \\\\\\\\sim N(0,1)$? The distribution of $|X|$ when $X \\\\\\\\sim N(\\\\\\\\mu,\\\\\\\\sigma^2)$?  But, regardless of the interpretation, if you aver that <em>\"The mean and variance of the folded normal \\ndistribution are known\"</em> to you, then rest assured that if $x \\\\\\\\sim N(\\\\\\\\mu,\\\\\\\\Sigma)$ has a multivariate normal distribution, then $x_i \\\\\\\\sim N(\\\\\\\\mu_i, \\\\\\\\Sigma_{i,i})$ and so whatever formulas are known to you as the mean and variance of $|X|$ where $X \\\\\\\\sim N(\\\\\\\\mu,\\\\\\\\sigma^2)$\\nalso can be used for the mean and variance of $|x_i|$ which has a folded normal distribution since $x_i \\\\\\\\sim N(\\\\\\\\mu_i, \\\\\\\\Sigma_{i,i})$.  </p>\\n\\n<ul>\\n<li><p>If you know only the mean and variance of $|X|$ when $X \\\\\\\\sim N(0,1)$ but not when \\n$X \\\\\\\\sim N(\\\\\\\\mu,\\\\\\\\sigma^2)$, then please edit your question to say so clearly.</p></li>\\n<li><p>If you know formulas for the mean and variance of $|X|$ where \\n$X \\\\\\\\sim N(\\\\\\\\mu,\\\\\\\\sigma^2)$, please apply the formulas to each $|x_i|$ since\\n$x_i \\\\\\\\sim N(\\\\\\\\mu_i, \\\\\\\\sigma_{i,i})$.  It would probably help the readers of\\nthis forum of you were to type in the formulas for the mean and variance\\nof $|X|$.</p></li>\\n<li><p>If you want to know the covariance of $|x_i|$ and $|x_j|$, please edit\\nyour question to say so clearly.  You have been asked the same question by\\ncardinal also.</p></li>\\n</ul>\\n',\n",
       " '<p>If you have a given autocovariance function, the best model (in terms of tractability) I can think of is a multivariate gaussian process, where, given the autocovariance function $R(\\\\\\\\tau)$ at lag $\\\\\\\\tau$, you can form the covariance matrix easily,</p>\\n\\n<p>$$\\\\\\\\Sigma=\\\\\\\\left[ \\\\\\\\begin{array}{cccc}\\nR(0) &amp; R(1) &amp; ... &amp; R(N) \\\\\\\\\\\\\\\\\\nR(1) &amp; R(0) &amp; ... &amp; R(N-1) \\\\\\\\\\\\\\\\\\n\\\\\\\\vdots &amp; \\\\\\\\ddots &amp; ... &amp; \\\\\\\\vdots \\\\\\\\\\\\\\\\\\nR(N) &amp; R(N-1) &amp; ... &amp; R(0) \\\\\\\\end{array} \\\\\\\\right]$$</p>\\n\\n<p>Given this covariance matrix, you sample data from a multivariate gaussian with the given covariance matrix $\\\\\\\\Sigma$, i.e., sample a vector from the distribution\\n$$f(\\\\\\\\vec{x})=\\\\\\\\frac{1}{(2\\\\\\\\pi)^{N/2}|\\\\\\\\Sigma|^{1/2}}\\\\\\\\text{exp}\\\\\\\\left(-\\\\\\\\frac{1}{2}(\\\\\\\\vec{x}-\\\\\\\\vec{\\\\\\\\mu})^T\\\\\\\\Sigma^{-1}(\\\\\\\\vec{x}-\\\\\\\\vec{\\\\\\\\mu})\\\\\\\\right)\\\\\\\\text{,}$$\\nwhere $\\\\\\\\vec{\\\\\\\\mu}$ is the mean vector.</p>\\n',\n",
       " '<p>I am wanting to examine whether religiosity moderates intervention effects on stigmatized attitudes.  Here are my variables:\\nX = group (1 = experimental; 0 = control)\\nZ = religiosity (14-item scale - using total score)\\nY = either post-test scores or pre-post change scores - Social Distance; Genderism/Transphobia</p>\\n\\n<p>I realize that I need to center my moderator variable and will also need to center my outcome variable (if I use post-test scores).</p>\\n\\n<p>My question is whether to use post-test scores or to use pre-post test change scores (and if I use the latter would I still center)</p>\\n',\n",
       " \"<p>Note that  $\\\\\\\\beta(\\\\\\\\mu)=P[(X_1,\\\\\\\\ldots,X_n)\\\\\\\\in W|\\\\\\\\mu\\\\\\\\in\\\\\\\\Omega_1]=P[|\\\\\\\\sqrt{n}(\\\\\\\\bar X-\\\\\\\\mu_0)/\\\\\\\\sigma|&gt;\\\\\\\\tau _{\\\\\\\\alpha/2}]$$=P[\\\\\\\\sqrt{n}(\\\\\\\\bar X-\\\\\\\\mu_0)/\\\\\\\\sigma&gt;\\\\\\\\tau _{\\\\\\\\alpha/2}$ or $&lt;-\\\\\\\\tau_{\\\\\\\\alpha/2}$]$=P[\\\\\\\\sqrt{n}(\\\\\\\\bar X-\\\\\\\\mu)/\\\\\\\\sigma+\\\\\\\\sqrt{n}(\\\\\\\\mu-\\\\\\\\mu_0)/\\\\\\\\sigma&gt;\\\\\\\\tau _{\\\\\\\\alpha/2}$ or $ &lt; -\\\\\\\\tau _{\\\\\\\\alpha/2}]$$$=P[\\\\\\\\sqrt{n}(\\\\\\\\bar X-\\\\\\\\mu)/\\\\\\\\sigma>\\\\\\\\tau _{\\\\\\\\alpha/2}+\\\\\\\\sqrt{n}(\\\\\\\\mu_0-\\\\\\\\mu)/\\\\\\\\sigma]+P[\\\\\\\\sqrt{n}(\\\\\\\\bar X-\\\\\\\\mu)/\\\\\\\\sigma>-\\\\\\\\tau _{\\\\\\\\alpha/2}+\\\\\\\\sqrt{n}(\\\\\\\\mu_0-\\\\\\\\mu)/\\\\\\\\sigma]$$$$=1-\\\\\\\\Phi(\\\\\\\\tau _{\\\\\\\\alpha/2}+\\\\\\\\sqrt{n}(\\\\\\\\mu_0-\\\\\\\\mu)/\\\\\\\\sigma)+\\\\\\\\Phi(-\\\\\\\\tau _{\\\\\\\\alpha/2}+\\\\\\\\sqrt{n}(\\\\\\\\mu_0-\\\\\\\\mu)/\\\\\\\\sigma)$$Now differentiate $\\\\\\\\beta(\\\\\\\\mu)$ w.r.t $\\\\\\\\mu$ we get $$\\\\\\\\beta'(\\\\\\\\mu)=[\\\\\\\\phi(\\\\\\\\tau _{\\\\\\\\alpha/2}+\\\\\\\\sqrt{n}(\\\\\\\\mu_0-\\\\\\\\mu)/\\\\\\\\sigma)-\\\\\\\\phi(-\\\\\\\\tau _{\\\\\\\\alpha/2}+\\\\\\\\sqrt{n}(\\\\\\\\mu_0-\\\\\\\\mu)/\\\\\\\\sigma)]\\\\\\\\frac{\\\\\\\\sqrt{n}}{\\\\\\\\sigma}$$ Now equating $\\\\\\\\beta'(\\\\\\\\mu)=0$ we get $$\\\\\\\\phi(\\\\\\\\tau _{\\\\\\\\alpha/2}+\\\\\\\\sqrt{n}(\\\\\\\\mu_0-\\\\\\\\mu)/\\\\\\\\sigma)=\\\\\\\\phi(\\\\\\\\tau _{\\\\\\\\alpha/2}+\\\\\\\\sqrt{n}(\\\\\\\\mu-\\\\\\\\mu_0)/\\\\\\\\sigma)$$ [Since $\\\\\\\\phi (-x)=\\\\\\\\phi(x)$]$$=>\\\\\\\\mu=\\\\\\\\mu_0$$ Note that if $\\\\\\\\mu&lt;\\\\\\\\mu_0$ then $\\\\\\\\beta'(\\\\\\\\mu)&lt;0 $ and if $\\\\\\\\mu>\\\\\\\\mu_0$ then $\\\\\\\\beta'(\\\\\\\\mu)>0 $ So from the first order derivative test we get  $\\\\\\\\beta(\\\\\\\\mu)$ is minimum at $\\\\\\\\mu=\\\\\\\\mu_0$. So,hence it is proved that power > size</p>\\n\",\n",
       " '<p>I think that some of the effect of omitting $x_2$ gets absorbed in the estimates of $β_0$ and $β_1$. The rest gets absorbed in the residuals.  I think that a good way to see if $x_2$ has an important effect on y that indicates it should be in the model is to fit the model including just $x_1$ and then plot the residuals vs $x_2$.  If there is a relationship rather than random variation $x_2$ is important and its omission causes omitted variable bias.</p>\\n',\n",
       " '<p>I think the Mann-Whitney/Wilcoxon ranked-sum test is the appropriate test.  The KS test is specifically for comparing continuous distributions - your ratings are ordinal, so it doesn\\'t seem appropriate here.</p>\\n\\n<p>The t-test and the Wilcoxon ranked-sum differ in that the t-test is comparing the <em>means</em> of the two distributions, while the Wilcoxon is comparing the \\'locations\\' by looking at how the values of the two distributions compare when ranked.  When your entire ratings distribution has only two values, one group has only ratings of 4 and your sample size is 14, the t-test seems less appropriate to me.  It works, but I just have a harder time with it conceptually.  This data is more binomial than it is continuous!</p>\\n\\n<p>Here\\'s how I\\'d do all of that in <a href=\"http://cran.r-project.org/index.html\" rel=\"nofollow\">R, which is a freely available software for statistical computing</a> (a step up from using websites to compute tests, I think...)</p>\\n\\n<pre><code># A vector of data for people with smartphone experience\\nsmartphone &lt;- c(4, 4, 4, 4)\\n\\n# A vector of data for people without smartphone experience\\ndumbphone &lt;- c(4, 3, 4, 3, 3, 3, 3, 3, 3, 3)\\n\\n# The Mann-Whitney/Wilcoxon ranked-sum test\\nwilcox.test(x = smartphone, y = dumbphone)\\n\\n# t-test for comparison\\nt.test(x = smartphone, y = dumbphone)\\n\\n# And, why not, a test of proportions\\n# Consider 4 as the event, comparing 4/4 to 2/10\\nprop.test(x = c(4, 2), n = c(4, 10))\\n</code></pre>\\n',\n",
       " '<p>The <code>arima()</code> function in R will do what you want:</p>\\n\\n<pre><code>fit &lt;- arima(y, xreg=x, order=c(1,0,1), seasonal=c(1,N,1))\\n</code></pre>\\n\\n<p>where <code>x</code> is the matrix of regressors and <code>y</code> is the time series you wish to model.</p>\\n',\n",
       " '<p>This is a well-known property of k-means. The initialization is usually <strong>randomized</strong>, so you might even get (and in fact, <em>intend</em> to get) different results for multiple runs. It is a common best practise with k-means to run it multiple times, and choose the one with the minimum average distances (or by some other internal metric).</p>\\n\\n<p><code>DBSCAN</code> is mostly order independent. Only for fringe points that could belong two two different clusters, the cluster assignment is not stable (unless you extend <code>DBSCAN</code> to support multi-assignments or use some other stable tie-braking rule).</p>\\n\\n<p>In general, it actually is even a good idea to shuffle your dataset before doing any kind of analysis. Say your method fails completely, and returns the data set in its original order. If the data was sorted before, the output will look meaningful. So always shuffle your data.</p>\\n',\n",
       " \"<p>If y is too far from x you should be able to define T independent of y. I wouldn't do it this way.  But if you want to update sigma and T and then calculate P(d>T) you can and decide whether or not to keep y based on that probability.</p>\\n\",\n",
       " '<p>I am using 2007\\'s Social Security Life Table: <a href=\"http://www.ssa.gov/oact/STATS/table4c6.html\" rel=\"nofollow\">http://www.ssa.gov/oact/STATS/table4c6.html</a></p>\\n\\n<p>I would like to use the table to generate a number for each scenario I ran. This number represents the remaining life (in years) scenario n will live. The way I plan to do this is to first input an age \\'x\\' let say age 60. I get the corresponding number on column \\'Number of Lives\\' which is 85,227. (This simply reveals that out of 100K people, 85,227 survived) Next, I randomly select one integer from 1:100K and if it is less than 85,227, then this scenario will continue on to next year else this case will die off. I repeat this process, at each age get the corresponding number from column \\'Number of Lives\\' and so on..</p>\\n\\n<p>I want to know if such method is sound from a probability perspective. Does it violate any axioms. Do I still preserve the underlying population distribution of the table? Any better ways of doing it?</p>\\n\\n<p>Thanks,</p>\\n',\n",
       " '<p>My two cents on this topic: if you are really testing hypotheses then I would <em>not</em> use information theoretic approaches, but rather classical hypothesis testing approaches (i.e., I would use <code>drop1</code> on <code>res1</code> to do likelihood ratio tests).  I personally prefer AIC to BIC in ecology because I think its logic more closely matches what we think about the systems: <a href=\"http://emdbolker.wikidot.com/blog:aic-vs-bic\" rel=\"nofollow\">http://emdbolker.wikidot.com/blog:aic-vs-bic</a> . But if I find myself trying to pick the model that most closely matches a \"true\" model with some number of non-zero parameters, I would hypothesis-test instead.</p>\\n',\n",
       " '<p>You can use the dispersion (ratio of variance to the mean) as a test statistic, since the Poisson should give a dispersion of 1. <a href=\"http://www.stats.uwo.ca/faculty/aim/2004/04-259/notes/DispersionTests.pdf\">Here is a link</a> to how to use it as a model test.</p>\\n',\n",
       " '<p>This is a quite interesting philosophical issue related to hypothesis testing (and thus in the frequentist setting also confidence intervals, as I explain <a href=\"http://stats.stackexchange.com/questions/31679/what-is-the-connection-between-credible-intervals-and-bayesian-hypothesis-tests\">here</a>).</p>\\n\\n<p>There are, of course, a lot of hypotheses that could be investigated - passive smoking causes coronary heart disease, drinking alcohol causes chd,  owning dogs causes chd, being a Capricorn causes chd...</p>\\n\\n<p><strong>If we choose one of all of these hypotheses at random, the probability of us choosing a hypothesis that happens to be true is virtually zero.</strong> This seems to be the argument in the quoted text - that it is very unlikely that we happened to test a true hypothesis.</p>\\n\\n<p><strong>But the hypothesis was not chosen at random.</strong> It was motivated by previous epidemiological and medical knowledge about coronary heart disease. There are theoretical mechanisms that explain how smoking could cause coronary heart disease, so it does not seem far-fetched to think that those would work for passive smoking as well.</p>\\n\\n<p>The criticism in the quote may be valid for exploratory studies where a data set is mined for hypotheses. That is the reason that we don\\'t accept such \"discoveries\" as facts - instead we require that the results can be replicated in new studies. Either way, the paper cited in the quote is a meta study and is therefore not affected by this problem.</p>\\n\\n<p>We have seen empirically over the last centuries that <strong>testing hypotheses motivated by theory by comparing the predicted results to the observed results works.</strong> The fact that we believe in this procedure is the reason that we have made so much progress in medicine, engineering and science. It is the reason that I can write this on my computer and that you can read it on yours. To argue that this procedure is wrong is to argue that the scientific method is fundamentally flawed - and we have <strong>plenty of evidence that says otherwise.</strong></p>\\n\\n<p>I doubt that there is anything that a person who isn\\'t willing to accept this kind of evidence actually will accept...</p>\\n',\n",
       " '<p>I\\'ll be careful with this question and suggest you to be careful with the answers. Recall that a $100\\\\\\\\alpha\\\\\\\\%$ confidence interval <strong>DOES NOT MEAN</strong> that there is a probability $\\\\\\\\alpha$ of finding the true mean in the interval: remember that the true mean, $\\\\\\\\mu$, is in the interval or it is not. This mean is not a random variable and, therefore, we cannot associate probabilities to it: the statistic, on the other hand, is random.</p>\\n\\n<p>With that stated, I don\\'t know if your question really makes sense. For more information about what confidence intervals really are, please read <a href=\"http://stats.stackexchange.com/questions/2272/whats-the-difference-between-a-confidence-interval-and-a-credible-interval\">this post</a>!</p>\\n',\n",
       " '<p>This is actually an extremely sophisticated problem and a tough ask from your lecturer!</p>\\n\\n<p>In terms of how you organise your data, a 1070 x 10 rectangle is fine.  For example, in R:</p>\\n\\n<pre><code>&gt; conflict.data &lt;- data.frame(\\n+ confl = sample(0:1, 1070, replace=T),\\n+ country = factor(rep(1:107,10)),\\n+ period = factor(rep(1:10, rep(107,10))),\\n+ landdeg = sample(c(\"Type1\", \"Type2\"), 1070, replace=T),\\n+ popincrease = sample(0:1, 1070, replace=T),\\n+ liveli =sample(0:1, 1070, replace=T),\\n+ popden = sample(c(\"Low\", \"Med\", \"High\"), 1070, replace=T),\\n+ NDVI = rnorm(1070,100,10),\\n+ NDVIdecl1 = sample(0:1, 1070, replace=T),\\n+ NDVIdecl2 = sample(0:1, 1070, replace=T))\\n&gt; head(conflict.data)\\n  confl country period landdeg popincrease liveli popden     NDVI NDVIdecl1 NDVIdecl2\\n1     1       1      1   Type1           1      0    Low 113.4744         0         1\\n2     1       2      1   Type2           1      1   High 103.2979         0         0\\n3     0       3      1   Type2           1      1    Med 109.1200         1         1\\n4     1       4      1   Type2           0      1    Low 112.1574         1         0\\n5     0       5      1   Type1           0      0   High 109.9875         0         1\\n6     1       6      1   Type1           1      0    Low 109.2785         0         0\\n&gt; summary(conflict.data)\\n     confl           country         period     landdeg     popincrease         liveli        popden         NDVI          NDVIdecl1        NDVIdecl2     \\n Min.   :0.0000   1      :  10   1      :107   Type1:535   Min.   :0.0000   Min.   :0.0000   High:361   Min.   : 68.71   Min.   :0.0000   Min.   :0.0000  \\n 1st Qu.:0.0000   2      :  10   2      :107   Type2:535   1st Qu.:0.0000   1st Qu.:0.0000   Low :340   1st Qu.: 93.25   1st Qu.:0.0000   1st Qu.:0.0000  \\n Median :1.0000   3      :  10   3      :107               Median :1.0000   Median :1.0000   Med :369   Median : 99.65   Median :1.0000   Median :0.0000  \\n Mean   :0.5009   4      :  10   4      :107               Mean   :0.5028   Mean   :0.5056              Mean   : 99.84   Mean   :0.5121   Mean   :0.4888  \\n 3rd Qu.:1.0000   5      :  10   5      :107               3rd Qu.:1.0000   3rd Qu.:1.0000              3rd Qu.:106.99   3rd Qu.:1.0000   3rd Qu.:1.0000  \\n Max.   :1.0000   6      :  10   6      :107               Max.   :1.0000   Max.   :1.0000              Max.   :130.13   Max.   :1.0000   Max.   :1.0000  \\n                  (Other):1010   (Other):428                                                                                                              \\n&gt; dim(conflict.data)\\n[1] 1070   10\\n</code></pre>\\n\\n<p>For fitting a model, the glm() function as @gui11aume suggests will do the basics...</p>\\n\\n<pre><code>mod &lt;- glm(confl~., family=\"binomial\", data=conflict.data)\\nanova(mod)\\n</code></pre>\\n\\n<p>... but this has the problem that it treats \"country\" (I\\'m assuming you have country as your 107 units) as a fixed effect, whereas a random effect is more appropriate.  It also treats period as a simple factor, no autocorrelation allowed.</p>\\n\\n<p>You can address the first problem with a generalized linear mixed effects model as in eg <a href=\"http://cran.r-project.org/web/packages/lme4/\">Bates et al\\'s lme4</a> package in R.  There\\'s a nice introduction to some aspects of this <a href=\"http://www.rensenieuwenhuis.nl/r-sessions-16-multilevel-model-specification-lme4/\">here</a>.  Something like</p>\\n\\n<pre><code>library(lme4)\\nmod2 &lt;- lmer(confl ~ landdeg + popincrease + liveli + popden + \\n    NDVI + NDVIdecl1 + NDVIdecl2 + (1|country) +(1|period), family=binomial,\\n    data=conflict.data)\\nsummary(mod2)\\n</code></pre>\\n\\n<p>would be a step forward.</p>\\n\\n<p>Now your last remaining problem is autocorrelation across your 10 periods.  Basically, your 10 data points on each country aren\\'t worth as much as if they were 10 randomly chosen independent and identicall distributed points.  I\\'m not aware of a widely available software solution to autocorrelation in the residuals of a multilevel model with a non-Normal response.  Certainly it isn\\'t implemented in lme4.  Others may know more than me.</p>\\n',\n",
       " '<p>For a simulation study I have to generate random variables that show a prefined (population) correlation to an existing variable $Y$.</p>\\n\\n<p>I looked into the <code>R</code> packages <code>copula</code> and <code>CDVine</code> which can produce random multivariate distributions with a given dependency structure. It is, however, not possible to fix one of the resulting variables to an existing variable.</p>\\n\\n<p>Any ideas and links to existing functions are appreciated!</p>\\n\\n<hr>\\n\\n<p><strong>Conclusion:</strong>\\nTwo valid answers came up, with different solutions:</p>\\n\\n<ol>\\n<li>An <code>R</code> <a href=\"http://stats.stackexchange.com/questions/15011/generate-a-random-variable-with-a-defined-correlation-to-an-existing-variable/15040#15040\">script</a> by caracal, which calculates a random variable with an <em>exact</em> (sample) correlation to a predefined variable</li>\\n<li>An <code>R</code> <a href=\"http://stats.stackexchange.com/questions/15011/generate-a-random-variable-with-a-defined-correlation-to-an-existing-variable/15035#15035\">function</a> I found myself, which calculates a random variable with a defined <em>population</em> correlation to a predefined variable</li>\\n</ol>\\n',\n",
       " '<p>I have about 5544 runs where I am trying to classify it as failure or success. Here the number of runs that lead to failure is only 64 and rest is sucess. In that case when I try to use SVM should I make the number of classes equal? Will this affect the results?</p>\\n',\n",
       " '<p>With the <a href=\"http://gamlss.org\">gamlss</a> package you can model the error distribution of the response as a linear, a non-linear, or a smooth function of the explanatory variables. This seems to be a <em>quite powerful</em> approach (I learned a <em>lot</em> about all the possibilities that might arise during the model selection process) and everything is nicely explained in the <a href=\"http://gamlss.org/images/stories/papers/Lancaster-booklet.pdf\">tutorial</a> that comes with the package.</p>\\n',\n",
       " '<p>First, you are not seeing genuine zeros in expression data.  Your biologist is saying that, like all biologists do, but when a biologist says \"it\\'s zero\" it actually means \"it\\'s below my detection threshold, so it doesn\\'t exist.\"  It\\'s a language issue due to the lack of mathematical sophistication in the field.  I speak from personal experience here.</p>\\n\\n<p>The explanation of the zero inflated Gamma in the link you provide is excellent.  The physical process leading to your data is, if I understand it, a donor is selected, then treated with a certain peptide, and the response is measured from that donor\\'s cells.  There are a couple layers here.  One is the overall strength of the donor\\'s response, which feeds into the expression level of each particular cell being measured.  If you interpret your Bernoulli variable in the zero inflated Gamma as \"donor\\'s response is strong enough to measure\", then it might be fine.  Just note that in that case you\\'re lumping the noise of the individual cell\\'s expression with the variation between strongly responding donors.  Since the noise in expression in a single cell is roughly gamma distributed, that may end up causing too much dispersion in your distribution -- something to check for.</p>\\n\\n<p>If the additional variation from donors vs cells doesn\\'t screw up your Gamma fit, and you\\'re just trying to get expression vs applied peptide, then there\\'s no reason why this shouldn\\'t be alright.</p>\\n\\n<p>If more detailed analysis is in order, then I would recommend constructing a custom hierarchical model to match the process leading to your measurements.</p>\\n',\n",
       " '<p><a href=\"http://en.wikipedia.org/wiki/Problem_of_points\">Blaise Pascal and Pierre de Fermat</a> for creating the theory of probability and inventing the idea of expected value (1654) in order to solve a problem grounded in statistical observations (from gambling).</p>\\n',\n",
       " '<p>This question concerns  an implementation of the topmoumoute natural gradient (tonga) algorithm as described in page 5 in the paper Le Roux et al 2007 <a href=\"http://research.microsoft.com/pubs/64644/tonga.pdf\" rel=\"nofollow\">http://research.microsoft.com/pubs/64644/tonga.pdf</a>.</p>\\n\\n<p>I understand that the basic idea is to augment stochastic gradient ascent with the covariance of the stochastic gradient estimates. Basically, the natural gradient approach multiplies a stochastic gradient with the inverse of the covariance of the gradient estimates in order to weight each component of the gradient by the variance of this component. We prefer moving into directions that show less variance during the stochastic gradient estimates: </p>\\n\\n<p>$ng \\\\\\\\propto C^{-1} g$</p>\\n\\n<p>Since updating and inverting the covariance in an online optimisation setting is costly, the authors describe a ``cheap\\'\\' approximate update algorithm as described on page 5 as:</p>\\n\\n<p>$C_t = \\\\\\\\gamma \\\\\\\\hat{C}_{t-1} + g_tg_t^T$ where  $C_{t-1}$ is the low rank approximation at time step t-1. Writing  $\\\\\\\\hat{C}_{t} = X_tX_t^T$ with $X_t =\\\\\\\\sqrt{\\\\\\\\gamma} X_{t-1}\\\\\\\\ \\\\\\\\ g_t]$ they use an iterative update rule for the  gram matrix $X_t^T X_T = G_t = $</p>\\n\\n<p>($\\\\\\\\gamma G_{t-1}$ &nbsp;&nbsp;&nbsp;&nbsp; $\\\\\\\\sqrt{\\\\\\\\gamma} X^T_{t-1}g_t$</p>\\n\\n<p>$\\\\\\\\sqrt{\\\\\\\\gamma} g^T_t X_{t-1}$ &nbsp;&nbsp;&nbsp;&nbsp; $g_t^Tg_t$)                                                                                </p>\\n\\n<p>They then state ``To keep a low-rank estimate of  $\\\\\\\\hat{C}_{t} = X_tX_t^T$, we can compute its eigendecomposition and keep only the first k eigenvectors. This can be made low cost using its relation to that of the Gram matrix</p>\\n\\n<p>$G_t= X_t^T X_T$:</p>\\n\\n<p>$G_t = VDV^T$</p>\\n\\n<p>$C_t = (X_tVD^{-\\\\\\\\frac12})D(X_tVD^{-\\\\\\\\frac12})^T$\\'\\'</p>\\n\\n<p>Because it\\'s cheaper than updating and decomposing G at every step, they then suggest that you should update X for several steps using \\n$C_{t+b} = X_{t+b}X_{t+b}^T$ with $X_{t+b} = \\\\\\\\left[\\\\\\\\gamma U_t, \\\\\\\\ \\\\\\\\gamma^{\\\\\\\\frac{b-1}{2}g_{t+1}},...\\\\\\\\ \\\\\\\\  \\\\\\\\gamma^{-\\\\\\\\frac12}g_{t+b-1}, \\\\\\\\ \\\\\\\\gamma^{\\\\\\\\frac{t+b}{2}g_{t+b}}\\\\\\\\right]$</p>\\n\\n<p>I can see why you can get $C_t$ from $G_t$ using the eigendecomposition. But I\\'m unsure about their update rule for X. The authors don\\'t explain where U is coming from. I assume (by notation) this is the first k eigenvectors of $C_t$, correct? But if so, why would the formula for $X_t$ be a good approximation for $X_t$? When I implement this update rule, $X_t$ does not seem to be a good approximation of the \\'\\'real\\'\\' $X_t$ (that you would get from $X_t = [\\\\\\\\sqrt{\\\\\\\\gamma} X_{t-1}\\\\\\\\ \\\\\\\\ g_t]$) at all. So why should I then be able to get a good approximation of $C^{-1}$ from this (I don\\'t)? The authors are also not quite clear about how they keep $G_t$ from growing (The size oft he matrix $G_t$ increases at each iterative update). I assume they replace $G_t$ by $\\\\\\\\hat V\\\\\\\\hat D\\\\\\\\hat V^T$ with $\\\\\\\\hat V$ and $\\\\\\\\hat D$ being the first k components of the eigendecomposition?</p>\\n\\n<p>So in summary:</p>\\n\\n<ul>\\n<li><p>I tried implementing this update rule, but I\\'m not getting good results and am unsure my implementation is correct</p></li>\\n<li><p>why should the update rule for $X_t$ be reasonable? Is $U_t$ really the first k eigenvectors of $C_t$? (Clearly I cannot let $X_t$ and $G_t$ grow for each observed gradient $g_t$)</p></li>\\n<li><p>This is far fetched, but has anyone implemented this low rank approximate update of the covariance before and has some code to share so I can compare it to my implementation?</p></li>\\n</ul>\\n\\n<p>As a simple example if I simulate having 15 gradients in matlab like: </p>\\n\\n<pre><code>X = rand(15, 5);\\nc = cov(X);\\ne = eig(c);\\n%if the eigenvectos of C would give a good approximation of the original , this should give an appoximation of c, no?:\\nc_r = e\\'*e; %no resemblence to c\\n</code></pre>\\n\\n<p>So I\\'m quite certainly doing it wrong, I guess U might actually not be the eigenvectors of C, but then what is U?\\nAny suggestions or references would be  most welcome!</p>\\n\\n<p>(sorry about the terrible layout, looks like only a subset of latex is supported, no arrays for matriced and embedded latex doesn\\'t look particularly good, all the formulas are much more readable on page 5 of the referenced paper :)\\nAlso, is this considered off-topic here? It\\'s really more related to optimisation and machine-learning...)</p>\\n',\n",
       " '<p>Is there any comprehensive reference on (or introduction to) how people have tried to model non-independent random variables? I already know about mixing processes, which express in various ways according to various coefficients how \"future\" events depend on \"past\" events, but that\\'s about it...</p>\\n',\n",
       " \"<p>The short version of my question is: How is it possible to control for a confounding variable that has a Dirichlet distribution?</p>\\n\\n<p>Suppose I have electoral and census data for a large set of cities. In this simplified example, say, there are two parties (Liberal and Conservative), and that someone is either college educated or not. I would like to test for correlation between  College Education and voting for the Liberal across all my cities. However I also have the income distribution per quintile, and want to control for this in my analysis. So the data looks something like:</p>\\n\\n<pre><code>City        Liberal Conservative CollegeEducated    Income Distribution\\nSpringfield 55      45           61                  15 33 37 10 5\\nShelbyville 38      62           43                  10 31 42 10 7\\n...\\nCapitol     70      30           57                  12 33 40 10 5\\n</code></pre>\\n\\n<p>If my confounding variable were categorical I could maybe perform an elaboration analysis, but here I don't know what is valid. Any advice? Let me know if any of this needs clarification.</p>\\n\",\n",
       " \"<p>I'm considering using a neural network on financial time series but rather than train the network on actual data I am going to train on a model of the data which is perturbed by random noise. This being the case, I will potentially have unlimited amounts of model training data. However I don't want to generate a huge amount of data and then train the model as it might take a very long time to reach a solution, and I have no idea how much data to actually generate.</p>\\n\\n<p>What I am thinking of doing is training on a small amount of model data (say 5000 examples) and get the values for the hidden nodes, record these values, and then repeat again, thereby building up a distribution of values for each node. These distributions could then be bootstrapped to get the mean value per node, and the whole process would stop once the change in the bootstrapped mean values falls below a given threshold.</p>\\n\\n<p>Edit - the purpose of the network will be to classify/label the time series over the recent past as being in one of a finite number of states, e.g. trending up/down, moving sideways, in congestion etc. These states can be modelled using synthetic data with known labels, and then on real data the network's job will be to identify which state the real data most closely resembles. This will be used as input to a separate decision making process.</p>\\n\\n<p>My question is - is there any reason why this <em>would not</em> be a valid approach to take?</p>\\n\",\n",
       " '<p>If you have several measurements of the <em>same</em> quantity several times in the two units, there is, in general, no way to estimate the transformation from one unit to the other. </p>\\n\\n<p>However, if you <em>knew</em> that that there is a multiplicative relationship between the two, <em>and</em> that the noise in the two sets if measurements is zero-mean normal (with equal variances or different but known variances), then you can estimate the multiplicative factor $k$ by maximum-likelihood.</p>\\n\\n<p>If you make the above assumptions you can proceed as follows. Let $X_B$ be the actual value of the quantity you repeatedly measure in units of $B$. Then $L_{Ai} = k X_B + e_i$, $i = 1, \\\\\\\\dots, n$, and  $L_{Bj} = X_B + f_j$, $j = 1, \\\\\\\\dots, m$. </p>\\n\\n<p>$e_i$ and $f_j$ are normal i.i.d., normal random variables with mean 0 and variance $\\\\\\\\sigma^2$. You can write the log-likelihood of the data as </p>\\n\\n<p>$$ L(data; k, X_B) =  const - \\\\\\\\frac{1}{\\\\\\\\sigma^2}\\\\\\\\sum_i (L_{Ai} - k X_B)^2  - \\\\\\\\frac{1}{\\\\\\\\sigma^2}\\\\\\\\sum_i (L_{Bi} - X_B)^2 $$ </p>\\n\\n<p>You should be able to maximize this quantity in terms of $k$ and $X_B$ to obtain your transformation (and an estimate of the quantity).</p>\\n\\n<p>In fact, if you go through the algebra of setting the partial derivatives of the log-likelihood function with respect to $k$ and $X_B$ to zero, you should get the expression for $k$ you have in your question.</p>\\n\\n<p>$X_B = \\\\\\\\frac{\\\\\\\\sum_j L_{Bj}}{m}$ and $k = \\\\\\\\frac{ m \\\\\\\\sum_i L_{Ai}}{n \\\\\\\\sum_j L_{Bj}}$</p>\\n',\n",
       " '<p>How do I do group wise clustering in R?</p>\\n\\n<p>Hi all,</p>\\n\\n<p>I have N x K data matrix, where N is the number of observations, K is the number of variables.</p>\\n\\n<p>The N observations fall into M categories or groups.</p>\\n\\n<p>Now I want to cluster the groups, instead of the observations, how do I do that?</p>\\n\\n<p>i.e. the clustering would be at the group level...</p>\\n\\n<p>Thanks a lot for your help!</p>\\n',\n",
       " '<p>These are statistical models,  So, of course an error term is assumed.  Most likely it is an additive error term.  The book may not make it explicit but the fact that it is not shown in the equation should not be interpreted to mean that no error term is assumed.  The author probably thinks that the error term is implicitly assumed.</p>\\n',\n",
       " '<p>I found this article to be a good review/primer on MCMC for machine learning:</p>\\n\\n<p><a href=\"http://cis.temple.edu/~latecki/Courses/RobotFall07/PapersFall07/andrieu03introduction.pdf\" rel=\"nofollow\">An Introduction to MCMC for Machine Learning</a> - Andrieu, de Freitas, Doucet &amp; Jordan (2003).</p>\\n',\n",
       " \"<p>I'm doing a simple AIC-based backward elimination model where some variables are categorical variables with multiple levels. These variables are modeled as a set of dummy variables. When doing backward elimination, should I be removing all the levels of a variable together? Or should I treat each dummy variable separately? And why?</p>\\n\\n<p>As a related question, step in R handles each dummy variable separately when doing backward elimination. If I wanted to remove an entire categorical variable at once, can I do that using step? Or are there alternatives to step which can handle this?</p>\\n\\n<p>Thanks in advance.</p>\\n\",\n",
       " '<p>As far as I can see (forgive me if i\\'m wrong), $p$ and $q$ are <em>independent</em> proportions, so you\\'re after a confidence interval for the difference between two independent proportions. A comparison of various methods for constructing such intervals was conducted by Robert Newcombe:</p>\\n\\n<p>Newcombe RG. <a href=\"http://www3.interscience.wiley.com/journal/3157/abstract\" rel=\"nofollow\">Interval estimation for the difference between independent proportions: comparison of eleven methods.</a> <em>Statistics in Medicine</em> 1998; 17(8):873-890.</p>\\n\\n<p>He found the simplest method with good performance was one involving combining Wilson score intervals for the two proportions (\\'Method 10\\' in the above paper).</p>\\n\\n<p>Newcombe wrote an Excel spreadsheet which implements his recommended formulae for this and other CIs of several estimates involving proportions, available at <a href=\"http://medicine.cf.ac.uk/primary-care-public-health/resources/\" rel=\"nofollow\">http://medicine.cf.ac.uk/primary-care-public-health/resources/</a> .</p>\\n',\n",
       " '<p>In case of expensive functions without derivative, a useful abstract framework for optimization is</p>\\n\\n<pre><code>Compute the function in few points (it may be a regular grid or even not)\\nRepeat\\n    Interpolate data/fit into a stochastic model\\n    Validate the model through statistical tests\\n    Find the model’s maximum.\\n    If it is better that the best one you previously have got, update the maximum\\n    Put this point in the dataset\\n</code></pre>\\n\\n<p>That should also fit well with the pseudo convexity you refered.</p>\\n\\n<p>References here:</p>\\n\\n<p><a href=\"http://www.ressources-actuarielles.net/EXT/ISFA/1226.nsf/9c8e3fd4d8874d60c1257052003eced6/f84f7ac703bf5862c12576d8002f5259/%24FILE/Jones98.pdf\">Efficient Global Optimization of Expensive Black-Box Functions</a></p>\\n\\n<p><a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.53.1791\">A Rigorous Framework for Optimization of Expensive Functions by Surrogates</a></p>\\n',\n",
       " '<p>Any time you do computer simulations to evaluate the performance of a statistical method (eg, power), you are approximating a calculation that might conceivably be calculated analytically (power <em>is</em> a probability).  You could also conceive of doing exact-ish numerical calculations: summing exhaustively across all possible outcomes.  </p>\\n\\n<p>In <a href=\"http://www.biostat.wisc.edu/~kbroman/publications/preCCprob_combined.pdf\" rel=\"nofollow\">a paper to appear in <em>Genetics</em></a>, I wrote:</p>\\n\\n<blockquote>\\n  <p>Simulations are most flexible and are generally simpler to obtain, but lack precision. Numeric calculations can be precise, but can be computationally intensive. Symbolic results are more general than numeric calculations, can enable quicker calculations in software, and have the potential to provide more clear insight.</p>\\n</blockquote>\\n\\n<p>I was trying to justify some crazy efforts I put into some analytical calulations that could easily be obtained numerically.  </p>\\n\\n<p>So regarding your question, I think numeric results can be quite compelling&mdash;not <em>real</em> proof, but likely sufficient to make the point in the range of parameter values considered.  Subjecting the results of numerical calculations to regression analysis can be really useful. (I\\'ve done that to figure out and/or verify an answer that ultimately I derived analytically.)  But again it\\'s not real proof, though if correct to within round-off error it would be pretty compelling just not completely satisfying.</p>\\n\\n<p>The advantage of numerical calculations over simulations is that you can get to $10^{-13}$ error whereas for simulations you\\'d never get that precise.</p>\\n',\n",
       " '<p>I\\'ve been estimating continuous positive outcome Poisson regressions with the Huber/White/Sandwich linearized estimator of variance fairly frequently. However, that\\'s not a particularly good reason to do anything, so here are some actual references.</p>\\n\\n<p>From the theory side, $y$ does <strong>not</strong> need to be an integer for for the estimator based on the Poisson likelihood function to be consistent. The data does not even need to be Poisson. This is shown in Gourieroux, Monfort and Trognon (1984). I believed this is called Poisson PMLE or QMLE, for Pseudo/Quasi Maximum Likelihood.  </p>\\n\\n<p>There\\'s also some encouraging simulation evidence from <a href=\"http://personal.lse.ac.uk/tenreyro/jensen08k.pdf\">Santos Silva and Tenreyro (2006),</a> where the Poisson comes in best-in-show. It also does well <a href=\"http://personal.lse.ac.uk/TENREYRO/ppml-fsr.pdf\">in a simulation with lots of zeros in the outcome</a>. You can also easily do your own simulation to convince yourself that this works in your snowflake case.</p>\\n\\n<p>Finally, you can also use a GLM with a log link function and Poisson family. This yields identical results and placates the count-data-only knee jerk reactions.</p>\\n\\n<p><strong>References Without Ungated Links:</strong></p>\\n\\n<p>Gourieroux, C., A. Monfort and A. Trognon (1984). “Pseudo Maximum Likelihood Methods: Applications to Poisson Models,” <em>Econometrica</em>, 52, 701-720. </p>\\n',\n",
       " '<p>I was reading about regression. I understand one of the how the variance of each $x_i$ needs to be normal distributed, but why doesn\\'t that related to the $s_b$? It seems like it should. I was reading the AP College Board paper about Inference. </p>\\n\\n<blockquote>\\n  <p>Two more assumptions are that the errors are normally distributed and that they have\\n  the same standard deviation for all x values. Although it is not at all obvious, this is\\n  not the same as saying that the residuals are normally distributed and all have the same\\n  standard deviation. The difference is that the “errors” are the deviations between the\\n  response variables and the “actual” underlying (invisible) linear phenomenon, whereas\\n  the “residuals” are the deviations between the response variables and the least-squares\\n  regression line, which is only an estimate of the “actual” line.</p>\\n</blockquote>\\n\\n<p>I understand (I think) that just because the error of the regression is normal doesn\\'t mean that the variance of $x_i$ is normal because the sample could just randomly follow normality.  (don\\'t know if I said that statistically correct), but if all the $x_i$ are normal and the variance are equal, then why doesn\\'t that variance equal the variance that one would get for the $\\\\\\\\beta$</p>\\n\\n<p>I have gotten some great answers here that are very clear and I am hoping that I could get another answer to help clear up my confusion.  </p>\\n\\n<p>The kernel of my question is <strong>What exactly is the relationship between the variance of the residuals and the $x_i$ and is there any connection to the <em>error, variance</em> of $\\\\\\\\beta$</strong></p>\\n\\n<p><img src=\"http://i.stack.imgur.com/i1Qzy.jpg\" alt=\"the graph\"></p>\\n',\n",
       " '<p>Your class sample sizes do not seem so unbalanced since you have 30% of observations in your minority class. Logistic regression should be well performing in your case. Depending on the number of predictors that enter your model, you may consider some kind of penalization for parameters estimation, like ridge (L2) or lasso (L1). For an overview of problems with very unbalanced class, see Cramer (1999), The Statistician, 48: 85-94 (<a href=\"http://www.tinbergen.nl/discussionpapers/98085.pdf\">PDF</a>).</p>\\n\\n<p>I am not familiar with credit scoring techniques, but I found some papers that suggest that you could use SVM with weighted classes, e.g. <a href=\"http://www.springerlink.com/content/uwx80880224n25w3/\">Support Vector Machines for Credit Scoring: Extension to Non Standard Cases</a>. As an alternative, you can look at <a href=\"http://mpra.ub.uni-muenchen.de/8156/1/MPRA_paper_8156.pdf\">boosting</a> methods with CART, or Random Forests (in the latter case, it is possible to adapt the sampling strategy so that each class is represented when constructing the classification trees). The paper by Novak and LaDue discuss the pros and cons of <a href=\"http://ageconsearch.umn.edu/bitstream/15129/1/31010109.pdf\">GLM vs Recursive partitioning</a>. I also found this article, <a href=\"http://fic.wharton.upenn.edu/fic/handpaper.pdf\">Scorecard construction with unbalanced class sizes</a> by Hand and Vinciotti.</p>\\n',\n",
       " '<p>I think your setup is not correct. Try this:</p>\\n\\n<pre><code>set.seed(1234)\\nr &lt;- rnorm(100)\\nX &lt;- r\\nu &lt;- -1*X + 0.5*rnorm(100)\\nMyModel &lt;- function(x)  dlmModReg(X, FALSE, dV = x[1]^2)\\nfit &lt;- dlmMLE(u, parm = c(0.3), build = MyModel)\\nmod &lt;- MyModel(fit$par)\\ndlmFilter(u,mod)$a\\n</code></pre>\\n\\n<p>You recover the estimate of the observation variance from the only element of \\nfit$par:</p>\\n\\n<pre><code>&gt; fit\\n$par\\n[1] 0.4431803\\n\\n$value\\n[1] -20.69313\\n\\n$counts\\nfunction gradient \\n      17       17 \\n\\n$convergence\\n[1] 0\\n\\n$message\\n[1] \"CONVERGENCE: REL_REDUCTION_OF_F &lt;= FACTR*EPSMCH\"\\n</code></pre>\\n\\n<p>while your estimate of the coefficient (should be around -1 in your case) can be obtained as the last element of <code>dlmFilter(u,mod)$a</code>, which gives the values\\nof the state as new observations are processed:</p>\\n\\n<pre><code>    &gt; dlmFilter(u,mod)$m\\n  [1]  0.0000000 -1.1486921 -1.2123431 -1.1172783 -1.1231454 -1.1170222\\n  [7] -1.0974931 -1.1377114 -1.0378758 -1.0927136 -1.0955372 -1.0120210\\n [13] -0.9874791 -1.0036429 -1.0765513 -1.0678725 -1.0795124 -1.1568597\\n [19] -1.2044821 -1.2056687 -1.2102896 -1.2938958 -1.2922945 -1.2670604\\n [25] -1.1789594 -1.1570172 -1.1601590 -1.1417200 -1.1585501 -1.1608675\\n [31] -1.1616278 -1.1744861 -1.1717561 -1.1715025 -1.1568086 -1.1451311\\n [37] -1.1520867 -1.1379211 -1.1270897 -1.1048035 -1.1015793 -1.1054597\\n [43] -1.0621750 -1.0621218 -1.0696813 -1.0807651 -1.0816893 -1.0647963\\n [49] -1.0643440 -1.0667282 -1.0626404 -1.0623697 -1.0586265 -1.0571205\\n [55] -1.0569135 -1.0579224 -1.0607623 -1.0582257 -1.0495232 -1.0494288\\n [61] -1.0539632 -1.0555427 -1.0553468 -1.0491239 -1.0488604 -1.0491036\\n [67] -1.0510551 -1.0576294 -1.0611296 -1.0628612 -1.0626451 -1.0573650\\n [73] -1.0629577 -1.0647724 -1.0658052 -1.0823839 -1.0753808 -1.0747229\\n [79] -1.0747762 -1.0615243 -1.0630352 -1.0697431 -1.0666448 -1.0617227\\n [85] -1.0585460 -1.0583981 -1.0563544 -1.0567715 -1.0544349 -1.0573228\\n [91] -1.0588404 -1.0639155 -1.0625845 -1.0578004 -1.0571034 -1.0602645\\n [97] -1.0604838 -1.0586019 -1.0580891 -1.0587096 -1.0577559  \\n</code></pre>\\n\\n<p>Hope this helps.</p>\\n',\n",
       " '<p><strong>Question 1</strong>\\nMake a table with those columns, and three rows.  Fill in the results. You don\\'t need both r and r-squared.</p>\\n\\n<p><strong>Question 2</strong>\\nYes.  A CI has more information that a p-value. Technically, a CI tells you \"if I did the same thing a whole lot of times, how big a range would I need to capture 95% of the results\" which is awkward. But it\\'s more information than the p-value, which just tells you \"if the null hypothesis of no correlation in the population were true, how often would I get a value as extreme or more extreme than this?\"  </p>\\n\\n<p><strong>Question 3</strong>\\nAs I\\'ve told you in another answer to a virtually identical question, if either A or B can be thought of as a dependent variable, you can do regression and include \"group\" as a covariate; you could also include the interaction of group and the independent variable</p>\\n\\n<p><strong>Bonus</strong>\\nThe correction is not needed for the p regarding the difference of correlations, as far as I can see. That\\'s ONE test.  You could argue for or against a correction for doing 3 analyses. I don\\'t see how you could get to 0.0006, even the Bonferroni would be .05/3 (or possibly 4).  However the null is NEVER supported, you only fail to reject. </p>\\n\\n<p>HOWEVER, it is completely irrelevant. The three correlations are virtually identical. </p>\\n\\n<p><strong>Added bonus</strong>\\nWhy do you keep asking the same question over and over? \\nDo you expect different results?</p>\\n',\n",
       " '<p>I would like to pose this question in two parts. Both deal with a generalized linear model, but the first deals with model selection and the other deals with regularization.</p>\\n\\n<p><b>Background:</b> I utilize GLMs (linear, logistic, gamma regression) models for both prediction and for description. When I refer to the \"<i>normal things one does with a regression</i>\" I largely mean description with (i) confidence intervals around coefficients, (ii) confidence intervals around predictions and (iii) hypothesis tests concerning linear combinations of the coefficients such as \"is there a difference between treatment A and treatment B?\". </p>\\n\\n<p>Do you legitimately lose the ability to do these things using the normal theory under each of the following? And if so, are these things really only good for models used for pure prediction?</p>\\n\\n<p><b>I.</b> When a GLM has been fit via some model selection process (for concreteness say its a stepwise procedure based on AIC).</p>\\n\\n<p><b>II.</b> When a GLM has been fit via a regularization method (say using glmnet in R).</p>\\n\\n<p>My sense is that for I. the answer is technically that you should use a bootstrap for the \"<i>normal things one does with a regression</i>\" but no one really abides by that.</p>\\n\\n<p>Thank you. </p>\\n\\n<p><b>Add:</b><br/>\\nAfter getting a few responses and reading elsewhere, here is my take on this (for anyone else benefit as well as to receive correction).<br/><br/>\\n<b>I.</b> <br/>\\n<b>A) RE: Error Generalize.</b> In order to generalize error rates on new data, when there is no hold out set, cross validation can work but you need to repeat the process completely for each fold - using nested loops - thus any feature selection, parameter tuning, etc. must be done independently each time. This idea should hold for any modeling effort (including penalized methods).<br/><br/>\\n<b>B) RE: Hypothesis testing and confidence intervals of GLM.</b> When using model selection (feature selection, parameter tuning, variable selection) for a generalized linear model and a hold out set exists, it is permissible to train the model on a partition and then fit the model on the remaining data or the full data set and use that model/data to perform hypothesis tests etc. If a hold out set does not exist, a bootstrap can be used, as long as the full process is repeated for each bootstrap sample. This limits the hypothesis tests that can be done though since perhaps a variable will not always be selected for example.<br/><br/>\\n<b>C) RE: Not carrying about prediction on future data sets</b>, then fit a purposeful model guided by theory and a few hypothesis tests and even consider leaving all  variables in the model (significant or not) (along the lines of Hosmer and Lemeshow). This is small variable set classical type of regression modeling and then allows the use of CI\\'s and hypothesis test.<br/><br/>\\n<b>D) RE: Penalized regression.</b> No advice, perhaps consider this suitable for prediction only (or as a type of feature selection to then apply to another data set as in B above) as the bias introduced makes CI\\'s and hypothesis tests unwise - even with the bootstrap.</p>\\n',\n",
       " '<p>You should not exclude outliers just because they cause problems, nor should you use a subset of your data because the full data causes problems. Neither of these solved the \"problem\" in your case, but even if they did, it wouldn\\'t be right.</p>\\n\\n<p>You haven\\'t given a lot of detail about what you are trying to do or how you are doing it, but can you add reaction time as a covariate? </p>\\n',\n",
       " '<p>On an attempt to solve this <a href=\"http://cs.stackexchange.com/questions/1076/how-to-approach-vertical-sticks-challenge\">problem</a> I\\'ve managed to reduce it to finding the expected number of white balls picked until one black ball is observed (let\\'s call that value $v$). Except that, unlike the geometric distribution, this needs to be done without replacement. Hypergeometric distribution doesn\\'t come to the rescue as the number of black balls picked is immaterial (and of course the white balls must be picked consecutively). So I am looking for a way to compute $v$, or at least its underlying distribution. I\\'ve tried to search online but couldn\\'t find a distribution that readily fits this. </p>\\n\\n<p>So the problem is:</p>\\n\\n<blockquote>\\n  <p>Consider an urn having total of $n$ balls, $w$ of them are white, and the rest is black. Pick $j$ balls <strong>without</strong> replacement. What is the expected number of white balls that will be picked before a black ball is observed ? Notice that the total number of balls picked, $j$, is fixed.</p>\\n</blockquote>\\n\\n<p>My guess is that cumulative hypergeometric distribution could be used, while dividing somewhere by the number of ways the black balls intersperse the white ones (after the first black ball is observed). But I have little intuition how to go through with this. I.e. to divide before taking the cumulative distribution or after?</p>\\n\\n<p><strong>Update</strong></p>\\n\\n<p>Based on a question in the comments below, to be clear: the desired value is the expected number of white balls observed until <strong>either</strong> a black ball is observed, or $j$ picks happened, whichever comes earlier.</p>\\n\\n<p>Thank you.</p>\\n',\n",
       " '<p><a href=\"http://en.wikipedia.org/wiki/Andrey_Kolmogorov\">Andrey Nikolayevich Kolmogorov</a>, for putting probability theory on a rigorous mathematical footing. While he was a mathematician, not a statistician, undoubtedly his work is important in many branches of statistics.</p>\\n',\n",
       " '<p>I want to fit mixed model using lme4, nlme, baysian regression package or any available. </p>\\n\\n<p><em><strong>Mixed model in Asreml- R  coding conventions</em></strong></p>\\n\\n<p>before going into specifics, we might want to have details on asreml-R conventions, for those who are unfamiliar with ASREML codes.</p>\\n\\n<pre><code>y = Xτ + Zu + e ........................(1) ; \\n</code></pre>\\n\\n<p>the usual mixed model with, y denotes the n × 1 vector of observations,where τ is the p×1 vector of ﬁxed eﬀects, X is an n×p design matrix of full column rank which associates observations with the appropriate combination of ﬁxed eﬀects, u is the q × 1 vector of random eﬀects, Z is the n × q design matrix which associates observations with the appropriate combination of random eﬀects, and e is the n × 1 vector of residual errors.The model (1) is called a linear mixed model or linear mixed eﬀects model. It is assumed </p>\\n\\n<p><img src=\"http://i.stack.imgur.com/gxdur.jpg\" alt=\"enter image description here\"></p>\\n\\n<p>where the matrices G and R are functions of parameters γ and φ, respectively.</p>\\n\\n<p>The parameter θ is a variance parameter which we will refer to as the scale parameter.</p>\\n\\n<p>In mixed eﬀects models with more than one residual variance, arising for example in the\\nanalysis of data with more than one section or variate, the parameter θ is\\nﬁxed to one. In mixed eﬀects models with a single residual variance then θ is equal to\\nthe residual variance (σ2). In this case R must be correlation matrix. Further details on the models are provided in the <a href=\"http://www.vsni.co.uk/downloads/asreml/release2/doc/asreml-R.pdf\">Asreml manual (link)</a>. </p>\\n\\n<p>Variance structures for the errors: R structure and Variance structures for the random eﬀects: G structures can be specified.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/or4Gj.jpg\" alt=\"enter image description here\"><img src=\"http://i.stack.imgur.com/oXTgc.jpg\" alt=\"enter image description here\"></p>\\n\\n<p>variance modelling in asreml() it is important to understand the formation of variance structures via direct products. The usual least squares assumption (and the default in asreml()) is that these are independently and identically distributed (IID). However, if the data was from a field experiment laid out in a rectangular array of r rows by c columns, say, we could arrange the residuals e as a matrix and potentially consider that they were autocorrelated within rows and columns.Writing the residuals as a vector in field order, that is, by sorting the residuals rows\\nwithin columns (plots within blocks) the variance of the residuals might then be</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/SPE5b.jpg\" alt=\"enter image description here\">\\n<img src=\"http://i.stack.imgur.com/IcikW.jpg\" alt=\"enter image description here\"> are correlation matrices for the row model (order r, autocorrelation parameter ½r) and column model (order c, autocorrelation parameter ½c)\\nrespectively. More specifically, a two-dimensional separable autoregressive spatial structure\\n(AR1 x \\xad AR1) is sometimes assumed for the common errors in a field trial analysis.</p>\\n\\n<p><em><strong>The example data:</em></strong></p>\\n\\n<p>nin89 is from asreml-R library, where different varities were grown in replications / blocks in rectangular field. To control additional variability in row or column direction each plot is referenced as Row and Column variables (row column design). Thus this row column design with blocking. Yield is measured variable. </p>\\n\\n<p><strong>Example models</strong> </p>\\n\\n<p>I need something equivalent to the asreml-R codes:</p>\\n\\n<p>The simple model syntax will look like the follows:</p>\\n\\n<pre><code> rcb.asr &lt;- asreml(yield ∼ Variety, random = ∼ Replicate, data = nin89)  \\n .....model 0\\n</code></pre>\\n\\n<p>The linear model is specified in the fixed (required), random (optional) and rcov (error\\ncomponent) arguments as formula objects.The default is a simple error term and does not need to be formally specified for error term as in the model 0. </p>\\n\\n<p>here the variety is fixed effect and random is replicates (blocks). Beside random and fixed terms we can specify error term. Which is default in this model 0. The residual or error component of the model is specified in a formula object through the rcov argument, see the following models 1:4. </p>\\n\\n<p>The following model1 is more complex in which both G (random) and R (error) structure are specified.</p>\\n\\n<p><strong>Model 1:</strong> </p>\\n\\n<pre><code>data(nin89)\\n\\n\\n # Model 1: RCB analysis with G and R structure\\n     rcb.asr &lt;- asreml(yield ~ Variety, random = ~ idv(Replicate), \\n      rcov = ~ idv(units), data = nin89)\\n</code></pre>\\n\\n<p>This model is equivalent to above model 0, and introduces the use of G and R variance model. Here the option random and rcov specifies random and rcov formulae to explicitly specify the G and R structures. where idv() is the special model function in asreml() that identifies the variance model. The expression idv(units) explicitly sets the variance matrix for e to a scaled identity.</p>\\n\\n<p><em><strong># Model 2: two-dimensional spatial model with correlation in one direction</em></strong></p>\\n\\n<pre><code>  sp.asr &lt;- asreml(yield ~ Variety, rcov = ~ Column:ar1(Row), data = nin89)\\n</code></pre>\\n\\n<p>experimental units of nin89 are indexed by Column and Row. So we expect random variation in two direction - row and column direction in this case. where ar1() is a special function specifying a first order autoregressive variance model for Row. This call specifies a two-dimensional spatial structure for error but with spatial correlation in the row direction only.The variance model for Column is identity (id()) but does not need to be formally\\nspecified as this is the default.</p>\\n\\n<p><em><strong># model 3: two-dimensional spatial model, error structure in both direction</em></strong></p>\\n\\n<pre><code> sp.asr &lt;- asreml(yield ~ Variety, rcov = ~ ar1(Column):ar1(Row),  \\n data = nin89)\\nsp.asr &lt;- asreml(yield ~ Variety, random = ~ units, \\n rcov = ~ ar1(Column):ar1(Row), data = nin89)\\n</code></pre>\\n\\n<p>similar to above model  2, however the correlation is two direction - autoregressive one. </p>\\n\\n<p>I am not sure how much of these models are possible with open source R packages. Even if solution of any one of these models will be of great help. <strong><em>Even if the bouty of +50 can stimulate to develop such package will be of great help !</em></strong></p>\\n\\n<p><em><strong>See MAYSaseen has provided output from each model and data  (as answer)  for comparision.</em></strong> </p>\\n\\n<p><em><strong>Edits: \\nThe following is suggestion I received in mixed model discussion forum:</em></strong>\\n\" You might look at the regress and spatialCovariance packages of David Clifford.  The former allows fitting of (Gaussian) mixed models where you can specify the structure of the covariance matrix very flexibly (for example, I have used it for pedigree data).  The spatialCovariance package uses regress to provide more elaborate models than AR1xAR1, but may be applicable.  You may have to correspond with the author about applying it to your exact problem.\" </p>\\n',\n",
       " \"<p>First, I don't think that the distances are distributed as a $\\\\\\\\chi(d)$, assuming that $d$ is the dimension of the space. We know that the $k$ nearest neighbors will be farther if they are sampled from $k$ neighbors than if they are sampled from $1,000,000,000$ neighbors. So it is not possible that they are both distributed as a $\\\\\\\\chi(d)$.</p>\\n\\n<p>In dimension $d$, say that the object $x_0$ is the center of a locally uniform ball of total probability $\\\\\\\\alpha$ and of radius $R$. The probability that a point of this ball is at a distance smaller than $\\\\\\\\delta$ from $x_0$ is $(\\\\\\\\delta/R)^d$, so the pdf is $\\\\\\\\delta^{d-1} d/R^d$. Given that all $n$ points of the sample are in the ball, the $k$ nearest neighbors the $k$ smallest order statistics, so their joint pdf is locally</p>\\n\\n<p>$$ \\\\\\\\propto y_1^{d-1}y_2^{d-1}...y_k^{d-1}(1-(y_k/R)^d)^{n-k},\\n(y_1 \\\\\\\\leq y_2 \\\\\\\\leq ... \\\\\\\\leq y_k).$$</p>\\n\\n<p>This looks nice, but you do not know $R$ if I understood. The local uniform domain may be extremely small ($R$ very small) so that the $k$ nearest neighbors may not <em>all</em> be contained within it. So you do not know how high the distances of the nearest neighbors can go before they stop following this distribution. Some of the neighbors may be in the domain, and some may not.</p>\\n\\n<p>Even if you knew $R$, note that the distances of the $k$ nearest neighbors are not independent. I am not aware of any test that does not assume IID sampling, so I do not think you can answer that question with so little information.</p>\\n\",\n",
       " '<p>No, you cannot say anything at all about $E[w(X)]$ from this.  </p>\\n\\n<p>Let $w_k(X)=w(X)+k$ for any constant $k$.</p>\\n\\n<p>Then $v(X)$ and $w_k(X)$ have the same correlation $\\\\\\\\rho$ as $v(X)$ and $w(X)$.  </p>\\n\\n<p>But this tells you nothing about $k$ or $E[w_k(X)]=E[w(X)]+k$, since $k$ can take any value. </p>\\n',\n",
       " '<p>One way to develop intuition about the genesis of masking is to imagine a one-dimensional problem with three classes each of which has truncated normal PDF (support between -5 and 5) with means of -1, 0, and 1 and the same variance. Now, if one fits a line (using OLS) to a sample drawn from each of these PDFs, it is easy to see that the middle class (the one with the mean of zero) will get a zero slope (flat line) whereas the leftmost class will get negative slope and the rightmost class will get positive slope. This means that, under certain conditions (a particular choice of variance), the middle class might never dominate the other two classes as far as the model value goes which could lead to the middle class being always mis-classified.</p>\\n\\n<p>Figure 4.3 from the book illustrates that point nicely (that 2-dimensional example can be easily reduced to one-dimensional by projecting the PDFs to the line connecting the centroids of the three classes from Figure 4.2) </p>\\n',\n",
       " '<p>I\\'m not aware of a formal definition, but I\\'ve been using the term \"contribution\" with a different meaning from <a href=\"http://stats.stackexchange.com/a/48236/4598\">AdamO\\'s</a> in this paper:</p>\\n\\n<p><a href=\"http://dx.doi.org/10.1007/s00216-011-4985-4\" rel=\"nofollow\">C. Beleites, K. Geiger, M. Kirsch, S. B. Sobottka, G. Schackert and R. Salzer,\\nAnal. Bioanal. Chem., 400 (2011), 2801 - 2816.</a>, you can get the <a href=\"http://softclassval.r-forge.r-project.org/2011/2011-07-01-ABC-Glioma-paper.html\" rel=\"nofollow\">authors\\' manuscript here</a>.</p>\\n\\n<p>This is what I use it for and why:</p>\\n\\n<p>Problem: I\\'m looking at spectroscopic data, i.e some kind of intensity at different wavelengths: each variate covers a different wavelength. As a spectroscopist, I can interprete  the physical/chemical/biological meaning of the different wavelengths.<br>\\nFor such data, each variate will span a different range, and it usually doesn\\'t make sense to scale variates to common range or variance (you\\'d blow up the noise in variates with low signal, and I also need these differences for my interpretation).<br>\\nWhen interpreting the meaning of linear models, I can look at the coefficients. However, coefficients alone are not the whole truth, I need to take into account the range of intensities for the respective variate.</p>\\n\\n<p>If I say the linear model produces some kind of scores $\\\\\\\\mathbf s$ by matrix-multiplying  $n$ spectra of $p$ wavelengths $\\\\\\\\mathbf X$ with coefficients $\\\\\\\\mathbf b$:</p>\\n\\n<p>$ \\\\\\\\mathbf s^{(n \\\\\\\\times 1)} =  \\\\\\\\mathbf X^{(n \\\\\\\\times p)} \\\\\\\\mathbf B^{(p \\\\\\\\times 1)}$<br>\\nwith<br>\\n$ s_{i} = \\\\\\\\sum_{j = 1}^p x_{i, j} \\\\\\\\cdot b_{j}$</p>\\n\\n<p>Now,  I can put the the $x_{i, j} \\\\\\\\cdot b_{j}$ into a matrix of the same size as $\\\\\\\\mathbf X$. The columns correspond to the wavelengths. And each entry tells me how much of the final score of that spectrum is picked up at which wavelength, or which spectral band contributes how much and in which direction to the final score.<br>\\nThis allows me to do a spectroscopic interpretation of the model. These contributions are not real spectra, but they behave mostly like difference spectra with some additional possibilities. But I can relate them e.g. to known differences in the composition of my specimen.\\nFor details, please look into the paper.</p>\\n\\n<hr>\\n\\n<p>Edit: If this already has a name, or you could suggest a better one, I\\'d be happy to hear it.</p>\\n',\n",
       " '<p>I\\'m working with the results of a survey which has multiple questions. All answers (in this case) are categorical and ordinal (such as very unhappy, unhappy, neutral, happy, very happy).</p>\\n\\n<p>I\\'m looking for a way to sort the questions from those with \"worst results\" to those with \"best results\". Getting the extremes is somewhat easy visually. If I plot the distribution of answers for each question, I can identify which questions have lots of \\'good\\' answers (distribution is negatively skewed) or those with lots of \\'bad\\' answers (positively skewed histogram). So <em>picking</em> the extremes is easy but this is also dependent on the data.</p>\\n\\n<p>Quantitatively however, I don\\'t know what to do. Since the answers are on an ordinal, but not an interval scale, I don\\'t know how to calculate an aggregate number for each question. Perhaps giving a numerical value to each category (such as -2, -1, 0, 1 or 2) and summing up the results might work if there\\'s nothing better, but I do realize that mathematically this is not accurate as this is not an interval scale.</p>\\n\\n<p>Oh, I\\'m not a statistician, just a programmer. I hope there is a reasonable option to this, I can imagine it\\'s a fairly common question with categorical data.</p>\\n\\n<p>Thanks in advance.</p>\\n\\n<p>PS I use R in case there is something built-in.</p>\\n',\n",
       " '<p>You could try:</p>\\n\\n<ul>\\n<li>Bagging <a href=\"http://en.m.wikipedia.org/wiki/Bootstrap_aggregating\" rel=\"nofollow\">http://en.m.wikipedia.org/wiki/Bootstrap_aggregating</a></li>\\n<li>Boosting <a href=\"http://en.m.wikipedia.org/wiki/Boosting\" rel=\"nofollow\">http://en.m.wikipedia.org/wiki/Boosting</a></li>\\n<li>Cross validation <a href=\"http://en.m.wikipedia.org/wiki/Cross-validation_(statistics\" rel=\"nofollow\">http://en.m.wikipedia.org/wiki/Cross-validation_(statistics</a>)</li>\\n</ul>\\n',\n",
       " '<p>It is hard to underestimate the contribution of <a href=\"http://en.wikipedia.org/wiki/Andrey_Kolmogorov\" rel=\"nofollow\">Andrey Kolmogorov</a>, for putting probability theory on a rigorous mathematical footing during the Soviet era.</p>\\n\\n<p>Also, <a href=\"http://en.wikipedia.org/wiki/Andrey_Markov\" rel=\"nofollow\">Andrey Markov</a>, for his contribution to stochastic processes and Markov chains in particular, though most of his work was in the pre-Soviet era.</p>\\n\\n<p>Possibly worth mentioning, though he was pre-Soviet Russia, <a href=\"http://en.wikipedia.org/wiki/Pafnuty_Chebyshev\" rel=\"nofollow\">Pafnuty Chebyshev</a> for the Chebyshev inequality.</p>\\n',\n",
       " '<p>Indeed, as you have mentioned it yourself, the lack of independence (and relevance) of the explanatory variables is crucial. Also, it is not a surprise at all that Random Forest is behaving in a much better way than a Naive Bayes classifier since it is much more robust to overfitting, especially in your situation where you have almost five times more explanatory variables than observations. Virtually any \\'ensemble method\\' will do better than a simple Naive Bayes classifier. You could try to do an ensembling of Naives Bayes classifier, in the spririt of what is described in this short <a href=\"http://www.kaggle.com/c/overfitting/forums/t/484/ensembling\">text</a>.</p>\\n',\n",
       " '<p><a href=\"http://jboost.sourceforge.net/\" rel=\"nofollow\">JBoost</a> is an awesome library. It is definitely not written in Python, however It is somewhat language agnostic, because it can be executed from the command line and such so it can be \"driven\" from Python. I\\'ve used it in the past and liked it a lot, particularly the visualization stuff.</p>\\n',\n",
       " '<p>Here is a great article and book that explains the rationale, theory, and application of most of the most popular methods:</p>\\n\\n<p><a href=\"http://www.cs.uvm.edu/~icdm/algorithms/index.shtml\">Top 10 Algorithms in Data Mining</a></p>\\n\\n<p>It\\'s especially neat because it\\'s a \"top 10\" chosen by polling experts in the field.</p>\\n\\n<p>Also, for gene data in general, feature selection is hugely important because of the many features. For example, SVM recursive feature elimination (SVM-RFE) and related methods are very popular and being actively developed and applied in the context of gene data.</p>\\n',\n",
       " \"<ol>\\n<li>Your assumption that the measures will only differ by a multiplicative constant strikes me as certainly false.  The fact that this would not work for converting from Fahrenheit to Celsius demonstrates that.  </li>\\n<li>(A.k.a. #3) You will need to assess more than one part.  You will not have enough degrees of freedom to determine the conversion between the two measurements if you only use one part.  Moreover, try to get parts where the true values of the measurements span as large a range as possible, and certainly span the range within which you will want to make the conversion in the future.  </li>\\n<li><p>(A.k.a. #2) You can determine the conversion equation by means of a regression analysis.  With multiple measures, you could use a multi-level model, but I suspect this is more than is necessary.  If you make several measures of each part with each measurement instrument you can just use the averages, as you describe, to get a more robust measure.  Then you can just use those two means as your $x$ and $y$ values for that part.  The beta estimates from the regression equation will give you the shift required.  </p>\\n\\n<p>Note that these won't be the same values that you could get via other conversion strategies, however, because the procedure differs; for example to convert from Fahrenheit to Celsius, you can subtract 32 and divide by 1.8, but to use a regression equation, $\\\\\\\\beta_0\\\\\\\\approx18$ and $\\\\\\\\beta_1\\\\\\\\approx.6$.  This doesn't matter, as long as you know which procedure you're using.  </p>\\n\\n<p>Another advantage of the regression approach, by the way, is the conversion between two measurement instruments won't necessarily be linear throughout the possible range, which a regression analysis may allow you to model.  </p></li>\\n</ol>\\n\",\n",
       " \"<p>So here's a question. lets say we have three datasets A, B, C, D(continuous, that take values within the data range [0, 5]) where C and D are actually the predicted values of A and B accordingly, taken from a model use. Both pairs (A and C, B and D) are significantly correlated and Spearman's Rho for pair (A,C) is 0.2 where Rho for pair (B,D) is 0.4. When computing the MAE though, the pair with biggest Rho also appears to have the maximum error. Shouldn't it be the other way round? I mean, the biggest the correlation coefficient, the smallest the error since we're talking about positively correlated data?</p>\\n\\n<p>Thank you in advance :)\\nA.</p>\\n\",\n",
       " \"<p>While @propofol's answer is correct for your given scenario with no covariates, if you wish to extend your analysis to include covariates like patient characteristics, contingency table analysis quickly becomes cumbersome.</p>\\n\\n<p>From there, you'd probably use either binomial or logistic regression. Given the relatively high prevalence of recurrence in your data set (~24%), I would probably go with binomial regression, which will provide you with a better estimate. But really, if you want to include covariates like that, my best advice is to go find a statistically-inclined epidemiologist or biostatistician.</p>\\n\",\n",
       " '<p>Suppose we have variables $(X,Y)$ and we have theory tell us that $X$ $\\\\\\\\overset{\\\\\\\\text{cause}}{\\\\\\\\implies} Y$. Perhaps they\\'re time-series variables and it would be common to see something like this:</p>\\n\\n<p>$$\\\\\\\\boxed{Y_{t+1} = a + b X_{t} + e_{t+1}.\\\\\\\\,\\\\\\\\,\\\\\\\\,\\\\\\\\,\\\\\\\\,(1)}$$</p>\\n\\n<p>My question is why it\\'s not just as legitimate to specify:</p>\\n\\n<p>$$\\\\\\\\boxed{X_t = \\\\\\\\alpha + \\\\\\\\beta Y_{t+1}+\\\\\\\\varepsilon_{t+1}\\\\\\\\,\\\\\\\\,\\\\\\\\,\\\\\\\\,\\\\\\\\,(2)}$$</p>\\n\\n<p>where $\\\\\\\\alpha = \\\\\\\\frac{-a}b$, $\\\\\\\\varepsilon_t = \\\\\\\\frac{-e_t}b$, $\\\\\\\\beta = \\\\\\\\frac1b$       ?</p>\\n\\n<p>I understand that in finite samples these equations usually don\\'t hold (for the estimates). However this doesn\\'t answer the question of why the former is more legitimate.  Also, the application isn\\'t forecasting, so I don\\'t need the ease of being able to plug in $X_t$ into $(1)$ and arriving at $\\\\\\\\text{forecast}(Y_{t+1})$ with no effort. </p>\\n\\n<p>This question occurred to me when I was reading an empirical paper. They had variables $(A,B,Y)$ and they thought that $A,B \\\\\\\\overset{\\\\\\\\text{cause}}{\\\\\\\\implies} Y$. Specifically they thought a good approximation of the relationship was:</p>\\n\\n<p>$$\\\\\\\\boxed{Y_{t+1} = a + b(A_t - B_t) + e_{t+1}.\\\\\\\\,\\\\\\\\,\\\\\\\\,\\\\\\\\,\\\\\\\\,(3)}$$</p>\\n\\n<p>However they had theoretical reasons to think that there is a break in $b$ when $(A_t - B_t)$ was \"large\" versus a \"small\" $(A_t - B_t)$. They then went on a big spree of testing all sorts of different dummy variable combos. They even went on to specify a non-linear smooth transition model. However, in this spot it seems far more sensible, interpretable, easier (and probably more statistically powerful) to just do:</p>\\n\\n<p>$$\\\\\\\\boxed{Q_\\\\\\\\tau(A_t-B_t|Y_{t+1}) = \\\\\\\\alpha(\\\\\\\\tau) + \\\\\\\\beta(\\\\\\\\tau)Y_{t+1} + \\\\\\\\varepsilon_{t+1}(\\\\\\\\tau)\\\\\\\\,\\\\\\\\,\\\\\\\\,\\\\\\\\,\\\\\\\\,(4)}$$</p>\\n\\n<p>where $Q$ is the quantile function.</p>\\n\\n<p>Is this wrong?</p>\\n',\n",
       " \"<p>The boxplot and histogram tell you all by themselves that your data are skewed, especially in group A.  The Shapiro-Wilk test is kind of pointless.  With data thusly skewed the ANOVA isn't really appropriate.  The Kruskal-Wallis rank sum test is based on the ranks, not the absolute values and doesn't require normality, either in the measures or residuals.  It is the more appropriate test.</p>\\n\\n<p>A quick Google search will tell you one requires normality and one does not.</p>\\n\\n<p>One thing you might consider is that durations are an arbitrary representation of time.  For example, you can indicate the duration of an event as 2s or you can say the event has a rate 0.5 events/s.  It's the exact same thing and both numbers can arbitrarily be interchanged for representation.  However, rates tend to be much less skewed and more appropriate for statistical analysis.  It's possible your rates are normally distributed and you can use ANOVA in that case.</p>\\n\\n<p>If you do decide to look at rates keep in mind that the direction of magnitude changes, a higher duration values = a lower rate value.  Some people use a negative rate just to avoid that confusion.</p>\\n\",\n",
       " \"<p>Suppose we have the following random process.  We start with two vectors $a_1=(0)$ and $b_1=(0)$.  In going from $i$ to $i+1$, we will a perturbation to $a_i$ and $b_i$.  With probability $p$, we perform Case 1, otherwise perform Case 2.</p>\\n\\n<ul>\\n<li><p>Case 1: We pick an element of $a_i$, say at coordinate $x$ (chosen uniformly at random from all coordinates $1,2,\\\\\\\\ldots,|a_i|$, where $|a_i|$ is the length of $a_i$), and append it to the end of $a_i$ to form $a_{i+1}$.  Similarly, to form $b_{i+1}$ append the element at coordinate $x$ from $b_i$ to the end of $b_i$.  The remainder of $a_{i+1}$ and $b_{i+1}$ is the same as $a_i$ and $b_i$, respectively.  [The copied element is at coordinate $x$ in both $a_i$ and $b_i$.]</p></li>\\n<li><p>Case 2: Choose $x$ and $y$ to be two coordinates chosen uniformly at random from $1,2,\\\\\\\\ldots,|a_i|$.  If $x \\\\\\\\neq y$ then set $a_{i+1}[x]=a_i[x]+1$ and $b_{i+1}[y]=b_i[y]+1$ (if $x=y$ then don't do anything [it's not clear at this point whether or not I want to enforce $x \\\\\\\\neq y$]).  Again, the remainder of $a_{i+1}$ and $b_{i+1}$ is the same as $a_i$ and $b_i$, respectively.</p></li>\\n</ul>\\n\\n<blockquote>\\n  <p><strong>Question</strong>: What is the expected dot product of $a_i$ and $b_i$?  That is, what is $\\\\\\\\mathrm{E}(\\\\\\\\sum_x a_i[x] \\\\\\\\cdot b_i[x])$?</p>\\n</blockquote>\\n\\n<p>So here's an example of what these vectors could look like:</p>\\n\\n<pre><code>i a_i b_i\\n1 (0) (0)\\n2 (0,0) (0,0) [case 1 x=1]\\n3 (0,0,0) (0,0,0) [case 1 x=1]\\n4 (1,0,0) (0,1,0) [case 2 x=1 y=2]\\n5 (1,0,0,1) (0,1,0,0) [case 1 x=1]\\n6 (1,0,0,1) (0,1,0,0) [case 2 x=4 y=4]\\n7 (1,0,0,2) (0,1,1,0) [case 2 x=4 y=3]\\n</code></pre>\\n\\n<p>I'm looking at a related problem studying evolving random graphs.  In the random graph problem, the two vectors represent the in-degrees and out-degrees of an evolving network over time.  In the graph, we can duplicate vertices or add or delete an edge (which is related to cases 1 and 2 above).  In this case, the dot product therefore counts the number of directed paths of length 3.</p>\\n\\n<p>However, the above problem differs from the one I'm considering since (a) it's not necessarily the case that $\\\\\\\\sum_x a_i[x]=\\\\\\\\sum_x b_i[x]$ and (b) there's nothing to stop parallel edges arising here.  Although, I'm hoping that this simplified question will be more answerable and the techniques can be re-used.</p>\\n\",\n",
       " \"<p>I have a data set consisting of reaction time (RT) measurements.  These will be used to calculate the duration of experimental trials in an MRI study.  In each block (experimental condition) there are 10 trials.  Now, due to various issues, there are some missing RTs.  Due to the nature of MRI analysis, I need values for all 10 trials per block.  Where more than 5 values are missing I will probably discard it from the analysis completely, but where only 1 or 2 are missing I plan on using the mean RT value for that condition in place of the missing value.  However, I want to make sure that this is a principled decision to make, as the RT values in some conditions, by the same participant, can be quite variable.</p>\\n\\n<p>How can I use the standard deviation or standard error of the mean to ensure that it is 'fair' to use the mean in place of the missing value?  For example, see the data below.</p>\\n\\n<p>Block 1 - Missing values: 2; Mean: 740; SD: 519; SEM: 196.\\nBlock 2 - Missing values: 1; Mean: 2245; SD: 292; SEM: 97.</p>\\n\\n<p>I'm trying to figure out an honest, consistent way of deciding whether the decision to replace the missing value with the mean is sound.  Where it is not, I would rather leave the block out of the analysis than skew the data.  </p>\\n\\n<p>Any advice?  I hope this makes sense. </p>\\n\",\n",
       " '<p>What do you mean by \"mean effect\"?  I think it\\'s important when doing an analyses like this to re-establish what question you are trying to answer by running a specific model.</p>\\n\\n<p>In the probit model, if I understand you correctly, you are trying to determine the impact of an independent variable on the propensity for an individual to have nonzero medical expenses.</p>\\n\\n<p>In the linear model, then, you are looking at the direct impact of an independent variable on personal medical expenses.  You might want to then throw out the zero medical expense observations before running the linear model.  That way, you can explain the coefficients in the linear model as:</p>\\n\\n<p>\"The impact of (explanatory variable X) on medical expenses, conditional on having nonzero medical expenses, is Z.\"</p>\\n\\n<p>By virtue of you using a probit model, it doesn\\'t appear you are interested in the \"mean impact\" on the entire population because you are implicitly dividing that population into two groups: nonzero and zero medical expense people.</p>\\n\\n<p>So, explanatory variable X first acts to \"push\" a person into (or out of) nonzero medical expense, then once they have been pushed into one of the two groups, acts on them separately in the linear model.</p>\\n',\n",
       " '<p>I have a series of boxplots that i am generating with ggplot2.  I want to control the order in which they are displayed.  Is there a way to control this order?  I have two genotypes and i want them displayed as WT then KO rather than the reverse (which is what i am getting as a default).  My code right now is:</p>\\n\\n<pre><code>p &lt;- qplot(genotype, activity.ratio, data = df) \\np + geom_boxplot() + geom_jitter()\\n</code></pre>\\n\\n<p>Also if this is better as a SO question than this forum then please let me know (and if it is the correct forum can someone create a ggplot tag).</p>\\n',\n",
       " '<p>I am having trouble generating a set of stationary colored time-series, given the covariance matrix (their PSDs and CSDs).  </p>\\n\\n<p>I know that, given two time-series $y_{I}(t)$ and $y_{J}(t)$, I can estimate their power spectral densities (PSDs)  and cross spectral densities (CSDs) using many widely available routines, such as <code>psd()</code> and <code>csd()</code> functions in Matlab, etc.  The PSDs and CSDs make up the covariance matrix:\\n$$                                                                             \\n\\\\\\\\mathbf{C}(f) = \\\\\\\\left(                                                                  \\n\\\\\\\\begin{array}{cc}                                                  \\nP_{II}(f) &amp; P_{IJ}(f)\\\\\\\\\\\\\\\\                                                                 \\nP_{JJ}(f) &amp; P_{JJ}(f)                                                                   \\n\\\\\\\\end{array}                                                                    \\n\\\\\\\\right)\\\\\\\\;,                                                                      $$\\nwhich is in general a function of frequency $f$.  </p>\\n\\n<p>What happens if I want to do the reverse?  <strong>Given the covariance matrix, how do I generate a realisation of $y_{I}(t)$ and $y_{J}(t)$?</strong> Please include any background theory, or point out any existing tools that do this (anything in Python would be great).</p>\\n\\n<h2>My attempt</h2>\\n\\n<p>Below is a description of what I have tried, and the problems I have noticed.  It is a bit of a long read, and sorry if it contains terms that have been misused. If what is erroneous can be pointed out, that would be very helpful.  But my question is the one in bold above.</p>\\n\\n<ol>\\n<li>The PSDs and CSDs can be written as the expectation value (or ensemble average) of the products of the Fourier transforms of the time-series.  So, the covariance matrix can be written as:\\n$$                                                                              \\\\\\\\mathbf{C}(f) = \\\\\\\\frac{2}{\\\\\\\\tau} \\\\\\\\langle \\\\\\\\mathbf{Y}^{\\\\\\\\dagger}(f)                          \\n\\\\\\\\mathbf{Y}(f) \\\\\\\\rangle \\\\\\\\;,                                                              \\n$$\\nwhere\\n$$                                                                              \\n\\\\\\\\mathbf{Y}(f) = \\\\\\\\left(                                                                  \\n\\\\\\\\begin{array}{cc}                                                               \\n\\\\\\\\tilde{y}_{I}(f) &amp; \\\\\\\\tilde{y}_{J}(f)                                                     \\n\\\\\\\\end{array}                                                                     \\n\\\\\\\\right) \\\\\\\\;.                                                                            \\n$$  </li>\\n<li>A covariance matrix is a Hermitian matrix, having real eigenvalues that are either zero or positive.  So, it can be decomposed into\\n$$                                                                              \\n\\\\\\\\mathbf{C}(f) = \\\\\\\\mathbf{X}(f) \\\\\\\\boldsymbol\\\\\\\\lambda^{\\\\\\\\frac{1}{2}}(f) \\\\\\\\:                    \\n\\\\\\\\mathbf{I} \\\\\\\\: \\\\\\\\boldsymbol\\\\\\\\lambda^{\\\\\\\\frac{1}{2}}(f)                                       \\n\\\\\\\\mathbf{X}^{\\\\\\\\dagger}(f) \\\\\\\\;,                                                             \\n$$\\nwhere $\\\\\\\\lambda^{\\\\\\\\frac{1}{2}}(f)$ is a diagonal matrix whose non-zero elements are the square-roots of $\\\\\\\\mathbf{C}(f)$\\'s eigenvalues; $\\\\\\\\mathbf{X}(f)$ is the matrix whose columns are the orthonormal eigenvectors of $\\\\\\\\mathbf{C}(f)$; $\\\\\\\\mathbf{I}$ is the identity matrix.  </li>\\n<li>The identity matrix is written as\\n$$                                                                              \\n\\\\\\\\mathbf{I} = \\\\\\\\langle \\\\\\\\mathbf{z}^{\\\\\\\\dagger}(f) \\\\\\\\mathbf{z}(f) \\\\\\\\rangle \\\\\\\\;,                  \\n$$\\nwhere\\n$$                                                                              \\n\\\\\\\\mathbf{z}(f) = \\\\\\\\left(                                                                  \\n\\\\\\\\begin{array}{cc}                                                               \\nz_{I}(f) &amp; z_{J}(f)                                                                     \\n\\\\\\\\end{array} \\\\\\\\right) \\\\\\\\;,                                                                \\n$$\\nand $\\\\\\\\{z_{i}(f)\\\\\\\\}_{i=I,J}$ are uncorrelated and complex frequency-series with zero mean and unit variance.  </li>\\n<li>By using 3. in 2., and then compare with 1.  The Fourier transforms of the time-series are:\\n$$\\n\\\\\\\\mathbf{Y}(f) = \\\\\\\\sqrt{ \\\\\\\\frac{\\\\\\\\tau}{2} } \\\\\\\\mathbf{z}(f) \\\\\\\\:\\n\\\\\\\\boldsymbol\\\\\\\\lambda^{\\\\\\\\frac{1}{2}}(f) \\\\\\\\: \\\\\\\\mathbf{X}^{\\\\\\\\dagger}(f) \\\\\\\\;.\\n$$</li>\\n<li>The time-series can then be obtained by using routines like the inverse fast Fourier transform.</li>\\n</ol>\\n\\n<p>I have written a routine in Python for doing this:</p>\\n\\n<pre><code>def get_noise_freq_domain_CovarMatrix( comatrix , df , inittime , parityN , seed=\\'none\\' , N_previous_draws=0 ) :\\n    \"\"\"                                                                                                          \\n    returns the noise time-series given their covariance matrix                                                  \\n    INPUT:                                                                                                       \\n    comatrix --- covariance matrix, Nts x Nts x Nf numpy array                                                   \\n      ( Nts = number of time-series. Nf number of positive and non-Nyquist frequencies )                     \\n    df --- frequency resolution\\n    inittime --- initial time of the noise time-series                                                           \\n    parityN --- is the length of the time-series \\'Odd\\' or \\'Even\\'                                                 \\n    seed --- seed for the random number generator                                                                \\n    N_previous_draws --- number of random number draws to discard first                                          \\n    OUPUT:                                                                                                       \\n    t --- time [s]                                                                                               \\n    n --- noise time-series, Nts x N numpy array                                                                 \\n    \"\"\"\\n    if len( comatrix.shape ) != 3 :\\n       raise InputError , \\'Input Covariance matrices must be a 3-D numpy array!\\'\\n    if comatrix.shape[0]  != comatrix.shape[1] :\\n        raise InputError , \\'Covariance matrix must be square at each frequency!\\'\\n\\n    Nts , Nf = comatrix.shape[0] , comatrix.shape[2]\\n\\n    if parityN == \\'Odd\\' :\\n        N = 2 * Nf + 1\\n    elif parityN == \\'Even\\' :\\n        N = 2 * ( Nf + 1 )\\n    else :\\n        raise InputError , \"parityN must be either \\'Odd\\' or \\'Even\\'!\"\\n    stime = 1 / ( N*df )\\n    t = inittime + stime * np.arange( N )\\n\\n    if seed == \\'none\\' :\\n        print \\'Not setting the seed for np.random.standard_normal()\\'\\n        pass\\n    elif seed == \\'random\\' :\\n        np.random.seed( None )\\n    else :\\n        np.random.seed( int( seed ) )\\n    print N_previous_draws\\n    np.random.standard_normal( N_previous_draws ) ;\\n\\n    zs = np.array( [ ( np.random.standard_normal((Nf,)) + 1j * np.random.standard_normal((Nf,)) ) / np.sqrt(2)\\n                 for i in range( Nts ) ] )\\n\\n    ntilde_p = np.zeros( ( Nts , Nf ) , dtype=complex )\\n    for k in range( Nf ) :\\n        C = comatrix[ :,:,k ]\\n        if not np.allclose( C , np.conj( np.transpose( C ) ) ) :\\n            print \"Covariance matrix NOT Hermitian! Unphysical.\"\\n        w , V = sp_linalg.eigh( C )\\n        for m in range( w.shape[0] ) :\\n            w[m] = np.real( w[m] )\\n            if np.abs(w[m]) / np.max(w) &lt; 1e-10 :\\n                w[m] = 0\\n            if w[m] &lt; 0 :\\n                print \\'Negative eigenvalue! Simulating unpysical signal...\\'\\n\\n        ntilde_p[ :,k ] =  np.conj( np.sqrt( N / (2*stime) ) * np.dot( V , np.dot( np.sqrt( np.diag( w ) ) , zs[ :,k ] ) ) )\\n\\n    zerofill = np.zeros( ( Nts , 1 ) )\\n    if N % 2 == 0 :\\n        ntilde = np.concatenate( ( zerofill , ntilde_p , zerofill , np.conj(np.fliplr(ntilde_p)) ) , axis = 1 )\\n    else :\\n        ntilde = np.concatenate( ( zerofill , ntilde_p , np.conj(np.fliplr(ntilde_p)) ) , axis = 1 )\\n    n = np.real( sp.ifft( ntilde , axis = 1 ) )\\n    return t , n\\n</code></pre>\\n\\n<p>I have applied this routine to PSDs and CSDs, the analytical expressions of which have been obtained from the modeling of some detector I\\'m working with. The important thing is that at all frequencies, they make up a covariance matrix (well at least they pass all those <code>if</code> statements in the routine).  The covariance matrix is 3x3. The 3 time-series have been generated about 9000 times, and the estimated PSDs and CSDs, averaged over all these realisations are plotted below with the analytical ones.  While the overall shapes agree, there are noticeable <em>noisy features</em> at certain frequencies in the CSDs (Fig.2).  After a close-up around the peaks in the PSDs (Fig.3), I noticed that the PSDs are actually <em>underestimated</em>, and that the noisy features in the CSDs occur at just about the same frequencies as the peaks in the PSDs.  I do not think that this is a coincidence, and that somehow power is leaking from the PSDs into the CSDs.  I would have expected the curves to lie on top of one another, with this many realisations of the data.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/nPisE.png\" alt=\"Figure 1: P11\"><br>\\n<img src=\"http://i.stack.imgur.com/axOf2.png\" alt=\"Figure 2: P12\">\\n<img src=\"http://i.stack.imgur.com/v48Zi.png\" alt=\"Figure 2: P11 (close-up)\"></p>\\n',\n",
       " '<p>The problem exists with more than just the first observation.  Take a pth order AR process. It is only the p+1st observation that can be expressed as the estimated function of the previous p values.  For the initial values you need a set of values estimated by the model as occurring before the first observation.  This is done by fitting the series and then predicting backwards.  Box and Jenkins call this backcasting.  So for the AR(p) process you need to generate p starting values prior to X1.  Then you can use these starting values and the model to fit the first p-1 observations.</p>\\n',\n",
       " '<p>Many randomised controlled trial (RCT) papers report significance tests on baseline parameters just after/before randomisation to show that the groups are indeed similar. This is often part of a \"baseline characteristics\" table. However, significance tests measure the probability of getting the observed (or a stronger) difference by chance, aren\\'t they? And if the test is significant we conclude that there is a <strong>true difference because a random difference of that extent would be unlikely</strong>. Does a significance test make any sense after randomisation when we know that any <strong>difference must be due to chance</strong>?</p>\\n',\n",
       " '<p>I am searching to get a detailed development for the multivariate normal likelihood function in order to enter it to <a href=\"http://en.wikipedia.org/wiki/Multivariate_normal_distribution#Likelihood_function\" rel=\"nofollow\">Wikipedia</a>.</p>\\n\\n<p>Can anyone suggest to me a good reference book (or if you have it handy, just copy paste the LaTeX of it?)</p>\\n\\n<p>I am failing to google-find a good, and detailed, reference.</p>\\n\\n<p>Thanks.</p>\\n',\n",
       " '<p>Could someone explain me which is the null hypothesis of the energy test of multivariate normality (R package <code>energy</code>)? By using this test I obtained a very different p-value from that I obtained with <code>mshapiro.test</code>.</p>\\n',\n",
       " '<p>I think the context is a MCMC algorithm, because this terminology is rather standard in such a context. The goal is to simulate a multivariate distribution, that is, the distribution of a random vector $(\\\\\\\\theta_1, \\\\\\\\ldots,\\\\\\\\theta_p)$. The full conditional distribution of $\\\\\\\\theta_1$ is then nothing but the conditional distribution of $\\\\\\\\theta_1$ given all other variables.</p>\\n',\n",
       " '<p>The question asks about a <a href=\"http://en.wikipedia.org/wiki/Hypergeometric_distribution\" rel=\"nofollow\">hypergeometric distribution</a>.  In R, the chance of obtaining <em>no</em> white balls in an urn with $w$ white balls and $b$ black balls after $k$ draws is</p>\\n\\n<pre><code>phyper(0, w, b, k)\\n</code></pre>\\n\\n<p>We need to find the smallest nonnegative integral $k$ for which this number is less than or equal to $\\\\\\\\alpha = 1-x$.  (<em>E.g.,</em> if $x$ is 95%, $\\\\\\\\alpha$ is $0.05$.)  Most standard root-finding methods will work, but (unless the urn has a huge number of balls), a brute-force search will be reasonable:</p>\\n\\n<pre><code>g &lt;- function(alpha,w,b) { # 0 &lt; alpha &lt; 1; 1 &lt;= w + b; 0 &lt;= w, b\\n    p &lt;- phyper(0,w,b,1:(w+b))\\n    k &lt;- min(which(p&lt;=alpha))\\n    list(count=k, probs=phyper(0,w,b,(k-1):k))\\n}\\n</code></pre>\\n\\n<p>This incarnation of the solution returns two things: the answer $k$ (as \"\\\\\\\\$count\") and a check of the answer (as \"\\\\\\\\$probs\") in the form of the hypergeometric probability for $k-1$ (which should be strictly greater than $\\\\\\\\alpha$) and the probability for $k$ (which should be less than or equal to $\\\\\\\\alpha$).</p>\\n\\n<p><strong>Example:</strong></p>\\n\\n<pre><code>&gt; set.seed(17)\\n&gt; table(replicate(10000, sum(sample(c(rep(1,3), rep(0,10)), 8))))\\n   0    1    2    3 \\n 355 2815 4844 1986 \\n</code></pre>\\n\\n<p>In this simulation with 10,000 independent trials, 8 balls were drawn without replacement from an urn with 3 white and 10 black balls.  In 355 (3.55%) of those trials no white balls were drawn.  What does our previous calculation suggest?</p>\\n\\n<pre><code>&gt; g(0.05, 3, 10)  \\n$count\\n[1] 8\\n\\n$probs\\n[1] 0.06993007 0.03496503\\n</code></pre>\\n\\n<p>It says that indeed, we need to withdraw 8 balls in order to have at least $x = 1-\\\\\\\\alpha = 95\\\\\\\\%$ chance of obtaining a white ball, and that the chance is 100 - 3.4965% of doing so (which is at least $x$; whereas the chance when only 7 balls are withdrawn is just 100 - 6.993%, which is less than $x$).  This is in close agreement with the simulated value.</p>\\n',\n",
       " '<p>Just try to remove the last value of the window and add the new one.</p>\\n\\n<p>If  </p>\\n\\n<p>$$MA(t)=\\\\\\\\frac{1}{w}\\\\\\\\sum\\\\\\\\limits_{i=t-w+1}^t{y_i}$$ </p>\\n\\n<p>then  </p>\\n\\n<p>$$MA(t+1)=MA(t)+\\\\\\\\frac{y(t+1)-y(t-w+1)}{w}.$$</p>\\n',\n",
       " '<p>I recently had a set of interval censored survival data, so I know exactly what you need. If you have ever used <code>R</code>, this should help.</p>\\n\\n<p>If you don\\'t want to assume a parametric form, how about an interval censored Cox proportional hazards model? The <code>intcox</code> package that would do this is no longer in the <code>R</code> repository. I would suggest imputing survival times and then using the <code>coxph</code> function from the <code>survival</code> library. Keep in mind that your standard errors will be too low using this method; you have not accounted for the uncertainty of not knowing the exact survival time. If you want interval censored survival estimates, use the <code>icfit</code> function from the <code>interval</code> package.</p>\\n\\n<p>Another way analyze the effect of covariates on survival time is by using interval censored, nonparametric regression. See the <code>R</code> package <code>ICE</code>: <a href=\"http://cran.r-project.org/web/packages/ICE/ICE.pdf\" rel=\"nofollow\">http://cran.r-project.org/web/packages/ICE/ICE.pdf</a>. You first need to impute the midpoints of survival time, then you do a local linear regression using the <code>locpoly</code> function from the <code>np</code> package. It\\'s not as hard as it sounds.</p>\\n',\n",
       " '<p>Disclaimer: I\\'m a recruiter and have been since 1982 so I understand your question very well. Let me break it down this way. Your resume is a screening out device. Companies get tons of resumes so they\\'re reading resumes with one question in mind, \"Why don\\'t I want to talk to this person?\" That reduces their pile to a few candidates who hold the best chances of meeting their needs. So if you\\'re getting interviews and your resume doesn\\'t show a PhD then there\\'s something else going on here. I say that because, just as a resume is a screening OUT device, the interview is a screening IN device. Once they\\'ve invited you to an interview then they\\'ve already concluded you have enough \"on paper\" to do the job. So when you\\'re walking in the interview the only quesion they\\'re really asking is \"why should I hire you?\" The person they hire will be the individual who addresses out they can best serve the company\\'s needs. </p>\\n\\n<p>My advice as a recruiter is to ask questions <strong>throughout</strong> the interview to identify their deeper needs. Believe me, the job description rarely resembles the truth so you\\'ll want to probe for their hot buttons then sell directly to those issues. Don\\'t allow the interview to feel like an interrogation, waiting for the end to ask questions. You\\'ll go down in flames and end up being told \"you don\\'t have a PhD\". Be respectful yet show your willingness to help them solve their problem.</p>\\n\\n<p>My favorite question is: \"What are the <em>traits</em> of the best person you\\'ve ever known in this role?\" Everyone has a dream team in mind so it\\'s important to figure out what <em>traits</em> they feel are necessary to succeed in this role. Keep in mind, this isn\\'t a question about experience, backgrounds or degrees. See, I can always find a mediocre PhD with tons of experience so this isn\\'t the holy grail. It\\'s just what companies continue to think is best because IMO they don\\'t know how else to write a job description that captures the essence of the person they need.</p>\\n',\n",
       " '<p>SPSS removes cases <a href=\"http://en.wikipedia.org/wiki/Listwise_deletion\" rel=\"nofollow\">list-wise</a> by default, and in my experience this is the case for the majority of statistical procedures. So if a case is missing data for any of the variables in the analysis it will be dropped entirely from the model. For generating correlation matrices or linear regression you <em>can</em> exclude cases pair-wise if you want (I\\'m not sure if that is ever really advised), but for logistic and generalized linear model regression procedures this isn\\'t an option. Hence you may want to look at techniques for imputing missing data.</p>\\n\\n<p>Below are some resources I came up quickly for missing data analysis in SPSS;</p>\\n\\n<ul>\\n<li>User <a href=\"http://stats.stackexchange.com/users/3277/ttnphns\">ttnphns</a> has a macro for hot-deck imputation on his <a href=\"http://rivita.ru/spssmacros_en.shtml\" rel=\"nofollow\">web site</a>. I also see <a href=\"http://www.afhayes.com/spss-sas-and-mplus-macros-and-code.html\" rel=\"nofollow\">Andrew Hayes</a> has a macro for hot-deck imputation.</li>\\n<li>Raynald Levesque\\'s site has a set of example syntax implementations of various <a href=\"http://spsstools.net/SampleSyntax.htm#WorkingWithMissingValues\" rel=\"nofollow\">missing values procedures</a>. Including another implementation of hot-deck imputation!</li>\\n<li>SPSS has various tools in-built for imputing missing values. See the commands <code>MVA</code>, <code>RMV</code>, and <code>MULTIPLE IMPUTATION</code>. See the <a href=\"http://publib.boulder.ibm.com/infocenter/spssstat/v20r0m0/index.jsp?topic=%2Fcom.ibm.spss.statistics.help%2Fidh_miss.htm\" rel=\"nofollow\">Missing Values Analysis</a> section in the HELP documentation.</li>\\n</ul>\\n\\n<p>I\\'m not quite sure what is available in base and what are available as add-ons. I believe the <code>MULTIPLE IMPUTATION</code> <strike>command is an add-on, but the others are part of the base package.</strike> and the <code>MVA</code> commands are add-ons, but the <code>RMV</code> procedure is part of the base package. </p>\\n\\n<p>For more general questions about missing data analysis, peruse the tag <a href=\"/questions/tagged/missing-data\" class=\"post-tag\" title=\"show questions tagged \\'missing-data\\'\" rel=\"tag\">missing-data</a>.</p>\\n',\n",
       " '<p>I am using the Mahalanobis distance to classify an unknown 64-dimensional vector into one of 75 classes. There are n samples of 64-dimensional vectors for each class, arranged into an Nx64 matrix format. Reading this into Matlab with no problem.</p>\\n\\n<p>However when the 64x64 covariance matrix is calculated using the cov() function it is not being positive-definite or non-singular. Hence the inverse of the covariance matrix required to calculate the Mahal. dist. is undefined.</p>\\n\\n<p>Now the data that is used to get the feature vector is basically co-ordinates of points taken from handwriting samples. In such a case it seems that the distribution is naturally giving rise to a singular covariance matrix. Is there any way to apply the Mahalanobis distance, or some way of modifying the vectors to yeild a non-singular covar. matrix?</p>\\n',\n",
       " '<p>In R I would like to solve a system of linear equations with constraints to preserve monotonicity.  I can do this easily with no constraints on the coefficients.  Here is an example:</p>\\n\\n<pre><code>       [,1]      [,2]        [,3]      [,4]       [,5]      [,6]      [,7]\\n[1,] 0.6945405 0.1157702 0.004973632 0.0000000 0.00000000 0.0000000 0.1625076\\n[2,] 0.9212828 0.3055870 0.026655560 0.0000000 0.00000000 0.0000000 0.3916894\\n[3,] 1.0000000 0.9987081 0.835572186 0.1767705 0.00000000 0.0000000 6.6993305\\n[4,] 1.0000000 1.0000000 0.992828243 0.5758778 0.02530867 0.0000000 7.6371723\\n[5,] 1.0000000 1.0000000 0.997171672 0.6412910 0.04668548 0.0000000 7.6628770\\n[6,] 1.0000000 1.0000000 1.000000000 0.9970624 0.90614523 0.4305434 7.6796152\\n</code></pre>\\n\\n<p>Columns 1-6 are the coefficients and column 7 is the right side of the linear system.  This can easily be solved by:</p>\\n\\n<pre><code>solve(mat[,-ncol(mat)],mat[,ncol(mat)])\\n</code></pre>\\n\\n<p>However if I want to put constraints where all coefficients must be greater than 0 is there an easy way to do this in R?  If the solve function returns a negative coefficient does this indicate no solution exists where all the coefficients are positive?</p>\\n',\n",
       " '<p>If you are searching for a alternative GUI for WEKA, you could try <a href=\"http://www.knime.org/\" rel=\"nofollow\" title=\"KNIME\">KNIME</a> too. It is a bit more stable and offers some extra features. Personally I think KNIME is more intuitive to use than the standard WEKA GUI.</p>\\n',\n",
       " \"<p>The ROC function (it is not necessarily a curve) allows you to assess the discrimination ability provided by a a specific statistical model (comprised of a predictor variable or a set of them). </p>\\n\\n<p>A main consideration of ROCs is that model predictions do not only stem from the model's ability to discriminate/make predictions based on the evidence provided by predictor variables. Also operating is a response criteria that defines how much evidence is necessary for the model to predict a response, and what is the outcome of these responses. The value that is established for the response criteria will greatly influence the model predictions, and ultimately the type of mistakes that it will make.</p>\\n\\n<p>Consider a generic model with predictor variables and a response criteria. This model is trying to predict the Presence of X,by responding Yes or No.\\nSo you have the following confusion matrix:</p>\\n\\n<pre><code>                                **X present               X absent**\\n **Model Predicts X Present**       Hit                   False Alarm\\n\\n **Model Predicts X Absent**      Miss                 Correct Rejection\\n</code></pre>\\n\\n<p>In this matrix, you only need to consider the proportion of Hits and the False Alarms (because the others can be derived from these, given that they have to some to 1). For each response criteria, you wil ave a different confusion matrix. The errors (Misses and False Alarms) are negatively related, which means that a response criteria that minimizes false alarms maximizes misses and vice-versa. The message is: there is no free lunch. </p>\\n\\n<p>So, in order to understand how well the model discriminates cases/makes predictions, independently of the response criteria established, you plot the Hits and False rates produced across the range of possible response criteria.</p>\\n\\n<p>What you get from this plot is the ROC function. The area under the function provides an unbiased, and non-parametric measure of the discrimination ability of the model. This measure is very important because it is free of any confounds that could have been produced by the response criteria.</p>\\n\\n<p>A second important aspect, is that by analyzing the function, one can define what response criteria is better for your objectives. What types of errors you want to avoid, and what are errors are OK. For instance, consider an HIV test: it is a test that looks up some sort of evidence (in this case antibodies) and  makes a discrimination/prediction based on the comparison of the evidence with response criterion. This response criterion is usually set very low, so that you minimize Misses. Of course this will result in more False Alarms, which have a cost, but a cost that is negligible when compared to the Misses.</p>\\n\\n<p>With ROCs you can assess some model's discrimination ability, independently of the response criteria, and also establish the optimal response criteria, given the needs and constraints of whatever that you are measuring.\\nTests like hi-square cannot help at all in this because even if your testing if the predictions are at chance level, many different Hit-False Alarm pairs are consistent with chance level.</p>\\n\\n<p>Some frameworks, like signal detection theory, assume a priori that the evidence available for discrimination has specific distribuiton (e.g., normal distribution, or gamma distribution). When these assumptions hold (or are pretty close), some really nice measures are available that make your life easier.</p>\\n\\n<p>hope this helps to elucidate you on the advantages of ROCs </p>\\n\",\n",
       " '<p>a sample representative of population cannot be obtained through internet as you will only get people interested in answering your survey online, which will give you a biased sample.</p>\\n',\n",
       " '<p>I am fitting a simple linear regression model with R &amp; JAGS:</p>\\n\\n<pre><code>model {\\n    for (i in 1:length(ri)){\\n        ri[i] ~ dnorm(mu[i],tau)\\n        mu[i] &lt;- alpha + b.vr*vr[i] + b.ir*ir[i]\\n    }\\n\\n    #posterior predictive distribution\\n    for (j in 1:length(pvr)){\\n        pmu[j] &lt;- alpha + b.vr*pvr[j] + b.ir*pir[j]\\n        pri[j] ~ dnorm(pmu[j],tau)\\n     }  \\n\\n    #priors for regression\\n    alpha ~ dnorm(0,0.01)\\n    b.vr ~ dunif(-10,0)\\n    b.ir ~ dunif(0,10)\\n\\n    #prior for precision\\n    tau ~ dgamma(0.001,0.001)\\n    sigma &lt;- 1/sqrt(tau)\\n}\\n</code></pre>\\n\\n<p>I am trying to calculate the posterior predictive distribution with new data (<code>pvr</code> and <code>pir</code> in the model). But somehow the reported results (<code>pri</code>) do not make sense (the means of <code>pri</code> are smaller than expected). </p>\\n\\n<p>Could someone explain me, is something wrong with the model specification?</p>\\n',\n",
       " '<p>What kind of function is:</p>\\n\\n<p>$f_X(x) = 2 \\\\\\\\lambda \\\\\\\\pi x e^{-\\\\\\\\lambda \\\\\\\\pi x ^2}$</p>\\n\\n<p>Is this a common distribution? I am trying to find a confidence interval of $\\\\\\\\lambda$ using the estimator $\\\\\\\\hat{\\\\\\\\lambda}=\\\\\\\\frac{n}{\\\\\\\\pi \\\\\\\\sum^n_{i=1} X^2_i}$ and I am struggling to prove if this estimator has Asymptotic Normality.</p>\\n\\n<p>Thanks</p>\\n',\n",
       " \"<p>This question might be very basic, but somehow I don't understand this point.</p>\\n\\n<p>Suppose initially I used a univariate regression equation  such as</p>\\n\\n<pre><code>GDP=a+b*Income \\n</code></pre>\\n\\n<p>I'll get some coefficient values (say 0.5). Now, I'm using the same structure of the regression model, but added another independent variable. So, the new equation is</p>\\n\\n<pre><code>GDP=a+b*Income+c*Investment\\n</code></pre>\\n\\n<p>Then the new coefficients value will be b=0.3 &amp; c=0.4.</p>\\n\\n<p>My question is why coefficient's value changes when we add another independent variable?</p>\\n\\n<p>Hope I can put my question clearly. </p>\\n\\n<p>Thanks in advance.</p>\\n\",\n",
       " '<p>I am creating a program to collect and analyze prices for goods and alert the user when the price decreases so that they can buy it on \"sale.\" I know a little statistics (e.g. standard deviation) but am looking for the best way to determine if a price drop is significant compared to historical prices - i.e. if I should bother alerting the user to the price decrease.</p>\\n\\n<p>Let\\'s say, for example, that my program is collecting prices for books. It searches the web, say, on a daily or weekly basis for prices for the book (both used and new), and stores this data in a database. I can make a graph of the prices over time (both lowest price, mean price, etc.) and visually determine when the price drops a significant amount - this is easy for a person to do. But what is the best statistical algorithm to determine this?</p>\\n\\n<p>I can think of a few cases in which a person would buy a product when the price drops:</p>\\n\\n<ol>\\n<li>If the price drops below a certain static barrier. For example, if the customer determines that they would buy the product at &lt; 15, then whenever the price dips below that amount they should be alerted to that. This scenario is based on the idea that the customer believes that the price will actually eventually get that low, at some point in the future.</li>\\n<li>If the price drops a significant amount below the current price. This is more realistic in some scenarios. For example, if there is a rare and relatively expensive book (for example, it was printed by an academic publisher with only a few thousand copies, all in hardcover, which sold for 150 when new and there are only a few copies on the market) - the book may be available (used) at the current time for an average price of 85. The price may never drop to 15, because the supply is so low. However, it may drop to a significantly lower price (40 or 50), but the customer doesn\\'t have a specific price where they will buy the book.</li>\\n<li>If the price of the book drops <em>very low</em>. A book may go today for 20, but the customer may want to wait and see if a bookseller is going to put it on sale for 5-8, because they don\\'t need this book right now - but it\\'s on their \"wish list.\"</li>\\n</ol>\\n\\n<p>I am interested in this second and third scenario. Essentially I want to be able to determine when an item is available for a price that is \"notable.\" I will store all of the historic prices in a database, and can compare this week\\'s prices with historic prices.</p>\\n\\n<p>What are your suggestions for statistical analytic tools which can be used to study this type of data and make automated recommendations?</p>\\n',\n",
       " '<p>The covariance matrix is not positive definite because it is singular.  That means that at least one of your variables can be expressed as a linear combination of the others.  You do not need all the variables as the value of at least one can be determined from a subset of the others.  I would suggest adding variables sequentially and checking the covariance matrix at each step.  If a new variable creates a singularity drop it and go on the the next one.  Eventually you should have a subset of variables with a postive definite covariance matrix.</p>\\n',\n",
       " '<p>The function <code>predictSurvProb</code> in the <code>pec</code> package can give you absolute risk estimates for new data based on an existing cox model if you use R.</p>\\n\\n<p>The mathematical details I cannot explain.</p>\\n\\n<p>EDIT: The function provides survival probabilities, which I have so far taken as 1-(Event probability).</p>\\n',\n",
       " \"<p>Doing everything in one regression is neat, and the assumption of independence is important. But calculating the point estimates in this way does <em>not</em> require constant variance. Try this R code;</p>\\n\\n<pre><code>x &lt;- rbinom(100, 1, 0.5)\\nz &lt;- rnorm(100)\\ny &lt;- rnorm(100)\\ncoef(lm(y~x*z))\\ncoef(lm(y~z, subset= x==1))[1] - coef(lm(y~z, subset= x==0))[1]\\ncoef(lm(y~z, subset= x==1))[2] - coef(lm(y~z, subset= x==0))[2]\\n</code></pre>\\n\\n<p>We get the same point estimate either way. Estimates of standard error may require constant variance (depending on which one you use) but the bootstrapping considered here doesn't use estimated standard errors.</p>\\n\",\n",
       " '<p>The $r$-th moment of a random variable $X$ is <strong>finite</strong> if\\n$$\\\\\\\\n\\\\\\\\mathbb E(|X^r|)&lt; \\\\\\\\infty\\\\\\\\n$$</p>\\n\\n<p>I am trying to show that for any positive integer $s&lt;r$, then the\\n$s$-th moment $\\\\\\\\mathbb E[|X^s|]$ is also finite.</p>\\n',\n",
       " \"<p>The question suggests a naive view of a statistician-—that it's all about checking to see if a p &lt; 0.05 and reporting some numbers and standard graphs.  If that's what you mean by statistician then you are correct in your implication that much of it could be entirely automated.  But that's not what statistician means.</p>\\n\\n<p>Define your term statistician though, and you might get better answers.</p>\\n\",\n",
       " '<p>Are they more efficient implementation -- is the difference important from practical point of view, there is R package which implements them. Is it new algorithm which overcomes \"generic\" implementation (RandomForest package from R) not only in terms of efficiency or also in some other areas? </p>\\n\\n<p>Extreme Random Forest <a href=\"http://link.springer.com/article/10.1007%2Fs10994-006-6226-1\" rel=\"nofollow\">http://link.springer.com/article/10.1007%2Fs10994-006-6226-1</a></p>\\n',\n",
       " '<p><a href=\"http://www.nytimes.com/2009/01/07/technology/business-computing/07program.html?_r=1&amp;adxnnl=1&amp;adxnnlx=1344344699-cRLYjykb1H9zJBnxmIvhlg&amp;pagewanted=all\">I think this quote</a> from Anne H. Milley sums up the way a lot of people feel about R:</p>\\n\\n<blockquote>\\n  <p>We have customers who build engines for aircraft. I am happy they are\\n  not using freeware when I get on a jet.</p>\\n</blockquote>\\n\\n<p>Unfortunately, I think this misconception (free==inferior) is common in the general public.</p>\\n',\n",
       " '<p>There is a good reason for still having both, which is that a good craftsman will want to select the best tool for the task at hand, and both Bayesian and frequentist methods have applications where they are the best tool for the job.</p>\\n\\n<p>However, often the wrong tool for the job is used because frequentist statistics are more amenable to a \"statistics cookbook\" approach which makes them easier to apply in science and engineering than their Bayesian counterparts, even though the Bayesian methods provide a more direct answer to the question posed (which is generally what we can infer from the particular sample of data we actually have).  I am not greatly in favour of this as the \"cookbook\" approach leads to using statistics without a solid understanding of what you are actually doing, which is why things like the p-value fallacy crop up again and again.</p>\\n\\n<p>However, as time progresses, the software tools for the Bayesian approach will improve and they will be used more frequently as jbowman rightly says.</p>\\n\\n<p>I am a Bayesian by inclination (it seems to make a lot more sense to me than the frequentist approach), however I end up using frequentist statistics in my papers, partly because I will have trouble with the reviewers if I use Bayesian statistics as they will be \"non-standard\".</p>\\n\\n<p>Finally (somewhat tongue in cheek ;o), to quote Max Plank <em>\"A new scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die, and a new generation grows up that is familiar with it.\"</em></p>\\n',\n",
       " '<p>Assuming you are speaking of second order moments (covariance), and assuming the rotation is about the mean (or equivalently, that the variable has zero mean - this does not loose generality), the rotation can be expressed as a matrix multiplication, so ${\\\\\\\\bf y} = Q {\\\\\\\\bf x}$ and then</p>\\n\\n<p>$\\\\\\\\Sigma_y = E[{\\\\\\\\bf y}{\\\\\\\\bf y^T}]=Q E[{\\\\\\\\bf x}{\\\\\\\\bf x^T}] Q^t= Q \\\\\\\\, \\\\\\\\Sigma_x Q^t $</p>\\n',\n",
       " '<p>1) The authors of the article have written that both $m_i$ and $n_i$ are distributed Poisson, as it\\'s a comparison of simulated to actual data; hence the difference between the two has variance $m_i + n_i$ instead of the usual case, where $m_i$ is the variance (assuming correctness of the model). Consequently, $m_i+n_i$ is the appropriate divisor. </p>\\n\\n<p>2) The test is indeed nonparametric.  No Gaussianity assumed!</p>\\n\\n<p>Edit: Michael Chernick makes a valuable point in comments, namely that the approximation will be poor if the cells are sparse.  The authors do mention the importance of bin size selection, but it\\'s certainly not as though you can take this statistic, apply it without care, and expect to get good results.</p>\\n\\n<p>Edit the Second:  Michael Chernick makes another valuable point in comments, which is that there are better ways of validating their model against the star data.  Even if they take the \"binning\" approach, they\\'d be better off using model-calculated expected values for bin occupancy counts and doing a more typical $\\\\\\\\chi^2$ test than comparing simulated data with actual data; the use of simulated data vs. expected values just adds randomness to the results and thereby reduces the power of the test.  It may be, however, that w/o access to the software tool\\'s code, they can\\'t actually do the needed calculations to get bin occupancy rates, in which case this may be about as well as they can do (I could be wrong about that, though.)  </p>\\n',\n",
       " \"<p>I have one data sample of non-negative random variable $X$ with unknown distribution and predefined expected value $y$. Is there any test able to check null hypothesis $\\\\\\\\mathbb{E}[X]\\\\\\\\geq y$ or $\\\\\\\\mathbb{E}[X]\\\\\\\\leq y$?</p>\\n\\n<p>Actual data samples are gathered in realtime. More specifically, it's an intervals between HTTP-requests coming to web-server from one client. Pearson's test shown that it is not normally distributed variable.</p>\\n\",\n",
       " '<p>I don\\'t think I know more than you do, but the links you posted do suggest answers. I\\'ll take <a href=\"http://www.cs.cornell.edu/~tomf/publications/supervised_kmeans-08.pdf\" rel=\"nofollow\">http://www.cs.cornell.edu/~tomf/publications/supervised_kmeans-08.pdf</a> as an example. Basically they state: 1) clustering depends on a distance. 2) successful use of k-means requires a carefully chosen distance. 3) <em>Given training data in the form of sets of items with their desired partitioning, we provide a structural SVM method that learns a distance measure so that k-means produces the desired clusterings.</em> In this case there is a supervised stage to the clustering, with both training data and learning. The purpose of this stage is to learn a distance function so that applying k-means clustering with this distance will be hopefully optimal, depending on how well the training data resembles the application domain. All the usual caveats appropriate to machine learning and clustering still apply.</p>\\n\\n<p>Further quoting from the article: <em>Supervised clustering is the task of automatically adapting a clustering algorithm with the aid of a training set consisting of item sets and complete partitionings of these item sets.</em>. That seems a reasonable definition.</p>\\n',\n",
       " \"<p>I'm not a mathematician and I need to prove what this theorem says. I think is easy and I know how it works, but definitively I'm not too much rigorous to make a demonstration. Can anybody help me?</p>\\n\\n<p>I enounce it:</p>\\n\\n<p>If the test statistic has a continuous distribution, then under H0 : θ = θ0, the p-value has a Uniform[0, 1] distribution. Therefore, if we reject H0 when the p-value is less than α, the probability of a type I error is α.</p>\\n\\n<ul>\\n<li><p>Therefore, when H0 is true, the p-value is like a random draw from a\\nUniform[0, 1].</p></li>\\n<li><p>On the other hand, if H0 is not true, the distribution of the p-value will\\ntend to concentrate closer to 0.</p></li>\\n<li><p>A large p-value can occur for two reasons:</p>\\n\\n<ul>\\n<li>H0 is true, or</li>\\n<li>H0 is false, but the test has low power</li>\\n</ul></li>\\n<li><p>Do not confuse the p-value with P(H0 | data). The p-value is not the\\nprobability that the null hypothesis is true.</p></li>\\n</ul>\\n\\n<p>Thank you so much!</p>\\n\",\n",
       " '<p>I have used an established 33-item empathy questionnaire. \\nI have modified the scale from a 9 point to a 4 point scale in order to force an answer and  reduce ambiguity. </p>\\n\\n<p><strong>Do I need to retest the scale properties after this reduction in the number of response options?</strong></p>\\n',\n",
       " '<p>The paper \"A heteroscedastic structural errors-in-variables model with equation error\" can be downloaded at the author\\'s page:</p>\\n\\n<p><a href=\"http://www.ime.usp.br/~patriota/curriculo-eng.html#Published_papers\" rel=\"nofollow\">http://www.ime.usp.br/~patriota/curriculo-eng.html#Published_papers</a></p>\\n\\n<p>basically you must take into account the variability of both variables to avoid inconsistent estimators, non-reliable hypothesis tests and confidence intervals.</p>\\n',\n",
       " '<p>I am studying the growth (in volume) of tumors in mice when they are given 1) saline solution (control), 2) treatment A, 3) treatment B, and 4) treatment A and B. The treatments are designed to inhibit the growth of the tumors. </p>\\n\\n<p>I am hoping to estimate whether there is a \"synergistic\" relationship between A and B. By synergy, I mean that the combination treatment of A and B leads to what we would call, above some uncertainty threshold, a greater than additive effect. My data will be in time series format, as the measurements will be made longitudinally over many days, with replicates in each group. </p>\\n\\n<p>Here\\'s a quick literature review. Many studies cite and use <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/16968952\" rel=\"nofollow\">Chou\\'s method</a>, but that will not work for us because it requires a separate dose-response curve for each drug, which we will not have. </p>\\n\\n<p>I also found one study that uses a regression-based model (<a href=\"http://www.ncbi.nlm.nih.gov/pubmed/15523707\" rel=\"nofollow\">Tan et al</a>), which employs the model </p>\\n\\n<p>$ y_i = \\\\\\\\alpha + y_{i0}1_{12}\\\\\\\\beta_0 + x^1_i\\\\\\\\beta_1 + x_i^2\\\\\\\\beta_2 + + x_i^3\\\\\\\\beta_3 + \\\\\\\\epsilon_i $</p>\\n\\n<p>where $y_i$ is the log tumor size, the $x_i$\\'s refer to the quantity of the treatments used, and the $\\\\\\\\beta$\\'s refer to the parameters. They then do expectation-maximization to get MLE\\'s of the parameters. In this setting, $\\\\\\\\beta_3$ is the crucial interaction parameter which we want to estimate as either significantly less than zero or not. </p>\\n\\n<p>However, the authors do not mention access to their implementation of the method and it seems difficult to re-implement. Moreover, our treatments will be binary rather than two-leveled as in their data set. </p>\\n\\n<p>I also found a study that uses an ANOVA (<a href=\"http://www.ncbi.nlm.nih.gov/pubmed/22025735\" rel=\"nofollow\">Soundararajan et al</a>) and presumably post-hoc tests, but I didn\\'t exactly understand their methods. </p>\\n\\n<p>I am not looking to reinvent the wheel and I would prefer to use existing R packages and/or software. If the latter, I\\'d prefer open-source, to boost reproducibility. Any suggestions?</p>\\n',\n",
       " '<p>I am trying to plot a two-component mixture distribution $F_X(x):.7N(0,1)+.3N(3,1)$ with density</p>\\n\\n<p>$$f_X(x) = {0.7 \\\\\\\\over {\\\\\\\\sqrt{2\\\\\\\\pi}}} e^{−x^2/2} + {0.3 \\\\\\\\over {\\\\\\\\sqrt{2\\\\\\\\pi}}}e^{−(x−3)^2/2} $$.</p>\\n\\n<p>I have done the histogram for it but having trouble with plotting the true density,so far this is my code</p>\\n\\n<pre><code>x &lt;- rep(na,2000)\\nmu1 &lt;- 0\\nsd1 &lt;- 1\\nmu2 &lt;- 3\\nsd2 &lt;- 1\\n\\nfor(i in 1:n){\\n    u &lt;- runif(1,0,1)\\n    if (u &lt; .7){\\n        x[i] &lt;- rnorm( 1, mu1, sd1)\\n    }else\\n        x[i] &lt;- rnorm( 1, mu2, sd2)\\n    }\\n\\nhist(x,prob=T)\\n</code></pre>\\n',\n",
       " \"<p>I have data coming from a sample that, for various reasons, don't follow the original sampling plan. Trying to calibrate this sample seems to be very difficult because deviations are hard in some cases.  </p>\\n\\n<p>At first the design aimed to include a number of small regions but not all of them have participated in the study. If we consider bigger areas we have data from all of them but the distribution in the sample is very different from the population (even for post-calibration techniques). </p>\\n\\n<p>As we have a large amount of data, would it be a good solution to draw a random subsample from this dataset following the probabilities observed in the population by different characteristics?</p>\\n\",\n",
       " '<ul>\\n<li>I guess if it is an Engineering audience, then the freely available online <a href=\"http://www.itl.nist.gov/div898/handbook/\" rel=\"nofollow\">Engineering Statistics Handbook</a> might be good. But I admit it\\'s more content than you asked for.</li>\\n<li>There\\'s also this <a href=\"http://matthias.vallentin.net/probability-and-statistics-cookbook/\" rel=\"nofollow\">probability and statistics cheat sheet</a></li>\\n</ul>\\n',\n",
       " '<p>Suppose I run a community center and I want to find the average length of time that someone is a rec center member. It is straightforward to do this with people who have already cancelled their accounts, but I\\'m not sure how to handle the data of people who still have accounts.</p>\\n\\n<p>For example, suppose the average duration of people who have cancelled is 500 days. But when I take the average of <em>all</em> account durations (assuming today is the \"end date\" for current members) that number become to 1000. The obvious problem with this is it is possible for a current member to cancel tomorrow or they may cancel in 10 years. Furthermore, someone who joined yesterday will have a duration of 1 day, which is not accurate. </p>\\n\\n<p>My hunch is that this involves predicting when current members will cancel, but that seems to add several layers of complexity to this (seemingly) simple question.</p>\\n\\n<p>I apologize in advance if this has been asked. I did try searching, but it\\'s possible I don\\'t know the correct terminology for this concept.</p>\\n',\n",
       " '<p>So I\\'m looking into methods in selecting the best number of hidden states for a hidden markov model, given I don\\'t know what how many states \"generated\" my data. One method I\\'ve seen a lot is to learn lots of models with different numbers of states and then perform BIC on them. Another method is too place a dirichlet process (which I know nothing about) over the number of states and learn the posterior of the states. </p>\\n\\n<p>So, why is obtaining the posterior of states worth the expense and time compared to using the BIC? Do I gain more information this way?</p>\\n\\n<p>Just trying to figure out if its worth, trying to build a HMM with a DP in my work</p>\\n',\n",
       " '<p>Certainly, if $p \\\\\\\\leq n$ and you run LARS until you\\'ve included all $p$ variables in the model and the correlations are zero, then the solution will be exactly the OLS solution.</p>\\n\\n<p>You can view LARS as just another \"regularized\" least-squares estimate. Of course, it has a very close connection to both forward-stagewise regression and the lasso. My suspicion is that most people use the LARS algorithm primarily to compute the lasso solutions and that LARS itself has gotten fairly little use as an estimation method in its own right.</p>\\n\\n<p>Cross-validation to choose $k$ and $t$ should be feasible if your interest is on prediction. Since the maximal correlation decreases monotonically, then $k$ is completely determined by $t$ in LARS (see, e.g., the right pane of Fig. 3 of the paper). Hence, you only need optimize over the threshold $t$ in the cross-validation. This is cleaner in a couple of ways: (1) There is only one parameter to optimize over and (2) any sensible objective function you choose should be pretty much continuous in $t$, whereas $k$ is discrete. Discrete-valued parameters can often be less pleasant to deal with in CV.</p>\\n\\n<p>For more details regarding some of the properties of least angle regression, you can also see the solution to <strong><a href=\"http://stats.stackexchange.com/questions/6795/least-angle-regression-keeps-the-correlations-monotonically-decreasing-and-tied\">this question</a></strong>.</p>\\n',\n",
       " '<p>Given a variable $X \\\\\\\\sim N(\\\\\\\\mu, \\\\\\\\Sigma)$ </p>\\n\\n<p>What is the density of $X$ given $HX = d$,<br>\\n$H$ and $d$ both constant</p>\\n\\n<p>I.e. we observe $X$ with rank-deficient linear operator $H$ and obtain some value $d$. Given this knowledge how do we update our knowledge/distribution of $X$?</p>\\n\\n<p>This is just one of the steps in the <a href=\"http://en.wikipedia.org/wiki/Kalman_filter#The_Kalman_filter\" rel=\"nofollow\">Kalman filter</a>. I\\'m having trouble separating it out.</p>\\n\\n<p>Ideally an answer would be analytic forms for both $\\\\\\\\mu$ and $\\\\\\\\Sigma$ after the update plus a rationale/derivation or link to such a derivation. </p>\\n',\n",
       " '<p>I found a harder but possible way to export editable charts.\\nYou have to choose graphs only and to set to .eps format. It would be good to create\\nnew folder and to place all charts in that folder.\\nThen, you select all exported files (charts) and drag them in word.\\nCharts will bee editable.</p>\\n',\n",
       " '<p>I think @ocram is on the right track here.  His first \"possible structure\" is also called <em>compound symmetry</em> (see here: <a href=\"http://stats.stackexchange.com/questions/15102/\">What is compound symmetry in plain English?</a> for more information).  This is the correlational structure that goes with what\\'s called a <em>repeated measures ANOVA</em>, that is, a mixed effects model with a random intercept only (no random effects / slopes for any other variable). However, it appears that the correlations between the sets differ from each other, so this covariance structure is not appropriate.  It is possible to determine power for situations like this, but it isn\\'t straightforward.  It appears that G*Power doesn\\'t have routines specified that can handle your situation.  The only way I know of to deal with these more complicated situations is to simulate.  However, the answer you get from G*Power may be good enough for your purposes.</p>\\n',\n",
       " '<p>Dear crossvalidated community!</p>\\n\\n<p>I am analyzing a data of a study in which every participant\\'s performance in a memory task was measured twice (once under placebo, once under drug treatment in random order).  Moreover every subject was tested in a blood test measuring the level of stable blood marker. I would like to analyze the data with respect to drug-effects and blood marker effects using lme() from the nlme() package. </p>\\n\\n<p>\\'treatment\\' codes placebo/drug\\n\\'session\\' codes the first or second testing in order to account for learning/re-test effects\\n\\'blood\\' codes for the blood marker level (continous variable), which should be entered as a covariate in the analysis\\n\\'id\\' codes for the subjects</p>\\n\\n<p>Would the following equation correct for the described data:</p>\\n\\n<pre><code># some data\\ndata &lt;- structure(list(id = structure(c(1L, 12L, 14L, 15L, 16L, 17L, \\n18L, 19L, 20L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 13L, \\n1L, 12L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 2L, 3L, 4L, 5L, 6L, \\n7L, 8L, 9L, 10L, 11L, 13L), .Label = c(\"1\", \"10\", \"11\", \"12\", \\n\"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"2\", \"20\", \"3\", \"4\", \\n\"5\", \"6\", \"7\", \"8\", \"9\"), class = \"factor\"), sess = structure(c(1L, \\n2L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, \\n2L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, \\n2L, 2L, 1L, 1L, 2L, 2L, 1L), .Label = c(\"a\", \"b\"), class = \"factor\"), \\n    treat = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, \\n    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, \\n    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L\\n    ), .Label = c(\"drug\", \"placebo\"), class = \"factor\"), memory = structure(c(2L, \\n    12L, 17L, 18L, 1L, 20L, 19L, 15L, 14L, 16L, 8L, 7L, 5L, 9L, \\n    4L, 6L, 10L, 3L, 11L, 13L, 2L, 12L, 17L, 18L, 1L, 20L, 19L, \\n    15L, 14L, 16L, 8L, 7L, 5L, 9L, 4L, 6L, 10L, 3L, 11L, 13L), .Label = c(\"1\", \\n    \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \\n    \"2\", \"20\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"), class = \"factor\"), \\n    blood = structure(c(3L, 11L, 17L, 18L, 20L, 19L, 9L, 10L, \\n    6L, 16L, 5L, 2L, 12L, 15L, 4L, 13L, 14L, 8L, 7L, 1L, 3L, \\n    11L, 17L, 18L, 20L, 19L, 9L, 10L, 6L, 16L, 5L, 2L, 12L, 15L, \\n    4L, 13L, 14L, 8L, 7L, 1L), .Label = c(\"1\", \"100\", \"17\", \"25\", \\n    \"38\", \"39\", \"40\", \"44\", \"55\", \"6\", \"62\", \"66\", \"70\", \"77\", \\n    \"81\", \"83\", \"85\", \"86\", \"89\", \"94\"), class = \"factor\")), .Names = c(\"id\", \\n\"sess\", \"treat\", \"memory\", \"blood\"), row.names = c(NA, -40L), class = \"data.frame\")\\n\\n# the model\\nm1 &lt;- lme(memory~sess*treat*blood, random=~1|id/treat, data=data)\\nsummary(m1)\\n</code></pre>\\n',\n",
       " '<p><em>The desired integral can be wrestled into submission by brute-force manipulations; here, we instead try to give an alternative derivation with a slightly more probabilistic flavor.</em></p>\\n\\n<p>Let $X \\\\\\\\sim \\\\\\\\mathrm{Exp}(k,\\\\\\\\lambda)$ be a noncentral exponential random variable with location parameter $k &gt; 0$ and rate parameter $\\\\\\\\lambda$. Then $X = Z + k$ where $Z \\\\\\\\sim \\\\\\\\mathrm{Exp}(\\\\\\\\lambda)$.</p>\\n\\n<p>Note that $\\\\\\\\log(X/k) \\\\\\\\geq 0$ and so, using a <a href=\"http://stats.stackexchange.com/questions/26020/geometric-distribution-without-replacement/26033#26033\">standard fact for computing the expectation of nonnegative random variables</a>,\\n$$\\n\\\\\\\\newcommand{\\\\\\\\e}{\\\\\\\\mathbb E}\\\\\\\\newcommand{\\\\\\\\rd}{\\\\\\\\mathrm d}\\\\\\\\renewcommand{\\\\\\\\Pr}{\\\\\\\\mathbb P}\\n\\\\\\\\e \\\\\\\\log(X/k) = \\\\\\\\int_0^\\\\\\\\infty \\\\\\\\Pr(\\\\\\\\log(X/k) &gt; z)\\\\\\\\,\\\\\\\\rd z = \\\\\\\\int_0^\\\\\\\\infty \\\\\\\\Pr(Z &gt; k(e^z - 1)) \\\\\\\\,\\\\\\\\rd z \\\\\\\\&gt;.\\n$$\\nBut, $\\\\\\\\Pr(Z &gt; k(e^z -1)) = \\\\\\\\exp(-\\\\\\\\lambda k(e^z - 1))$ on $z \\\\\\\\geq 0$ since $Z \\\\\\\\sim \\\\\\\\mathrm{Exp}(\\\\\\\\lambda)$ and so\\n$$\\n\\\\\\\\e \\\\\\\\log(X/k) = e^{\\\\\\\\lambda k} \\\\\\\\int_0^\\\\\\\\infty \\\\\\\\exp(-\\\\\\\\lambda k e^z) \\\\\\\\, \\\\\\\\rd z = e^{\\\\\\\\lambda k} \\\\\\\\int_{\\\\\\\\lambda k}^\\\\\\\\infty t^{-1} e^{-t} \\\\\\\\,\\\\\\\\rd t \\\\\\\\&gt;,\\n$$\\nwhere the last equality follows from the substitution $t = \\\\\\\\lambda k e^z$, noting that $\\\\\\\\rd z = \\\\\\\\rd t / t$.</p>\\n\\n<p>The integral on the right-hand size of the last display is just $\\\\\\\\Gamma(0,\\\\\\\\lambda k)$ <a href=\"http://en.wikipedia.org/wiki/Incomplete_gamma_function\" rel=\"nofollow\">by definition</a> and so\\n$$\\n\\\\\\\\e \\\\\\\\log X = e^{\\\\\\\\lambda k} \\\\\\\\Gamma(0,\\\\\\\\lambda k) + \\\\\\\\log k \\\\\\\\&gt;,\\n$$\\nas confirmed by <a href=\"http://stats.stackexchange.com/questions/30684/expected-log-value-of-noncentral-exponential-distribution#comment59130_30684\">@Procrastinator\\'s Mathematica computation</a> in the comments to the question.</p>\\n\\n<p><strong>NB</strong>: The equivalent notation <a href=\"http://en.wikipedia.org/wiki/Exponential_integral#Definitions\" rel=\"nofollow\">$\\\\\\\\mathrm E_1(x)$</a> is also often used in place of $\\\\\\\\Gamma(0,x)$.</p>\\n',\n",
       " '<p>I recently came across this work: </p>\\n\\n<p><a href=\"http://cvlab.epfl.ch/alumni/oezuysal/ferns.html\" rel=\"nofollow\">http://cvlab.epfl.ch/alumni/oezuysal/ferns.html</a></p>\\n\\n<p>which is a balance between the full naive Bayes assumption (full independence between the features) and absolutely no independence assumption. This seems like a very appealing idea. Is there more work like this?</p>\\n',\n",
       " '<p>If all series have the same number of data points at the same x positions, then a LOESS smoother with lambda=0 (essentially a weighted moving average) will satisfy your constraints.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/jEGFF.png\" alt=\"0th degree lambda smoothers\"></p>\\n\\n<p>At each position, you\\'re computing $z_i = \\\\\\\\frac{w_i * y_i}{n}$, where $w$ is the local smoothing function around the position. Given that $w_i$ and $n$ are the same for each series, if all the $y_i$ in one series are greater than or equal to the corresponding $y_i$ in another series, then the $z_i$ will follow the same rule. The external bounds condition is met by applying the above logic with a series of all min values or all max values.</p>\\n\\n<p>This smoother will not usually go through the data points.</p>\\n',\n",
       " '<p>I\\'m doing a time series analysis. I\\'m doing most of my analysis in R, where I can use \"NA\" to represent \"not available\" (e.g. a missing data point). But I\\'m doing some data preparation in OpenOffice; currently, I\\'m leaving cells blank for missing data. Is there a better way to \"declare\" that a cell is NA?</p>\\n',\n",
       " '<p>I am trying to read up about AdaBoost from <a href=\"http://www.stanford.edu/~hastie/local.ftp/Springer/ESLII_print5.pdf\" rel=\"nofollow\">Tibshirani</a> (page 337 onwards), and would appreciate some help in understanding it better.  </p>\\n\\n<p>The book says that \"For each successive iteration m = 2, 3, . . . , M the observation weights are individually modiﬁed and the classiﬁcation algorithm is reapplied to the weighted observations. At step m, those observations that were misclassiﬁed by the classiﬁer Gm−1 (x) induced at the previous step have their weights increased, whereas the weights are decreased for those that were classiﬁed correctly. Thus as iterations proceed, observations that are diﬃcult to classify correctly receive ever-increasing inﬂuence. Each successive classiﬁer is thereby forced to concentrate on those training observations that are missed by previous ones in the sequence.\" </p>\\n\\n<p>I am not able to understand what it means to \"reapply the algorithm to the weighted observations\". Say for example if I am doing 2-class text classification, what am I doing to my observations (document vectors)? How does this \"force\" the classifier to concentrate on some samples vs others?</p>\\n',\n",
       " '<p>I will answer each of your queries in turn. </p>\\n\\n<p><em><strong>Is the syntax correctly specifying the clustering and random effects?</em></strong></p>\\n\\n<p>The model you\\'ve fit here is, in mathematical terms, the model </p>\\n\\n<p>$$ Y_{ijk} = {\\\\\\\\bf X}_{ijk} {\\\\\\\\boldsymbol \\\\\\\\beta} + \\\\\\\\eta_{i} + \\\\\\\\theta_{ij} + \\\\\\\\varepsilon_{ijk}$$</p>\\n\\n<p>where </p>\\n\\n<ul>\\n<li><p>$Y_{ijk}$ is the reaction time for observation $k$ during session $j$ on individual $i$. </p></li>\\n<li><p>${\\\\\\\\bf X}_{ijk}$ is the predictor vector for observation $k$ during session $j$ on individual $i$ (in the model you\\'ve written up, this is comprised of all main effects and all interactions). </p></li>\\n<li><p>$\\\\\\\\eta_i$ is the person $i$ random effect that induces correlation between observations made on the same person. $\\\\\\\\theta_{ij}$ is the random effect for individual $i$\\'s session $j$ and $\\\\\\\\varepsilon_{ijk}$ is the leftover error term. </p></li>\\n<li><p>${\\\\\\\\boldsymbol \\\\\\\\beta}$ is the regression coefficient vector. </p></li>\\n</ul>\\n\\n<p>As noted <a href=\"http://lme4.r-forge.r-project.org/book/Ch2.pdf\">on page 14-15 here</a> this model is correct for specifying that sessions are nested within individuals, which is the case from your description. </p>\\n\\n<p><em><strong>Beyond syntax, is this model appropriate for the above within-subject design?</em></strong></p>\\n\\n<p>I think this model is reasonable, as it does respect the nesting structure in the data and I do think that individual and session are reasonably envisioned as random effects, as this model asserts. You should look at the relationships between the predictors and the response with scatterplots, etc. to ensure that the linear predictor (${\\\\\\\\bf X}_{ijk} {\\\\\\\\boldsymbol \\\\\\\\beta}$) is correctly specified. The other standard regression diagnostics should possibly be examined as well. </p>\\n\\n<p><em><strong>Should the full model specify all interactions of fixed effects, or only the ones that I am really interested in?</em></strong> </p>\\n\\n<p>I think starting with such a heavily saturated model may not be a great idea, unless it makes sense substantively. As I said in a comment, this will tend to overfit your particular data set and may make your results less generalizable. Regarding model selection, if you do start with the completely saturated model and do backwards selection (<a href=\"http://stats.stackexchange.com/questions/17111/classic-linear-model-model-selection/17113#17113\">which some people on this site, with good reason, object to</a>) then you have to make sure to <a href=\"http://stats.stackexchange.com/questions/11009/including-the-interaction-but-not-the-main-effects-in-a-model\">respect the hierarchy</a> in the model. That is, if you eliminate a lower level interaction from the model, then you should also delete all higher level interactions involving that variable. For more discussion on that, see the linked thread. </p>\\n\\n<p><em><strong>I have not included the STIM factor in the model, which characterizes the specific stimulus type used in a trial, but which I am not interested to estimate in any way - should I specify that as a random factor given it has 123 levels and very few data points per stimulus type?</em></strong> </p>\\n\\n<p>Admittedly not knowing anything about the application (so take this with a grain of salt), that sounds like a fixed effect, not a random effect. That is, the treatment type sounds like a variable that would correspond to a fixed shift in the mean response, not something that would induce correlation between subjects who had the same stimulus type. But, the fact that it\\'s a 123 level factor makes it cumbersome to enter into the model. I suppose I\\'d want to know how large of an effect you\\'d expect this to have. Regardless of the size of the effect, it will not induce bias in your slope estimates since this is a linear model, but leaving it out may make your standard errors larger than they would otherwise be. </p>\\n',\n",
       " '<p>You can use a credible interval (or HPD region) for Bayesian hypothesis testing.  I don\\'t think it is common; though, to be fair I do not see much nor do I use formal Bayesian Hypothesis testing in practice.  Bayes factors are occasionally used (and in Robert\\'s \"Bayesian Core\" somewhat lauded) in hypothesis testing set up.</p>\\n',\n",
       " \"<p>For large $p$ a spherical rank $p$ Gaussian in $\\\\\\\\mathbb{R}^{p}$ looks like the uniform distribution on the surface of the hypersphere $\\\\\\\\mathbb{S}^{p-1}$ with radius $\\\\\\\\sigma\\\\\\\\sqrt{p}$, while a rank $p-1$ spherical Gaussian embedded in $\\\\\\\\mathbb{R}^{p}$ will give Saturn's rings (the points will look like the hypersphere $\\\\\\\\mathbb{S}^{p-2}$).</p>\\n\\n<p>So I think you can generate this data by drawing from two spherical Gaussians $\\\\\\\\mathcal{N}(0,\\\\\\\\sigma_{p}^{2}I_{p})$ and $\\\\\\\\mathcal{N}(0,\\\\\\\\sigma_{p-1}^{2}I_{p-1})$ having set  $\\\\\\\\sigma_{p}^{2}$ and $\\\\\\\\sigma_{p-1}^{2}$ to get the separation you want.</p>\\n\\n<p>The concentration in the norms is exponentially fast w.r.t $p$, so you probably won't have to throw away any points if $p$ is large enough.</p>\\n\",\n",
       " '<blockquote>\\n  <p><strong>Possible Duplicate:</strong><br>\\n  <a href=\"http://stats.stackexchange.com/questions/11691/how-to-tell-if-data-is-clustered-enough-for-clustering-algorithms-to-produce-m\">How to tell if data is &ldquo;clustered&rdquo; enough for clustering algorithms to produce meaningful results?</a>  </p>\\n</blockquote>\\n\\n\\n\\n<p>I have used hierarchical clustering, e.g, Ward\\'s method, single,complete, etc. I have the same problem, I do not know how to assess my clustering algorithm. How would I know that the clusters are meaningful?</p>\\n\\n<p>I do not know what FPC software is, can any one use FPC, and, what is the programming language of this software?</p>\\n\\n<p>If I\\'m not familiar with FPC, is there another means of assessment?</p>\\n',\n",
       " 'Hierarchical Bayesian models specify priors on parameters and hyperpriors on the parameters of the prior distributions',\n",
       " \"<p>If your data conform to the assumptions that a t-test makes (normality, also equal variances if you do not use Welch's version), then a t-test on the data will have greater statistical power than its non-parametric (i.e. less assumption-heavy) equivalent, the Mann-Whitney test.  In other words, if your data are normal and have equal variances, and there really is a difference between the smoking and non-smoking groups, then it's perfectly possible to get a significant result from a t-test but not from a Mann-Whitney test.  This becomes more likely the smaller the difference between the two groups - even if your sample size is large.</p>\\n\\n<p>Therefore, if your data are normal or can be made to be normal by transformation, use the t-test.  If this is not possible, then use the Mann-Whitney.</p>\\n\\n<p>However, you should note that while the Mann-Whitney test does not assume normality, it does assume that the distribution of data is similar in the two groups.  For example, if the data from smokers are right-skewed and the data from non-smokers are left-skewed, the Mann-Whitney test is not valid.  In that case, you'd use randomisation or permutation as Michael Lew says.</p>\\n\",\n",
       " '<p>What about the determinant of the sample variance-covariance matrix: a measure of the\\nsquared volume enclosed by the matrix within the space of dimension of the measurement vector. Also, an often used scale invariant version of that measure is the determinant of the sample correlation matrix: the volume of the space occupied within the dimensions of the measurement vector. </p>\\n',\n",
       " '<p>I think the R package <a href=\"http://www.ggobi.org/rggobi/introduction.pdf\" rel=\"nofollow\">rggobi</a> is exactly what you\\'re looking for.  Audo recognition is even their <a href=\"http://www.ggobi.org/book/2007-infovis/05-clustering.pdf\" rel=\"nofollow\">example problem</a>!</p>\\n',\n",
       " '<p>I\\'m very in to Sports analysis and am keen to look at finessing my analysis models that I have worked up  (I don\\'t have a great maths background, I\\'ve just done a little bit of reading).</p>\\n\\n<p>Standard deviation of Game Results about prediction from a Ratings system (in this case least squares regression with a few tweaks) equals the stochastic element in a normal distribution function where:</p>\\n\\n<p>x = the margin of victory that you are trying to get a % against (e.g. it could be a point spread e.g. % to cover)\\nthe mean = the predicted margin of victory derived in this case from my least squares regression to create power ratings for teams\\nsigma = standard deviation of error on the difference between the predicted margin of victory and actual margin of victory based on past results.</p>\\n\\n<p>Based on the above I would treat in Excel terms a win % for team A as:</p>\\n\\n<p>1-NORMDIST(0.5,mean,sigma,true) - with the mean and sigma having values obviously.</p>\\n\\n<p>Based on Stern \"The Probability of Winning an American Football Game\" American Statistician (August 1991) - the sigma value (stochastic element? - sorry this isn\\'t my usual thing) is approximately 13.86. How would you go about refining this for other sports? - e.g. Australian Rules Football. I said this could be done for any sport (and I believe it can within reason unless the results or the sport doesn\\'t lend itself to this type of analysis).</p>\\n\\n<p>Looking at past AFL results from the end of the season point of view the standard deviations are as follows (have only started looking at these in the last two days):</p>\\n\\n<p>2009: 31.34 (150+ results)\\n2010: 33.44 (150+ results\\n2011: 34.91 (150+ results)\\n2012: 25.62 (only approximately 70 results so far as it is mid season)</p>\\n\\n<p>As you feed more data into the model for 2012 it will build up a more accurate picture but looking at past data I\\'m putting the sigma value for AFL at around 33.5 (above the 25.62 value). Is there anyway of defining a best fit value? Where I don\\'t have a great background in this kind of thing, if someone could give me some gentle nudges in the right direction in terms of where I should be looking or what I could be applying I would be grateful.</p>\\n\\n<p>I believe this can definitely be applied to other sports as well e.g.:</p>\\n\\n<p>English Premiership:</p>\\n\\n<p>2008/2009: 1.45 (Season with over 300+ results)\\n2009/2010: 1.52\\n2010/2011: 1.54\\n2011/2012: 1.64 (but when looking over the last 100 results around the 1.50 mark)</p>\\n\\n<p>I\\'ve also looked at the percentages that this then gives for home wins, away wins and draws and they are virtually identical to the percentages for bookmakers odds (except they work to 105-106% instead of 100% so they have an edge which accounts for the small differences).</p>\\n\\n<p>NBA:</p>\\n\\n<p>2008/2009: 11.30 (Season with 1000+ results)\\n2009/2010: 11.56\\n2010/2011: 10.88\\n2011/2012: 11.36</p>\\n\\n<p>Thanks in advance for any advice,</p>\\n',\n",
       " \"<p>I have spent much time looking for a special package that could run the Pesaran(2007) unit root test (which assumes cross-sectional dependence unlike most others) and I have found none. So, I decided to do it manually; however, I don't know where I'm going wrong, because my results are very different from Microsoft Excel's results (in which it is done very easily).</p>\\n\\n<p>My data frame is made up of 22 countries with 506 observations of daily price indices. Following is the model to run using the Pesaran(2007) unit root test:</p>\\n\\n<p>(i) With an intercept only</p>\\n\\n<p>$$\\\\\\\\Delta Y_{i,t} = a_i + b_iY_{i,t-1} + c_i\\\\\\\\overline{Y}_{t-1} + d_i\\\\\\\\Delta\\\\\\\\overline{Y}_{t-1}+ e_i\\\\\\\\Delta\\\\\\\\overline{Y}_{t-2}+ f_i\\\\\\\\Delta\\\\\\\\overline{Y}_{i,t-1}+ g_i\\\\\\\\Delta\\\\\\\\overline{Y}_{i,t-2} + \\\\\\\\varepsilon_{i,t}$$</p>\\n\\n<p>where $\\\\\\\\overline{Y}$ is the cross-section average of the observations across countries at each time $t$ and $b$ is the coefficient of interest to us because it will allow us to compute the ADF test statistic and then determine whether the process is stationary or not.</p>\\n\\n<p>I constructed each of these variables in the following way:</p>\\n\\n<p>$\\\\\\\\Delta Y_t$</p>\\n\\n<pre><code>dif.yt = diff(yt) \\n## yt is the object containing all the observations for a specific country \\n## (e.g. Australia)\\n</code></pre>\\n\\n<p>$Y_{t-1}$</p>\\n\\n<pre><code>yt.lag.1 = lag(yt, -1)\\n</code></pre>\\n\\n<p>$\\\\\\\\overline{Y}_{t-1}$</p>\\n\\n<pre><code>ybar.lag.1 = lag(c(rowMeans(x)), -1) \\n## x is the object containing my entire data frame\\n</code></pre>\\n\\n<p>$\\\\\\\\Delta \\\\\\\\overline{Y}_{t-1}$</p>\\n\\n<pre><code>dif.ybar.lag.1 = diff(ybar.lag.1)\\n</code></pre>\\n\\n<p>$\\\\\\\\Delta \\\\\\\\overline{Y}_{t-2}$</p>\\n\\n<pre><code>dif.ybar.lag.2 = diff(lag(c(rowMeans(x)), -2))\\n</code></pre>\\n\\n<p>$\\\\\\\\Delta Y_{t-1}$</p>\\n\\n<pre><code>dif.yt.lag.1 = diff(yt.lag.1)\\n</code></pre>\\n\\n<p>$\\\\\\\\Delta Y_{t-2}$</p>\\n\\n<pre><code>dif.yt.lag.2 = diff(lag(yt, -2)\\n</code></pre>\\n\\n<p>After constructing each variable individually, I then run the linear regression</p>\\n\\n<pre><code>reg = lm(dif.yt ~ yt.lag.1[-1] + ybar.lag.1[-1] + dif.ybar.lag.1 + \\n                  dif.ybar.lag.2 + dif.yt.lag.1 + dif.yt.lag.2)\\nsummary(reg)\\n</code></pre>\\n\\n<p>It is obvious that the explanatory variables in my regression equation differ in length, so I'd like to know whether there is a way in R to make all the variables of equal length (perhaps with a function).</p>\\n\\n<p>Also, I'd like to know whether the procedure I used was correct and if there are more optimal ways.</p>\\n\",\n",
       " \"<p>Is there any information out there about the distribution whose $n$th cumulant is given by $\\\\\\\\frac 1 n$? The cumulant-generating function is of the form \\n$$\\n\\\\\\\\kappa(t) = \\\\\\\\int_0 ^ 1 \\\\\\\\frac{e^{tx} - 1}{x} \\\\\\\\ dx.\\n$$\\nI've run across it as the limiting distribution of some random variables but I haven't been able to find any information on it. </p>\\n\",\n",
       " '<p>this too:\\n<img src=\"http://i.stack.imgur.com/6Z3QQ.png\" alt=\"alt text\"></p>\\n',\n",
       " '<p>In fact in the notation $$p(\\\\\\\\theta|y)\\\\\\\\propto p(y,\\\\\\\\theta)$$ it is understood that the symbol \"$\\\\\\\\propto$\" means that the two members are <strong>proportional functions of the variable $\\\\\\\\theta$</strong>. This is not ambiguous because it is clearly understood that we are dealing with a distribution on the space of the parameter $\\\\\\\\theta$. This notation could become ambiguous when dealing with a two parameters model, say $\\\\\\\\theta$ and $\\\\\\\\mu$. In such a case I personally use the notation $\\\\\\\\underset{\\\\\\\\theta}{\\\\\\\\propto}$, $\\\\\\\\underset{\\\\\\\\mu}{\\\\\\\\propto}$ or $\\\\\\\\underset{\\\\\\\\mu,\\\\\\\\theta}{\\\\\\\\propto}$ for precising what variable is considered in the proportionality statement.</p>\\n',\n",
       " '<p>Your wording is implying causality, which is not what the R^2 represents. \"Does price (x) determine square footage (y)?\" is implying causality which is not what is captured through a correlation. \"Price explains X% of the variation in square footage\" describes that there is a relationship between price and square footage, but not a causal one. This only implies that these variables vary together, not that price causes square footage. Its more akin to saying \"In general, When price goes up X amount square footage goes up Y amount\"</p>\\n',\n",
       " '<p>The following doesn\\'t answer exactly the question. It may give you some ideas, though. It\\'s something I recently did in order to assess the fit of several regression models using one to four independent variables (the dependent variable was in the first column of the df1 dataframe). </p>\\n\\n<pre><code># create the combinations of the 4 independent variables\\nlibrary(foreach)\\nxcomb &lt;- foreach(i=1:4, .combine=c) %do% {combn(names(df1)[-1], i, simplify=FALSE) }\\n\\n# create formulas\\nformlist &lt;- lapply(xcomb, function(l) formula(paste(names(df1)[1], paste(l, collapse=\"+\"), sep=\"~\")))\\n</code></pre>\\n\\n<p>The contents of as.character(formlist) was</p>\\n\\n<pre><code> [1] \"price ~ sqft\"                     \"price ~ age\"                     \\n [3] \"price ~ feats\"                    \"price ~ tax\"                     \\n [5] \"price ~ sqft + age\"               \"price ~ sqft + feats\"            \\n [7] \"price ~ sqft + tax\"               \"price ~ age + feats\"             \\n [9] \"price ~ age + tax\"                \"price ~ feats + tax\"             \\n[11] \"price ~ sqft + age + feats\"       \"price ~ sqft + age + tax\"        \\n[13] \"price ~ sqft + feats + tax\"       \"price ~ age + feats + tax\"       \\n[15] \"price ~ sqft + age + feats + tax\"\\n</code></pre>\\n\\n<p>Then I collected some useful indices </p>\\n\\n<pre><code># R squared\\nmodels.r.sq &lt;- sapply(formlist, function(i) summary(lm(i))$r.squared)\\n# adjusted R squared\\nmodels.adj.r.sq &lt;- sapply(formlist, function(i) summary(lm(i))$adj.r.squared)\\n# MSEp\\nmodels.MSEp &lt;- sapply(formlist, function(i) anova(lm(i))[\\'Mean Sq\\'][\\'Residuals\\',])\\n\\n# Full model MSE\\nMSE &lt;- anova(lm(formlist[[length(formlist)]]))[\\'Mean Sq\\'][\\'Residuals\\',]\\n\\n# Mallow\\'s Cp\\nmodels.Cp &lt;- sapply(formlist, function(i) {\\nSSEp &lt;- anova(lm(i))[\\'Sum Sq\\'][\\'Residuals\\',]\\nmod.mat &lt;- model.matrix(lm(i))\\nn &lt;- dim(mod.mat)[1]\\np &lt;- dim(mod.mat)[2]\\nc(p,SSEp / MSE - (n - 2*p))\\n})\\n\\ndf.model.eval &lt;- data.frame(model=as.character(formlist), p=models.Cp[1,],\\nr.sq=models.r.sq, adj.r.sq=models.adj.r.sq, MSEp=models.MSEp, Cp=models.Cp[2,])\\n</code></pre>\\n\\n<p>The final dataframe was</p>\\n\\n<pre><code>                      model p       r.sq   adj.r.sq      MSEp         Cp\\n1                price~sqft 2 0.71390776 0.71139818  42044.46  49.260620\\n2                 price~age 2 0.02847477 0.01352823 162541.84 292.462049\\n3               price~feats 2 0.17858447 0.17137907 120716.21 351.004441\\n4                 price~tax 2 0.76641940 0.76417343  35035.94  20.591913\\n5            price~sqft+age 3 0.80348960 0.79734865  33391.05  10.899307\\n6          price~sqft+feats 3 0.72245824 0.71754599  41148.82  46.441002\\n7            price~sqft+tax 3 0.79837622 0.79446120  30536.19   5.819766\\n8           price~age+feats 3 0.16146638 0.13526220 142483.62 245.803026\\n9             price~age+tax 3 0.77886989 0.77173666  37884.71  20.026075\\n10          price~feats+tax 3 0.76941242 0.76493500  34922.80  21.021060\\n11     price~sqft+age+feats 4 0.80454221 0.79523470  33739.36  12.514175\\n12       price~sqft+age+tax 4 0.82977846 0.82140691  29640.97   3.832692\\n13     price~sqft+feats+tax 4 0.80068220 0.79481991  30482.90   6.609502\\n14      price~age+feats+tax 4 0.79186713 0.78163109  36242.54  17.381201\\n15 price~sqft+age+feats+tax 5 0.83210849 0.82091573  29722.50   5.000000\\n</code></pre>\\n\\n<p>Finally, a Cp plot (using library wle)</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/JbQyk.png\" alt=\"\"></p>\\n',\n",
       " '<p>The canonical reference for this kind of thing is <a href=\"http://amzn.com/0127466525\" rel=\"nofollow\">White (2001)</a>.</p>\\n\\n<p>The model you have is \\n$$\\nY_i = \\\\\\\\boldsymbol{X}_i\\'\\\\\\\\boldsymbol{\\\\\\\\beta}_0 + \\\\\\\\varepsilon_i\\n$$\\ntogether with the conditional exogeneity condition $\\\\\\\\mathbb{E}(\\\\\\\\varepsilon_i \\\\\\\\mid \\\\\\\\boldsymbol{X}_i) = 0$. Here $\\\\\\\\boldsymbol{X}_i$ is a $k \\\\\\\\times 1$ vector.</p>\\n\\n<p>Your question is about the minimal conditions for the consistency of the OLS estimator. In particular, you would like to know if $\\\\\\\\mathbb{E}(\\\\\\\\varepsilon_i \\\\\\\\mid \\\\\\\\boldsymbol{X}_i) = 0$ is sufficient to prove that $\\\\\\\\widehat{\\\\\\\\boldsymbol{\\\\\\\\beta}} \\\\\\\\overset{p}{\\\\\\\\to} \\\\\\\\boldsymbol{\\\\\\\\beta}_0$. We can go through the steps of proving the consistency to show that this condition is not sufficient.</p>\\n\\n<hr>\\n\\n<ul>\\n<li><p>We have that \\n$$\\n\\\\\\\\begin{align}\\n\\\\\\\\widehat{\\\\\\\\boldsymbol{\\\\\\\\beta}} &amp;= \\\\\\\\boldsymbol{\\\\\\\\beta}_0 + \\\\\\\\left(\\\\\\\\frac{\\\\\\\\mathbf{X}\\'\\\\\\\\mathbf{X}}{n}\\\\\\\\right)^{-1} \\\\\\\\left(\\\\\\\\frac{\\\\\\\\mathbf{X}\\'\\\\\\\\boldsymbol{\\\\\\\\varepsilon}}{n}\\\\\\\\right) \\\\\\\\\\\\\\\\\\n&amp;= \\\\\\\\boldsymbol{\\\\\\\\beta}_0 +\\\\\\\\left(\\\\\\\\frac{\\\\\\\\sum_{i=1}^n \\\\\\\\boldsymbol{X}_i\\\\\\\\boldsymbol{X}_i\\'}{n}\\\\\\\\right)^{-1} \\\\\\\\left(\\\\\\\\frac{\\\\\\\\sum_{i=1}^n\\\\\\\\boldsymbol{X}_i\\\\\\\\varepsilon_i}{n}\\\\\\\\right)\\n\\\\\\\\end{align}\\n$$\\nwhere $\\\\\\\\mathbf{X} = [\\\\\\\\boldsymbol{X}_1, \\\\\\\\ldots, \\\\\\\\boldsymbol{X}_n]\\'$, and $\\\\\\\\boldsymbol{\\\\\\\\varepsilon} = [\\\\\\\\varepsilon_1, \\\\\\\\ldots, \\\\\\\\varepsilon_n]\\'$.<br>\\nAs long as I can prove that\\n$$\\n\\\\\\\\frac{\\\\\\\\sum_{i=1}^n\\\\\\\\boldsymbol{X}_i\\\\\\\\varepsilon_i}{n} \\\\\\\\overset{p}{\\\\\\\\to} \\\\\\\\boldsymbol{0}\\n$$\\nand that \\n$$\\n\\\\\\\\frac{\\\\\\\\sum_{i=1}^n \\\\\\\\boldsymbol{X}_i\\\\\\\\boldsymbol{X}_i\\'}{n} \\\\\\\\overset{p}{\\\\\\\\to} \\\\\\\\mathbf{m}_{\\\\\\\\boldsymbol{X}\\\\\\\\boldsymbol{X}} \\n$$\\nwhere $\\\\\\\\mathbf{m}_{\\\\\\\\boldsymbol{X}\\\\\\\\boldsymbol{X}}$ is a finite positive definite matrix, I can make two applications of Slutsky\\'s theorem and get that $\\\\\\\\widehat{\\\\\\\\boldsymbol{\\\\\\\\beta}} \\\\\\\\overset{p}{\\\\\\\\to} \\\\\\\\boldsymbol{\\\\\\\\beta}_0$. What guarantees this?</p></li>\\n<li><p>The first requirement is guaranteed to hold by an application of your favorite LLN (in the case of IID observations, you can use Khintchine\\'s WLLN) to state that\\n$$\\n\\\\\\\\begin{align}\\n\\\\\\\\frac{\\\\\\\\sum_{i=1}^n\\\\\\\\boldsymbol{X}_i\\\\\\\\varepsilon_i}{n} &amp;\\\\\\\\overset{p}{\\\\\\\\to} \\\\\\\\mathbb{E}(\\\\\\\\boldsymbol{X}_i \\\\\\\\varepsilon_i) \\\\\\\\\\\\\\\\\\n&amp;= \\\\\\\\boldsymbol{0}\\n\\\\\\\\end{align}\\n$$\\nNote that your conditional exogeneity condition guarantees that $\\\\\\\\mathbb{E}(\\\\\\\\boldsymbol{X}_i \\\\\\\\varepsilon_i)$ exists (why?) and that it is equal to $\\\\\\\\boldsymbol{0}$. So far, we have not needed any additional assumptions.</p></li>\\n<li><p>The second requirement also requires use of an appropriate LLN to state that\\n$$\\n\\\\\\\\begin{align}\\n\\\\\\\\frac{\\\\\\\\sum_{i=1}^n \\\\\\\\boldsymbol{X}_i\\\\\\\\boldsymbol{X}_i\\'}{n} &amp;\\\\\\\\overset{p}{\\\\\\\\to} \\\\\\\\mathbb{E}(\\\\\\\\boldsymbol{X}_i\\\\\\\\boldsymbol{X}_i\\') \\\\\\\\\\\\\\\\\\n&amp;\\\\\\\\equiv \\\\\\\\mathbf{m}_{\\\\\\\\boldsymbol{X}\\\\\\\\boldsymbol{X}} \\n\\\\\\\\end{align}\\n$$\\nwhere now we have to show that this convergence takes place and to a matrix with the required properties.<br>\\nAn application of the LLN in this context requires the RHS of the limit to be finite (whence comes the finiteness of the $\\\\\\\\mathbf{m}_{\\\\\\\\boldsymbol{X}\\\\\\\\boldsymbol{X}}$ matrix requirement).<br>\\nNext, even the pointwise full column rank of the design matrices $\\\\\\\\mathbf{X}$, that is, the positive definiteness of $\\\\\\\\mathbf{X}\\'\\\\\\\\mathbf{X}$ does not guarantee that the limit is going to be positive definite, so that assumption will have to be made in addition.</p></li>\\n</ul>\\n\\n<p>Once you make these assumptions, then, you can claim consistency of the OLS estimator. Proving the consistency of the OLS estimator is possible under much weaker conditions of this type, for these, see White (2001, Ch. 2).</p>\\n',\n",
       " '<p>There are many ways to estimate the accuracy of such predictions and the optimal choice  really depends on what will the estimation implemented for. </p>\\n\\n<p>For example, if you plan to select a few high score hits for an expensive follow-up study you may want to maximize the precision at high scores. On the other hand, if the follow-up study is cheap you may want to maximize the recall (sensitivity) at lower scores. The ROC AUC may be suitable if you are comparing different method, etc.</p>\\n\\n<p>On the practical side, <code>R</code>\\'s <code>ROCR</code> package contains 2 useful functions</p>\\n\\n<pre><code>pred.obj &lt;- prediction(predictions, labels,...)\\nperformance(pred.obj, measure, ...)\\n</code></pre>\\n\\n<p>Together, these functions can calculate a wide range of accuracy measures, including global scalar values (such as <em>\"auc\"</em>) and score-dependent vectors for plotting Recall-precision and ROC curves (<em>\"prec\"</em>, <em>\"rec\"</em>, <em>\"tpr\"</em> and <em>\"fpr\"</em>, etc.)</p>\\n',\n",
       " '<p>If the marks 1 through 5 can be reasonably called interval-level, a t-test should be fine. (Interval level data means that the gap between 1 and 2 is the same as that between 2 and 3 and so on).  In <code>r</code> this is <code>t.test</code>. </p>\\n\\n<p>If the marks 1 through 5 can only be assumed to be ordinal, then some test of the median is probably going to be good. See <a href=\"http://stats.stackexchange.com/questions/97/what-are-good-basic-statistics-to-use-for-ordinal-data\">this post</a> and its many comments for some ideas on this. </p>\\n\\n<p>(Note, I added <code>ordinal</code> to your tags, as I suspect that these are only ordinal). </p>\\n',\n",
       " '<p>I think the <a href=\"http://cran.r-project.org/web/packages/hdrcde/index.html\" rel=\"nofollow\"><code>hdrcde</code></a> package does what you want. Here is something quite similar to your example:</p>\\n\\n<pre><code>require(hdrcde)\\nn &lt;- 23000\\nx &lt;- c(runif(n,0,1),runif(n,0,.6))\\ny &lt;- c(x[1:n], 7*x[n+(1:n)]) + rnorm(n)\\ny &lt;- (y-min(y))/(max(y)-min(y))\\nplot(x,y)\\n\\nden &lt;- hdr.boxplot.2d(x,y,prob=.30,h=c(2.,2),pch=\".\",pointcol=\"red\")\\nj &lt;- (den$fxy &gt; den$falpha)\\npoints(x[j],y[j],col=\"green\",pch=\".\")\\n</code></pre>\\n\\n<p>That will give you the points within the contour of 30% probability. It is very fast, even with 46000 observations.</p>\\n',\n",
       " '<p>This is an open problem in psychology/psychometrics. There are a number of methods commonly used. Two that I have found useful are parallel analysis and the minimum avaerage partial criterion. Both of these are implemented in the <code>psych</code> package for R.</p>\\n\\n<p>Typically, the MAP criterion suggests less factors (say $l$) than does parallel analysis $m$. In that case, I typically use all the solutions between $l$ and $m$ factors. If you were willing to carry out a cross validation process, you could also investigate structural equation modelling, which is implemented in the packages <code>lavaan</code>, <code>sem</code> and <code>OpenMx</code>. </p>\\n',\n",
       " '<p>I am using rlm in the R MASS package to regress a multivariate linear model. It works well for a number of samples but I am getting quasi-null coefficients for a particular model:</p>\\n\\n<pre><code>Call: rlm(formula = Y ~ X1 + X2 + X3 + X4, data = mymodel, maxit = 50, na.action = na.omit)\\nResiduals:\\n       Min         1Q     Median         3Q        Max \\n-7.981e+01 -6.022e-03 -1.696e-04  8.458e-03  7.706e+01 \\n\\nCoefficients:\\n             Value    Std. Error t value \\n(Intercept)    0.0002   0.0001     1.8418\\nX1             0.0004   0.0000    13.4478\\nX2            -0.0004   0.0000   -23.1100\\nX3            -0.0001   0.0002    -0.5511\\nX4             0.0006   0.0001     8.1489\\n\\nResidual standard error: 0.01086 on 49052 degrees of freedom\\n  (83 observations deleted due to missingness)\\n</code></pre>\\n\\n<p>For comparison, these are the coefficients calculated by lm():</p>\\n\\n<pre><code>Call:\\nlm(formula = Y ~ X1 + X2 + X3 + X4, data = mymodel, na.action = na.omit)\\n\\nResiduals:\\n    Min      1Q  Median      3Q     Max \\n-76.784  -0.459   0.017   0.538  78.665 \\n\\nCoefficients:\\n              Estimate Std. Error t value Pr(&gt;|t|)    \\n(Intercept)  -0.016633   0.011622  -1.431    0.152    \\nX1            0.046897   0.004172  11.240  &lt; 2e-16 ***\\nX2           -0.054944   0.002184 -25.155  &lt; 2e-16 ***\\nX3            0.022627   0.019496   1.161    0.246    \\nX4            0.051336   0.009952   5.159  2.5e-07 ***\\n---\\nSignif. codes:  0 \\'***\\' 0.001 \\'**\\' 0.01 \\'*\\' 0.05 \\'.\\' 0.1 \\' \\' 1 \\n\\nResidual standard error: 2.574 on 49052 degrees of freedom\\n  (83 observations deleted due to missingness)\\nMultiple R-squared: 0.0182, Adjusted R-squared: 0.01812 \\nF-statistic: 227.3 on 4 and 49052 DF,  p-value: &lt; 2.2e-16 \\n</code></pre>\\n\\n<p>The lm plot doesn\\'t show any particularly high outlier, as measured by Cook\\'s distance:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/98Hxj.png\" alt=\"lm diagnostic\"></p>\\n\\n<p><strong>EDIT</strong></p>\\n\\n<p>For reference and after confirming results based on the answer provided by Macro, the R command to set the tuning parameter, <code>k</code>, in the Huber estimator is (<code>k=100</code> in this case):</p>\\n\\n<pre><code>rlm(y ~ x, psi = psi.huber, k = 100)\\n</code></pre>\\n',\n",
       " '<p>With your new information, that <strong>all the components of the positive-definite matrix are positive</strong>, it becomes easy. While it follows directly from the Perron-Frobenius theorem (Which is valid for square matrices with non-negative elements, symmetric or not), in the symmetric case it is much easier.</p>\\n\\n<p>Let the positive-definit matrix bee $S$. the eigenvector corresponding to the largest eigenvector is the value of the vector $x$ obtaining the maximum in the following problem:\\n$$\\n\\\\\\\\lambda_{\\\\\\\\text{max}} = max_{\\\\\\\\{x \\\\\\\\colon \\\\\\\\| x\\\\\\\\|=1\\\\\\\\}} x^T S x\\n$$(that is, the \"argmax\") where $\\\\\\\\lambda_{\\\\\\\\text{max}}$ is the largest eigenvalue.</p>\\n\\n<p>Suppose to get a contradiction that $x_1$ is negative, while the other components of $x$ are non-negative. We can write\\n$$\\nx^T S x = x_1 S_{11} x_1+2x_1 \\\\\\\\sum_{j=2}^m s_{1j} x_j + \\\\\\\\sum_{i=2}^m \\\\\\\\sum_{j=2}^m x_i s_{ij} x_j\\n$$\\nNote that the first and third terms are positive while the second term is negative, and we can get a strictly larger value by switching the sign of $x_1$, which respects the restriction on norm. That gives the contradiction you need. A similar argument can be written for any other pattern of negative/positive sign.</p>\\n',\n",
       " \"<p>Let's say I am trying to estimate the treatment effect of rude waiters on how often customers frequent a restaurant. To simplify, let's assume that rudeness is binary, observable, and random. I consider the population of all transactions in October, and I see that some customers have eaten multiple meals and others only once. My outcome will be number of meals in November-Dec period.</p>\\n\\n<p>I would like to construct a customer-level cross section out of my panel data since the estimation method I am using gets complicated with non-independent observations. I have three options available to me:</p>\\n\\n<ol>\\n<li>Grab a random transaction for each customer. Use the waiter from that transaction to determine if a diner was treated or not.</li>\\n<li>Aggregate the October data by calculating the # of rude waiters/# of meals for each customer</li>\\n<li>For each person, grab a random rude transactions if there was a rude one at any time in October. Otherwise, grab a random transaction. </li>\\n</ol>\\n\\n<p>Unfortunately (2) is off the table since there are other attributes of the meal that are hard to aggregate.</p>\\n\\n<p>Method (1) has the problem that 2 people who both dined twice and had rude transactions half of the time might wind up in separate groups, which seems strange since they have the exact same history. </p>\\n\\n<p>Method (3) seems the most sensible, but it is giving me an estimate that have the wrong sign, and seem fragile.    </p>\\n\\n<p>I have three related questions:</p>\\n\\n<ul>\\n<li>Is there anything wrong with (3) from a theoretical sampling perspective? </li>\\n<li>Can I rationalize using (1), since on average the mis-assignment bias might cancel out?</li>\\n<li>Can I get my cross section in some other way, perhaps by adding weights of some sort?</li>\\n</ul>\\n\",\n",
       " '<p>I asked a question on StackOverflow for which I was suggested to use Kalman Filter. The question is as follows:</p>\\n\\n<p><a href=\"http://stackoverflow.com/questions/5726358/what-class-of-algorithms-reduce-margin-of-error-in-continuous-stream-of-input/5728373#5728373\">http://stackoverflow.com/questions/5726358/what-class-of-algorithms-reduce-margin-of-error-in-continuous-stream-of-input/5728373#5728373</a></p>\\n\\n<blockquote>\\n  <p>A machine is taking measurements and\\n  giving me discrete numbers\\n  continuously like so:</p>\\n  \\n  <p>1 2 5 7 8 10 11 12 13 14 18</p>\\n  \\n  <p>Let us say these measurements can be\\n  off by 2 points and a measurement is\\n  generated every 5 seconds. I want to\\n  ignore the measurements that may\\n  potentially be same</p>\\n  \\n  <p>Like continuous 2 and 3 could be same\\n  because margin of error is 2 so how do\\n  I partition the data such that I get\\n  only distinct measurements but I would\\n  also want to handle the situation in\\n  which the measurements are\\n  continuously increasing like so:</p>\\n  \\n  <p>1 2 3 4 5 6 7 8 9 10</p>\\n  \\n  <p>In this case if we keep ignoring the\\n  consecutive numbers with difference of\\n  less than 2 then we might lose actual\\n  measurements.</p>\\n</blockquote>\\n\\n<p>Now how do I apply Kalman Filter to solve this? All examples I see take multiple error estimations while I know a single thing that each measurement can be off by a value Q thats it and all the examples also work on multi dimensional vectors, that too multiple vectors.</p>\\n',\n",
       " '<p>I\\'m pretty sure that I\\'m missing something obvious here, but I\\'m rather confused with different terms in the time series field. If I understand it correctly, serially autocorrelated errors are a problem in regression models (see for example <a href=\"http://stats.stackexchange.com/questions/19321/can-i-trust-a-regression-if-variables-are-autocorrelated\">here</a>). My question is now what exactly defines an autocorrelated error? I know the definition of autocorrelation and I can apply the formulas, but this is more a problem of comprehension with time series in regressions.</p>\\n\\n<p>For example, let\\'s take the time series of daily temperatures: If it is a hot day today (summer time!), it\\'s probably hot tomorrow as well, and vice versa. I guess I have a problem to call this phenomenon a phenomenon of \"serially autocorrelated errors\" because it just doesn\\'t strike me as an error, but as something expected.</p>\\n\\n<p>More formally, let\\'s assume a regression set-up with one dependent variable $y_t$ and one independent variable $x_t$ and the model.</p>\\n\\n<p>$$\\\\\\\\ny_t = \\\\\\\\alpha + \\\\\\\\beta x_t + \\\\\\\\epsilon_t\\\\\\\\n$$</p>\\n\\n<p>Is it possible that $x_t$ is autocorrelated, while $\\\\\\\\epsilon_t$ is i.i.d? If so, what does that mean for all that methods that adjust standard errors for autocorrelation? Do you still have to do that or do they only apply to autocorrelated errors? Or would you always model the autocorrelation in such a setting in the error term, so it basically doesn\\'t make a difference if $x_t$ is autocorrelated or $e_t$?</p>\\n\\n<p>This is my first question here. I hope it\\'s not too confusing and I hope I didn\\'t miss anything obvious...I also tried to google it and found some interesting links (for instance, here on <a href=\"http://stats.stackexchange.com/questions/23704/time-series-regression-serially-correlated-errors-vs-autocorrelation-of-residua\">SA</a>), but nothing really helped me.</p>\\n',\n",
       " '<p>The micromap example that @karl suggested is very nice; I think it would, though, be similar to a dot plot, which are easy to implement in R or SAS (and probably other software). Something like </p>\\n\\n<pre><code>corr &lt;- c(.4, .4, .3, .2)\\ngroup = c(\"Education\", \"gender\", \"age\", \"more\")\\ndotchart(corr, labels = group)\\n</code></pre>\\n\\n<p>works in R</p>\\n',\n",
       " '<p>A Normal approximation works extremely well, <em>especially</em> in the tails.  Use a mean of $\\\\\\\\alpha/(\\\\\\\\alpha+\\\\\\\\beta)$ and a variance of $\\\\\\\\frac{\\\\\\\\alpha\\\\\\\\beta}{(\\\\\\\\alpha+\\\\\\\\beta)^{2} (1+\\\\\\\\alpha+\\\\\\\\beta)}$.  For example, the absolute relative error in the tail probability in a tough situation (where skewness might be of concern) such as $\\\\\\\\alpha = 10^6, \\\\\\\\beta = 10^8$ peaks around $0.00026$ and is less than $0.00006$ when you\\'re more than 1 SD from the mean.  (This is <em>not</em> because beta is so large: with $\\\\\\\\alpha = \\\\\\\\beta = 10^6$, the absolute relative errors are bounded by $0.0000001$.)  Thus, this approximation is excellent for essentially any purpose involving 99% intervals.</p>\\n\\n<p>In light of the edits to the question, note that one does not compute beta integrals by actually integrating the integrand: of course you\\'ll get underflows (although they don\\'t really matter, because they don\\'t contribute appreciably to the integral). There are many, many ways to compute the integral or approximate it, as documented in Johnson &amp; Kotz (Distributions in Statistics). An online calculator is found at <a href=\"http://www.danielsoper.com/statcalc/calc37.aspx\" rel=\"nofollow\">http://www.danielsoper.com/statcalc/calc37.aspx</a>.  You actually need the inverse of this integral.  Some methods to compute the inverse are documented on the Mathematica site at <a href=\"http://functions.wolfram.com/GammaBetaErf/InverseBetaRegularized/\" rel=\"nofollow\">http://functions.wolfram.com/GammaBetaErf/InverseBetaRegularized/</a>.  Code is provided in Numerical Recipes (www.nr.com). A really nice online calculator is the Wolfram Alpha site (www.wolframalpha.com ): enter <code>inverse beta regularized (.005, 1000000, 1000001)</code> for the left endpoint and <code>inverse beta regularized (.995, 1000000, 1000001)</code> for the right endpoint ($\\\\\\\\alpha=1000000, \\\\\\\\beta=1000001$, 99% interval).</p>\\n',\n",
       " '<p>\"What the use of a p-value implies, therefore, is that a hypothesis that may be true may be rejected because it has not predicted observable results that have not occurred.\"</p>\\n\\n<p>Harold Jeffreys (Theory of Probability)</p>\\n',\n",
       " '<p>A quick response to the bulleted content:</p>\\n\\n<p>1) Power / Type 1 error in a Bayesian analysis vs. a frequentist analysis</p>\\n\\n<p>Asking about Type 1 and power (i.e. one minus the probability of Type 2 error) implies that you can put your inference problem into a repeated sampling framework.  Can you?  If you can\\'t then there isn\\'t much choice but to move away from frequentist inference tools.  If you can, and if the behavior of your estimator over many such samples is of relevance, and if you are not particularly interested in making probability statements about particular events, then I there\\'s no strong reason to move.</p>\\n\\n<p>The argument here is not that such situations never arise - certainly they do - but that they typically don\\'t arise in the fields where the methods are applied.</p>\\n\\n<p>2) The trade-off in complexity of the analysis (Bayesian seems more complicated) vs. the benefits gained. </p>\\n\\n<p>It is important to ask where the complexity goes.  In frequentist procedures the <em>implementation</em> may be very simple, e.g. minimize the sum of squares, but the <em>principles</em> may be arbitrarily complex, typically revolving around which estimator(s) to choose, how to find the right test(s), what to think when they disagree.  For an example. see the still lively discussion, picked up in this forum, of different confidence intervals for a proportion!</p>\\n\\n<p>In Bayesian procedures the <em>implementation</em> can be arbitrarily complex even in models that look like they \\'ought\\' to be simple, usually because of difficult integrals but the <em>principles</em> are extremely simple.  It rather depends where you\\'d like the messiness to be.</p>\\n\\n<p>3) Traditional statistical analyses are straightforward, with well-established guidelines for drawing conclusions. </p>\\n\\n<p>Personally I can no longer remember, but certainly my students <em>never</em> found these straightforward, mostly due to the principle proliferation described above.  But the question is not really whether a procedure is straightforward, but whether is closer to being right given the structure of the problem.</p>\\n\\n<p>Finally, I strongly disagree that there are \"well-established guidelines for drawing conclusions\" in either paradigm.  And I think that\\'s a <em>good</em> thing.  Sure, \"find p&lt;.05\" is a clear guideline, but for what model, with what corrections, etc.? And what do I do when my tests disagree?  Scientific or engineering judgement is needed here, as it is elsewhere.</p>\\n',\n",
       " '<p>At the bottom of the Wikipedia page you linked are a few examples:</p>\\n\\n<ul>\\n<li><p>If $X_1$ and $X_2$ are IID exponential distributions, $X_1 - X_2$ has a Laplace distribution.</p></li>\\n<li><p>If $X_1, X_2, X_3, X_4$ are IID standard normal distributions, $X_1X_4 - X_2X_3$ has a standard Laplace distribution. So, the determinant of a random $2\\\\\\\\times 2$ matrix with IID standard normal entries $\\\\\\\\begin{pmatrix}X_1 &amp; X_2 \\\\\\\\\\\\\\\\\\\\\\\\ X_3 &amp; X_4 \\\\\\\\end{pmatrix} $ has a Laplace distribution.</p></li>\\n<li><p>If $X_1, X_2$ are IID uniform on $[0,1]$, then $\\\\\\\\log \\\\\\\\frac{X_1}{X_2}$ has a standard Laplace distribution. </p></li>\\n</ul>\\n',\n",
       " '<p>You may know that weighting generally aims at ensuring that a given sample is representative of its target population. If in your sample some attributes (e.g., gender, SES, type of medication) are less well represented than in the population from which the sample comes from, then we may adjust the weights of the incriminated statistical units to better reflect the hypothetical target population.</p>\\n\\n<p>RIM weighting (or raking) means that we will equate the sample marginal distribution to the theoretical marginal distribution. It bears some idea with post-stratification, but allows to account for many covariates. I found a good overview in this handout about <a href=\"http://sekhon.berkeley.edu/causalinf/sp2010/section/week9.pdf\" rel=\"nofollow\">Weighting Methods</a>, and here is an example of its use in a real study: <a href=\"http://www.fcsm.gov/01papers/Greene.pdf\" rel=\"nofollow\">Raking Fire Data</a>.</p>\\n\\n<p>Propensity weighting is used to compensate for unit non-response in a survey, for example, by increasing the sampling weights of the respondents in the sample using estimates of the probabilities that they responded to the survey. This is in spirit the same idea than the use of propensity scores to adjust for treatment selection bias in observational clinical studies: based on external information, we estimate the probability of patients being included in a given treatment group and compute weights based on factors hypothesized to influence treatment selection. Here are some pointers I found to go further: </p>\\n\\n<ul>\\n<li><a href=\"http://www.statistics.su.se/modernsurveys/publ/11.pdf\" rel=\"nofollow\">The propensity score and estimation in nonrandom surveys - an overview</a> </li>\\n<li><a href=\"http://www.amstat.org/sections/srms/proceedings/y2009/Files/304345.pdf\" rel=\"nofollow\">A Simulation Study to Compare Weighting Methods for Nonresponses in the National Survey of Recent College Graduates</a> </li>\\n<li><a href=\"http://www.jds-online.com/file_download/94/JDS-233.pdf\" rel=\"nofollow\">A Comparison of Propensity Score and Linear Regression Analysis of Complex Survey Data</a>.</li>\\n</ul>\\n\\n<p>As for a general reference, I would suggest </p>\\n\\n<blockquote>\\n  <p>Kalton G, Flores-Cervantes I.\\n  Weighting Methods. J. Off. Stat.\\n  (2003) 19: 81-97. Available on\\n  <a href=\"http://www.jos.nu/\" rel=\"nofollow\">http://www.jos.nu/</a></p>\\n</blockquote>\\n',\n",
       " \"<p>Other than a calibration plot, is there a way to decide how good one models' predictive probabilities as compared to another model. </p>\\n\\n<p>I'm not interested in error rates as I find them ineffective for the level of precision I'm looking for. </p>\\n\\n<p>The only quantity of interest is the predictive probability distribution, as I am pricing contracts using them.</p>\\n\\n<p>EDIT:</p>\\n\\n<p>I have no faith in scoring rules based on the experience below with several different classifiers. </p>\\n\\n<p>I've simulated data from a known model. Trained the known model and a worse model using the simulated data, and the brier and log rules don't agree that the known model is superior. The class probabilities are materially different.</p>\\n\",\n",
       " '<p>I have a set of counts as a response variable (anywhere from 0 to 85). I want to find the easiest way to plot count on the y-axis vs frequency of that count on the x-axis, the type of plot you see in \"power law\" plots. For example, I have 2 people with count = 100, 5 people with count 75, ..., 20 people with count 5.</p>\\n\\n<p>This is really just for some exploratory data analysis. I can almost achieve the same thing by reversing the order of the y-axis is a horizontal bar chart (but I can\\'t figure out how to reverse the y-axis). </p>\\n\\n<p>So, any ideas about either one?</p>\\n',\n",
       " '<p>It\\'s the $L^p$ norm.  See for example the Wikipedia articles:</p>\\n\\n<ul>\\n<li><a href=\"http://en.wikipedia.org/wiki/Lp_space\">$L^p$ space</a></li>\\n<li><a href=\"http://en.wikipedia.org/wiki/Minkowski_distance\">Minkowski distance</a></li>\\n</ul>\\n\\n<p>If you use $p = 2$, you\\'ll find it resolves to the more familiar Euclidean norm -- i.e. the most familiar measure used as length of the vector $a$.  Other values of p give others ways of measuring length as outlined in the article -- see the sections on Euclidean norm, Taxicab norm, etc.</p>\\n',\n",
       " '<p>I\\'m trying to solve this problem out loud. I\\'m not great at statistics but I try sometimes to understand where things are and where they\\'re going.</p>\\n\\n<p>I\\'m trying to figure out the best method to compare the two following datasets. The first is the actual observation and the second is of what my sensor determined the value was. Basically an object travels across the sensor that with properties. Each object can have anywhere from 2 to 5 components. We know the exact value of each one of those components. </p>\\n\\n<p>The big table below shows the data results from running those objects on the sensor. Observations are independent so observation 5 has no affect on observation 26. The dataset has 31 objects in it, and each object was measure using the sensor. We know the exact properties of the object and I put it under \"Actual\"</p>\\n\\n<p><strong>Note:</strong></p>\\n\\n<p>Total = 1 + 2 + 3 + 4 + 5</p>\\n\\n<p>All what I\\'m trying to figure out is if the sensor was adequate or does it need further calibration? The old way of doing things was to get an average difference less than 5% to be acceptable. I don\\'t agree with that methodology as I believe it\\'s prone to error and does not capture the essence of measurements. So therefore, I\\'m investigating better data analysis tools for the problem.</p>\\n\\n<p><strong>Hypotheses:</strong></p>\\n\\n<ol>\\n<li>\"Is the sensor performing adequately?\"</li>\\n<li>Is there an overall statistical difference between Actual and Sensor values?</li>\\n<li>Is there a statistical difference between each observed pair?</li>\\n<li>At which point is the significance broken? i.e. at which confidence interval do the two datasets become different?</li>\\n</ol>\\n\\n<p><strong>Analysis:</strong></p>\\n\\n<p>The first thing I did was to compare the average differences for each column. I also did the mean of the absolute differences and here are the results:</p>\\n\\n<pre><code>   Difference between Actual and Sensor (%)\\n                      Total        1        2      3         4        5\\n   Mean of Diff        1.06     2.80    -4.00   7.12    -10.86    17.01\\n   Mean of abs diff    7.31     6.96     7.56   8.52      0.70     1.10\\n</code></pre>\\n\\n<p>These numbers don\\'t tell me much really but they show somewhat of a relevant comparison. I then proceeded to do a two paired t-test using alpha = 0.05 with the following results:</p>\\n\\n<pre><code>        df      Mean Actual     Mean Sensor     Var Actual      Var Sensor      pearson     t stat      P 1 tail    t Crit 1 tail   P 2 tail    t Crit 2 tail\\nTotal   30      37,858          89,829,589      89,829,589      77,766,451      0.9463      1.4871      0.0737      1.6973          0.1474      2.0423\\n1       30      5,411.61        5,261           254,000         407,118         0.7829      2.1076      0.0218      1.6973          0.0435      2.0423\\n2       30      15,246          15,719          4,762,169       3,878,279       0.7845      -1.9103     0.0328      1.6973          0.0657      2.0423\\n3       27      17,943          15,396          40,196,438      33,216,653      0.4268      2.0742      0.0239      1.7033          0.0477      2.0518\\n4       1       6,770.00        7,600           14,257,800      20,480,000      1.0000      -1.5661     0.1809      6.3138          0.3618      12.7062\\n5       1       5,910.00        4,900           64,800          0.00            N/A         5.6108      0.0561      6.3138          0.1123      12.7062\\n</code></pre>\\n\\n<p>I basically concluded that the means are not significantly different for \"Total\", \"\", \"4\", and \"5\". Am I correct on the analysis? </p>\\n\\n<p>I then did an F test analysis for two means to compare those results. Note that I wasn\\'t sure if I needed to do the F-test but I did it to compare the variance. I concluded that according to the F test, it appears that my variance is high on all accounts that the results are statistically significant.</p>\\n\\n<pre><code>        F   P 1 tail    F Crit 1 tail\\nTotal   1.1551  0.3477      1.8409\\n1       0.6239  0.1011      0.5432\\n2       1.2279  0.2887      1.8409\\n3       1.0299  0.4698      1.9048\\n4       0.6962  0.4427      0.0062\\n5       65,535  N/A         161.4476\\n</code></pre>\\n\\n<p><strong>Data Table</strong></p>\\n\\n<pre><code>            Actual                                             Sensor\\nObservation   Total       1       2       3       4       5    Total      1       2       3       4       5\\n          1   36820    5860   14840   16120                   36200    5500   15900   14800\\n          2   42530    5160   12000    9840    9440    6090   43300    5200   13100    9200   10800    4900\\n          3   48650    4770   18260   25620                   52000    4700   18300   29000\\n          4   42290    5020   16080   21190                   48800    5600   18400   24800\\n          5   40530    5350   15670   19510                   37800    4800   15300   17700\\n          6   41610    5140   16000   20470                   41500    5700   17100   18700\\n          7   35210    5310   15150   14750                   34000    5100   15600   13300\\n          8   36150    5430   15520   15200                   33600    4900   15400   13300\\n          9   17600    6770   10830                           19300    6600   12800\\n         10   35920    5280   15540   15100                   28700    4700   13000   11000\\n         11   37900    5740   15440   16720                   35100    5500   14700   14900\\n         12   41530    5590   16560   19380                   39300    5400   16600   17200\\n         13   33580    5330   14870   13380                   33400    5100   14800   13500\\n         14   25550    4990   13450    7110                   24600    4800   13000    6800\\n         15   45920    5420   16200   24300                   43300    5100   15800   22400\\n         16   17750    5740   12010                           21700    6500   15200\\n         17   29570    5230   14140   10200                   28800    4600   14600    9600\\n         18   44870    5150   16990   22730                   47500    5600   18900   23000\\n         19   35500    5780   14930   14790                   32200    5600   14200   12500\\n         20   54070    5310   17170   31590                   50100    5000   16700   28300\\n         21   35440    4590   13920   16930                   35000    3900   14400   16600\\n         22   40680    5330   12840   12680    4100    5730   40000    5400   13400   11900    4400    4900\\n         23   16540    7160    9380                           20200    6900   13300\\n         24   47820    4960   17480   25380                   47100    4700   18100   24300\\n         25   39410    5350   18070   15990                   42100    5900   20700   15500\\n         26   40790    5470   16480   18840                   36400    5000   15700   15700\\n         27   31760    5480   14570   11710                   33700    5900   16200   11600\\n         28   33000    5090   14760   13150                   30900    4800   14600   11500\\n         29   49390    5350   18110   25930                   45500    4900   18100   22400\\n         30   54290    5110   17540   31640                   48800    4800   16700    27300   20900\\n         31   40930    5500   17850   17580                   37300    4900   16700   15700\\n</code></pre>\\n',\n",
       " '<p>Consider the following model</p>\\n\\n<p>$$\\\\\\\\nX \\\\\\\\sim |\\\\\\\\mathcal{N}(X;0,1)|\\\\\\\\n$$\\n$$\\\\\\\\nY|X \\\\\\\\sim Q(Y;X)\\\\\\\\n$$\\nwhere I define $Q(Y=-x|X=x)$ with probability mass $\\\\\\\\int_{-\\\\\\\\infty}^{-x}\\\\\\\\mathcal{N}(x;0,1)dx$, $Q(Y=+x|X=x)$ with probability mass $1-\\\\\\\\int_{x}^{\\\\\\\\infty}\\\\\\\\mathcal{N}(x;0,1)dx$, and the density of $Q(Y|X=x)$ equivalent to a truncated Normal distribution in $(-x,x)$.  </p>\\n\\n<p>Assume now that I an unknown $x_{unk}$ is sampled from $|\\\\\\\\mathcal{N}(X;0,1)|$ and that $y$ is sampled from $Q(Y|X=x_{unk})$.  I am given $y$ and would like to approximate the posterior distribution, $P(X|Y=y)$.</p>\\n\\n<p>Using importance sampling, I would like to take $N$ sample values for $X$ then weight them according to the \"probability\" of $Y|X$ such that for sufficiently many samples, $\\\\\\\\mathbb{E}[X|Y=y] \\\\\\\\approx \\\\\\\\frac{1}{\\\\\\\\sum_{i}w_{i}} \\\\\\\\sum_{i} x_{i} w_{i}$.  </p>\\n\\n<p>Were $Q(Y|X)$ entirely continuous, I would use $w_{i} = Q(Y=y|X=x_{i})$.  Were it entirely discrete, I would use the probability mass function instead.  As $Y|X$ is distributed according to a sort of hybrid, what should my weights be?</p>\\n',\n",
       " '<p>After consulting a statistics professor specialized in GLMs, I realized this is not a longitudinal set with respect to the ordered value (because grades are assigned only in period 3) but a cross-section with lags, so a proportional odds logistic regression will make it.</p>\\n',\n",
       " '<p>The Julia language is pretty new; it\\'s time in the spot light can be measured in weeks (even though its development time can of course be measured in years). Now those weeks in the spot light were very exciting weeks---see for example the recent talk at Stanford where \"it had just started\"---but what you ask for in terms of broader infrastructure and package support will take much longer to materialize. </p>\\n\\n<p>So I\\'d keep using R, and be mindful of the developing alternatives. Last year a lot of people went gaga over Clojure; this year Julia is the reigning new flavour. We\\'ll see if it sticks.</p>\\n',\n",
       " '<blockquote>\\n  <p><strong>Possible Duplicate:</strong><br>\\n  <a href=\"http://stats.stackexchange.com/questions/7527/change-point-analysis-using-rs-nls\">Change point analysis using R&#39;s nls()</a>  </p>\\n</blockquote>\\n\\n\\n\\n<p>I want to do a nonlinear regression with nls() but also include a specific type of segmented or piecewise regression. The Formula I want to implement is:</p>\\n\\n<pre><code>S ~ b0 + (A &gt; T) * b1 * (A - T)\\n</code></pre>\\n\\n<p><code>T</code> should be the threshold value or breakpoint as identified by the nonlinear-segmented regression. I know that I can use <code>\"algorithm = plinear\"</code> but that does not work at all.</p>\\n\\n<hr>\\n\\n<p>The data I have is:</p>\\n\\n<pre><code>A   S\\n0.000809371 1\\n0.003642171 3\\n0.009712455 4\\n0.010521827 2\\n0.004046856 4\\n0.015378054 5\\n0.000404686 0\\n0.000404686 0\\n0.000404686 0\\n0.000809371 0\\n0.000809371 3\\n0.037635765 3\\n0.008903084 2\\n0.016187426 5\\n0.043301364 1\\n0.000404686 1\\n0.002428114 1\\n0.003642171 1\\n0.013759312 4\\n0.051395077 9\\n0.394568501 9\\n0.005665599 1\\n0.013354626 1\\n0.028732681 3\\n0.026304567 2\\n0.004451542 1\\n0.050585705 2\\n0.00647497  1\\n0.010926512 0\\n0.013354626 1\\n1.695632841 4\\n0.013354626 2\\n</code></pre>\\n',\n",
       " '<p>These aren\\'t complete tutorials on Bayesian statistics, but rather isolated explanations of individual concepts that I like. Just thought I\\'d add in case it helps.</p>\\n\\n<ul>\\n<li><a href=\"http://lesswrong.com/lw/2b0/bayes_theorem_illustrated_my_way\" rel=\"nofollow\">http://lesswrong.com/lw/2b0/bayes_theorem_illustrated_my_way</a> - Graphical explanation of Bayes\\' theorem.</li>\\n<li><a href=\"http://stats.stackexchange.com/questions/2272/whats-the-difference-between-a-confidence-interval-and-a-credible-interval/2287#2287\">What&#39;s the difference between a confidence interval and a credible interval?</a> - Great example of the difference between confidence intervals and Bayesian credible intervals (and of the difference between frequentist statistics and Bayesian statistics in general).</li>\\n<li><a href=\"http://behind-the-enemy-lines.blogspot.com/2008/01/are-you-bayesian-or-frequentist-or.html\" rel=\"nofollow\">http://behind-the-enemy-lines.blogspot.com/2008/01/are-you-bayesian-or-frequentist-or.html</a> - A simple example of how frequentists and Bayesians approach a problem differently, and how it leads to a different answer.</li>\\n</ul>\\n',\n",
       " '<p>I ran an ad campaign on Facebook and I have impressions (ad-views), clicks, and sign-ups broken down by gender and age.</p>\\n\\n<p>Certain demographics have a higher rate of clicks and sign-ups when compared to their respective percent of impressions. What is the best way to determine if these differences represent significant results instead of random noise?</p>\\n\\n<p>I am trying to see how age and gender predict sign-up.</p>\\n',\n",
       " '<p>What you are doing is <a href=\"http://en.wikipedia.org/wiki/Exploratory_data_analysis\">exploratory data analysis</a>.  Because you looked at your data first, and then decided to test something specific based on what you just saw, sample statistics (like r-scores) are going to be <a href=\"http://en.wikipedia.org/wiki/Bias_%28statistics%29\">biased</a>, and inferential statistics (like p-values) would not mean what they are purported to mean.  (If that doesn\\'t make sense, you may want to read my answer to a different question <a href=\"http://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection/20856#20856\">here</a>, which should make the underlying ideas more understandable, although the ideas are applied in a different context.)  To be clear, I have nothing against exploratory data analysis--I\\'m quite fond of it, it\\'s just important to realize which game you\\'re playing and what that means for how you think about what you\\'re doing and what you find.  With that in mind, I would take the log of all your counts (looking at your numbers, I would probably use log base-2, then each unit would be interpretable as a <em>doubling</em> of the underlying count).  Then I would simply make a scatterplot matrix overlaid with loess lines.  In R, in the car package, <code>scatterplot.matrix()</code> will do this for you and make it pretty.  I would then just use my judgment to assess the situation because of the bias issues noted above.  If you want a formal model and hypothesis tests, I would figure out what I thought <em>might</em> be true (and if it were interesting &amp; important, etc.) and then gather a <em>new</em> dataset specifically to test that hypothesis.  </p>\\n',\n",
       " \"<p>I have simple model for some medical test characteristics calculation.</p>\\n\\n<ul>\\n<li>$T$=test</li>\\n<li>$D$=Disease </li>\\n<li>$D+$= Disease present</li>\\n<li>$T+$= Test positive (yes\\\\\\\\no)</li>\\n</ul>\\n\\n<p>$P(D+)$ is very small (0.000001).<br>\\nThe problem is I don't know what the probability of Test being positive in the general population. This is a very new test and we don't know the exact profile of this very expensive test yet. </p>\\n\\n<p>Test result can be affected by many factors.  Given this problem definition, how can I incorporate these many factors that may change the test result (cause a false positive or a false positive) into my general basic Bayesian model below?</p>\\n\\n<p>My model is $P(T+|D+) = P(D+|T+)P(D+) / P(D+)$</p>\\n\\n<p>Thank you</p>\\n\",\n",
       " \"<p><strong>NB</strong> The stated result does not depend on any assumption of normality or even independence of the coordinates of $\\\\\\\\newcommand{\\\\\\\\x}{\\\\\\\\mathbf{x}}\\\\\\\\newcommand{\\\\\\\\e}{\\\\\\\\mathbb{E}}\\\\\\\\newcommand{\\\\\\\\tr}{\\\\\\\\mathbf{tr}}\\\\\\\\newcommand{\\\\\\\\A}{\\\\\\\\mathbf{A}}\\\\\\\\x$. It does not depend on $\\\\\\\\A$ being positive definite either. Indeed, suppose <em>only</em> that the coordinates of $\\\\\\\\x$ have zero mean, variance of one and are <em>uncorrelated</em> (but not necessarily independent); that is, $\\\\\\\\e \\\\\\\\x_i = 0$, $\\\\\\\\e \\\\\\\\x_i^2 = 1$, and $\\\\\\\\e \\\\\\\\x_i \\\\\\\\x_j = 0$ for all $i \\\\\\\\neq j$.</p>\\n\\n<p><strong>Bare-hands approach</strong></p>\\n\\n<p>Let $\\\\\\\\A = (a_{ij})$ be an arbitrary $n \\\\\\\\times n$ matrix. By definition $\\\\\\\\tr(\\\\\\\\A) = \\\\\\\\sum_{i=1}^n a_{ii}$. Then,\\n$$\\\\\\\\n\\\\\\\\tr(\\\\\\\\A) = \\\\\\\\sum_{i=1}^n a_{ii} = \\\\\\\\sum_{i=1}^n a_{ii} \\\\\\\\e \\\\\\\\x_i^2 = \\\\\\\\sum_{i=1}^n a_{ii} \\\\\\\\e \\\\\\\\x_i^2 + \\\\\\\\sum_{i\\\\\\\\neq j} a_{ij} \\\\\\\\e \\\\\\\\x_i \\\\\\\\x_j ,\\\\\\\\n$$\\nand so we are done.</p>\\n\\n<p>In case that's not quite obvious, note that the right-hand side, by linearity of expectation, is\\n$$\\\\\\\\n\\\\\\\\sum_{i=1}^n a_{ii} \\\\\\\\e \\\\\\\\x_i^2 + \\\\\\\\sum_{i\\\\\\\\neq j} a_{ij} \\\\\\\\e \\\\\\\\x_i \\\\\\\\x_j = \\\\\\\\e\\\\\\\\Big(\\\\\\\\sum_{i=1}^n \\\\\\\\sum_{j=1}^n a_{ij} \\\\\\\\x_i \\\\\\\\x_j \\\\\\\\Big) = \\\\\\\\e(\\\\\\\\x^T \\\\\\\\A \\\\\\\\x)\\\\\\\\n$$</p>\\n\\n<p><strong>Proof via trace properties</strong></p>\\n\\n<p>There is another way to write this that is suggestive, but relies, conceptually on slightly more advanced tools. We need that both expectation and the trace operator are linear and that, for any two matrices $\\\\\\\\A$ and $\\\\\\\\newcommand{\\\\\\\\B}{\\\\\\\\mathbf{B}}\\\\\\\\B$ of appropriate dimensions, $\\\\\\\\tr(\\\\\\\\A\\\\\\\\B) = \\\\\\\\tr(\\\\\\\\B\\\\\\\\A)$. Then, since $\\\\\\\\x^T \\\\\\\\A \\\\\\\\x = \\\\\\\\tr(\\\\\\\\x^T \\\\\\\\A \\\\\\\\x)$, we have\\n$$\\\\\\\\n\\\\\\\\e(\\\\\\\\x^T \\\\\\\\A \\\\\\\\x) = \\\\\\\\e( \\\\\\\\tr(\\\\\\\\x^T \\\\\\\\A \\\\\\\\x) ) = \\\\\\\\e( \\\\\\\\tr(\\\\\\\\A \\\\\\\\x \\\\\\\\x^T) ) = \\\\\\\\tr( \\\\\\\\e( \\\\\\\\A \\\\\\\\x \\\\\\\\x^T ) ) = \\\\\\\\tr( \\\\\\\\A \\\\\\\\e \\\\\\\\x \\\\\\\\x^T ),\\\\\\\\n$$\\nand so,\\n$$\\\\\\\\n\\\\\\\\e(\\\\\\\\x^T \\\\\\\\A \\\\\\\\x) = \\\\\\\\tr(\\\\\\\\A \\\\\\\\mathbf{I}) = \\\\\\\\tr(\\\\\\\\A) .\\\\\\\\n$$</p>\\n\\n<p><strong>Quadratic forms, inner products and ellipsoids</strong></p>\\n\\n<p>If $\\\\\\\\A$ is positive definite, then an inner product on $\\\\\\\\mathbf{R}^n$ can be defined via $\\\\\\\\langle \\\\\\\\x, \\\\\\\\mathbf{y} \\\\\\\\rangle_{\\\\\\\\A} = \\\\\\\\x^T \\\\\\\\A \\\\\\\\mathbf{y}$ and $\\\\\\\\mathcal{E}_{\\\\\\\\A} = \\\\\\\\{\\\\\\\\x: \\\\\\\\x^T \\\\\\\\A \\\\\\\\x = 1\\\\\\\\}$ defines an ellipsoid in $\\\\\\\\mathbf{R}^n$ centered at the origin.</p>\\n\",\n",
       " \"<p>I am trying to extract variance components for selection and chance in a bifactorial design with <strong>Generation as a fixed factor</strong> and <strong>Replicate as a random term,</strong> for early fecundity. </p>\\n\\n<p>Since I am using the individual level (because I want to make confidence intervals for Chance), the sample size is not equal between Generations so it's an unbalanced design and it's is not a repeated measures ANOVA because my individuals are different.</p>\\n\\n<p>The problem is that when I compare the results between Statistica and R the Mean Squares(MS) for the Generation and Replicate Factors are different (but the MS for Gen*Rep and the residuals are not!)</p>\\n\\n<p>My data consist of 2 terms: </p>\\n\\n<ul>\\n<li>Generation - 2 level fixed factor (6 and 11)</li>\\n<li>Replicate - 3 level random factor (Ad1, Ad2, Ad3)</li>\\n</ul>\\n\\n<p>The model I am using is</p>\\n\\n<pre><code>g &lt;- Anova (aov(Fec~ Gen+Rep+ Gen*Rep), random=~1|Gen*Rep)\\n</code></pre>\\n\\n<p>I am not sure if this is how to phrase that I want the terms with Replicate to be random (maybe that's one of my errors when comparing with the Statistica results?).</p>\\n\\n<ul>\\n<li><p>Might the differences be due to the unbalanced design? Do R and Statistica have different ways of handling the different number of individuals in each replicate and generation?</p></li>\\n<li><p>Why aren't all MS equal? Is there a way to make the results be the same? </p></li>\\n<li><p>How can I indicate that a factor (and all its interactions) are random?  </p></li>\\n<li><p>How can I tell R that I want it to test the Generation factor against the Gen*Rep term or to put it in another way to make it test the significance of the Gen factor against the heterogeneity between my replicates?</p></li>\\n</ul>\\n\",\n",
       " '<p>here is an unconventional but apparently workable idea. wondering if anyone has tried something like it, esp looking for references, examples, or nearby related work. </p>\\n\\n<p>am working on a mathematical induction problem with a sequence $x_i$ of strings which are \"generally increasing\" in size as $i$ goes to infinity. from inspection the function is loosely related to the Fibonacci sequence.</p>\\n\\n<p>these strings could be converted to very large multidigit base-N numbers (ie with something in the range of dozens of digits or more) and one could do a curve fit using standard algorithms eg Marquardt-Levenburg nonlinear least squares algorithm, powells algorithm, etc.</p>\\n\\n<p>of course its preferable to get an <em>exact</em> curve fit such that it matches $x_i$ for some finite $i \\\\\\\\leq k$ and then generalizes to unseen data $i&gt;k$. ie <em>induction</em>. </p>\\n\\n<blockquote>\\n  <p>so what are some of the largest magnitude numbers that curve fitting has been applied to, and esp cases where exact formulas have been obtained? are there some researchers, refs, or some software that specializes in this? etc </p>\\n</blockquote>\\n\\n<p>my question should not be taken to be asking about datasets with many datapoints. this question is a much different regime of interest where there may be \"not so many datapoints\", but each datapoint has a very large numerical magnitude with lots of digits. so its \"big data\" in a sense but with a key twist.</p>\\n\\n<p>note however equivalently this question <em>is</em> the same as fitting floating point values to very high degrees of precision and the tight fit is not arbitrary but instead meaningful to the problem in some way.</p>\\n\\n<p>(this question is partly inspired by the AMS ref on <em>experimental mathematics</em>.[1])</p>\\n\\n<p>[1] <a href=\"http://www.ams.org/notices/200505/fea-borwein.pdf\" rel=\"nofollow\">Experimental Mathematics: Examples, Methods and\\nImplications</a> by Bailey/Borwein, notes of the AMS, 2005</p>\\n',\n",
       " \"<p>I'm trying to finish a proof for a review exercise and I'm asked to show that </p>\\n\\n<p>$E\\\\\\\\left[(y-E(y|x))(E(y|x)-f(x))\\\\\\\\right]=0$</p>\\n\\n<p>where $y$ is the dependent variable and $f(x)$ is a linear predictor of $y$.</p>\\n\\n<p>I'm almost finished, but I just want to check whether or not</p>\\n\\n<p>$E\\\\\\\\left[ E \\\\\\\\left[ y E(y|x)\\\\\\\\,|\\\\\\\\,x \\\\\\\\right] \\\\\\\\right]=E\\\\\\\\left[ E(y|x)E \\\\\\\\left[ E(y|x)\\\\\\\\,|\\\\\\\\,x \\\\\\\\right] \\\\\\\\right]$</p>\\n\\n<p>Basically, can I take $y$ out of the conditional expectation as $E(y|x)$, or worded differently, are conditional expectations multiplacative in this way?</p>\\n\\n<p>Ordinarily you can back a function out of the expectation if it is a function of the variable being conditioned on - i.e., $E\\\\\\\\left[ f(x)y|x \\\\\\\\right]=f(x)E(y|x)$, but I think what I'm trying to do above is different.</p>\\n\\n<p>Also, a second, related question – if $E\\\\\\\\left[E(y|x)\\\\\\\\right]=E(y)$ by the Law of Total Expectations, then does $E\\\\\\\\left[E(y|x)E(y|x)\\\\\\\\right]=E(y^2)$ by the same token?</p>\\n\",\n",
       " '<p>NNMF is unique to within a permutation and a scaling, per Laurberg, 2007, 14th IEEE/SP Workshop on Statistical Signal Processing, August 2007. Related, Donoho, Stodden \"When does NNMF give a correct decomposition into parts?\", Stanford, 2003.</p>\\n',\n",
       " '<p>Fredric M. Wolf\\'s little green <a href=\"http://rads.stackoverflow.com/amzn/click/0803927568\" rel=\"nofollow\">Sage book</a> is worth the $18 or so.  \"Pleasantly mathematical\" but not too technical, not too dogmatic either (it\\'s a fiercely contested field, you probably know), good for a person with what I\\'d call intermediate-level stats/research experience.</p>\\n',\n",
       " '<p>I think no extra package is needed for the task, just use the basic <code>sample</code> function, e.g.:</p>\\n\\n<p>Get sample from the first group:</p>\\n\\n<pre><code>sample &lt;- sample(data[data$\"Care Type\" == \"Acute Care\",], size = 25)\\n</code></pre>\\n\\n<p>Get the choosen IDs out of the orig. dataset (making a backup could be a good idea before that):</p>\\n\\n<pre><code>data &lt;- data[setdiff(data$pat_id, sample_pat_id),]\\n</code></pre>\\n\\n<p>Get sample from second group in the rest of the dataset and concatenate to sample:</p>\\n\\n<pre><code>sample &lt;- rbind(sample, sample(data[(data$\"Care Type\" == \"Acute Care\"),], size = 25)\\n</code></pre>\\n\\n<p>Repeat for each segment:</p>\\n\\n<pre><code>data &lt;- data[setdiff(data$pat_id, sample_pat_id),]\\nsample &lt;- rbind(sample, sample(data[(data$\"Care Type\" == \"?\"),], size = ?)\\n</code></pre>\\n\\n<p>Sorry, not tested, but I think the point can be seen. And also: I am sure the above code could be improved and minified.</p>\\n',\n",
       " '<p>I have a weighted sample, for which I wish to calculate quantiles.<sup>1</sup></p>\\n\\n<p>Ideally, where the weights are equal (whether = 1 or otherwise), the results would be consistent with those of <code>scipy.stats.scoreatpercentile()</code> and R\\'s <code>quantile(...,type=7)</code>.</p>\\n\\n<p>One simple approach would be to \"multiply out\" the sample using the weights given. That effectively gives a locally \"flat\" ecdf in the areas of weight > 1, which intuitively seems like the wrong approach when the sample is actually a sub-sampling. In particular, it means that a sample with weights all equal to 1 has different quantiles than one with weights all equal to 2, or 3. (Note, however, that the paper referenced in [1] does appear to use this approach.)</p>\\n\\n<p><a href=\"http://en.wikipedia.org/wiki/Percentile#Weighted_percentile\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Percentile#Weighted_percentile</a> gives an alternative formulation for weighted percentile. Its not clear in this formulation whether adjacent samples with identical values should first be combined and their weights summed, and in any case its results do not appear to be consistent with R\\'s default type 7 <code>quantile()</code> in the unweighted/equally weighted case. The wikipedia page on quantiles doesn\\'t mention the weighted case at all.</p>\\n\\n<p>Is there a weighted generalisation of R\\'s \"type 7\" quantile function?</p>\\n\\n<p>[using Python, but just looking for an algorithm, really, so any language will do]</p>\\n\\n<p>M</p>\\n\\n<p>[1] Weights are integers; the weights are those of the buffers which are combined in the \"collapse\" and \"output\" operations as described in <a href=\"http://infolab.stanford.edu/~manku/papers/98sigmod-quantiles.pdf\" rel=\"nofollow\">http://infolab.stanford.edu/~manku/papers/98sigmod-quantiles.pdf</a>. Essentially the weighted sample is a sub-sampling of the full unweighted sample, with each element x(i) in the sub-sample representing weight(i) elements in the full sample.</p>\\n',\n",
       " '<p>I would guess that he did not include an intercept in his model, which allows you to identify all seven day-of-week coefficients. In any case, the model with an intercept and six day-of-week coefficients gives the same results, when appropriately calculated, as a model with seven day-of-week dummies. See a related derivation <a href=\"http://stats.stackexchange.com/questions/8135/dummy-variable-trap-issues/8165#8165\">here</a>.</p>\\n',\n",
       " '<p>To be general, let us consider a time series with arbitrary steps.  Therefore, let $k$ be the number of steps in three months and write $\\\\\\\\omega_1$ for the weight for the immediately preceding time, $\\\\\\\\omega_2$ for the weight preceding it, and so on, so that the sequence of weights </p>\\n\\n<p>$$(\\\\\\\\omega_1, \\\\\\\\omega_2, \\\\\\\\ldots, \\\\\\\\omega_k)$$</p>\\n\\n<p>is applied to the preceding three months.  The <em>total</em> weight for those three months is the sum of these.</p>\\n\\n<p><strong>Assumption I:</strong>  It is natural to hope that the next sequence of weights starting at $\\\\\\\\omega_{k+1}$ be in proportion to the first sequence; the question specifies that the constant of proportionality be $1/2$, entailing</p>\\n\\n<p>$$\\\\\\\\frac{1}{2}(\\\\\\\\omega_1, \\\\\\\\omega_2, \\\\\\\\ldots, \\\\\\\\omega_k) = (\\\\\\\\omega_{k+1}, \\\\\\\\omega_{k+2}, \\\\\\\\ldots, \\\\\\\\omega_{2k}).$$</p>\\n\\n<p>The requirement of \"smoothness,\" together with the natural idea of a monotonic decrease in weights over time, suggests that $\\\\\\\\omega_{k+1} \\\\\\\\lt \\\\\\\\omega_k$.  This leads to</p>\\n\\n<p><strong>Assumption II:</strong> We might hope that the sequence of weights could be arranged in geometric proportion, say with constant of proportionality $\\\\\\\\rho$, whence</p>\\n\\n<p>$$\\\\\\\\frac{1}{2}\\\\\\\\omega_1 = \\\\\\\\omega_{k+1} = \\\\\\\\rho^k \\\\\\\\omega_1.$$</p>\\n\\n<p><strong>The unique solution</strong> is </p>\\n\\n<p>$$\\\\\\\\rho = 2^{-1/k}.$$ </p>\\n\\n<p>Summing these weights over $n$ time steps and requiring them to sum to unity gives</p>\\n\\n<p>$$1 = \\\\\\\\omega_1 + \\\\\\\\omega_2 + \\\\\\\\cdots + \\\\\\\\omega_n = \\\\\\\\omega_1(1 + \\\\\\\\rho + \\\\\\\\rho^2 + \\\\\\\\cdots + \\\\\\\\rho^{n-1}) = \\\\\\\\omega_1 \\\\\\\\frac{1-\\\\\\\\rho^n}{1-\\\\\\\\rho}.$$</p>\\n\\n<p>It follows that</p>\\n\\n<p>$$\\\\\\\\omega_1 = \\\\\\\\frac{1-\\\\\\\\rho}{1-\\\\\\\\rho^n} = \\\\\\\\frac{1-2^{-1/k}}{1-2^{-n/k}}$$</p>\\n\\n<p>and</p>\\n\\n<p>$$\\\\\\\\omega_i = \\\\\\\\rho^{i-1}\\\\\\\\omega_1 = 2^{(1-i)/k}\\\\\\\\omega_1, \\\\\\\\quad i=1, 2, \\\\\\\\ldots, n.$$</p>\\n\\n<p><strong>For example</strong>, with $k=3$ (monthly data) and $n=6$ (giving two full three-month periods), $\\\\\\\\rho = 2^{-1/3} = 0.793701$, $\\\\\\\\omega_1 = 0.275066$, and the sequence of weights for the first three-month period is $0.275066$, $0.21832$, $0.173281$ followed by half these weights for the second three-month period, $0.137533$, $0.10916$, $0.0866403$.</p>\\n\\n<hr>\\n\\n<p><strong>Other solutions to the problem are possible</strong>; in particular, there are infinitely many available when we do not make Assumption II.  But Assumption II has the particularly nice property that if you were to combine the data into sequential groups, taking $m$ of them at a time (such as combining daily data into monthly data, with $m \\\\\\\\approx 30$), and recalculate the weights, then <em>the sum of the weights for each group will equal the recalculated group weights.</em></p>\\n',\n",
       " '<p><strong>First (and easy) solution:</strong> If you are not keen to stick with classical RF, as implemented in Andy Liaw\\'s <code>randomForest</code>, you can try the <a href=\"http://cran.r-project.org/web/packages/party/index.html\">party</a> package which provides a different implementation of the original RF<sup>&#153;</sup> algorithm (use of conditional trees and aggregation scheme based on units weight average). Then, as reported on this <a href=\"http://r.789695.n4.nabble.com/Re-Fwd-Re-Party-extract-BinaryTree-from-cforest-td3878100.html\">R-help post</a>, you can plot a single member of the list of trees. It seems to run smoothly, as far as I can tell. Below is a plot of one tree generated by <code>cforest(Species ~ ., data=iris, controls=cforest_control(mtry=2, mincriterion=0))</code>.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/3OWx1.png\" alt=\"enter image description here\"></p>\\n\\n<p><strong>Second (but probably not so harder) solution:</strong> Most of tree-based techniques in R (<code>tree</code>, <code>rpart</code>, <code>TWIX</code>, etc.) offers a <code>tree</code>-like structure for printing/plotting a single tree. The idea would be to convert the output of <code>randomForest::getTree</code> to such an R object, even if it is nonsensical from a statistical point of view. Basically,  it is easy to access the tree structure from a <code>tree</code> object, as shown below. Please note that it will slightly differ depending of the type of task--regression vs. classification--where in the later case it will add class-specific probabilities as the last column of the <code>obj$frame</code> (which is a <code>data.frame</code>).</p>\\n\\n<pre><code>&gt; library(tree)\\n&gt; tr &lt;- tree(Species ~ ., data=iris)\\n&gt; tr\\nnode), split, n, deviance, yval, (yprob)\\n      * denotes terminal node\\n\\n 1) root 150 329.600 setosa ( 0.33333 0.33333 0.33333 )  \\n   2) Petal.Length &lt; 2.45 50   0.000 setosa ( 1.00000 0.00000 0.00000 ) *\\n   3) Petal.Length &gt; 2.45 100 138.600 versicolor ( 0.00000 0.50000 0.50000 )  \\n     6) Petal.Width &lt; 1.75 54  33.320 versicolor ( 0.00000 0.90741 0.09259 )  \\n      12) Petal.Length &lt; 4.95 48   9.721 versicolor ( 0.00000 0.97917 0.02083 )  \\n        24) Sepal.Length &lt; 5.15 5   5.004 versicolor ( 0.00000 0.80000 0.20000 ) *\\n        25) Sepal.Length &gt; 5.15 43   0.000 versicolor ( 0.00000 1.00000 0.00000 ) *\\n      13) Petal.Length &gt; 4.95 6   7.638 virginica ( 0.00000 0.33333 0.66667 ) *\\n     7) Petal.Width &gt; 1.75 46   9.635 virginica ( 0.00000 0.02174 0.97826 )  \\n      14) Petal.Length &lt; 4.95 6   5.407 virginica ( 0.00000 0.16667 0.83333 ) *\\n      15) Petal.Length &gt; 4.95 40   0.000 virginica ( 0.00000 0.00000 1.00000 ) *\\n&gt; tr$frame\\n            var   n        dev       yval splits.cutleft splits.cutright yprob.setosa yprob.versicolor yprob.virginica\\n1  Petal.Length 150 329.583687     setosa          &lt;2.45           &gt;2.45   0.33333333       0.33333333      0.33333333\\n2        &lt;leaf&gt;  50   0.000000     setosa                                  1.00000000       0.00000000      0.00000000\\n3   Petal.Width 100 138.629436 versicolor          &lt;1.75           &gt;1.75   0.00000000       0.50000000      0.50000000\\n6  Petal.Length  54  33.317509 versicolor          &lt;4.95           &gt;4.95   0.00000000       0.90740741      0.09259259\\n12 Sepal.Length  48   9.721422 versicolor          &lt;5.15           &gt;5.15   0.00000000       0.97916667      0.02083333\\n24       &lt;leaf&gt;   5   5.004024 versicolor                                  0.00000000       0.80000000      0.20000000\\n25       &lt;leaf&gt;  43   0.000000 versicolor                                  0.00000000       1.00000000      0.00000000\\n13       &lt;leaf&gt;   6   7.638170  virginica                                  0.00000000       0.33333333      0.66666667\\n7  Petal.Length  46   9.635384  virginica          &lt;4.95           &gt;4.95   0.00000000       0.02173913      0.97826087\\n14       &lt;leaf&gt;   6   5.406735  virginica                                  0.00000000       0.16666667      0.83333333\\n15       &lt;leaf&gt;  40   0.000000  virginica                                  0.00000000       0.00000000      1.00000000\\n</code></pre>\\n\\n<p>Then, there are methods for pretty printing and plotting those objects. The key functions are a generic <code>tree:::plot.tree</code> method (I put a triple <code>:</code> which allows you to view the code in R directly) relying on <code>tree:::treepl</code> (graphical display) and <code>tree:::treeco</code> (compute nodes coordinates). These functions expect the <code>obj$frame</code> representation of the tree. Other subtle issues: (1) the argument <code>type = c(\"proportional\", \"uniform\")</code> in the default plotting method, <code>tree:::plot.tree</code>, help to manage vertical distance between nodes (<code>proportional</code> means it is proportional to deviance, <code>uniform</code> mean it is fixed); (2) you need to complement <code>plot(tr)</code> by a call to <code>text(tr)</code> to add text labels to nodes and splits, which in this case means that you will also have to take a look at <code>tree:::text.tree</code>.</p>\\n\\n<p>The <code>getTree</code> method from <code>randomForest</code> returns a different structure, which is documented in the online help. A typical output is shown below, with terminal nodes indicated by <code>status</code> code (-1). (Again, output will differ depending on the type of task, but only on the <code>status</code> and <code>prediction</code> columns.)</p>\\n\\n<pre><code>&gt; library(randomForest)\\n&gt; rf &lt;- randomForest(Species ~ ., data=iris)\\n&gt; getTree(rf, 1, labelVar=TRUE)\\n   left daughter right daughter    split var split point status prediction\\n1              2              3 Petal.Length        4.75      1       &lt;NA&gt;\\n2              4              5 Sepal.Length        5.45      1       &lt;NA&gt;\\n3              6              7  Sepal.Width        3.15      1       &lt;NA&gt;\\n4              8              9  Petal.Width        0.80      1       &lt;NA&gt;\\n5             10             11  Sepal.Width        3.60      1       &lt;NA&gt;\\n6              0              0         &lt;NA&gt;        0.00     -1  virginica\\n7             12             13  Petal.Width        1.90      1       &lt;NA&gt;\\n8              0              0         &lt;NA&gt;        0.00     -1     setosa\\n9             14             15  Petal.Width        1.55      1       &lt;NA&gt;\\n10             0              0         &lt;NA&gt;        0.00     -1 versicolor\\n11             0              0         &lt;NA&gt;        0.00     -1     setosa\\n12            16             17 Petal.Length        5.40      1       &lt;NA&gt;\\n13             0              0         &lt;NA&gt;        0.00     -1  virginica\\n14             0              0         &lt;NA&gt;        0.00     -1 versicolor\\n15             0              0         &lt;NA&gt;        0.00     -1  virginica\\n16             0              0         &lt;NA&gt;        0.00     -1 versicolor\\n17             0              0         &lt;NA&gt;        0.00     -1  virginica\\n</code></pre>\\n\\n<p>If you can manage to convert the above table to the one generated by <code>tree</code>, you will probably be able to customize <code>tree:::treepl</code>, <code>tree:::treeco</code> and <code>tree:::text.tree</code> to suit your needs. In particular, you probably want to get ride of the use of deviance, class probabilities, etc. which are not meaningful in RF. All you want is to set up nodes coordinates and split values. You could use <code>fixInNamespace()</code> for that, but, to be honest, I\\'m not sure this is the right way to go.</p>\\n\\n<p><strong>Third (and certainly cleaver) solution:</strong> Write a true <code>as.tree</code> helper\\n  function which will alleviates all of the above \"patches\". You could then use R\\'s plotting methods or, probably better, <a href=\"http://rosuda.org/Klimt/intro.html\">Klimt</a> (directly from R) to display individual trees.</p>\\n',\n",
       " '<p>Boxplots were really designed for normal data, or at least unimodal data. The Beanplot shows you the actual density curve, which is more informative.</p>\\n\\n<p>The shape is the density, and the short horizontal lines represent each data point. This combines the best of a boxplot, density plot, and rug plot all in one and is very readable.</p>\\n\\n<p>Unfortunately, the example that you\\'ve chosen decided to add a bunch of longer lines which clutter the graph beyond recognition (for me). [snip]</p>\\n\\n<p>EDIT: Having now worked with beanplot a bit more, the longer thick lines are the mean (or optionally median) for each bean. The longer thin lines are the data, with a sort of \"stacking\" where wider lines indicate more duplicate values. (You can also jitter them, which I prefer, but at least the \"normal\" category already has a fair density of points that jittering might make worse.)</p>\\n\\n<p>I still think the example you chose is a rather cluttered, which could perhaps be cleared up by using jittering instead of stacking.</p>\\n\\n<p>The <a href=\"http://www.jstatsoft.org/v28/c01/paper\" rel=\"nofollow\">paper that describes the R package for making bean plots</a> is a nice read.</p>\\n',\n",
       " '<p>You could try out read.table or read.csv <a href=\"http://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html\" rel=\"nofollow\">http://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html</a> is the link to the R manual. </p>\\n\\n<p>The important specification parameters include whether the file contains headers (head = T in case file contains headers), the separator specification - sep = \",\" for a csv file would work well. The specification on how to handle missing strings is by the na.strings parameter.</p>\\n\\n<p>say the variable filename contains the full path to the csv file then</p>\\n\\n<p><strong>Eg: read.table(filename, head = T, sep = \",\", na.strings = \"\")</strong> would readin a csv file with headers and replace the missing fields with blanks </p>\\n',\n",
       " '<p>For McKay\\'s distribution $X$ is a Gamma variate that is the sum of a subset of squares taken from the other, $Y$, which is the sum of a larger set of squares. Implying that $Y&gt;X$ with probability $1$.  See McKay\\'s original paper:</p>\\n\\n<blockquote>\\n  <p>McKay, A. T. (1934) <a href=\"http://www.jstor.org/stable/2983603\" rel=\"nofollow\">Sampling from batches</a>. <em>Journal of the Royal Statistical Society—Supplement</em> <strong>1</strong>: 207–216. </p>\\n</blockquote>\\n',\n",
       " '<p>My problem with understanding this expression might come from the fact that English is not my first language, but I don\\'t understand why it\\'s used in this way.</p>\\n\\n<p>The marginal mean is typically the mean of a group or subgroup\\'s measures of a variable in an experiment, but why not just use the word mean? What\\'s the marginal here for?</p>\\n\\n<p>See the <a href=\"http://en.wiktionary.org/wiki/marginal\" rel=\"nofollow\">definition of marginal from wiktionary</a>. </p>\\n',\n",
       " \"<p>You're in big trouble if you're asking us for gift advice.</p>\\n\",\n",
       " '<p>It is about sas programming. Thank you in advance.</p>\\n\\n<p>I build an ordinal logistic regression model on my data. </p>\\n\\n<pre><code>Level  Meaning\\n  3     Alert\\n  2    Unusual Event\\n  1    Non Emergency\\n</code></pre>\\n\\n<p>As you can see, it is my ordinal response variable. 3 means large loss and Alert situation, etc. I build a very simple model, the graph below shows the result:</p>\\n\\n<pre><code> proc logistic data = sasuser.Pacific_West_ outmodel=sasuser.Pacifici_West_model;;\\n     model Numerical_Emergency(event = \\'1\\') = AGE;\\n run;\\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/xBihb.jpg\" alt=\"enter image description here\"></p>\\n\\n<p>Everything looks fine. Now I would like to apply the BOOTSTRAP method to validate this model. What thing I would like to see is the test c value as ROC value. As you may see in the graph above, the c (ROC) value is 0.640. The basic idea of bootstrap is to generate new data set by random picking process from the data we used to train the model as we all know. I prefer to generate 1000 copies of data with same size of original data for building the model in graph above.</p>\\n\\n<p>THE PROBLEM:</p>\\n\\n<p>In the SAS Score statement, I just found outroc method which does not support the ordinal regression. Is there a simple procedure to help me to output the statistics about fitting of trained model on the validation dataset build by myself?</p>\\n\\n<p>Thank you so much for your time. Have a nice day!</p>\\n\\n<p>Wenhao SHE</p>\\n',\n",
       " '<p>I know most of you probably feel that Google Docs is still a primitive tool.  It is no Matlab or R and not even Excel.  Yet, I am baffled at the power of this web based software that just uses the operating capability of a browser (and is compatible with many browsers that work very differently).</p>\\n\\n<p>Mike Lawrence, active in this forum, has shared a spreadsheet with us using Google Docs doing some pretty fancy stuff with it.  I personally have replicated a pretty thorough hypothesis testing framework (including numerous parametric and nonparametric tests) originally done in Excel in Google Docs.  </p>\\n\\n<p>I am interested if any of you have given Google Docs a try and have pushed it to its limits in interesting applications.  I am also interested to hear about the bugs or flaws you have encountered with Google Docs</p>\\n\\n<p>I am designating this question \"for community wiki\" denoting that there are no best answers for this.  It is more of a survey than anything.  </p>\\n',\n",
       " '<p>This is a followup to <a href=\"http://stats.stackexchange.com/q/2597/977\">this</a> question. I am currently trying to implement the C-Index in order to find a near-optimal number of clusters from a hierarchy of clusters. I do this by calculating the C-Index for every step of the (agglomerative) hierarchical clustering. The problem is that the C-Index is minimal (0 to be exact) for very degenerated clusterings. Consider this:</p>\\n\\n<p>$c = \\\\\\\\frac{S-S_{min}}{S_{max}-S_{min}}$</p>\\n\\n<p>In this case $S$ is the sum of all distances between pairs of observations in the same cluster over all clusters. Let $n$ be the number of these pairs. $S_{min}$ and $S_{max}$ are the sums of $n$ lowest/highest distances across all pairs of observations. In the first step of the hierarchical clustering, the two closest observations (minimal distance) are merged into a cluster. Let $d$ be the distance between these observations. Now there is one pair of observations in the same cluster, so $n=1$ (all other clusters are singletons). Consequently $S=d$. The problem is that $S_{min}$ also equals $d$, because $d$ is the smallest distance (that is why the observations where merged first). So for this case, the C-Index is always 0. It stays 0 as long as only singleton clusters are merged. This means the optimal clustering according the C-Index would always consist of a bunch of clusters containing two observations, and the rest singletons. Does this mean that the C-Index is not applicable to hierarchical clustering? Am I doing something wrong? I have searched a lot, but could not find any suitable explanation. Can someone refer me to some resource that is freely available on the internet? Or, if not, at least a book I may try to get at my universities library?</p>\\n\\n<p>Thanks in advance!</p>\\n',\n",
       " '<p>I am performing simple (first-order terms) linear regression on data having several categorical variables (i.e. factors), and it is often desired that for each factor, one of the levels should add nothing to the regressand, and the other levels should add positive values to the regressand.  When I perform a regression analyses, however, I often get a lot of negative coefficients.</p>\\n\\n<p>Is there a <strong>non-manual</strong> way of choosing which levels of a factor should be used as regressor variables in order to maximize the number of positive coefficients in the equation?  In other words, how can I get R to do this (somewhat tedious) task for me?</p>\\n',\n",
       " '<p>The first thing you can do is, for example, interpret $\\\\\\\\hat{\\\\\\\\beta_2}$ as the estimated effect of $sex$ on the logit of the quantile you\\'re looking at. </p>\\n\\n<p>$\\\\\\\\exp\\\\\\\\{\\\\\\\\hat{\\\\\\\\beta_2}\\\\\\\\}$, similarly to \"classic\" logistic regression, is the odds ratio of median (or any other quantile) outcome in males versus females. The difference with \"classic\" logistic regression is how the odds are calculated: using your (bounded) outcome instead of a probability.</p>\\n\\n<p>Besides, you can always look at the predicted quantiles according to one covariate. Of course you have to fix (condition on) the values of the other covariates in your model (like you did in your example).</p>\\n\\n<p>By the way, the transformation should be $\\\\\\\\log(\\\\\\\\frac{y-y_{min}}{y_{max}-y})$.</p>\\n\\n<p>(This is not really intended to be an answer, as it\\'s just a (poor) rewording of what it\\'s written in <a href=\"http://onlinelibrary.wiley.com/doi/10.1002/sim.3781/abstract;jsessionid=5C98E7EAD10B98086592DF817A979D21.d03t03\" rel=\"nofollow\">this paper</a>, that you cited yourself. However, it was too long to be a comment and someone who doesn\\'t have access to on-line journals could be interested anyway).</p>\\n',\n",
       " '<p>Minimize SSE = &Sigma;<sub>i</sub> (z<sub>i</sub>-a)<sup>2</sup> with respect to a and you get the average of the two sample means, so it looks like your MLE is also your minimum MSE estimator, whether you have one (x,y) pair or several.  Not surprising, since the expression for the SSE and the log likelihood function are almost identical.</p>\\n',\n",
       " \"<p>I am having a conceptual hard time understanding where this formula came from. It does not seem to make any sense to me. Could someone shed some light on this:</p>\\n\\n<blockquote>\\n  <p>The natural estimator of $p$ is $\\\\\\\\hat p = \\\\\\\\frac{X}{n}$, the same fraction\\n  of success. Since $\\\\\\\\hat p$ is just $X$ multiplied by a constant, $\\\\\\\\hat p$ has an approximately normal distribution. $E(\\\\\\\\hat p) = p$ and\\n  $\\\\\\\\sigma_{\\\\\\\\hat p} = \\\\\\\\sqrt{\\\\\\\\frac{p(1-p)}{n}}$. Standardizing, this\\n  implies that:</p>\\n  \\n  <p>$$P\\\\\\\\left(-z_{\\\\\\\\alpha/2} \\\\\\\\lt \\\\\\\\frac{\\\\\\\\hat p- p}{\\\\\\\\sqrt{p(1-p)/n}} \\\\\\\\lt  z_{\\\\\\\\alpha/2}\\\\\\\\right) \\\\\\\\approx 1 - \\\\\\\\alpha $$</p>\\n</blockquote>\\n\\n<p>Could someone derive this equation so that it makes sense to someone who's never seen this before?  </p>\\n\",\n",
       " '<p><strong>Q1. Which to use? lme or lmer? does it matter?</strong>\\nEither is fine.  They will give you same fits.  <code>lme</code> will give you p-values, and <code>lmer</code> won\\'t, but that\\'s more than I want to get into here.  The most famous reference is one of Doug Bates\\'s posts to the R-help mailing list <a href=\"https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html\" rel=\"nofollow\">here</a>.</p>\\n\\n<p>(caveat: They do use slightly different algorithms so there are potentially some computationally difficult cases where one or the other might do better, but those are very rare in practice, and actually, most likely point to some kind of model misspecification. See <a href=\"http://stats.stackexchange.com/q/38524/3601\">Completely different results from lmer() and lme()</a>.)</p>\\n\\n<p><strong>Q2. is the main reason to use ML over REML because of use with ANOVA? This is not clear to me.</strong>\\nML is necessary because comparisons using REML are not valid when the fixed effects change. A possible useful related question is here: <a href=\"http://stats.stackexchange.com/a/16015/3601\">http://stats.stackexchange.com/a/16015/3601</a>.  To answer your question in the comment above, yes, when comparing models, REML should only be used if the models have the same fixed effects (that is to say, when only the random effects change).  The REML likelihood depends on which fixed effects are in the model, and so are not comparable if the fixed effects change.  REML is generally considered to give better estimates for the random effects, though, so the usual advice is to fit your best model using REML for your final inference and reporting.</p>\\n\\n<p><strong>Q3. is there a way to use stepAIC at this point?</strong>\\nTo compare between your 19 models that make sense in your situation, just compare the AIC for all of them.  No reason to use a stepwise procedure at all.  Stepwise procedures are generally seen as old-fashioned nowadays as they do not guarantee that the best model is found, and computers make it easy to compare lots of models.</p>\\n',\n",
       " '<p>The ASW is a measure of the coherence of a clustering solution. A high ASW value means that the clusters are homogeneous (all observations are close to cluster center), and that they are well separated. According to Kaufmann and Rousseuw (1990), a value below 0.25 means that the data are not structured. Between 0.25 and 0.5, the data might be structured, but it might also be an artifice. Please keep in mind that these values are indicative and should not be used as a decision threshold. These values are not theoretically defined (there are not based on some p-value) but are based on the experience of the authors. Hence, according to these low ASW values, your data seems to be quite unstructured. If the purpose of the cluster analysis is only descriptive, then you can argue that it reveals some (but only some) of the most salient patterns. However, I think that in your case, you should not draw any theoretical conclusions from your clustering.</p>\\n\\n<p>You can also try to have a look at the \"per cluster\" ASW values (this is given by the function <code>wcClusterQuality</code>). Maybe some of your clusters are well-defined and some may be \"spurious\" (ASW&lt;0), resulting in a low overall ASW value.</p>\\n\\n<p>You can try to use bootstrap strategies, which should give you a better hint. In R, the function <code>clusterboot</code> in the package <code>fpc</code> can be used for this purpose (look at the help page). However, it does not work with weighted data. If your data are unweighted, I think it is worth to give it a try.</p>\\n\\n<p>Finally, you may want to have a closer look at your data and your categorization. Maybe, your categories are too instable or not well defined. However, it does not seem to be the case here.</p>\\n\\n<p>As you have said, \"lack of clearly differentiated clusters is not the same thing as a lack of interesting variation\". There are other methods to analyse the variability of your sequences such as discrepancy analysis. These methods allow you to study the links between sequences and explanatory factors. You may, for instance, try to build sequence regression trees (function \"seqtree\" in package TraMineR).</p>\\n',\n",
       " '<p>Yes, A hidden Markov model is a bivariate stochastic process\\n                 $\\\\\\\\{(X_t,Y_t)\\\\\\\\}_{t\\\\\\\\geq0}$, defined on a product space $(\\\\\\\\mathsf{X}\\n                 \\\\\\\\times \\\\\\\\mathsf{Y},\\\\\\\\mathcal{X} \\\\\\\\otimes \\\\\\\\mathcal{Y})$, where\\n                 $\\\\\\\\{X_t\\\\\\\\}_{t\\\\\\\\geq0}$ is a Markov chain taking\\n                 values in $\\\\\\\\mathsf{X}$ and $\\\\\\\\{Y_t\\\\\\\\}_{t\\\\\\\\geq0}$, defined on some state\\n                 space $\\\\\\\\mathsf{Y}$, is a sequence of observable random\\n                 variables conditional on $\\\\\\\\{X_t\\\\\\\\}_{t\\\\\\\\geq0}$.</p>\\n',\n",
       " '<p>Is it OK to use the Kolmogorov-Smirnov goodness-of-fit test to compare two empirical distributions to determine whether they appear to have come from the same underlying distribution, rather than to compare one empirical distribution to a pre-specified reference distribution?</p>\\n\\n<p>Let me try asking this another way.  I collect N samples from some distribution at one location.  I collect M samples at another location.  The data is continuous (each sample is a real number between 0 and 10, say) but not normally distributed. I want to test whether these N+M samples all come from the same underlying distribution.  Is it reasonable to use the Kolmogorov-Smirnov test for this purpose?</p>\\n\\n<p>In particular, I could compute the empirical distribution $F_0$ from the $N$ samples, and the empirical distribution $F_1$ from the $M$ samples.  Then, I could compute the Kolmogorov-Smirnov test statistic to measure the distance between $F_0$ and $F_1$: i.e., compute $D = \\\\\\\\sup_x |F_0(x) - F_1(x)|$, and use $D$ as my test statistic as in the Kolmogorov-Smirnov test for goodness of fit.  Is this a reasonable approach?</p>\\n\\n<p>(I read elsewhere that the Kolmogorov-Smirnov test for goodness of fit <a href=\"http://stats.stackexchange.com/questions/1047/is-kolmogorov-smirnov-test-valid-with-discrete-distributions\">is not valid for discrete distributions</a>, but I admit I don\\'t understand what this means or why it might be true.  Does that means my proposed approach is a bad one?)</p>\\n\\n<p>Or, do you recommend something else instead?</p>\\n',\n",
       " '<p>I have performance review data and scores (ranging from 1 to 4) for employees of a company. I need to show the company average over the past year. However, the employees were only ever reviewed for a few weeks in a row at a time.</p>\\n\\n<p>For example:<br>\\nRoger was reviewed from Jan 1 through March 1<br>\\nSteve was reviewed from August 15 through September 15<br>\\n...etc...</p>\\n\\n<p>Right now, I\\'m taking all of the reviews over the year (Jan 1 through Dec 31) and for each day I\\'m computing the average for that day. I then take all of those days and plot the average score for that day on a line graph with time on the X axis and score on the Y axis.</p>\\n\\n<p>The problems I\\'m having are:</p>\\n\\n<ul>\\n<li>there are gaps of time in these reviews (no one was reviewed in February, 20 people were reviewed in March)</li>\\n<li>the higher performing employees may have been reviewed at one part of the year, and the lower performing employees at another</li>\\n<li>when I plot this data the average produces a zigzag looking line chart. for example, day 1 the average is 3.4, but day 2 is 1.3. over time this looks like a lot of spikes.</li>\\n</ul>\\n\\n<p>So, where I\\'m at now is thinking of taking each employee as a \"series\" (where some series (employees) have data ranging for a few weeks, and some have data ranging a few months) and normalizing them together and computing the average of that normalization.</p>\\n\\n<p>I\\'d greatly appreciate any help on this and the best way to present this data! </p>\\n',\n",
       " '<p>Ok I am guessing this is a trivial question however having pondered it for a few days the only thing I have become clear on is my lack of statistical prowess. Yesterday I asked a question on fitting linear regression to a dataset piecewise and <a href=\"http://stackoverflow.com/questions/14337439/piece-wise-linear-and-non-linear-regression-in-r\">this was successfully answered</a>. Now I would like to compare these fits to a lognormal fit of the whole dataset with the reason that this is a standard method for comparing such data (even if it really doesn\\'t fit well); I basically want to highlight that it does not fit as well. So without further ado here is the data:</p>\\n\\n<pre><code>x&lt;-c(1e-08, 1.1e-08, 1.2e-08, 1.3e-08, 1.4e-08, 1.6e-08, 1.7e-08, \\n1.9e-08, 2.1e-08, 2.3e-08, 2.6e-08, 2.8e-08, 3.1e-08, 3.5e-08, \\n4.2e-08, 4.7e-08, 5.2e-08, 5.8e-08, 6.4e-08, 7.1e-08, 7.9e-08, \\n8.8e-08, 9.8e-08, 1.1e-07, 1.23e-07, 1.38e-07, 1.55e-07, 1.76e-07, \\n1.98e-07, 2.26e-07, 2.58e-07, 2.95e-07, 3.25e-07, 3.75e-07, 4.25e-07, \\n4.75e-07, 5.4e-07, 6.15e-07, 6.75e-07, 7.5e-07, 9e-07, 1.15e-06, \\n1.45e-06, 1.8e-06, 2.25e-06, 2.75e-06, 3.25e-06, 3.75e-06, 4.5e-06, \\n5.75e-06, 7e-06, 8e-06, 9.25e-06, 1.125e-05, 1.375e-05, 1.625e-05, \\n1.875e-05, 2.25e-05, 2.75e-05, 3.1e-05)\\n\\ny2&lt;-c(-0.169718017273307, 7.28508517630734, 71.6802510299446, 164.637259265704, \\n322.02901173786, 522.719633360006, 631.977073772459, 792.321270345847, \\n971.810607095548, 1132.27551798986, 1321.01923840546, 1445.33152600664, \\n1568.14204073109, 1724.30089942149, 1866.79717333592, 1960.12465709003, \\n2028.46548012508, 2103.16027631327, 2184.10965255236, 2297.53360080873, \\n2406.98288043262, 2502.95194879366, 2565.31085776325, 2542.7485752473, \\n2499.42610084412, 2257.31567571328, 2150.92120390084, 1998.13356362596, \\n1990.25434682546, 2101.21333152526, 2211.08405955931, 1335.27559108724, \\n381.326449703455, 430.9020598199, 291.370887491989, 219.580548355043, \\n238.708972427248, 175.583544448326, 106.057481792519, 59.8876372379487, \\n26.965143266819, 10.2965349811467, 5.07812046132922, 3.19125838983254, \\n0.788251933518549, 1.67980552001939, 1.97695007279929, 0.770663673279958, \\n0.209216903989619, 0.0117903221723813, 0.000974437796492681, \\n0.000668823762763647, 0.000545308757270207, 0.000490042305650751, \\n0.000468780182460397, 0.000322977916070751, 0.000195423690538495, \\n0.000175847622407421, 0.000135771259866332, 9.15607623591363e-05)\\n</code></pre>\\n\\n<p>You can see a plot of the data at the link above for quick interpretation.</p>\\n\\n<p>So to my question- how to regress a lognormal curve to it? My school stats says this is not univariate data so I cannot use something like fitdistr in R. Am I right? If this is the case how might a find an approximate lognormal curve for this data if at all? Any help or pointers in a relevant direction would be great.</p>\\n\\n<p>[Edit]\\nWith reference to the negative value I am wondering whether there is an approach that can handle it? This is because I have a second set of why values as shown below that are all negative and I would like to fit a similar model to these as well.</p>\\n\\n<pre><code>y3&lt;-c(0.0530500094068018, 0.160928860126123, -0.955328071740233, \\n-2.53940686389203, -7.3241148240459, -8.32055726147533, -10.1979192158835, \\n-12.0304091519687, -16.2527095992605, -19.9106624262052, -24.5089014234888, \\n-28.7705733263437, -31.4294017506492, -35.8411743936776, -37.9005809712801, \\n-40.2384353560669, -42.5902603334382, -44.6732472915729, -47.6606530197197, \\n-54.2197956720375, -58.6590075712884, -61.5736755669354, -65.7179971091261, \\n-63.1056155765268, -65.5404821687269, -59.2950249004724, -57.7385677458644, \\n-63.3729518981994, -56.9303570422243, -69.2878310104119, -60.3990492926747, \\n-14.1184714024129, -8.29495412660418, -3.44061704811896, 0.473805205298244, \\n-5.47720584050456, -4.02534147802113, -2.13820456379, -0.641737730083625, \\n0.25648844079225, 0.697033621376916, 0.622208830019496, 0.276775608633299, \\n0.219104625574544, -0.0411577088679307, -0.0191732850594168, \\n-0.0210255752601919, -0.0156084146143567, -0.00423245275017332, \\n-0.000297816450447215, -1.24063266749809e-05, -1.53585832955629e-05, \\n-2.32966128710771e-05, -2.39386641905819e-05, -2.15491949944693e-05, \\n-1.50167366691665e-05, -8.3066700184436e-06, -7.89314461608438e-06, \\n-6.20937293357605e-06, -3.64313623751525e-06)\\n</code></pre>\\n',\n",
       " \"<p>This kind of task is solved by ANCOVA (analysis of covariance). According to its model, weight is dependent on two effects (apart from constant), the group effect and the covariate (height) effect: $weight=constant+group+height$. Here group effect is clean, in the sense that the possible difference the two groups in average height is washed out. So you may safely rely on significance of group effect. Before you do the above analysis you should make sure that the strength of dependency of weight on height doesn't differ in the two groups. To do it, try the model with the interaction term: $weight=constant+group+height+group*height$. If the interaction is nonsignificant you can turn to the above two-effect model (while if it is significant you should apply a nested model that is a bit more complex).</p>\\n\\n<p>The above simple approach however assumes that weight is dependent on height linearly, and we know that it is certainly not true. Another nasty thing is that weight depends on height heteroscedastically, that is, variation of weight is larger for big heights than for small heights. What to do? One way is to transform weight prior to the analysis so that weight by height scatter-cloud is about linear and homoscedastic. Another opportunity is to try generalized linear model instead of classic ANCOVA. That procedure offers various link functions which in fact perform the transformation for you implicitly.</p>\\n\",\n",
       " '<p><strong>Variogram</strong> is simple but effective way to investigated the spatial variation in the variable of interest in the field being studied. However it has been reported that variogram cannot be considered as the most satisfactory model especially where there are curvature <em>(e.g., permeable channels in reservoir engineering)</em> in the medium being studied. That is different patterns could have more and less exact copy of the same variogram.<br>\\nA suggestion for development an approach was <strong>multi-point statistics</strong> (<strong>geostatistics</strong>), in which for example, a <strong>training image</strong>, a 2D matrix template is being used for inference of spatial variation. It was shown in the literature that training images are useful to dictate the output the desired pattern honoring conditioning data and being based on stochastic approaches to satisfy statistical inferences.  </p>\\n\\n<ul>\\n<li>Questions:</li>\\n</ul>\\n\\n<blockquote>\\n  <p><strong>Q1:</strong> What else about advantages and or disadvantages of multi-point statistics ?<br>\\n  <strong>Q2:</strong> How to obtain training images?<br>\\n  __<strong>Q2.1:</strong> How to generate training images using existing knowledge etc?<br>\\n  <strong>Q3:</strong> How about 3D and further dimensions?<br>\\n  <strong>Q3:</strong> What is the next stage after training image?  </p>\\n</blockquote>\\n',\n",
       " '<p>I am comparing tree species use vs. availability within bird territories. I am using the R <code>compana()</code> function which uses Aebischer et al. (1993) method (package <a href=\"http://cran.r-project.org/web/packages/adehabitatHS/index.html\" rel=\"nofollow\">adehabitatHS</a>) to determine if there are significant differences in proportional availability and proportional use of different tree species. </p>\\n\\n<p>I am trying to determine if the input to the program needs to be proportions (of both use and availability) or log-ratios of use and availability.</p>\\n\\n<p>Additionally, I am also trying to figure out whether or not the <code>compana</code> code for compositional analysis automatically replaces missing values in the available matrix by using the weighted mean $\\\\\\\\lambda$ when you specify <code>test=\"randomization\"</code>, which means that randomisation tests are performed for both the habitat ranking and the test of habitat selection (check out CRAN website for more information). I just can’t figure out if <code>compana</code> does the replacement of zeros in the available matrix automatically or if you need to do something else with the code.</p>\\n',\n",
       " '<p>Besides the various measures of <strong>Minimum Description Length</strong> (e.g., normalized maximum likelihood, Fisher Information approximation), there are two other methods worth to mention:  </p>\\n\\n<ol>\\n<li><p><strong>Parametric Bootstrap</strong>. It\\'s a lot easier to implement than the demanding MDL measures. A nice paper is by Wagenmaker and colleagues:<br>\\nWagenmakers, E.-J., Ratcliff, R., Gomez, P., &amp; Iverson, G. J. (2004). <a href=\"http://www.ejwagenmakers.com/2004/PBCM.pdf\">Assessing model mimicry using the parametric bootstrap</a>. <em>Journal of Mathematical Psychology</em>, 48, 28-50.<br>\\nThe abstract:</p>\\n\\n<blockquote>\\n  <p>We present a general sampling\\n  procedure to quantify model mimicry,\\n  defined as the ability of a model to\\n  account for data generated by a\\n  competing model. This sampling\\n  procedure, called the parametric\\n  bootstrap cross-fitting method (PBCM;\\n  cf. Williams (J. R. Statist. Soc. B 32\\n  (1970) 350; Biometrics 26 (1970) 23)),\\n  generates distributions of differences\\n  in goodness-of-fit expected under each\\n  of the competing models. In the data\\n  informed version of the PBCM, the\\n  generating models have specific\\n  parameter values obtained by fitting\\n  the experimental data under\\n  consideration. The data informed\\n  difference distributions can be\\n  compared to the observed difference in\\n  goodness-of-fit to allow a\\n  quantification of model adequacy. In\\n  the data uninformed version of the\\n  PBCM, the generating models have a\\n  relatively broad range of parameter\\n  values based on prior knowledge.\\n  Application of both the data informed\\n  and the data uninformed PBCM is\\n  illustrated with several examples.  </p>\\n</blockquote>\\n\\n<p><strong>Update: Assessing model mimicry in plain English.</strong> You take one of the two competing models and randomly pick a set of parameters for that model (either data informed or not). Then, you produce data from this model with the picked set of parameters. Next, you let both models fit the produced data and check which of the two candidate models gives the better fit. If both models are equally flexible or complex, the model from which you produced the data should  give a better fit. However, if the other model is more complex, it could give a better fit, although the data was produced from the other model. You repeat this several times with both models (i.e., let both models produce data and look which of the two fits better). The model that \"overfits\" the data produced by the other model is the more complex one.</p></li>\\n<li><p><strong>Cross-Validation</strong>: It is also quite easy to implement. <a href=\"http://stats.stackexchange.com/questions/1826/cross-validation-in-plain-english\">See the answers to this question</a>. However, note that the issue with it is that the choice among the sample-cutting rule (leave-one-out, K-fold, etc) is an unprincipled one. </p></li>\\n</ol>\\n',\n",
       " \"<p>From wikipedia: </p>\\n\\n<p>Tests of univariate normality include D'Agostino's K-squared test, the Jarque–Bera test, the Anderson–Darling test, the Cramér–von Mises criterion, the Lilliefors test for normality (itself an adaptation of the Kolmogorov–Smirnov test), the Shapiro–Wilk test, the Pearson's chi-squared test, and the Shapiro–Francia test. A 2011 paper from The Journal of Statistical Modeling and Analytics [1] concludes that Shapiro-Wilk has the best power for a given significance, followed closely by Anderson-Darling when comparing the Shapiro-Wilk, Kolmogorov-Smirnov, Lilliefors, and Anderson-Darling tests.</p>\\n\",\n",
       " \"<p>I have (trivariate: multivariate with three variables) data that appears to be good empirical and reasonable theoretical fit for a (univariate) convolution of an exponential and a normal distribution (some times called exp-norm or exGauss distributions).  </p>\\n\\n<p>My data are samples from the joint distribution:  J(R,G,B)</p>\\n\\n<p>It appears that the marginals of R,G,B follow:</p>\\n\\n<p>$R=R_N+R_E$ with $R_N \\\\\\\\sim N(\\\\\\\\mu_R,\\\\\\\\sigma^2_R)$ and $R_E \\\\\\\\sim Exp(\\\\\\\\lambda_R)$</p>\\n\\n<p>(and likewise for G,B).</p>\\n\\n<p>I would like to effectively summarize the marginal, conditional, and joint distributions.  The main purpose of the summary is to compare these distributions (marginal, conditional, and joint) with other distributions generated in a like manner.  My difficulty is that (1) I don't know a form for the joint (or the conditional) distribution and (2) following from that, I have no parameters to estimate.</p>\\n\\n<p>Thus, I need either a distribution free (non-parametric) approach to working with this data or I need to figure out a multivariate distribution that matches the data and the form of the marginals.  Or should I think about other options?</p>\\n\",\n",
       " '<p>Why not specify it further? I don\\'t think that any one model you would produce could be better or good enough than a specific choice.</p>\\n\\n<p>With that said, if you can narrow down your choices a bit to those you can test for, and the data input can be standardized, then why not write an automated testing procedure in R?</p>\\n\\n<p>Say you decide your data will fall within a range to be estimated by five models as well as one \"fallback\". Say you can characterize the input by different tests. Then just go ahead and write an R (or a program like that) algorithm that runs this for you. This works if you could produce a flowchart of which model to run based on test data, that is if any point of the decision tree is binary.</p>\\n\\n<p>If this is not an option because the decision may not be binary, I suggest you implement a rating system based on applicable tests and run some \"extreme cases\" simulated data through your grid to see if the results are what you are looking for. </p>\\n\\n<p>You can combine these things obviously, for example testing for non-stationarity may be give a definitve yes-no, while other attributes may fall into a range such as multicollinearity.<br>\\nYou can draw this out on paper first, then build it, simulate it with known distributions you expect to have. </p>\\n\\n<p>Then just run the R program everytime new data arrives.\\nI see no need to combine several models with the computational capabilities you most likely have at hand.</p>\\n',\n",
       " '<p>Here is the description of the LARS algorithm: <a href=\"http://www-stat.stanford.edu/~tibs/lasso/simple.html\" rel=\"nofollow\">http://www-stat.stanford.edu/~tibs/lasso/simple.html</a> It kind of ignores the correlation between the regressors so I would venture to guess that it might miss out on the fit in case of multicollinearity.</p>\\n',\n",
       " '<p>I have collected a bunch of statistical results in the form of \"YES\" and \"NO\" strings.</p>\\n\\n<p>Now I would like to have a summary cell displaying \"YES\" if all these cells equal \"YES\" (or are empty).</p>\\n',\n",
       " '<p>Sums of bernoullis are distributed exactly binomial, so one often would use logistic regression.</p>\\n',\n",
       " '<p>If I difference a time series and take out trend and seasonality ... does it mean we are left with only irregularity on which we plot the acf and pacf to arrive at the MA and AR order?</p>\\n\\n<p>Do 1st difference, 2nd difference always detrend the series, or do we need to detrend separately?</p>\\n',\n",
       " \"<p>The overall complexity of RF is something like $\\\\\\\\text{ntree}\\\\\\\\cdot\\\\\\\\text{mtry}\\\\\\\\cdot(\\\\\\\\text{# objects})\\\\\\\\log( \\\\\\\\text{# objects})$; if you want to speed your computations up, you can try the following:</p>\\n\\n<ol>\\n<li>Use <code>randomForest</code> instead of <code>party</code>.</li>\\n<li>Don't use formula, i.e. call <code>randomForest(predictors,decision)</code> instead of <code>randomForest(decision~.,data=input)</code>.</li>\\n<li>Use <code>do.trace</code> argument to see the OOB error in real-time; this way you may detect that you can lower <code>ntree</code>.</li>\\n<li>About factors; RF (and all tree methods) try to find an optimal subset of levels thus scanning $2^\\\\\\\\text{(# of levels-1)}$ possibilities; to this end it is rather naive this factor can give you so much information -- not to mention that randomForest won't eat factors with more than 32 levels. Maybe you can simply treat it as an ordered one (and thus equivalent to a normal, numeric variable for RF) or cluster it in some groups, splitting this one attribute into several?</li>\\n<li>Check if your computer haven't run out of RAM and it is using swap space. If so, buy a bigger computer.</li>\\n<li>Finally, you can extract some random subset of objects and make some initial experiments on this.</li>\\n</ol>\\n\",\n",
       " \"<p>Similarity is quantity that reflects the strength of relationship between two objects or two features. This quantity is usually having range of either -1 to +1 or normalized into 0 to 1.\\nThan you need to calculate the distance of two features by one of the methods below:</p>\\n\\n<ol>\\n<li>Simple Matching distance</li>\\n<li>Jaccard's distance</li>\\n<li>Hamming distance</li>\\n<li>Jaccard's coefficient</li>\\n<li>simple matching coefficient </li>\\n</ol>\\n\\n<p>For line... you can represent it by angle (a) and length (l) properties or L1= P1(x1,y1), P2(x2,y2) below is the similarity with a and l.</p>\\n\\n<p>now measure the angle for angles and lengths</p>\\n\\n<ul>\\n<li>A_user =20 and Length_User =50</li>\\n<li>A_teacher30 and Length_Teacher =55</li>\\n<li>Now, normalize the values.</li>\\n</ul>\\n\\n<p>Using euclidean distance</p>\\n\\n<p><strong>similarity = SquareRoot((A_user - A_teacher30 )^2 +(Length_User - Length_Teacher )^2)</strong></p>\\n\\n<p>gives the similarity measure. You can also use above mentioned methods based on the problem and the features.</p>\\n\",\n",
       " \"<p>What question are you trying to answer?</p>\\n\\n<p>If you want an overall test of anything going on, The null is that both main effects and the interaction are all 0, then you can replace all the data points with their ranks and just do a regular ANOVA to compare against an intercept/grand mean only model.  This is basically how many of the non-parametric tests work, using the ranks transforms the data to a uniform distribution (under the null) and you get a good approximation by treating it as normal (the Central Limit Theorem applies for the uniform for sample sizes above about 5 or 6).</p>\\n\\n<p>For other questions you could use permutation tests.  If you want to test one of the main effects and the interaction together (but allow the other main effect to be non-zero) then you can permute the predictor being tested.  I you want to test the interaction while allowing both main effects to be non-zero then you can fit the reduced model of main-effects only and compute the fitted values and residuals, then randomly permute the residuals and add the permuted residuals back to the fitted values and fit the full anova model including the interaction.  Repeat this a bunch of times to get the null distribution for the size of the interaction effect to compare with the size of the interaction effect from the original data. </p>\\n\\n<p>There may be existing SAS code for doing things like this, I have seen some basic tutorials on using SAS for bootstrap and permutation tests (the quickest way seems to be using the data step to create all the datasets in one big table, then using by processing to do the analyses).  Personally I use R for this type of thing so can't be of more help in using SAS.</p>\\n\",\n",
       " \"<p>Yes, it probably shouldn't be used as an estimate of hazard, and as Elvis points out the rest of the paper notes that Breslow didn't really intend it for that purpose.</p>\\n\\n<p>Continuing the quotation of Burr's note;</p>\\n\\n<p><em>The estimator should not be viewed as such, for it is inconsistent as an estimator of $\\\\\\\\lambda$ in the Cox model (although erroneous use of (1.2) has occurred in the literature). This inconsistency of $\\\\\\\\hat\\\\\\\\lambda$ is well known, but the result has not been written down explicitly. The purpose of this note is to do that.</em></p>\\n\",\n",
       " '<p>Could anybody please shed some light on me? I have $N$ individuals, divided into $K$ groups with $N&gt;K$. Some groups have 1 individuals only, some other groups have more individuals.</p>\\n\\n<p>Each individual has 4 features (4 variables): $F_0, F_1, F_2,$ and $F_3$, where $F_0$ is simply the group indicator. Therefore the data matrix is of size $N \\\\\\\\times 4$. There is also a weight vector $w$ of length $N$, which gives the weights for each of the $N$ individuals in the regression.</p>\\n\\n<p>May I ask if the following model is a random-intercept model?</p>\\n\\n<ol>\\n<li>There is a common beta for all N individuals.</li>\\n<li>Each group has a different within group regression line (same slope but different intercepts).</li>\\n<li>The regression line within each group crosses the \"cloud\" consisting of the group members. And the individual residuals scatter around the regression line, within each group.</li>\\n</ol>\\n\\n<p>This sounds like a \"random-intercept\" model to me. However, how do I explicitly write out the equation?</p>\\n\\n<p>$$ {\\\\\\\\boldsymbol y} = {\\\\\\\\bf X} \\\\\\\\beta + {\\\\\\\\bf Z} b + \\\\\\\\varepsilon $$ </p>\\n\\n<p>More specifically, with the three feature variables $F_1, F_2$ and $F_3$ and the group indicator variable $F_0$. I am having difficulty writing out $X$ and $Z$ explicitly. Moreover, my $F_2$ is a factor variable.</p>\\n\\n<p>Could anybody please show me how the $X$ and $Z$ matrices look like explicitly? And what do the \"$b$\"\\'s represent here?</p>\\n\\n<p>And how do I set up the weights in LME in <code>R</code>? If I would like to have \"group\"-weights and \"individual\"-weights, how shall I do it?</p>\\n\\n<p>Thank you!</p>\\n\\n<p>[ps1 @ Macro]\\nThanks a lot Macro for your very comprehensive answer. I am continuing digesting your writings and I will consult with you  and your expertise with more questions. My first question is: you think the condition 2 and condition 3 conflict each other. </p>\\n\\n<p>I don\\'t understand why. Here is my thinking: even though for a certain group, the overall beta is a very crappy one for that group. </p>\\n\\n<p>By adjusting the intercept for that group, I at least still can get the regression line passing the cloud, am I right? </p>\\n\\n<p>More specifically, in a random intercept model (all group sharing the same beta), will the following situation arise?</p>\\n\\n<p>All data points in a group reside on one side of the regression line of that group?</p>\\n\\n<hr>\\n\\n<p>In a random intercept model, for each group, which point does the regression line pass thru? In the general OLS, we knew that it\\'s the \\npoint (xbar, ybar) that is passed thru by the regression line. </p>\\n\\n<p>But for each group in a random intercept model, which point is that \"central\" point? </p>\\n\\n<hr>\\n\\n<p>Moreover, in R, what\\'s a convenient way to visualize what\\'s happening within a certain group in a mixed model? </p>\\n\\n<p>Thanks a lot for your help!</p>\\n',\n",
       " '<p>The objective of this research was to investigate the long-term effects of irrigation with treated waste water on some chemical soil properties. </p>\\n\\n<p>The investigation was carried out by comparison of soil properties in two different fields: one irrigated with the effluent from Parkan Waste water Treatment Plant over a period of six years, and the other one irrigated with water over the same period of time. Soil samples were taken from different depths of 0-15, 15-30, 30-60, 60-100 and 100-150 cm in both fields, and analyzed for various chemical properties. </p>\\n\\n<p>For visual summaries, I am going to plot depths of soil and element (with line and bar plots). However, I also need to fit a linear model with one categorical and one continuous predictor. How can this be done in R?</p>\\n\\n<p>Thanks for your answer!</p>\\n',\n",
       " \"<p>You should almost certainly go for Spearman's rho or Kendall's tau. Often, if the data is non-normal but variances are equal, you can go for Pearson's r as it doesnt make a huge amount of difference. If the variances are significantly different, then you need a non parametric method. </p>\\n\\n<p>You could probably cite almost any introductory statistics textbook to support your use of Spearman's rho.</p>\\n\\n<p>Update: if the assumption of linearity is violated, then you should not be using a correlation coefficient on your data, as they all assume a linear relationship (AFAIK). You could look at log transforming your data as this might deal with the non-linearity. </p>\\n\",\n",
       " '<p>You can use Markov chains. You will have a to specify a density $p(x_t|x_{t-1})$. Of course you will have to be able to sample from that marginal. Then just sample...</p>\\n',\n",
       " '<p>I am using R to replicate a study and obtain mostly the same results the author reported. At one point, however, I calculate marginal effects that seem to be unrealistically small. I would greatly appreciate if you could have a look at my reasoning and the code below and see if I am mistaken at one point or another.</p>\\n\\n<p>My sample contains 24535 observations, the dependent variable <code>x028bin</code> is a binary variable taking on the values 0 and 1, and there are furthermore 10 explaining variables. Nine of those independent variables have numeric levels, the independent variable <code>f025grouped</code> is a factor consisting of different religious denominations.</p>\\n\\n<p>I would like to run a probit regression including dummies for religious denomination and then compute marginal effects. In order to do so, I first eliminate missing values and use cross-tabs between the dependent and independent variables to verify that there are no small or 0 cells. Then I run the probit model which works fine and I also obtain reasonable results:</p>\\n\\n<pre><code>probit4AKIE &lt;- glm(x028bin ~ x003 + x003squ + x025secv2 + x025terv2 + x007bin + x04chief + x011rec + a009bin + x045mod + c001bin + f025grouped, family=binomial(link=\"probit\"), data=wvshm5red2delna, na.action=na.pass)\\n\\nsummary(probit4AKIE)\\n</code></pre>\\n\\n<p>However, when calculating marginal effects with all variables at their means from the probit coefficients and a scale factor, the marginal effects I obtain are much too small (e.g. 2.6042e-78). The code looks like this:</p>\\n\\n<pre><code>ttt &lt;- cbind(wvshm5red2delna$x003,\\n    wvshm5red2delna$x003squ, wvshm5red2delna$x025secv2, wvshm5red2delna$x025terv2,\\nwvshm5red2delna$x007bin, wvshm5red2delna$x04chief, wvshm5red2delna$x011rec,\\n    wvshm5red2delna$a009bin, wvshm5red2delna$x045mod, wvshm5red2delna$c001bin,\\nwvshm5red2delna$f025grouped, wvshm5red2delna$f025grouped,wvshm5red2delna$f025grouped,\\n    wvshm5red2delna$f025grouped,wvshm5red2delna$f025grouped,wvshm5red2delna$f025grouped,\\nwvshm5red2delna$f025grouped,wvshm5red2delna$f025grouped,\\nwvshm5red2delna$f025grouped) #I put variable \"f025grouped\" 9 times because this variable consists of 9 levels\\n\\nttt &lt;- as.data.frame(ttt)\\n\\nxbar &lt;- as.matrix(mean(cbind(1,ttt[1:19]))) #1:19 position of variables in dataframe ttt\\n\\nbetaprobit4AKIE &lt;- probit4AKIE$coefficients\\n\\nzxbar &lt;- t(xbar) %*% betaprobit4AKIE\\n\\nscalefactor &lt;- dnorm(zxbar)\\n\\nmarginprobit4AKIE &lt;- scalefactor * betaprobit4AKIE[2:20] \\n\\n#(2:20 are the positions of variables in the output of the probit model \\'probit4AKIE\\' \\n#(variables need to be in the same ordering as in data.frame ttt), the constant in   \\n#the model occupies the first position)\\n\\nmarginprobit4AKIE #in this step I obtain values that are much too small\\n</code></pre>\\n',\n",
       " '<p>I find chapter 7 of Hastie, Tibshirani, Friedman\\'s <a href=\"http://www-stat.stanford.edu/~tibs/ElemStatLearn/\" rel=\"nofollow\">Elements of Statistical Learning</a> to be a good reference for CV and how and why it is used.</p>\\n',\n",
       " '<p>I am not sure, but I think your probability model is a special case of <a href=\"http://en.wikipedia.org/wiki/Multinomial_logit\" rel=\"nofollow\">Multinomial logit</a> model with no covariates and only the intercept terms ($w_i$ will be the intercepts).</p>\\n\\n<p>This is model is a special case of GLM and hence there exits an iteratively weighted least square method (IRWLS) to get the maximum likelihood estimates of $w_i$.</p>\\n\\n<p>If you don\\'t want to code the IRWLS algorithm yourself, please check the <code>polr</code> function in the <code>MASS</code> library in R to accomplish the ML estimation of $w_i$.</p>\\n\\n<p><strong>EDIT</strong></p>\\n\\n<p>As @whuber points out, unless you adopt a Bayesian approach, there is no distribution for the parameters (assumed fixed), but there exits one for the estimates (here: ML) as they are estimated from the data, hence the randomness.</p>\\n\\n<p>HTH</p>\\n\\n<p>S.</p>\\n',\n",
       " '<p><code>pnorm</code> is giving you the cumulative probability distribution at a specified value of $x$.  This is the cumulative probability for a standard normal distribution. So in your example the quantity I call $x$ is specified as $0$ and $0.5$ as the answer is $P[X\\\\\\\\leq 0]$ for a random variable $X$ with a $N(0,1)$ distribution.If you took $x=1.96$ you would get $0.975$ because a standard normal distribution has probability $0.025$ in the upper right tail above $1.96$.</p>\\n',\n",
       " '<p>I understand your difficulty as I have a similar problem when I try to do something new in statistics (I\\'m also a grad student, but in a different field). I have found examining the R code quite useful to get an idea how something is calculated. For example, I have been recently learning how to use <code>kmeans</code> clustering and have many basic questions, both conceptual and how it is implemented. Using an <code>R</code> installation (I recommend <code>R Studio</code>, <a href=\"http://www.rstudio.org/\" rel=\"nofollow\">http://www.rstudio.org/</a>, but any installation works), just type <code>kmeans</code> in to the command line. Here is an example of <em>part</em> of the output:</p>\\n\\n<pre><code>x &lt;- as.matrix(x)\\n    m &lt;- nrow(x)\\n    if (missing(centers)) \\n        stop(\"\\'centers\\' must be a number or a matrix\")\\n    nmeth &lt;- switch(match.arg(algorithm), `Hartigan-Wong` = 1, \\n        Lloyd = 2, Forgy = 2, MacQueen = 3)\\n    if (length(centers) == 1L) {\\n        if (centers == 1) \\n            nmeth &lt;- 3\\n        k &lt;- centers\\n        if (nstart == 1) \\n            centers &lt;- x[sample.int(m, k), , drop = FALSE]\\n        if (nstart &gt;= 2 || any(duplicated(centers))) {\\n            cn &lt;- unique(x)\\n            mm &lt;- nrow(cn)\\n            if (mm &lt; k) \\n                stop(\"more cluster centers than distinct data points.\")\\n            centers &lt;- cn[sample.int(mm, k), , drop = FALSE]\\n        }\\n    } \\n</code></pre>\\n\\n<p>I\\'m not sure how practical it is to examine the source every time, but it really helps me get an idea what is going on, assuming you have some familiarity with the syntax. </p>\\n\\n<p>A previous question I asked on <a href=\"http://stackoverflow.com/questions/5297889/how-do-i-find-the-parameters-used-in-kmeans-to-create-clusters\">stackoverflow</a> pointed me in this direction, but also helpfully told me that the comments  about the code are sometimes included <a href=\"http://svn.r-project.org/R/trunk/src/library/stats/R/kmeans.R\" rel=\"nofollow\">here</a>.</p>\\n\\n<hr>\\n\\n<p>More generally, the <a href=\"http://www.jstatsoft.org/\" rel=\"nofollow\">Journal of Statistical Software</a> illustrates this link between theory and implementation, but it is frequently about advanced topics (that I personally have difficulty understanding), but is useful as an example.</p>\\n',\n",
       " \"<p>The most correct way to handle this is to model the probability of overshooting the threshold separately (note: handling the overshoots as NA would put them at the same position as missing data, which is also very common in biomarkers, but need a whole different handling. Either way this kind of 'missing data' is, if I might coin the term, 'missing completely not at random'). This is <em>not</em> an easy undertaking. A colleague of mine is working on this, and has already shown that different results can be obtained with the correct analysis.</p>\\n\\n<p>Apart from that: if you are not aiming high (in statistical correctness), I fear that the 'accepted standard', in many fields, for this kind of situation is indeed one of your first two options. I may not like it, but there are worse 'accepted practices' around. Check the literature of your field of interest to see what others do, or chose to do hard work on getting elaborate models to fit.</p>\\n\",\n",
       " '<p>If you are comparing two groups and want to show no significant difference, this is called equivalence testing.  It essentially reverses the null and alternative hypotheses. The idea is to define an interval of insignificance called the window of equivalence.  This is used a lot when trying to show that a generic drug is a suitable replacement for a marketed drug.  A good source to read about this is William Blackwelder\\'s paper titled <a href=\"http://www.sciencedirect.com/science/article/pii/0197245682900241\" rel=\"nofollow\">“Proving the null hypothesis” in clinical trials</a>.</p>\\n',\n",
       " '<p>After a cluster analysis I´m trying to plot for each cluster the Index plot of the Silhouette value instead of for the complete dataset \\n(like in the WeightedCluster Library Manual by Matthias Studer). First of all, is that  theoretically correct? If yes...</p>\\n\\n<p>I create the object \"sil\" with the wcSilhouetteObs command:</p>\\n\\n<pre><code>sil &lt;- wcSilhouetteObs(distance.matrix, cluster.object)\\n</code></pre>\\n\\n<p>Then I plot the Index Plot for the complete dataset (this line works, even if I can´t label the clusters, but for now I don´t care):</p>\\n\\n<pre><code>seqIplot(seq.data, group = group.p(cluster.object), sortv = sil)\\n</code></pre>\\n\\n<p>But when I try to plot the Index plot sorted by the silhouette I don´t know (but as I said I´m not sure that it´s theoretically correct...) how to impose the restriction for the group argument selecting only the silhouette values e.g. for the first cluster.I have created a sequence object for each cluster separately (let´s say <code>cluster1.seq</code>), but what should I do then?  </p>\\n\\n<p>Thank you!\\nEmanuela</p>\\n',\n",
       " \"<p>I am currently calculating reliability estimates for test-retest data.</p>\\n\\n<p>My question is regarding the difference between standard error of measurement (SEM) versus minimum detectable change (MDC) when seeking to determine if there is a 'real' difference between two measurements.</p>\\n\\n<p>Here is my thinking thus far:</p>\\n\\n<p>Each measurement has an error band about it. For two measurements, if error bands overlap then there is no 'real' difference between the measurements.</p>\\n\\n<ol>\\n<li><p>For example, at 95% confidence, each measurement has an error band of $\\\\\\\\pm 1.96 \\\\\\\\times SEM$. So, two measurements would need to be more than $2 \\\\\\\\times 1.96 \\\\\\\\times SEM =3.92 \\\\\\\\times SEM$ apart to avoid each measurement's confidence interval overlapping and for their to be a real difference between the two measurements.</p></li>\\n<li><p>Another method for determining if two measurements are 'different' is to use MDC where </p></li>\\n</ol>\\n\\n<p>$$MDC = 1.96 \\\\\\\\times \\\\\\\\sqrt{2} \\\\\\\\times SEM =2.77 \\\\\\\\times SEM$$</p>\\n\\n<p>[EDIT: for second formula see e.g. p. 238 of Weir, J. P. (2005). Quantifying test-retest reliability using the intraclass correlation coefficient and the SEM. Journal of strength and conditioning research / National Strength &amp; Conditioning Association, 19(1), 231–240. doi:10.1519/15184.1]</p>\\n\\n<p>If the difference between the two measurements is greater than MDC then there is a real difference between the measurements.</p>\\n\\n<p>Obviously the two formulas are different and would produce different results. So which formula is correct?</p>\\n\",\n",
       " '<p><a href=\"http://rads.stackoverflow.com/amzn/click/019928511X\" rel=\"nofollow\">Experimental Design for the Life Sciences</a>, by Ruxton &amp; Colegrave, is a nice book and is aimed primarily at undergraduates.</p>\\n',\n",
       " '<p>Your reasoning sounds reasonable to me, although I have the feeling you are stretching the independence assumptions of t tests a little. Therefore, you should keep two things in mind.</p>\\n\\n<p>First, the size of both groups (weeks with event versus weeks without event) should be comparable. E.g., 20 versus 30. would be fine I guess.</p>\\n\\n<p>Second, your observations are not independent but follow a rule (weeks follow deterministically each other). Therefore, the occurrence of the events should be uncorrelated with the this rule (i.e., order of the weeks). This would be especially important if the dv (your measured variable) is influenced by the order of the week. But if you can negate both of these issues (correlation of event with order of weeks and of order with dv) you are good to go.</p>\\n',\n",
       " '<p>I loathe these sort of cut-off-based rules. I think it depends on design and what your <em>a priori</em> hypotheses and expectations were. If you expecting the outcome to vary with time then I\\'d say you should keep time in, as you would for any other \\'blocking\\' factor. On the other hand, if you were replicating the same experiments at different times and had no reason to think the outcome would vary with time but wished to check this was the case, then having done so and found little or no evidence for it varying with time, i\\'d say it\\'s quite entirely reasonable to then drop time. </p>\\n\\n<p>I\\'ve never heard of Underwood before. It may be a standard text for \\'Experiments in Ecology\\' (the book\\'s title), but there\\'s no obvious reason that experiments in ecology should be treated any differently from any other experiments in this respect, so to view it as \"<em>the</em> standard text on this issue\" seems unjustified.</p>\\n',\n",
       " '<p>I’m using Stata 12.0, and I’ve downloaded the <code>polychoricpca</code> command written by Stas Kolenikov, which I wanted to use with data that includes a mix of categorical and continuous variables. Given the number of variables (around 25), my hunch is that I will need to generate more than 3 components. Ultimately, I would like to generate a handful of meaningful components (rather than dozens of variables) and use the components as independent variables in logistic regression.</p>\\n\\n<p>Using <code>polychoricpca</code>, I am able to generate a table showing the eigenvalues and the eigenvectors (loadings) for each variable for the first three (3) components only. <code>polychoricpca</code> appears to call these loadings “scoring coefficients” and produces these for every level of the variable, such that if a variable has three categories you’ll see three scoring coefficients (“loadings”) for that variable.  Never having worked with polychoric PCA before, I’m used to only seeing one loading per variable/item. I want to examine these coefficients (“loadings”) to try to understand what the components are and how they might be labelled. </p>\\n\\n<p>My questions:</p>\\n\\n<p>(1) What if it looks as if I should generate 4 components? It seems as if I wouldn’t be able to examine and understand what that 4th component is because I can’t see how each of the items load on that 4th component, only the first 3. Is there a way to see how each item loads on more than the first three components?</p>\\n\\n<p>(2) Can I simply use the polychoric correlation matrix combined with Stata’s <code>pcamat</code> command to examine how each item loads on each component (the eigenvector table). I thought this might be a way of being able to examine loadings if I have more than 3 components. The idea came from <a href=\"http://www.ats.ucla.edu/stat/stata/faq/efa_categorical.htm\" rel=\"nofollow\">this UCLA stats help post</a> on using <code>factormat</code> with a polychoric correlation matrix. <code>pcamat</code> in Stata, however, produces only 1 loading (coefficient) per variable, not 1 loading for every level of the variable. Any thoughts on whether it would be appropriate just to report the single loading from <code>pcamat</code>? </p>\\n',\n",
       " \"<p>To be able to estimate to a population from a survey, the sampling method needs to be understood. The sampling method is used to create the sampling weights, which are then used to basically multiply up the survey estimates to be population estimates. There are all sorts of ways of creating weights, but they need to be based on the survey design.</p>\\n\\n<p>If you don't have the survey design information and/or you don't know how the weights were constructed, the key pieces of information you need to create the population-level estimates are missing. In particular for the two surveys, you need to be confident that the sampling method was appropriate, e.g. no quotas were used to stop sampling people of a particular age/sex combination, a convenience sample wasn't used. If either survey had these particular characteristics in the design, any population estimates (and even subpopulation estimates) are going to be wrong. </p>\\n\\n<p>There are some aspects of your question that I don't understand. For example, why do you wish to combine the two surveys - did they ask different questions? And surveys don't routinely sample the entire population - when that happens, we call it a census, so I don't understand your comment about the second survey.</p>\\n\\n<p>Can you give any more information about the survey design, and also if there are weights in the datasets and what these weights look like? </p>\\n\\n<p>Update for clarity: I am not sure that survey 2 will add anything other than bias to survey 1. In your question you state that survey 2 was not meant to estimate anything - that makes it sound like survey 2 has a convenience sample design. When dealing with convenience samples, it is not possible to weight up to the population because the sampling method used is biased rather than random. For example, a survey of supermarket shoppers at 10am on a weekday is biased (it will undercount full-time workers and overcount adult females, for example). With a biased sample it is not possible to weight the data to take account of the bias because the probability of being sampled is unknown for some groups, and may even be zero for others, <strong>but you don't know what these probabilities are</strong>. Therefore it is impossible to construct weights to account for the sampling when a biased sample has been used.</p>\\n\\n<p>Because it sounds like survey 1 has the better design for population estimation, I recommend that you use survey 1 for your estimates.  </p>\\n\",\n",
       " '<p>In order to calibrate a confidence level to a probability in supervised learning (say to map the confidence from an SVM or a decision tree using oversampled data) one method is to use Platt\\'s Scaling (e.g., <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.60.5153&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">Obtaining Calibrated Probabilities from Boosting</a>).</p>\\n\\n<p>Basically one uses logistic regression to map $[-\\\\\\\\infty;\\\\\\\\infty]$ to $[0;1]$. The dependent variable is the true label and the predictor is the confidence from the uncalibrated model. What I don\\'t understand is the use of a target variable other than 1 or 0. The method calls for creation of a new \"label\": </p>\\n\\n<blockquote>\\n  <p>To avoid overfitting to the sigmoid train set, an out-of-sample model is used.  If there are $N_+$ positive examples and $N_-$ negative examples in the train set, for each training example Platt Calibration uses target values $y_+$ and $y_-$ (instead of 1 and 0, respectively), where\\n  $$\\ny_+=\\\\\\\\frac{N_++1}{N_++2};\\\\\\\\quad\\\\\\\\quad y_-=\\\\\\\\frac{1}{N_-+2}\\n$$</p>\\n</blockquote>\\n\\n<p>What I don\\'t understand is how this new target is useful. Isn\\'t logistic regression simply going to treat the dependent variable as a binary label (regardless of what label is given)?</p>\\n\\n<p><strong>UPDATE:</strong></p>\\n\\n<p>I found that in SAS changing the dependent from $1/0$ to something else reverted back to the same model (using <code>PROC GENMOD</code>). Perhaps my error or perhaps SAS\\'s lack of versatility. I was able to change the model in R. As an example:</p>\\n\\n<pre><code>data(ToothGrowth) \\nattach(ToothGrowth) \\n\\n  # 1/0 coding \\ndep          &lt;- ifelse(supp == \"VC\", 1, 0) \\nOneZeroModel &lt;- glm(dep~len, family=binomial) \\nOneZeroModel \\npredict(OneZeroModel) \\n\\n  # Platt coding \\ndep2           &lt;- ifelse(supp == \"VC\", 31/32, 1/32) \\nplattCodeModel &lt;- glm(dep2~len, family=binomial) \\nplattCodeModel \\npredict(plattCodeModel) \\n\\ncompare        &lt;- cbind(predict(OneZeroModel), predict(plattCodeModel)) \\n\\nplot(predict(OneZeroModel), predict(plattCodeModel))\\n</code></pre>\\n',\n",
       " '<p>Your final formula is missing a left parenthesis.</p>\\n\\n<p>This is a standard problem that requires no difficult work. The wikipedia page on <a href=\"http://en.wikipedia.org/wiki/Bayesian_linear_regression#Posterior_distribution\" rel=\"nofollow\">Bayesian regression</a> solves a harder problem; you should be able to use the same trick (which is basically just a form of completing the square, since you want it in terms of $(\\\\\\\\beta - m)\\' V^{-1} (\\\\\\\\beta - m)$ for some $m$ and $V$), with fewer terms to worry about. That is, you get to something like this:</p>\\n\\n<p>\\\\\\\\begin{align}(\\\\\\\\mathbf{y}- \\\\\\\\mathbf{X} \\\\\\\\boldsymbol\\\\\\\\beta)^{\\\\\\\\rm T}(\\\\\\\\mathbf{y}- \\\\\\\\mathbf{X} \\\\\\\\boldsymbol\\\\\\\\beta)&amp;= (\\\\\\\\boldsymbol\\\\\\\\beta - \\\\\\\\hat{\\\\\\\\boldsymbol\\\\\\\\beta})^{\\\\\\\\rm T}(\\\\\\\\mathbf{X}^{\\\\\\\\rm T}\\\\\\\\mathbf{X})(\\\\\\\\boldsymbol\\\\\\\\beta - \\\\\\\\hat{\\\\\\\\boldsymbol\\\\\\\\beta})\\n + \\\\\\\\mathbf{S} \\\\\\\\end{align}</p>\\n\\n<p>where </p>\\n\\n<p>\\\\\\\\begin{align}\\\\\\\\mathbf{S} = (\\\\\\\\mathbf{y}- \\\\\\\\mathbf{X} \\\\\\\\hat{\\\\\\\\boldsymbol\\\\\\\\beta})^{\\\\\\\\rm T}(\\\\\\\\mathbf{y}- \\\\\\\\mathbf{X} \\\\\\\\hat{\\\\\\\\boldsymbol\\\\\\\\beta})\\\\\\\\end{align}</p>\\n\\n<p>in the exponent.</p>\\n\\n<p>See also the references at the wikipedia article.</p>\\n\\n<p>If this is for some subject, please mark it as homework.</p>\\n',\n",
       " '<p>One of my favorites is the <a href=\"http://montyhallproblem.com/\" rel=\"nofollow\">Monty Hall problem.</a>  I remember learning about it in an elementary stats class, telling my dad, as both of us were in disbelief I simulated random numbers and we tried the problem.  To our amazement it was true.  </p>\\n\\n<p>Basically the problem states that if you had three doors on a game show, behind which one is a prize and the other two nothing, if you chose a door and then were told of the remaining two doors one of the two was not a prize door and allowed to switch your choice if you so chose you should switch you current door to the remaining door.</p>\\n\\n<p>Here\\'s the link to an R simulation as well: <a href=\"http://bayesianbiologist.com/2012/02/03/monty-hall-by-simulation/\" rel=\"nofollow\">LINK</a></p>\\n',\n",
       " '<p>First, $\\\\\\\\bar X$ is the best linear unbiased estimator of $\\\\\\\\mu$, the population mean. You can improve the mean squared error by shrinkage, but among unbiased estimates, you cannot get any better in terms of variance than $\\\\\\\\sigma^2/n$ ($\\\\\\\\sigma^2$ being the population variance). Moreover, from the asymptotic theory of estimating equations, I believe you cannot do better than $\\\\\\\\sigma^2/n$ asymptotically, anyway, so the bias, if any, has to go away at a rate faster than $O(n^{-1/2})$. A technically valid source to improve efficiency of the estimator of the mean is knowledge of the distributional form of your data (gamma? Poisson? double exponential?), whereas the mean is expressed as a function of the (estimated) population parameters, but of course practicality of any such assumption is dubious, at best.</p>\\n\\n<p>Second, the estimate $V_1$ is only good as an unbiased estimate of the variance. As far as I recall my stat theory classes, the estimate $$V_3=\\\\\\\\frac1{n+1} \\\\\\\\sum_i (X_i-\\\\\\\\bar X)^2$$ has the smallest MSE as the estimator of the sampling variance $V[\\\\\\\\bar X]$ (you probably have to assume normality of $X_i$\\'s to get a specific answer, as the MSE of the variance estimator depends on the kurtosis of the original distribution).</p>\\n\\n<p>So you can do all sorts of things with biased estimates, and improve the MSE of either the estimator of $\\\\\\\\mu$ or the estimator of $V[\\\\\\\\hat\\\\\\\\mu]$. But the unbiased estimation theory is fairly rigid, and <a href=\"http://en.wikipedia.org/wiki/Cramer_Rao_bound\" rel=\"nofollow\">Cramer-Rao bound</a> together with <a href=\"http://en.wikipedia.org/wiki/Rao-Blackwell_theorem\" rel=\"nofollow\">Rao-Blackwell theorem</a> give strict limits for your efficiency.</p>\\n',\n",
       " '<p>Both packages use <code>Lattice</code> as the backend, but <code>nlme</code> has some nice features like <code>groupedData()</code> and <code>lmList()</code> that are lacking in <code>lme4</code> (IMO). From a practical perspective, the two most important criteria seem, however, that</p>\\n\\n<ol>\\n<li><code>lme4</code> extends <code>nlme</code> with other link functions: in <code>nlme</code>, you cannot fit outcomes whose distribution is not gaussian, <code>lme4</code> can be used to fit mixed-effects logitic regression, for example.</li>\\n<li>in <code>nlme</code>, it is possible to specify the variance-covariance matrix for the random effects (e.g. an AR(1)); it is not possible in <code>lme4</code>.</li>\\n</ol>\\n\\n<p>Now, <code>lme4</code> can easily handle very huge number of random effects (hence, No. individuals in a given study) thanks to its C part and the use of sparse matrices. The <code>nlme</code> package has somewhat been superseded by <code>lme4</code> so I won\\'t expect people spending much time developing add-ons on top of <code>nlme</code>. Personally, when I have a continuous response in my model, I tend to use both packages, but I\\'m now versed to the <code>lme4</code> way for fitting GLMM.</p>\\n\\n<p>Rather than buying a book, take a look first at the Doug Bates\\' draft book on R-forge: <a href=\"http://lme4.r-forge.r-project.org/book/\">lme4: Mixed-effects Modeling with R</a>.</p>\\n',\n",
       " \"<p>The authors' paper about the method (Hyndman and Khandakar 2008) states that an extended an extended Canova-Hansen test is used, although the helpfile updates this to say than an OCSB test is used. </p>\\n\\n<p>I have two questions, therefore:</p>\\n\\n<ol>\\n<li>What is an OCSB test?</li>\\n<li>Is the method of determining seasonal differencing the same for regression with ARIMA errors as it is for ARIMA modelling with no covariates?</li>\\n</ol>\\n\",\n",
       " \"<p>I'm looking to use a multivariate regression for prediction, but making use of (possibly) superior estimates of variance for both the independent and extraneous variables.</p>\\n\\n<p>My approach is to standardize the dependent and extraneous variables (by dividing their respective standard deviations derived from the full data history).  Once I have the standardized regression coefficients, I use these, together with my separately sourced variance estimates, to get back to an equation that will be used for generating the predictions. Do I simply multiply each coefficient by my separately sourced variance estimate?  But what about the dependent variable variance estimate?</p>\\n\\n<p>As background info, my separate sourcing of variances estimates is because I believe I have better (more up-to-date) estimates than available from the full data history (let's say, because the variances aren't stable through time, I can estimate a more timely measure of variance using higher frequency data over a shorter, recent period).</p>\\n\\n<ul>\\n<li>Is this approach a sensible standard practice?</li>\\n<li>How to go from the standardized betas, to un-standardized ones using the high frequency variance estimates?</li>\\n</ul>\\n\\n<p>I read here (http://stats.stackexchange.com/questions/29781/when-should-you-center-your-data-when-should-you-standardize) about WHEN to standardize, but not clear to me if this covers the case of standardizing both dependent and extraneous variables.  </p>\\n\\n<p>Thank you!</p>\\n\",\n",
       " '<p>Does the autocorrelation function have any meaning with a non-stationary time series?</p>\\n\\n<p>The time series is generally assumed to be stationary before autocorrelation is used for Box and Jenkins modeling purposes.</p>\\n',\n",
       " '<p>I have 3 millions instances with 30 features each and I am trying to reduce it  in a sensible size for my computer for a classification problem. What are possible methods I can use to reduce the data while keeping the quality of classification reasonable.</p>\\n',\n",
       " '<p>The course from Hinton at <a href=\"https://class.coursera.org/neuralnets-2012-001/class\" rel=\"nofollow\">https://class.coursera.org/neuralnets-2012-001/class</a> is the best basic tutorial I found online. I think before you use tutorials from deeplearning.net, this would offer more insight.</p>\\n',\n",
       " \"<p>I'm currently attending the <em>An Introduction to Operations Management</em> course in Coursera.org. At some point in the course, the professor started to deal with variation in the operations' time. </p>\\n\\n<p>The measurement he uses is the <em>Coefficient of Variation</em>, the ratio between the standard deviation and the mean:</p>\\n\\n<p>$c_v = \\\\\\\\frac{\\\\\\\\sigma}{\\\\\\\\mu}$ </p>\\n\\n<p>Why would this measurement be used? What are the advantages and disadvantages of working with <em>CV</em> besides working with, say, standard deviation? What is the intuition behind this measurement?</p>\\n\",\n",
       " '<p>If you\\'re looking for an elementary text, i.e. one that doesn\\'t have a calculus prerequisite, there\\'s Don Berry\\'s <a href=\"http://rads.stackoverflow.com/amzn/click/0534234720\" rel=\"nofollow\">Statistics: A Bayesian Perspective</a>.</p>\\n',\n",
       " '<p>The probability all $3$ are distinct is $\\\\\\\\frac{(D-1)(D-2)}{D^2}$.</p>\\n\\n<p>The probability the third is not equal to the first two is $\\\\\\\\frac{(D-1)^2}{D^2}$. Imagine choosing $C$ first. Then there are $D-1$ choices for $A$ so that $A\\\\\\\\ne C$, and $D-1$ choices for $B$ so that $B\\\\\\\\ne C$.</p>\\n\\n<p>If you are given that the first two are distinct, the conditional probability that the third is different from both is $\\\\\\\\frac{D-2}{D}$. </p>\\n\\n<p>You are correct that if you are given the first two are equal, the conditional probability that the third is different is $\\\\\\\\frac{D-1}{D}$. </p>\\n\\n<p>If $D=10$, these are $72\\\\\\\\%, 81\\\\\\\\%, 80\\\\\\\\%,$ and $90\\\\\\\\%,$ respectively. The $81\\\\\\\\%$ is naturally a weighted average between $80\\\\\\\\%$ and $90\\\\\\\\%$, weighted by the probabilities that the first two are distinct or not. The $72\\\\\\\\%$ can also be viewed as a weighted average between $81\\\\\\\\%$ and $0$, weighted by the conditional probability that the first two are distinct from each other given that they are distinct from the third. </p>\\n',\n",
       " '<p>The analysis of irregularly sampled time series can be tricky, as there aren\\'t many tools available. Sometimes the practice is to apply regular algorithms and hope for the best. This isn\\'t necessarily the best approach. Other times people try to interpolate the data in the gaps. I have even seen cases where gaps are filled with random numbers which have the same distribution as the known data. One algorithm specifically for irregularly sampled series is the Lomb-Scargle Periodogram which gives a periodogram (think power spectrum) for unevenly sampled time series. Lomb-Scargle doesn\\'t require any \"gap conditioning\". </p>\\n',\n",
       " '<ul>\\n<li>What is a stationary VAR (vector autoregression)?</li>\\n<li>Can a VAR with non-stationary variables be stationary?</li>\\n<li>How do you test whether a VAR is stationary or non-stationary? (Example in <code>R</code> language if possible/applicable).</li>\\n</ul>\\n',\n",
       " '<p>I have several questions regarding the usual gaussianity (<em>broad normality</em>) assumptions in econometrics. Though people often check for normality (with apparently weak tests), I\\'ve seen just one example of \"gaussianity\" testing.</p>\\n\\n<blockquote>\\n  <p>1.Is the \"finite variance\" assumption the same as a gaussianity assumption? Ie., is it the same to assume that the variable follows any distribution in the family of Elliptically Symmetric distributions?\\n  2.If gaussianity is a requirement, how do you test for it?</p>\\n</blockquote>\\n\\n<p>The one example I know of doing some informal testing for gaussianity is NN Taleb\\'s <em>Errors, Robustness, and The Fourth Quadrant</em>. Here\\'s SSRN PDF version <a href=\"http://maint.ssrn.com/?abstract_id=1343042\" rel=\"nofollow\">http://maint.ssrn.com/?abstract_id=1343042</a> and here the technical part of the paper in a friendly html format <a href=\"http://www.fooledbyrandomness.com/EDGE/index.html\" rel=\"nofollow\">http://www.fooledbyrandomness.com/EDGE/index.html</a>.</p>\\n\\n<p>It that paper Taleb uses some measurements in a big number of financial time series trying to show that gaussianity is implausible.</p>\\n\\n<p>He does so by:</p>\\n\\n<ul>\\n<li>Trying to see if the data is consistent with the central limit theorem seeing if kurtosis converges when increasing data aggregation.</li>\\n<li>Trying to see if the data is consistent with either a gaussian decay in the conditional expectations of the variable or a power law (I think, page 8 of the PDF).</li>\\n<li>Trying to see a non gaussian incidence of rare events.</li>\\n</ul>\\n\\n<p>Finally, questions #3 and #4:</p>\\n\\n<blockquote>\\n  <p>3.Taleb performs his tests on data of a much higher frequency than what is commonly found in macroeconomics. What would the appropriate tests be for monthly data?\\n  4.Are there other necessary conditions for the usual econometric models besides finiteness of variance not typically tested for?</p>\\n</blockquote>\\n\\n<p>Please bear in mind that I\\'m a graduate student taking a fairly basic time series course, this is just for intellectual curiosity.</p>\\n\\n<p>Thanks!</p>\\n',\n",
       " '<p>Sounds a lot like n-gram to me.</p>\\n\\n<p>Extract all n-grams, then find the most frequent n-grams?</p>\\n',\n",
       " '<p>Which distributions are their own Fourier transform besides the <em>normal distribution</em> and the <em>generalized arcsine distribution</em>?</p>\\n',\n",
       " '<p>I\\'d suggest starting with\\n<a href=\"http://en.wikipedia.org/wiki/Cosine_similarity\">Cosine distance</a>,\\nnot Euclidean, for any data with most vectors nearly orthogonal,\\n$x \\\\\\\\cdot y \\\\\\\\approx$ 0.<br>\\nTo see why, look at\\n$|x - y|^2 = |x|^2 + |y|^2 - 2\\\\\\\\ x \\\\\\\\cdot y$.<br>\\nIf $x \\\\\\\\cdot y \\\\\\\\approx$ 0, this reduces to\\n$|x|^2 + |y|^2$: \\na crummy measure of distance, as Anony-Mousse points out.</p>\\n\\n<p>Cosine distance amounts to using $x / |x|$,\\nor projecting the data onto the surface of the unit sphere, so all $|x|$ = 1.\\nThen \\n$|x - y|^2 = 2 - 2\\\\\\\\ x \\\\\\\\cdot y$<br>\\na quite different and usually better metric than plain Euclidean.\\n$ x \\\\\\\\cdot y$ may be small, but it\\'s not masked by noisy $|x|^2 + |y|^2$.</p>\\n\\n<p>$x \\\\\\\\cdot y$ is mostly near 0 for sparse data.\\nFor example, if $x$ and $y$ each have 100 terms non-zero and 900 zeros,\\nthey\\'ll both be non-zero in only about 10 terms \\n(if the non-zero terms scatter randomly).</p>\\n\\n<p>Normalizing $x$ /= $|x|$ may be slow for sparse data; it\\'s fast in\\n<a href=\"http://scikit-learn.org/stable/developers/utilities.html#efficient-routines-for-sparse-matrices\">scikit-learn</a>.</p>\\n\\n<p>Summary: start with cosine distance, but don\\'t expect wonders on any old data.<br>\\nSuccessful metrics require evaluation, tuning, domain knowledge.</p>\\n',\n",
       " \"<p>I intend to use factor scores as derived from exploratory factor analysis in subsequent multivariate regression analysis, as an explanatory variable.</p>\\n\\n<p>I've read in multiple books/papers that Structural Equation Modeling is the appropriate method to model relationships AMONG LATENT VARIABLES (using factor scores being incorrect). I haven't, so far, found a negative answer as to whether it is a problem to use factor scores solely as an explanatory variable, my dependent variable being observed.</p>\\n\\n<p>Does anyone have a clear answer regarding this issue? </p>\\n\",\n",
       " '<p>We performed an experiment on bacterial growth whereby optical density was recorded for three flasks of <em>E. coli</em> bacteria every 30 minutes for 4 hours. 2 flasks upon the 2 hour mark were inoculated with two different antibiotics, whilst the other served as a control.</p>\\n\\n<p>Would a one-way ANOVA be appropriate in this case? Should I, for example, select two time points: one before adding the antibiotic, one after and do two separate ANOVAs?</p>\\n\\n<p>I will be using class data so I will have 5 replicates for each time point.</p>\\n',\n",
       " \"<p>We can write Bayes' theorem as</p>\\n\\n<p>$$p(\\\\\\\\theta|x) = \\\\\\\\frac{f(X|\\\\\\\\theta)p(\\\\\\\\theta)}{\\\\\\\\int_{\\\\\\\\theta} f(X|\\\\\\\\theta)p(\\\\\\\\theta)d\\\\\\\\theta}$$</p>\\n\\n<p>where $p(\\\\\\\\theta|x)$ is the posterior, $f(X|\\\\\\\\theta)$ is the conditional distribution, and $p(\\\\\\\\theta)$ is the prior.</p>\\n\\n<p>or</p>\\n\\n<p>$$p(\\\\\\\\theta|x) = \\\\\\\\frac{L(\\\\\\\\theta|x)p(\\\\\\\\theta)}{\\\\\\\\int_{\\\\\\\\theta} L(\\\\\\\\theta|x)p(\\\\\\\\theta)d\\\\\\\\theta}$$</p>\\n\\n<p>where $p(\\\\\\\\theta|x)$ is the posterior, $L(\\\\\\\\theta|x)$ is the likelihood function, and $p(\\\\\\\\theta)$ is the prior.</p>\\n\\n<p>My question is</p>\\n\\n<ol>\\n<li>Why is Bayesian analysis done using the likelihood function and not the conditional distribution?</li>\\n<li>Can you say in words what the difference between the likelihood and conditional distribution is?  I know the likelihood is not a probability distribution and $L(\\\\\\\\theta|x) \\\\\\\\propto f(X|\\\\\\\\theta)$.</li>\\n</ol>\\n\",\n",
       " '<blockquote>\\n  <p><strong>Possible Duplicate:</strong><br>\\n  <a href=\"http://stats.stackexchange.com/questions/32531/r-sequence-recognition-for-univariate-data-sequence-mining-arules\">r sequence recognition for univariate data sequence mining arules</a>  </p>\\n</blockquote>\\n\\n\\n\\n<p>I am looking for a method to detect sequences within univariate discrete data without specifying the length of the sequence or the exact nature of the sequence beforehand (see e.g. <a href=\"http://en.wikipedia.org/wiki/Sequence_mining\" rel=\"nofollow\">Wikipedia - Sequence Mining</a>)</p>\\n\\n<p>Here is example data</p>\\n\\n<pre><code>x &lt;- c(round(rnorm(100)*10),\\n       c(1:5),\\n       c(6,4,6),\\n       round(rnorm(300)*10), \\n       c(1:5), \\n       round(rnorm(70)*10),\\n       c(1:5), \\n       round(rnorm(100)*10),\\n       c(6,4,6),\\n       round(rnorm(200)*10),\\n       c(1:5), \\n       round(rnorm(70)*10),\\n       c(1:5),\\n       c(6,4,6),\\n       round(rnorm(70)*10),\\n       c(1:5), \\n       round(rnorm(100)*10),\\n       c(6,4,6),\\n       round(rnorm(200)*10),\\n       c(1:5), \\n       round(rnorm(70)*10),\\n       c(1:5),\\n       c(6,4,6))\\n</code></pre>\\n\\n<p>The method should be able to identify the fact that x contains the sequence 1,2,3,4,5 at least eight times and the sequence 6,4,6 at least five times (\"at least\" because the random normal part can potentially generate the same sequence).</p>\\n\\n<p>I have found the <code>arules</code> and <code>arulesSequences</code> package but I could\\'nt make them work with univariate data. Are there any other packages that might be more appropriate here ? </p>\\n\\n<p>I\\'m aware that only eight or five occurrences for each sequence is not going to be enough to generate statistically significant information, but my question was to ask if there was a good method of doing this, assuming the data repeated several times. </p>\\n\\n<p>Also note the important part is that the method is done without knowing beforehand that the structure in the data had the sequences <code>1,2,3,4,5</code> and <code>6,4,6</code> built into it. The aim was to find those sequences from <code>x</code> and identify where it occurs in the data.</p>\\n\\n<p>Any help would be greatly appreciated! </p>\\n\\n<p>P.S This was put up here upon suggestion from a stackoverflow comment...</p>\\n',\n",
       " '<p>If I find that my covariate (reaction time) alters over the length of my experiment (e.g. due to fatigue), can I somehow build that into my model?<br>\\nSo what I am saying is that the effect of my covariate is not constant (between subjects and within subjects).</p>\\n',\n",
       " '<p>(This is not yet an answer - but it may develop into one...)</p>\\n\\n<p>What kind of measurements do you have? </p>\\n\\n<p>If I had to compare spectra (just because that\\'s the data I work with and you only told us what your data isn\\'t, and 630 measurements per observation may very well be spectra - I\\'m quite willing to delete the answer if it doesn\\'t fit the question), I\\'d probably do something along these lines:</p>\\n\\n<ul>\\n<li><p>for a PCA, I\\'d rather center on the average before spectrum as that is a meaningful \"base point\"</p></li>\\n<li><p>If I had to search for differences between the data sets before and after, I\\'d try to set up directly a supervised model, either a classification (LDA is related to PCA and PLS) or a regression (PLS, particularly if the inversion in the LDA is a problem) and then interprete that model.</p></li>\\n<li><p>then I\\'d <a href=\"http://stats.stackexchange.com/questions/13548/present-results-from-a-multinomial-model-graphically/13558#13558\">check how much the models change if I change the data set \"a bit\"</a>- technically that\\'s a cross validation or bootstrap. Just make sure you pick the correct \"base unit\" for splitting the data (do you need to leave out batches or days instead of single measurements?). You may need Procrustes-like roations for this. The variation between models give a very good idea what could be real and what is spurious.</p></li>\\n<li><p>If the measurements have known physico-chemical meaning wrt. your process, checking this against the results / interpreting the models under this knowledge is a tremendous advantage over the usual settings of gene analysis. I\\'d consider \"I\\'m not interested in which of them did change\" throwing away much information that could help to judge what could be caused by the process change and what difference may just be random.</p></li>\\n<li><p>With spectra I\\'d never consider univariate tests for each wavelength as something I\\'d seriously want to do (unless there are physico-chemical reasons to look for a difference at a particular wavelength): good spectra have a lot of correlation among neighbour wavelengths. This is a very different situation from \"just\" multiple testing (and if you have 630 different pH, T, p, ... measurements I imagine them being  correlated about as much as spectra)</p></li>\\n</ul>\\n',\n",
       " '<p>High risk by your definition means likely to have short survival times.</p>\\n',\n",
       " \"<p>If you keep all the components from a PCA - then the Euclidean distances between patients in the new PCA-space will equal their Mahalanobis distances in the observed-variable space. If you'll skip some components, that will change a little, but anyway. Here I refer to to unit-variance PCA-components, not the kind whose variance is equal to eigenvalue (I am not sure about your PCA implementation).</p>\\n\\n<p>I just mean, that if you want to evaluate Mahalanobis distance between the patients, you can apply PCA and evaluate Euclidean distance. Evaluating Mahalanobis distance after applying PCA seems something meaningless to me.</p>\\n\",\n",
       " '<p>I\\'m running a simple McNemar\\'s test for agreement on the McNemar\\'s example from Wikipedia. The test uses the following data:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/w1CHJ.png\" alt=\"table\"></p>\\n\\n<p>for patients of Hodgkin\\'s disease. Running this SAS code:</p>\\n\\n<pre><code>data hodgkins;\\ninput hodgkins $ sibling $ count;\\ndatalines;\\nyes yes 26\\nyes no 15\\nno yes 7\\nno no 37\\n;\\n\\nproc freq data=hodgkins;\\n    tables hodgkins * sibling /agree;\\n    weight count;\\nrun;\\n</code></pre>\\n\\n<p>which generates this plot. How do I interpret this plot? I know that McNemar\\'s test deals with probabilities of discordant pairs, which I understand. In this case, the simple null would be that the probability that a Hodgkin’s patient having tonsil’s removed while sibling not having them removed is equal to a Hodgkin’s patient not having tonsils removed while sibling did, but I\\'m not entirely sure how this applies to the plot. </p>\\n\\n<p><img src=\"http://i.stack.imgur.com/E94QJ.png\" alt=\"plot\"></p>\\n',\n",
       " '<p>You can also do this in spherical coordinates, in which case there is no rejection. First you generate the radius and the two angles at random, then you use the transition formula to recover $x$, $y$ and $z$ ($x = r \\\\\\\\sin \\\\\\\\theta \\\\\\\\cos \\\\\\\\phi$, $y = r \\\\\\\\sin \\\\\\\\theta \\\\\\\\sin \\\\\\\\phi$, $z = r \\\\\\\\cos \\\\\\\\theta$).</p>\\n\\n<p>You generate $\\\\\\\\phi$ unifomly between $0$ and $2\\\\\\\\pi$. The radius $r$ and the inclination $\\\\\\\\theta$ are not uniform though. The probability that a point is inside the ball of radius $r$ is $r^3$ so the probability density function of $r$ is $3 r^2$. You can easily check that the cubic root of a uniform variable has exactly the same distribution, so this is how you can generate $r$. The probability that a point lies within a spherical cone defined by inclination $\\\\\\\\theta$ is $(1-\\\\\\\\cos\\\\\\\\theta)/2$ or $1 - (1-\\\\\\\\cos (-\\\\\\\\theta))/2$ if $\\\\\\\\theta &gt; \\\\\\\\pi/2$. So the density $\\\\\\\\theta$ is $sin(\\\\\\\\theta)/2$. You can check that minus the arccosine of a uniform variable has the proper density.</p>\\n\\n<p>Or more simply, we can simulate the cosine of $\\\\\\\\theta$ uniformly beteen $-1$ and $1$.</p>\\n\\n<p>In R this would look as shown below.</p>\\n\\n<pre><code>n &lt;- 10000 # For example n = 10,000.\\nphi &lt;- runif(n, max=2*pi)\\nr &lt;- runif(n)^(1/3)\\ncos_theta &lt;- runif(n, min=-1, max=1)\\nx &lt;- r * sqrt(1-cos_theta^2) * cos(phi)\\ny &lt;- r * sqrt(1-cos_theta^2) * sin(phi)\\nz &lt;- r * cos_theta\\n</code></pre>\\n\\n<p>In the course of writing and editing this answer, I realized that the solution is less trivial that I thought.</p>\\n\\n<p>I think that the easiest and computationally most efficient method is to follow @whuber\\'s method to generate $(x,y,z)$ on the unit sphere as shown on <a href=\"http://stats.stackexchange.com/questions/7977/how-to-generate-uniformly-distributed-points-on-the-surface-of-the-3-d-unit-sphe\">this post</a> and scale them with $r$.</p>\\n\\n<pre><code>xyz &lt;- matrix(rnorm(3*n), ncol=3)\\nlambda &lt;- runif(n)^(1/3) / sqrt(rowSums(xyz^2))\\nxyz &lt;- xyz*lambda\\n</code></pre>\\n',\n",
       " '<p>You should read up on copulas! That will give you a way to construct as many counterexamples as you wish. You can start with\\n<a href=\"http://en.wikipedia.org/wiki/Copula_(probability_theory\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Copula_(probability_theory</a>)</p>\\n',\n",
       " '<p>Be careful with medians: they are biased estimators and the degree of bias can change depending on the skew of the distribution and the sample size (see <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/2971778\" rel=\"nofollow\">Miller, 1988</a>). This means that if you are comparing two conditions that have either different skew or different sample sizes, you may find a difference that is in fact attributable to bias rather than a real difference, or you may fail to find a difference when there is a real one when the direction of the difference in bias is opposite to the direction of the real difference between the conditions.</p>\\n',\n",
       " '<p>If we are to classify 2 separate classes/actions using HMMs, we design 2 separate HMMs (one for each class). Do they share <strong>same set or a different set</strong> of observations-symbols for each of the HMM?</p>\\n\\n<p>If they share different set of symbols, during the time of extracting a symbol (using vector quantization) how do we know which set of symbols to generate in order to classify them?</p>\\n',\n",
       " '<p>I\\'ve solved this problem in the past by ensuring that either my SAS working directory is set to a disk I know I can write to (especially if SAS is running on an application server like Citrix) or by submitting</p>\\n\\n<p>ODS LISTING GPATH = \"C:\\\\\\\\Documents\";</p>\\n\\n<p>For example, assuming that you can access the C:\\\\\\\\Documents folder.</p>\\n',\n",
       " '<p>Intuitive explanation is such: multiplying by constant does not change information content of X and Y, so also their mutual information -- and thus it is invariant to scaling. Still Srikant gave you a strict proof of this fact.</p>\\n',\n",
       " '<p>You ran the regression for Fig. 2 that uses <code>confbord</code> variable, but you try to get the margins from regression for Fig. 1 that uses <code>nciwvar</code> variable.</p>\\n',\n",
       " '<p>If you want to create a multivariate Gaussian with a covariance matrix $\\\\\\\\Gamma$, find $A$ such that $ATA=\\\\\\\\Gamma$ using e.g. Cholesky decomposition. Then if Z is a vector of independent standard normals, AZ has the desired covariance.</p>\\n',\n",
       " '<p>There is a statistics one course  starting up tomorrow at coursera. I guess it\\'s much like the one at udacity, though I haven\\'t followed that one. \\nIt will be with small assignments throughout the course. 75,000 have already signed up. </p>\\n\\n<p>It\\'s not a book, but it takes you directly back to the classroom again :-)\\nHere\\'s the link: <a href=\"https://www.coursera.org/course/stats1\" rel=\"nofollow\">https://www.coursera.org/course/stats1</a></p>\\n',\n",
       " '<p>I was thinking of the smoothing idea also.  But there is a whole area called response surface methodology that searches for peaks in noisy data (it does primarily involve using local quadratic fits to the data) and there was a famous paper I recall with \"Bump hunting\" in the title.  Here are some links to books on response surface methodology.  Ray Myer\\'s books are particularly well-written.  I will try to find the bump hunting paper.</p>\\n\\n<p><a href=\"http://rads.stackoverflow.com/amzn/click/0470174463\" rel=\"nofollow\">Response Surface Methodology: Process and Product Optimization Using Designed Experiments </a></p>\\n\\n<p><a href=\"http://rads.stackoverflow.com/amzn/click/9812564586\" rel=\"nofollow\">Response Surface Methodology And Related Topics</a></p>\\n\\n<p><a href=\"http://rads.stackoverflow.com/amzn/click/B0006C2TLK\" rel=\"nofollow\">Response surface methodology </a></p>\\n\\n<p><a href=\"http://rads.stackoverflow.com/amzn/click/0471810339\" rel=\"nofollow\">Empirical Model-Building and Response Surfaces</a></p>\\n\\n<p>Although not the article I was looking for, <a href=\"http://www-stat.stanford.edu/~jhf/ftp/prim.pdf\" rel=\"nofollow\">here is</a> a very relevant article by Jerry Friedman and Nick Fisher that deals with these ideas applied to high-dimensional data.</p>\\n\\n<p><a href=\"http://www.science20.com/quantum_diaries_survivor/subtle_art_bump_hunting_part_i\" rel=\"nofollow\">Here is</a> an article with some online comments.</p>\\n\\n<p>So I hope you at least appreciate my response.  I think your ideas are good and on the right track but yes I do think you might be reinventing the wheel and I hope you and others will look at these excellent references.</p>\\n',\n",
       " '<p>The above answer may be useful but is complicated and I am not sure how to apply it. Thinking in simple terms this is a special case of testing whether or not a person has psychic powers (really no different from a scientific evaluation of the mechanics and physics of the coin flip).  Obviously have psychic capability would have to be defined as doing something much better than by random chance.  I would first want to define what is enough.  The difficulty is to decide how much better than random guessing is and what random guessing will do.  If all the coins were fair random choice would be 0.5 and so maybe saying anything over 0.75 is  Then testing the hypothesis that the person has psychic powers is doing a one-sided hypothesis that a binomial parameter p is &lt;= to 0.75 versus the alternative that it is greater.  As an estimator I have chosen the binomial parameter for successfullu calling heads or tails and the variance of my estimator is p(1-p)/n. The added difficulty is that the coins are not fair and the individual pis are unknown. I would still define chance as random guessing of heads or tails and 0.5 is what I would test against.  However with unfair coins there may be statistical strategies that would lead to a better than chance success rate but neither indicate skill with the individual coin flips or psychic powers.  To illustrate suppose the average for the pis is 0.80.  Then after seing heads come up much more frequently than tails we could switch to an all heads strategy and tend to be correct close to 80% of the time.  This assumes randomguessing until we are convinced that head occurs much more often than tails and at that point we switch to all heads.  So without knowing the pis or at least their averages I cannot tell what success rate would indicate skill.  Comparing against random guessing is not the standard to beat in this case. Note that my argument only makes sense if the stack of coins is very large.</p>\\n',\n",
       " '<p>A deep understanding of <strong>survival analysis</strong> requires knowledge of counting processes, martingales, Cox processes... See e.g. Odd O. Aalen, Ørnulf Borgan, Håkon K. Gjessing. <em>Survival and event history analysis: a process point of view</em>. Springer, 2008. <a href=\"http://en.wikipedia.org/wiki/Special%3aBookSources/9780387202877\" rel=\"nofollow\">ISBN 9780387202877</a> </p>\\n\\n<p>Having said that, many applied statisticians (including me) <em>use</em> survival analysis without any understanding of stochastic processes. I\\'m not likely to make any advances to the theory though.</p>\\n',\n",
       " '<p>Are there any tools for remote collaboration in prediction or machine learning settings? </p>\\n\\n<p>I am looking for a computing environment that includes appropriate source control, keeps track of how different datasets are being matched to different algorithms, and facilitates blending of various predictions.  What tools would be useful for this?</p>\\n',\n",
       " '<p>I believe the threshold is selected as the point at which the MRL plot becomes kind of linear, which to my eye is 2.75 on the MRL plot.</p>\\n',\n",
       " '<p>Is this all I have to do to implement 2 fold stratified cross validation:</p>\\n\\n<ol>\\n<li>Assign random number from uniform distribution [0...1] to each row.</li>\\n<li>Assign row to fold 1 if random number &lt;= 0.5 otherwise assign to fold 2</li>\\n</ol>\\n\\n<p>Does this ensure that the class priors are roughly the same in each fold?</p>\\n',\n",
       " '<p>In a dataset of two non-overlapping populations (patients &amp; healthy, total $n=60$) I would like to find (out of $300$ independent variables) significant predictors for a continuous dependent variable. Correlation between predictors is present. I am interested in finding out if any of the predictors are related to the dependent variable \"in reality\" (rather than predicting the dependent variable as exactly as possible). As I got overwhelmed with the numerous possible approaches, I would like to ask for which approach is most recommended. </p>\\n\\n<ul>\\n<li><p>From my understanding stepwise inclusion or exclusion of predictors is <a href=\"http://www.stata.com/support/faqs/statistics/stepwise-regression-problems/\" rel=\"nofollow\">not recommended</a></p></li>\\n<li><p>E.g. run a linear regression separately for every predictor and correct p-values for multiple comparison using FDR (probably very conservative?)</p></li>\\n<li><p>Principal-component regression: difficult to interpret as I won\\'t be able to tell about the predictive power of individual predictors but only about the components. </p></li>\\n<li><p>any other suggestions?</p></li>\\n</ul>\\n',\n",
       " '<p>I assume that the survey will be answered independently by the participants. First, you need estimates for the baseline probabilities $p_{i}$ that an answer $i$ will be answered \"yes\". The probability of two persons answering \"yes\" for question $i$ is then $p_{i}^{2}$. Likewise, the probability of two persons answering \"no\" for question $i$ is $(1-p_{i})^{2}$, hence the probability of agreement is $p_{i}^{2} + (1-p_{i})^{2}$.</p>\\n\\n<p>If you assume that all $p_{i} = 0.5$, then you get the answer given by carlosdc since $0.5^{2} + (1-0.5)^{2} = 0.5$. If you allow the $p_{i}$ to vary, an answer can probably be given in closed form as well, but with only 7 questions, it\\'s easy to simply enumerate all possibilities to get 4 or more agreements, and calculate the probability for each case.</p>\\n\\n<pre><code>&gt; n &lt;- 7            # number of questions\\n&gt; p &lt;- rep(0.5, n)  # probabilities p_i, here: set all to 0.5\\n# p &lt;- c(0.4, 0.4, 0.4, 0.4, 0.1, 0.1, 0.1) # alternative: let p_i vary\\n&gt; k &lt;- 4:7          # number of agreements to check\\n# k &lt;- 0:7          # check: result (total probability) should be 1\\n\\n# vector to hold probability for each number of agreements\\n&gt; res &lt;- numeric(length(k))\\n\\n# function to calculate the probability for an event with agreement on the\\n# questions x and disagreement on the remaining questions\\n&gt; getP &lt;- function(x) {\\n+     tf &lt;- 1:n %in% x              # convert numerical to logical index vector\\n+     pp &lt;- p[tf]^2 + (1-p[tf])^2   # probabilities of agreeing on questions x\\n+\\n+     # probabilities of disagreeing on remaining questions\\n+     qq &lt;- 1 - (p[!tf]^2 + (1-p[!tf])^2)\\n+     prod(pp) * prod(qq)           # total probability\\n+ }\\n\\n# for each number of agreements: calculate probability\\n&gt; for(i in seq(along=res)) {\\n+     # all choose(n, k) possibilities to have k agreements\\n+     poss &lt;- combn(1:n, k[i])\\n+\\n+     # probability for each of those possibilities, edit: take 0-length into account\\n+     if (length(poss) &gt; 0) {\\n+         res[i] &lt;- sum(apply(poss, 2, getP))\\n+     } else {\\n+         res[i] &lt;- getP(numeric(0))\\n+     }\\n+ }\\n\\n&gt; res                # probability for 4, 5, 6, 7 agreements\\n[1] 0.2734375 0.1640625 0.0546875 0.0078125\\n\\n&gt; dbinom(k, n, 0.5)  # check: all p_i = 0.5 -&gt; binomial distribution\\n[1] 0.2734375 0.1640625 0.0546875 0.0078125\\n\\n&gt; sum(res)           # probability for 4 or more agreements\\n[1] 0.5\\n</code></pre>\\n\\n<p>The R code could certainly be simplified, also <code>prod()</code> might be worse in terms of error propagation with small numbers than <code>exp(sum(log()))</code>, although I\\'m not sure on that one.</p>\\n',\n",
       " \"<p>I think piecewise regression means fitting several different lines at various cut points.  It is not clear whether the number of cutoffs is prespecified and whether their locations are prespecified.  Even if they are all prespecified it seems that each piece would be fit by ordinary regression and the problem of correlationed residuals and under or overestimated residual variance would be there in each line.  There is also another assumption not stated.  Are the residuals for each line assumed to have the same variance as for all the others?  So the problem exists and may be worse in this more complex type of regression.  Regarding IrishStat's comment.  I think he is right for the piecewise model because the breakpoints could be time interventions that affect the stationary component and possibly also the nonstationary component of the model.  But in ordinary harmonic regression the nonstationary seasonal component is modelled by sine and cosine functions of t and the residuals could be white noises or they could be model as a stationary process in time such as an AR(p) model.  </p>\\n\",\n",
       " '<p>Detecting a drift in a random walk is a lot harder than you make it out to be. Random walks are nonstationary sequences that can have long excursions in one direction or another. So they appear to drift even without any change in the process. Since the random walk fits an ARIMA first difference model, you could apply the model to the data and look to see where the residuals seem to go astray.</p>\\n',\n",
       " \"<p>Yes, it is possible to use any arbitrary distribution to get a p-value for <em>any statistic</em>. Theoretically and practically you can calculate (one-sided) p-value by this formula. </p>\\n\\n<p>$$\\\\\\\\mathrm{p-value} = P[T &gt; T_{observed} | H_0 \\\\\\\\quad \\\\\\\\mathrm{holds}]$$</p>\\n\\n<p>Where $T$ is the test-statistic of interest and $T_{observed}$ is the value that you have calculated for the observed data.</p>\\n\\n<p>If you know the theoretical distribution of $T$ under $H_0$, great! Otherwise, you can use MCMC simulation to generate from the <em>null distribution</em> of $T$ and calculate the Monte Carlo integral to obtain <em>p-value</em>. Numerical integration techniques will also work in case you don't want to use (may be) easier Monte Carlo methods (especially in R; in Mathematica integration may be easier, but I have no experience using it)</p>\\n\\n<p>The only assumption you are making here is -- you know the <em>null distribution</em> of T (which may not be in the standard R random number generator formats). That's it -- as long as you know the null distribution, p-value can be calculated.</p>\\n\",\n",
       " \"<p>Studying for a test. Couldn't answer this one. </p>\\n\\n<p>Let $X_{1,i},X_{2,i},X_{3,i}, i=1,\\\\\\\\ldots,n$ be iid $\\\\\\\\mathcal{N}(0,1)$ random variables. Define </p>\\n\\n<p>$W_i = (X_{1,i} + X_{2,i}X_{3,i})/\\\\\\\\sqrt{1 + X_{3,i}^2}, i = 1, \\\\\\\\ldots, n$, </p>\\n\\n<p>and $\\\\\\\\overline{W}_n = n^{-1}\\\\\\\\sum_{i=1}^nW_i$, </p>\\n\\n<p>$S_n^2 = (n-1)^{-1}\\\\\\\\sum_{i=1}^n(W_i - \\\\\\\\overline{W}_n)^2, n \\\\\\\\ge 2.$</p>\\n\\n<p>What is the distribution of $\\\\\\\\overline{W}_n$, $S_n^2$?</p>\\n\\n<p>How do I get an idea of the best method to use when starting a problem like this?</p>\\n\",\n",
       " \"<p>I am used to performing power analysis with unweighted data. In R, I use the pwr package, which is based on Cohen (1988).</p>\\n\\n<p>How would you do power analysis on weighted survey data? I don't think you could use the same calculations. The weights would probably somehow have to figure into it? Is there a reference that discusses this?</p>\\n\",\n",
       " '<p>Finding an unbiased estimator without having a parametric model would be difficult!  But you could use bootstrapping, and use that to correct the empirical median to get an approximately unbiased estimator. </p>\\n',\n",
       " \"<p>I'm implementing a non-linear SVM classifier with RBF kernel. I was told that the only difference from a normal SVM was that I had to simply replace the dot product with a kernel function: \\n$$\\nK(x_i,x_j)=\\\\\\\\exp\\\\\\\\left(-\\\\\\\\frac{||x_i-x_j||^2}{2\\\\\\\\sigma^2}\\\\\\\\right)\\n$$\\nI know how a normal linear SVM works, that is, after solving the quadratic optimization problem (dual task), I compute the optimal dividing hyperplane as \\n$$\\nw^*=\\\\\\\\sum_{i \\\\\\\\in SV} h_i y_i x_i\\n$$\\nand the offset of the hyperplane \\n$$\\nb^*=\\\\\\\\frac{1}{|SV|}\\\\\\\\sum_{i \\\\\\\\in SV}\\\\\\\\left(y_i - \\\\\\\\sum_{j=1}^N\\\\\\\\left(h_j y_j x_j^T x_i\\\\\\\\right)\\\\\\\\right)\\n$$\\nrespectively, where $x$ is a list of my training vectors, $y$ are their respective labels ($y_i \\\\\\\\in \\\\\\\\{-1,1\\\\\\\\}$), $h$ are the Lagrangian coefficients and $SV$ is a set of support vectors. After that, I can use $w^*$ and $b^*$ alone to easily classify: $c_x=\\\\\\\\text{sign}(w^Tx+b)$.</p>\\n\\n<p>However, I don't think I can do such a thing with an RBF kernel. I found some materials suggesting that $K(x,y)=\\\\\\\\phi(x)\\\\\\\\phi(y)$. That would make it easy. Nevertheless, I don't think such a decomposition exists for this kernel and it's not mentioned anywhere. Is the situation so that all the support vectors are needed for the classification? If so, how do I classify in that case?</p>\\n\",\n",
       " '<p>See <a href=\"http://stats.stackexchange.com/questions/257/what-is-the-easiest-way-to-create-publication-quality-plots-under-linux/\">this related question</a>.  All the same advice applies in your case.  Just to highlight a few points:</p>\\n\\n<ol>\\n<li>Using R is a good way to go, especially with the <strong><a href=\"http://had.co.nz/ggplot2/\" rel=\"nofollow\">ggplot2</a></strong> package.  This is both flexible and produces very high quality output.  There are plenty of examples on the ggplot2 website and across the web (including on this website).</li>\\n<li>To improve the quality of your plots, you should consider using a different device driver than the default, and choose a high-quality output (e.g. SVG).  The Cairo package is one good option. You simply call your plot function before plotting and it redirects to Cairo as the output device.  <strong><a href=\"http://cairographics.org/\" rel=\"nofollow\">Cairo</a></strong> can be used with any plotting software, not just with R. </li>\\n<li>In terms of putting your plots together in a LaTeX publication, that\\'s the role that <a href=\"http://www.stat.uni-muenchen.de/~leisch/Sweave/\" rel=\"nofollow\"><strong>Sweave</strong></a> plays. It makes combining plots with your paper a trivial operation (and has the added benefit of leaving you with something that is reproducible and understandable). Use cacheSweave if you have long-running computations.</li>\\n</ol>\\n\\n<p>If you want more specific help on a particular plot, I advise giving more detail.</p>\\n\\n<p><em>Edit</em></p>\\n\\n<p>If you want a specific example, it is best that you provide more specfics.  For more examples of using ggplot, you can also refer to <a href=\"http://learnr.wordpress.com/\" rel=\"nofollow\">the LearnR blog</a>.  Here\\'s an example combining ggplot with Cario for high quality output.</p>\\n\\n<pre><code>CairoPDF(\"plot.pdf\", 6, 6, bg=\"transparent\")\\nqplot(factor(cyl), wt, geom=c(\"boxplot\", \"jitter\"), color=am, data=mtcars)\\ndev.off()\\n</code></pre>\\n\\n<p>You can look at the documentation for these packages by using the R help functions.  </p>\\n',\n",
       " \"<p>I'm not sure the discrepancy is worth worrying about. The exact p-value is clearly 1 for a 2-sided test of p=0.5 given 17 positive responses out of 33, as there's no integer closer to 33/2 than 17. With small or moderate N as here there's no good reason for not doing the exact test (even without a PC, as the cdf of the binomial is commonly included in statistical tables). With a modern PC and decent software it's no problem even for large N, but for large N the resulting p-value will be very similar to that from a z-test anyway.</p>\\n\",\n",
       " \"<p>I am doing a project on finding out whether there is a difference in job satisfaction between employees of non-profit organisations and for-profit organisations. I also need to find whether job satisfaction varies as a function of age (categorized as below 20, 21-30 and so on), gender, and type of job (paid or volunteer). </p>\\n\\n<p>Regarding the job satisfaction, I have summed up their answers and labelled them as being satisfied, ambivalent and dissatisfied. Within the job satisfaction, I also measured their satisfaction on different aspects of their job (eg, pay, promotion) and each of these aspects summed up makes the job satisfaction score. </p>\\n\\n<p>I am so confused on how to analyze the data. I have analyzed them using an independent samples t-test but I have just remembered how my supervisor told me I could use factor analysis. But I have no experience in using factor analysis. Can anyone please help me and advice me on how I should analyze my data? </p>\\n\\n<p>@matt parker: thanks so much for your suggestion. I've only realized that I've done the wrong analysis, 1 week before the deadline! I'll try reading up on factor analysis now and <em>fingers crossed</em> that I'll get everything done by next week! :S</p>\\n\",\n",
       " '<p>Another option seems to be <a href=\"http://en.wikipedia.org/wiki/Dataverse\" rel=\"nofollow\">Dataverse</a>, which is available as a service and as open source software. I did not try it, though.</p>\\n',\n",
       " '<p>I\\'m reading the paper <a href=\"http://www-stat.wharton.upenn.edu/~tcai/paper/Sparse-Covariance-Matrix.pdf\" rel=\"nofollow\">Optimal Rates of Convergence for Sparse Covariance Matrix Estimation</a> by T. Cai and H. Zhou.</p>\\n\\n<p>They refer to a specific version of Assouad\\'s Lemma (see lemma 2, p. 6 of the linked paper), which differs from the one I\\'ve found in Yu [1] and Le Cam [2]. I\\'ve also ordered van der Vaart [3], but it has yet to arrive.</p>\\n\\n<p>Can do you know of a resource that contains a proof of the version Cai states? Or can you give a proof?</p>\\n\\n<p>I\\'m especially interested because I\\'m having trouble with their proof of lemma 3 (p. 7 of linked paper, proof p. 21) and the lemmas seem very related. Specifically, I don\\'t understand how the use of</p>\\n\\n<p>$\\\\\\\\hat{\\\\\\\\theta} = \\\\\\\\mbox{argmin}_{\\\\\\\\theta \\\\\\\\in \\\\\\\\Theta} d(T,\\\\\\\\psi (\\\\\\\\theta))$</p>\\n\\n<p>in eq. (37) is meaningful as $T$ is a random variable.</p>\\n\\n<p>[1] B. Yu (1997). Assouad, Fano, and Le Cam. Festschrift for Lucien Le Cam. D. Pollard, E. Torgersen, and G. Yang (eds), pp. 423-435, Springer-Verlag.</p>\\n\\n<p>[2] Le Cam, L. (1986). Asymptotic Methods in Statistical Decision Theory. SpringerVerlag, New York.</p>\\n\\n<p>[3] van der Vaart, A.W. (1998). Asymptotic Statistics. Cambridge University Press, Cambridge.</p>\\n',\n",
       " \"<p>I'm trying to fit a multivariate multiple regression model where the independent variable X is latent but I don't know where to start (I have prior information about the coefficient matrix so I can use some iterative method).</p>\\n\\n<p>The dependent variable Y is a NxM matrix denoting N observations each from M variables. The latent variable X is a NxP matrix about which we know nothing except the dimensions. In addition to these, we have an initial estimate of the coefficient matrix beta based on prior knowledge. My goal is to find the estimates of latent X and coefficient matrix beta by both using the data matrix Y and the initial coefficient matrix. I thought of constructing an EM algorithm but because of the complexity of multivariate data and latent variable concept, I am totally confused.</p>\\n\\n<p>Thank you.</p>\\n\",\n",
       " '<p>(Sorry for posting a new answer, I can\\'t reply to comments directly yet)</p>\\n\\n<p>I don\\'t really agree with the statement:</p>\\n\\n<p>\"Apparently, if you cross either the UCL or LCL, there has to be an assignable cause\" </p>\\n\\n<p>To keep things simple, if your in control distribution is N(0,1), then you will still obtain false alarms once every 370 observations, on average, using a UCL of 3 and LCL of -3. When the chart signals, the process needs to be investigated. Only then can a reason for the signal be assigned (ie process change or random error.) Setting the UCL and LCL requires the user to balance the desired false alarm/missed detection rate (analogous to the Type I/Type II error trade off in hypothesis testing.) </p>\\n\\n<p>You can also wait until a few signals to actually stop and investigate the process, but in that case, you may detect the shift too late if it really occurred at the first signal. Again, you can\\'t have something for nothing and the user must use their judgment to decide on how to set up the control chart and monitor the process.</p>\\n',\n",
       " '<p>What is the best way to select parameters for a binary neural network classifier? More specifically I have 265 features ranked according to Mutual Information Criterion. I have to determine the optimal number of inputs and the optimal number of hidden layer nodes for use in a multi-layer perceptron (MLP). </p>\\n\\n<p>Note: The selecting of the optimal number of hidden nodes and optimal number of input features are not independent. </p>\\n',\n",
       " '<p>I\\'m confused because I don\\'t understand the context. What is \"accuracy\"? What are you trying to measure? </p>\\n\\n<p>If you want both to be on the \"same scale\" then you  could just transform both to z scores and add them. Then they are on a standard deviation scale.  But multiplying the two sort of gets rid of the \"problem\" (I am not sure if it is a problem, or what the problem is). Let\\'s take a case that\\'s easier to understand: Area of a rectangle is length*width, right? Now, if you have the two measured on different scales, it makes no difference to the product - it just scales up</p>\\n\\n<p>A               B              C                     D       E<br>\\nht in inches    ht in feet     width in inches       A*C     B*C</p>\\n\\n<p>12                 1                12               144      12\\n120                10               12               1440     120\\n12                 1                120              1440     120\\n12                 1                1200             14,440   1200</p>\\n\\n<p>If you multiply A by 10, you multiply B, D and E by 10. Scale has nothing to do with it.</p>\\n\\n<p><em>Variation</em> has something to do with it, and you can make the two measures have equal sd by the z transform, if that\\'s what you want.</p>\\n',\n",
       " \"<p>No, there isn't. In essence, having the columns-wise means is equivalent to having the sums along the columns. With that, you cannot get the sums along the rows. </p>\\n\\n<p>In general, to recover the sum along the rows you'll need to recover the full matrix. Knowing the other sum (in your example, $(5.25 2.75 3.5)$ ) is not enough. This can be seen in matrix form. Essentially, you have $A U = M$  where A is the data matrix, U is a row column with ones (or 1/N) and M is the sums or means along the colums or rows. If you could augment U by adding other operations, so that U is square, you could invert U and recover A. For example, if you had not only the sum of the four rows, but the sum of the first three rows, and the first two rows ... etc... you'd have 4 vector of 3 components, and then you could recover your matrix.</p>\\n\",\n",
       " \"<p>What you are seeing in the example is just a function of looking at a simple two-way table where there aren't any particularly interesting log linear models between independence and saturation.  </p>\\n\\n<p>In a three way table things may be a bit clearer.  Take for example R's built-in data set <code>HairEyeColor</code>.  This has dimensions <code>Hair</code>, <code>Eye</code> and <code>Sex</code>.  </p>\\n\\n<pre><code>ftable(addmargins(HairEyeColor)) ## the data with all margins\\n\\n        Sex Male Female Sum\\nHair  Eye                      \\nBlack Brown       32     36  68\\n      Blue        11      9  20\\n      Hazel       10      5  15\\n      Green        3      2   5\\n      Sum         56     52 108\\nBrown Brown       53     66 119\\n      Blue        50     34  84\\n      Hazel       25     29  54\\n      Green       15     14  29\\n      Sum        143    143 286\\nRed   Brown       10     16  26\\n      Blue        10      7  17\\n      Hazel        7      7  14\\n      Green        7      7  14\\n      Sum         34     37  71\\nBlond Brown        3      4   7\\n      Blue        30     64  94\\n      Hazel        5      5  10\\n      Green        8      8  16\\n      Sum         46     81 127\\nSum   Brown       98    122 220\\n      Blue       101    114 215\\n      Hazel       47     46  93\\n      Green       33     31  64\\n      Sum        279    313 592\\n</code></pre>\\n\\n<p>There are quite a few log linear models we might consider, all of which can be fitted by IPF and some of which need to be (or at least need an iterative procedure of some form).  R's <code>loglin</code> function does fit in fact fit them all with IPF, whether or not a closed form is available.  Let's consider two models.</p>\\n\\n<p>IPF estimates cell values subject to marginal constraints.  This corresponds to log linear models containing (or rather lacking) particular interaction terms.  For example, suppose you got the idea that the connection between hair and eye colour was due entirely to (and thus conditionally independent given) sex.  You'd then fit a model that contained a grand mean, main effects for <code>Sex</code>, <code>Eye</code>, and <code>Hair</code>, and two interaction terms <code>Hair-Sex</code> and <code>Eye-Sex</code>.  This model can be written $[Hair,Sex][Eye,Sex]$ (or as the undirected graphical model $Hair-Sex-Eye$).</p>\\n\\n<p>If you were to add <code>Eye-Hair</code> and <code>Eye-Hair-Sex</code> terms in the model then it would be saturated. If you were to take out the interaction terms then you'd be back to the independence model $[Hair][Sex][Eye]$.</p>\\n\\n<p>Here's the conditional independence model in R:</p>\\n\\n<pre><code>mod1 &lt;- loglin(HairEyeColor, list(c('Eye', 'Sex'), c('Hair', 'Sex')), \\n               fit=TRUE)\\n</code></pre>\\n\\n<p>Looking at its fitted values shows which margins are recovered and which not:</p>\\n\\n<pre><code>ftable(addmargins(mod1$fit))\\n\\n            Sex       Male     Female        Sum\\nHair  Eye                                       \\nBlack Brown      19.670251  20.268371  39.938622\\n      Blue       20.272401  18.939297  39.211699\\n      Hazel       9.433692   7.642173  17.075864\\n      Green       6.623656   5.150160  11.773816\\n      Sum        56.000000  52.000000 108.000000\\nBrown Brown      50.229391  55.738019 105.967410\\n      Blue       51.767025  52.083067 103.850092\\n      Hazel      24.089606  21.015974  45.105580\\n      Green      16.913978  14.162939  31.076918\\n      Sum       143.000000 143.000000 286.000000\\nRed   Brown      11.942652  14.421725  26.364378\\n      Blue       12.308244  13.476038  25.784282\\n      Hazel       5.727599   5.437700  11.165298\\n      Green       4.021505   3.664537   7.686042\\n      Sum        34.000000  37.000000  71.000000\\nBlond Brown      16.157706  31.571885  47.729591\\n      Blue       16.652330  29.501597  46.153927\\n      Hazel       7.749104  11.904153  19.653257\\n      Green       5.440860   8.022364  13.463224\\n      Sum        46.000000  81.000000 127.000000\\nSum   Brown      98.000000 122.000000 220.000000\\n      Blue      101.000000 114.000000 215.000000\\n      Hazel      47.000000  46.000000  93.000000\\n      Green      33.000000  31.000000  64.000000\\n      Sum       279.000000 313.000000 592.000000\\n</code></pre>\\n\\n<p>Notice that all the margins except the one in the right hand column are the same as in the original data.</p>\\n\\n<p>An alternative model that captures this margin while still being neither the pure independence model nor saturated is $[Hair,Sex][Eye,Sex][Eye,Hair]$:</p>\\n\\n<pre><code>mod2 &lt;- loglin(HairEyeColor, list(c('Eye', 'Sex'), c('Hair', 'Sex'), \\n               c('Hair','Eye')), fit=TRUE)\\n</code></pre>\\n\\n<p>This model assumes that all odds ratios between any two variables are the same at each level of the third ('homogeneous association').  This takes six IPF iterations to fit and has no closed form estimates.  </p>\\n\\n<pre><code>ftable(addmargins(mod2$fit))\\n\\n            Sex       Male     Female        Sum\\nHair  Eye                                       \\nBlack Brown      32.793454  35.206546  68.000000\\n      Blue       11.743353   8.256647  20.000000\\n      Hazel       8.444542   6.555458  15.000000\\n      Green       3.018490   1.981510   5.000000\\n      Sum        55.999839  52.000161 108.000000\\nBrown Brown      52.524536  66.475464 119.000000\\n      Blue       45.930572  38.069428  84.000000\\n      Hazel      28.196301  25.803699  54.000000\\n      Green      16.348411  12.651589  29.000000\\n      Sum       142.999820 143.000180 286.000000\\nRed   Brown      10.760594  15.239406  26.000000\\n      Blue        8.819781   8.180219  17.000000\\n      Hazel       6.916815   7.083185  14.000000\\n      Green       7.502806   6.497194  14.000000\\n      Sum        33.999996  37.000004  71.000000\\nBlond Brown       1.926579   5.073421   7.000000\\n      Blue       34.500428  59.499572  94.000000\\n      Hazel       3.443330   6.556670  10.000000\\n      Green       6.129759   9.870241  16.000000\\n      Sum        46.000097  80.999903 127.000000\\nSum   Brown      98.005164 121.994836 220.000000\\n      Blue      100.994134 114.005866 215.000000\\n      Hazel      47.000988  45.999012  93.000000\\n      Green      32.999465  31.000535  64.000000\\n      Sum       278.999751 313.000249 592.000000\\n</code></pre>\\n\\n<p>As expected, its fitted values now also recover the third column margins that <code>mod1</code> did not because of the extra interaction term.</p>\\n\\n<p>As @gjabel notes, Agresti 2002 contains a nice clear discussion.  Hope that helps. </p>\\n\",\n",
       " '<p>I have the following problem: two groups A and B.</p>\\n\\n<p>The proportions of married and single are ($p_1$) and ($p_2$) respectively, and the standard deviations ($s_1$) and ($s_2$). The sample consists of  ($n_1$) A and  ($n_2$) B observations.</p>\\n\\n<p>Now I want to test whether the proportions are equal. I use the following formula to compute the z-statistic:</p>\\n\\n<p>z = $abs(p_1 - p_2) / \\\\\\\\sqrt{ \\\\\\\\frac{p_1(1-p_1)}{n_1} + \\\\\\\\frac{p_2(1-p_2)}{n_2} }$</p>\\n\\n<p>and test the hypothesis:\\nh0: $p_1 \\\\\\\\neq p_2$</p>\\n\\n<p>An alternative approach I tried is to estimate the t-value based on the standard deviations (hence just comparing means), that is by the formula (assuming unequal variances):</p>\\n\\n<p>t = $abs(p_1-p_2)/\\\\\\\\sqrt{ s_1^2/n_1 + s_2^2/n_2 }$</p>\\n\\n<p>My question are: </p>\\n\\n<ol>\\n<li>do you agree with my approach or;</li>\\n<li>should it be two-sided or one-sided?</li>\\n</ol>\\n\\n<p>Thank you.</p>\\n',\n",
       " \"<p>Let's say I am analyzing behavioral patterns over the course of an hour.  I have recorded three different behaviors and the time stamps (start end) they occurred at.  Something like:</p>\\n\\n<pre><code>yawning       stretching    whispering\\n2:21-2:22     3:31-3:33     1:21-1:30\\n3:42-3:45     8:23-8:59     9:27-9:33\\n9:20-925      9:34-9:44     14:04-14:07\\n14:45-14:32   15:01-15:06   18:00-18:22\\n.\\n.\\n.\\n45:40-45-43   45:23-45:30   44:19-44:44\\n</code></pre>\\n\\n<p>Is there a statistical method for determining if certain behaviors correlate or cluster around certain time periods/to each other?  For instance maybe I want to know if these three (or just 2) behaviors are found in close proximity to one another or maybe I want to know if these which behaviors are not in close proximity to each other.  Which of the three behaviors tend to cluster together?</p>\\n\\n<p>I don't even know what field of stats I'm looking at with this.</p>\\n\",\n",
       " '<p>If we assume that our data points were sampled from the surface of a sphere (with some perturbation), how can we recover the center of that sphere?  </p>\\n\\n<p>In my searching, I found papers on something labeled \"spherical regression\", but it didn\\'t quite seem like that was doing the same thing.  Maybe I just didn\\'t understand it.</p>\\n\\n<p>Is there a straightforward formula, similar to linear regression, that finds a sphere center point and radius that minimize the sum-squared distance of a set of data points from the surface of the sphere?</p>\\n\\n<hr>\\n\\n<p>Edit 1:</p>\\n\\n<p>We can assume that the noise will be 2 or 3 orders of magnitude smaller than the radius of the sphere and uniformly spherically Gaussian.  However, the samples themselves will definitely not be drawn uniformly from the sphere\\'s surface, but will likely be clustered in a few patches on the surface, likely all within one hemisphere.  A solution that works for data in $\\\\\\\\mathbb R^3$ is fine, but a general solution for arbitrary dimensionality is great too.</p>\\n\\n<hr>\\n\\n<p>Edit 2:</p>\\n\\n<p>What are the chances that I might get a sensible answer if I were to use linear regression, $y = X\\\\\\\\beta + \\\\\\\\epsilon$, in the 7 dimensional space pretending that the squared components are independent from the other parameters:</p>\\n\\n<p>$\\\\\\\\begin{align} X &amp;= \\\\\\\\begin{array}{ccccccc}[-2x&amp; -2y&amp;-2z&amp;1&amp;1&amp;1&amp;-1]\\\\\\\\end{array}\\\\\\\\\\\\\\\\ \\\\\\\\beta &amp;= \\\\\\\\begin{array}{ccccccc}[x_0 &amp; y_0 &amp; z_0 &amp; x_0^2 &amp; y_0^2 &amp; z_0^2 &amp; r^2]&#39;\\\\\\\\end{array}\\\\\\\\\\\\\\\\\\\\\\\\ny &amp;= x^2+y^2+z^2\\\\\\\\end{align}$</p>\\n\\n<p>At best, I suppose that my error metric will be a bit wacky.  At worst the solution won\\'t be even close to consistent.<br>\\n...or that\\'s silly because with four identical columns, we get a singular matrix when we try to do regression.</p>\\n\\n<hr>\\n\\n<p>Edit 3:</p>\\n\\n<p>So, it looks like these are my options:</p>\\n\\n<ol>\\n<li>Non-linear numerical optimization using some cost function: $f(x_0,y_0,z_0,r|X) = \\\\\\\\frac{1}{2}\\\\\\\\sum_{i=1}^n \\\\\\\\left(r - \\\\\\\\sqrt{(x_i-x_0)^2+(y_i-y_0)^2+(z_i-z_0)^2}\\\\\\\\right)^2$</li>\\n<li>Hough-transform: discretize the plausible space or possible centers and radii around the data points.  Each point casts a vote for the potential centers that it could be part of at each specific radius discretization.  Most votes wins.  This might be okay if there were potentially an unknown number of spheres, but with just one it\\'s a messy solution.</li>\\n<li>Randomly (or systematically) select groups of 4 points and <a href=\"http://paulbourke.net/geometry/spherefrom4/\" rel=\"nofollow\">analytically compute the center</a>.  Reject the sampling if ill-conditioned (points are nearly co-planar).  Reject outliers and find the mean center.  From that we can find the mean radius.</li>\\n</ol>\\n\\n<p>Does anyone have a better method?</p>\\n',\n",
       " '<p>You are asking quite a tricky question!</p>\\n\\n<p>This is outside my area of expertise, but I know that <a href=\"http://www.mcs.open.ac.uk/People/c.p.farrington\" rel=\"nofollow\">Prof Farrington</a> does some work on this problem. So I would look at a some of his papers and follow a few of his references. To get you started, this <a href=\"http://stats-www.open.ac.uk/TechnicalReports/Cusum.pdf\" rel=\"nofollow\">report</a> looks relevant.</p>\\n',\n",
       " '<pre><code>library(xts)\\n?endpoints\\n</code></pre>\\n\\n<p>Fo instance</p>\\n\\n<pre><code>tmp=zoo(rnorm(1000), as.POSIXct(\"2010-02-1\")+(1:1000)*60)\\ntmp[endpoints(tmp, \"minutes\", 20)]\\n</code></pre>\\n\\n<p>to subsample every 20 minutes</p>\\n\\n<p>You might also want to check out to.minutes, to.daily, etc.</p>\\n',\n",
       " '<p>Another piece of advise might be to look at packages yours will be depending on or interacting with, especially if these implement some <a href=\"http://stats.stackexchange.com/questions/5418/first-r-packages-source-code-to-study-in-preparation-for-writing-own-package/5433#5433\">items</a> <a href=\"http://stats.stackexchange.com/users/1657/joshua-ulrich\">Joshua Ulrich</a> mentioned or have been written by renowned authors. It might be helpful to learn how things are done in your field, to ensure some compatibility. Often people will have thought about certain issues and reading their solution migth be helpful. </p>\\n',\n",
       " \"<p>Your example is a little weaker than it could be, because there don't really seem to be any actions, however, we can work with that; I'll preserve the $\\\\\\\\max_a$ and $R(s,a)$ pieces of notation, although $a$ isn't operative in the example.  I'm assuming that from state 1 there is a 50-50 chance of transitioning to either state 2 or state 3, and that the discount rate is also 50%, as per your example.  I also assume the rewards $R(s,a)$ for being in states 1, 2, and 3 are 0, 2, and 0 respectively.</p>\\n\\n<p>Value iteration iterates until the values converge.  At the first iteration, as you have, $V_0(s) = 0 \\\\\\\\space \\\\\\\\forall s$.  </p>\\n\\n<p>At iteration 1:</p>\\n\\n<p>$V_1(s=3) = \\\\\\\\max_a\\\\\\\\{R(3,a)\\\\\\\\} = 0$ as state 3 is a terminal state, so no transitions.</p>\\n\\n<p>$V_1(s=2) = \\\\\\\\max_a\\\\\\\\{R(2,a)\\\\\\\\} = 2$ as state 2 is a terminal state, so no transitions.</p>\\n\\n<p>$V_1(s=1) = \\\\\\\\max_a\\\\\\\\{R(1,a) + 0.5(0.5*V_0(s=2) + 0.5*V_0(s=3))\\\\\\\\} = 0$</p>\\n\\n<p>At iteration 2:</p>\\n\\n<p>$V_2(s=3) = \\\\\\\\max_a\\\\\\\\{R(3,a)\\\\\\\\} = 0$ as state 3 is a terminal state, so no transitions.</p>\\n\\n<p>$V_2(s=2) = \\\\\\\\max_a\\\\\\\\{R(2,a)\\\\\\\\} = 2$ as state 2 is a terminal state, so no transitions.</p>\\n\\n<p>$V_2(s=1) = \\\\\\\\max_a\\\\\\\\{R(1,a) + 0.5(0.5*V_1(s=2) + 0.5*V_1(s=3))\\\\\\\\} = 0.5$</p>\\n\\n<p>At iteration 3:</p>\\n\\n<p>$V_3(s=3) = \\\\\\\\max_a\\\\\\\\{R(3,a)\\\\\\\\} = 0$ as state 3 is a terminal state, so no transitions.</p>\\n\\n<p>$V_3(s=2) = \\\\\\\\max_a\\\\\\\\{R(2,a)\\\\\\\\} = 2$ as state 2 is a terminal state, so no transitions.</p>\\n\\n<p>$V_3(s=1) = \\\\\\\\max_a\\\\\\\\{R(1,a) + 0.5(0.5*V_2(s=2) + 0.5*V_2(s=3))\\\\\\\\} = 0.5$</p>\\n\\n<p>And we have convergence!  All the $V_3(s) = V_2(s)$.  In a more complex example, with actions included, $V_3(s) = V_2(s)$ implies that the actions selected at iteration 3 and at iteration 2 are either equal or equivalent in terms of value; having part of the algorithm be a fixed way of choosing between actions that are tied, in terms of expected value, reduces the implication to the actions being the same between iterations. </p>\\n\",\n",
       " '<p>I would consider <a href=\"http://onlinelibrary.wiley.com/doi/10.1002/sim.3781/abstract;jsessionid=3EFE478F77EF4F805F8479F0C5DF1F28.d03t01\" rel=\"nofollow\">logistic quantile regression for bounded outcome</a>. If you\\'re a Stata user, there\\'s also a <a href=\"http://www.stata-journal.com/article.html?article=st0231\" rel=\"nofollow\">command</a> (<code>lqreg</code>) for the estimation, prediction and graphical representation of a logistic quantile regression.  </p>\\n',\n",
       " \"<p>I am comparing two distributions with KL divergence which returns me a non-standardized number that, according to what I read about this measure, is the amount of information that is required to transform one hypothesis into the other. I have two questions:</p>\\n\\n<p>a) Is there a way to quantify a KL divergence so that it has a more meaningful interpretation, e.g. like an effect size or a R^2? Any form of standardization?</p>\\n\\n<p>b) In R, when using KLdiv (flexmix package) one can set the 'esp' value (standard esp=1e-4) that sets all points smaller than esp to some standard in order to provide numerical stability. I have been playing with different esp values and, for my data set, I am getting an increasingly larger KL divergence the smaller a number I pick. What is going on? I would expect that the smaller the esp, the more reliable the results should be since they let more 'real values' become part of the statistic. No? I have to change the esp since it otherwise does not calculate the statistic but simply shows up as NA in the result table...</p>\\n\\n<p>Thanks in advance,\\nAmpleforth </p>\\n\",\n",
       " '<p>Asymptotically, the ratio of positive to negative patterns is essentially irrelevant.  The problem arises principally when you have too few samples of the minority class to adequately describe its statistical distribution.  Making the dataset larger generally solves the problem (where that is possible).</p>\\n\\n<p>If this is not possible, the best thing to do is to re-sample the data to get a balanced dataset, and then apply a multiplicative adjustment to the output of the classifier to compensate for the difference between training set and operational relative class frequencies.  While you can calculate the (asymptotically) optimal adjustment factor, in practice it is best to tune the adjustment using cross-validation (as we are dealing with a finite practical case rather than an asymptotic one).</p>\\n\\n<p>In this sort of situation, I often use a committee of models, where each is trained on all of the minority patterns and a different random sample of the majority patterns of the same size as the minority patterns.  This guards against bad luck in the selection of a single subset of the majority patterns.</p>\\n',\n",
       " '<h2>Introduction</h2>\\n\\n<p>For some time now I have been struggling to understand how theoretical results can be applied in practice. Fortunately in most cases the link between theory and practice is not hard to find, for instance:</p>\\n\\n<ul>\\n<li>You can directly use a theoretical result for your calculations.</li>\\n<li>You cannot actually find a solution, but you at least have an upper or lower bound that can indicate how good your real solution is.</li>\\n<li>By observing the theoretical formulas you can make something \\'similar\\' and hopefully with similar performance</li>\\n<li>The general case is only theoretical, but in special cases it can be applied directly.</li>\\n</ul>\\n\\n<p>Well, this is not everything, but I won\\'t try to make a complete list of how statistical theory can be useful. Until now I have always succeeded in at least grasping the idea of how theory can be applied in practice.</p>\\n\\n<p>However, it now appears that I have stumbled opon something of which I cannot see how it could possibly be applied.</p>\\n\\n<p>I was studying some smoothing methods, amongst which (nonparametric) kernel smoothing, as described on <a href=\"http://en.wikipedia.org/wiki/Kernel_density_estimation\" rel=\"nofollow\">Wikipedia</a> , and found that there is a theoretical solution for the <code>AMISE</code>, as well as the bandwidth,<code>hAMISE</code> that optimizes it. However, both in the notes that I was reading, as well as the Wikipedia page I have been unable to find any practical application of this. </p>\\n\\n<h2>The Question</h2>\\n\\n<p><strong>Is there any practical application for the <code>AMISE</code> and its optimal bandwidth?</strong></p>\\n\\n<p>Discovering that there actually is an application for would be very motivating so I  hope it can be achieved!</p>\\n',\n",
       " '<p>I have 6 random number generators. They are \"black boxes\", i.e. I do not know if they are the same or different. For example I do not know if they provide the same arithmetic averages and/or root mean square deviations. My goal is to check if they are the same or different. The problem is that I can generate a limited set of numbers (every random number generator gives me only 20 numbers).</p>\\n\\n<p>The procedure that I use is as following.</p>\\n\\n<ol>\\n<li>I choose a pair of generators.</li>\\n<li>I use each of them to generate 20 numbers.</li>\\n<li>For every generator I use these numbers to calculate the mean.</li>\\n<li>Then I calculate the difference between the means (mean1 - mean2). I will call this difference as \"reference difference\".</li>\\n</ol>\\n\\n<p>Now I want to know if this difference is \"real\". In other words I want to know if the observed difference in means is caused by the fact that I do have two different random number generators or I have identical generators and the mean is different just because of chance (small number of numbers).</p>\\n\\n<p>To answer the last question I use the following procedure.</p>\\n\\n<ol>\\n<li>I combine the two sets of numbers (20 numbers + 20 numbers).</li>\\n<li>Then I randomly split this set into two subsets of equal size.</li>\\n<li>For each subset I calculate means and, as before, I calculate the difference between the two means (I will call this difference as \"test difference\").</li>\\n<li>I compare this test difference with the reference difference.</li>\\n<li>I repeat this procedure many times and to see in home many cases the test difference is smaller then the reference difference.</li>\\n</ol>\\n\\n<p>I thought that if the reference difference is larger than the test difference in most of the cases than it is conditioned not by chance but by the fact that the compared random number generators are different. In contrast, if the generators are identical than the test difference has 50% of chances to be larger (or smaller) then the reference one.</p>\\n\\n<p>However, I see that for the most of the considered pairs of the generators the reference difference between the averages is smaller than the test differences. How could it be? <strong>It looks like the the generators are more similar than identical generators?</strong></p>\\n',\n",
       " '<p>I have random sets of $N$ random 32-bit strings,\\nwhere all bits are i.i.d. with $\\\\\\\\mathbb{P}(0) = \\\\\\\\mathbb{P}(1) = 1/2$.\\nDefine<br>\\n$\\\\\\\\ \\\\\\\\ \\\\\\\\ \\\\\\\\ $weight( 32-bit x ) = number of 1 bits in x, i.e. Hamming distance to 0<br>\\n$\\\\\\\\ \\\\\\\\ \\\\\\\\ \\\\\\\\ $minweight( set $X$ ) = min$_{\\\\\\\\text{x} \\\\\\\\in X}$ weight( x )</p>\\n\\n<p>How does the average minweight( $X$ ) vary with $N$, the size of $X$ ?</p>\\n\\n<p>My $N$ is small enough that it doesn\\'t matter whether $X$ has duplicates, or not.<br>\\nA back-of-the envelope estimate would be fine.</p>\\n\\n<p>(A harder question is to analyze the algorithm for\\n<a href=\"http://stackoverflow.com/questions/9959728/bit-string-nearest-neighbour-searching\">bit-string-nearest-neighbour-searching</a>\\non stackoverflow,\\nbut that seems rather specialized.\\nIf anyone could suggest another forum or reference,\\nI\\'d appreciate it.)</p>\\n',\n",
       " '<p>I have  a regression with two continuous predictors and one dichotomous predictor in Model 1 and two interactions of each of the continuous predictors with the dichotomous predictor in Model 2. The coefficient for one of the interaction terms is significant. However, the F test is not significant (neither for Model 1 nor for Model 2). Should I still interpret that significant interaction or should I just assume that neither the predictors, nor the interaction terms are useful in predicting the outcome variable?</p>\\n',\n",
       " '<p>I\\'m working on a problem that I\\'ve cast as an HMM, except that unlike the \"traditional\" case where the transition probabilities $a(i,j) = p(s_i = j \\\\\\\\,|\\\\\\\\, s_{i-1}=i)$, emission probabilities $b(j,o) = p(o_i | s_i = j)$ and initial probabilities $\\\\\\\\pi(j) = p(s_0 = j)$ are all just independent numbers, in my case they are (relatively complicated) functions of a set of common parameters. I\\'m reasonably sure that this means I can\\'t rely on Baum-Welch to find the optimal parameters (because Baum-Welch assumes that the three sets of probabilities can be optimized independently), and I\\'ve tried my hand at directly optimizing the likelihood by just computing the likelihood using the forward algorithm. However, this has proved slow (my sequence is extremely long) and somewhat unreliable (the likelihood surface is a bit weird looking). A friend suggested that both of those problems could be solved if I used an EM algorithm (using forward-backward probabilities in the E-step and numerically optimizing in the M-step).</p>\\n\\n<p>However, I\\'m having a hell of a time figuring out exactly what the Q function should be. I believe it is \\n$$\\nQ(\\\\\\\\theta\\\\\\\\,|\\\\\\\\, \\\\\\\\theta^{t}) = \\\\\\\\sum_j\\\\\\\\log(\\\\\\\\pi(j))p(s_0 = j \\\\\\\\,|\\\\\\\\, x, \\\\\\\\theta^{t}) \\\\\\\\\\\\\\\\+ \\\\\\\\sum_t\\\\\\\\left[\\\\\\\\sum_{i,j} \\\\\\\\log(a(i,j))p(s_t = j, s_{t-1} = i \\\\\\\\,|\\\\\\\\, x, \\\\\\\\theta^{t}) \\\\\\\\right]\\\\\\\\\\\\\\\\+\\\\\\\\sum_t\\\\\\\\left[\\\\\\\\sum_j\\\\\\\\log(b(j,o))p(s_t = j \\\\\\\\,|\\\\\\\\, x, \\\\\\\\theta^{t}) \\\\\\\\right].\\n$$\\nHowever, it\\'s not clear to me that this will make life any easier, as it still looks like it\\'s $O(nm^2)$, where $n$ is the length of the sequence and $m$ is the number of hidden states, time to evaluate. And that\\'s in addition to forward-backward run to calculate all the posterior probabilities necessary during the E step.</p>\\n\\n<p>So I feel like I must be doing something wrong or else missing the point of using the EM algorithm rather than directly optimizing the likelihood in this problem. </p>\\n',\n",
       " \"<p>Agresti 2007 discusses them.  They're in chapter 9 and 10. The 2002 edition probably discusses them too, as @suncoolsu mentioned.</p>\\n\\n<p>Agresti refers to the group of response variables is referred to as a <em>cluster</em> and discusses their analysis with marginal models, conditional models and generalized estimating equations.</p>\\n\",\n",
       " \"<p>It will depend on what kind of GLM you're using and your data. For example, the Wald chi-square and likelihood test are good statistics for categorical data. </p>\\n\",\n",
       " '<p>Use Laplacian Eigenmaps, Locally Linear Embedding or Local Tangent Space Alignment. These methods map the distances in a low-dimensional space non-linearly, thereby preserving the local neighborhood/ local geometry. A very preliminary technique would be classical metric multidimensional scaling- but this would be a linear projection, and hence it is fairly possible that the local neighborhood is not preserved accurately, unless the intrinsic dimensionality of the data is low (one way of interpreting this is- the data can be represented as a kernel gram matrix, which has a low-error low rank approximation). </p>\\n',\n",
       " '<p>Check out the following, where an ad hoc transformation is mentioned <a href=\"https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDAQFjAA&amp;url=http://maartenbuis.nl/presentations/berlin10.pdf&amp;ei=HeL5UPCDIcjg2QWNkIGwBw&amp;usg=AFQjCNEvhafbtVVcFRPcr91hWOYLL4G2wg&amp;bvm=bv.41248874,d.b2I\" rel=\"nofollow\">maartenbuis.nl/presentations/berlin10.pdf</a> on slide 17. Also you could modeling 0 and 1 with two separate logistic regressions and then use Beta regression for those not at the boundary.</p>\\n',\n",
       " \"<p>The proportions $p_1$ and $p_2$ you use in the variance should be the true proportions under the null hypothesis, where $p_1=p_2+c$.  Obviously you don't know what these are (other than $c$) so you have to estimate them from your data.  Either of the formulae you have reasonable estimates, but it is possible to combine them.</p>\\n\\n<p>Consider we start with the obvious estimates $\\\\\\\\hat{p_1}=\\\\\\\\frac{X_1}{n_1}$ and $\\\\\\\\hat{p_2}=\\\\\\\\frac{X_2}{n_2}$.  This won't do because they are unlikely to be exactly $c$ different.  Consider an alternative estimate of $\\\\\\\\hat{p_1}=\\\\\\\\frac{X_2}{n_2}+c=\\\\\\\\hat{p_2}+c$.  Then you might construct a weighted average of your two estimates of $p_1$:</p>\\n\\n<p>$\\\\\\\\hat{p_{1b}}=\\\\\\\\frac{n_2(\\\\\\\\hat{p_2}+c)+n_1\\\\\\\\hat{p_1}}{n_1+n_2}$</p>\\n\\n<p>which reduces to:</p>\\n\\n<p>$\\\\\\\\hat{p_{1b}}=\\\\\\\\frac{X_2+n_{2}c+X_1}{n_1+n_2}$</p>\\n\\n<p>So then I'd use that in the formula to estimate the standard error of the distribution</p>\\n\\n<p>$\\\\\\\\sigma_{\\\\\\\\hat{p}_{1b} -  \\\\\\\\hat{p}_2}=\\\\\\\\sqrt{\\\\\\\\frac{\\\\\\\\hat{p}_{1b}(1-\\\\\\\\hat{p}_{1b})}{n_1}+\\\\\\\\frac{\\\\\\\\hat{p}_2(1-\\\\\\\\hat{p}_2)}{n_2}}$</p>\\n\\n<p>In practice, my experience is that this sort of thing doesn't make much difference...</p>\\n\\n<p>With regard to your post script, the reason the difference is asymptotically normal is, as you guess, because the linear combination of two normal distributed random variables is also normally distributed (and the same goes when the normality is only asymptotic).</p>\\n\",\n",
       " '<p>I think the advice you got is wrong.</p>\\n\\n<p>First, it\\'s rather strange to think of the skewness of a variable that is ordinal and has 5 categories. Skewness implies interval level data; your scale is not interval level as it stands, but you could make it so by converting it into \"times per month met\" then\\ndaily = 30,\\ntwice a week = 8, \\nweekly = 4,\\nfortnightly = 2\\n6% monthly = 1.</p>\\n\\n<p>But dichotomizing the data makes \"monthly\" the same as \"weekly\" and \"daily\" the same as \"twice a week\". These aren\\'t the same. Also, dichotomizing differently (e.g. putting \"weekly\" with \"daily\" and \"2x a week\" might give very different results. That can\\'t be a good thing.</p>\\n',\n",
       " \"<p>I am graduating with a bachelor's degree in applied math and I will pursue a master's degree in statistics this fall. </p>\\n\\n<p>There are many specialized fields in applied statistics. I realize I may be more interested in becoming a data analyst, working with big data in market research or IT, but I'm not sure what kinds of skills are needed and what are the core courses I should take to become a data analyst.</p>\\n\\n<p>I've learned some MatLab, R, SPSS, C, ruby, can't say I'm an expert on these. I'm learning SAS, because SAS seems to be what every stats master should know. (Is this really true?)</p>\\n\\n<p>I'm a little confused right now and need some specific advice about what skills I should intensively learn and what courses I should take.</p>\\n\\n<p>TIA.</p>\\n\",\n",
       " '<p>All three methods should produce the same result for the house edge.  To check yours, you should find </p>\\n\\n<pre><code>Number 1s     Prob\\n   0        125/216 \\n   1         75/216\\n   2         15/216\\n   3          1/216\\n</code></pre>\\n\\n<p>which would make your expected winnings (for a stake of 1) </p>\\n\\n<p>$$\\\\\\\\frac{-1\\\\\\\\times 125 + 1\\\\\\\\times 75 + 2 \\\\\\\\times 15 + 3 \\\\\\\\times 1}{216} = - \\\\\\\\frac{17}{216} \\\\\\\\approx -0.0787.$$</p>\\n\\n<p>You may need to check your calculations again.</p>\\n',\n",
       " '<p>As @gung notes, there are varying conventions regarding the meaning of ($\\\\\\\\beta$, i.e., \"beta\"). In the broader statistical literature, beta is often used to represent unstandardised coefficients. However, in psychology (and perhaps other areas), there is often a distinction between b for unstandardised and beta for standardised coefficients. This answer assumes that the context indicates that beta is representing standardised coefficients: </p>\\n\\n<ul>\\n<li><p><strong>Beta weights:</strong> As @whuber mentioned, \"beta weights\" are by convention standardised regression coefficients (see <a href=\"http://en.wikipedia.org/wiki/Standardized_coefficient\" rel=\"nofollow\">wikipedia on standardised coefficient</a>).\\nIn this context, $b$ is often used for unstandardised coefficients and $\\\\\\\\beta$ is often used for standardised coefficients.</p></li>\\n<li><p><strong>Basic interpretation</strong>: A beta weight for a given predictor variable is the predicted difference in the outcome variable in standard units for a one standard deviation increase on the given predictor variable holding all other predictors constant.</p></li>\\n<li><p><strong>General resource on multiple regression:</strong> The question is elementary and implies that you should read some general material on multiple regression (<a href=\"http://www.statisticshell.com/multireg.pdf\" rel=\"nofollow\">here is an elementary description by Andy Field</a>).</p></li>\\n<li><p><strong>Causality:</strong> Be careful of language like \"the dependent variable has increased in response to greater use of the independent variable\". Such language has causal connotations. Beta weights by themselves are not enough to justify a causal interpretation. You would require additional evidence to justify a causal interpretation.</p></li>\\n</ul>\\n',\n",
       " '<p>If you have \"continuous\" (seemingly, as they could still be discrete) values in between 0 and 1 there are at least two cases:</p>\\n\\n<ol>\\n<li>They came from a number of independent binary trials and the \"continuous\" value is the number of successes divided by trials. Then a binomial GLM might be appropriate. In this case you need to fit it in R as <code>glm(cbind(numberSuccesses,numberFailures)~x,family=binomial)</code> </li>\\n<li>If that is not the case, then you might have something for which a <a href=\"http://epub.wu.ac.at/726/\" rel=\"nofollow\">Beta Model</a> might be more appropriate. The link I provided shows how to do that in R.</li>\\n</ol>\\n\\n<p>Note that in R <code>glm(y~x,family=binomial)</code> with a \"continuous\" $y$ will throw a warning and in general the result will not be the same as in the case with number of successes and trials:</p>\\n\\n<pre><code>set.seed(1)\\nsuccesses&lt;-sample(1:10,100,replace=TRUE)\\nx&lt;-1:100\\nn&lt;-12\\nfailures&lt;-n-successes\\n\\nsummary(glm(cbind(successes,failures)~x,family=binomial))\\nCall:\\nglm(formula = cbind(successes, failures) ~ x, family = binomial)\\n\\nDeviance Residuals: \\n    Min       1Q   Median       3Q      Max  \\n-2.8197  -0.9434   0.0454   0.9358   2.4921  \\n\\nCoefficients:\\n            Estimate Std. Error z value Pr(&gt;|z|)  \\n(Intercept) -0.24622    0.11349   -2.17     0.03 *\\nx            0.00080    0.00195    0.41     0.68  \\n---\\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 \\n\\n(Dispersion parameter for binomial family taken to be 1)\\n\\n    Null deviance: 134.99  on 99  degrees of freedom\\nResidual deviance: 134.82  on 98  degrees of freedom\\nAIC: 422.2\\n\\nNumber of Fisher Scoring iterations: 3\\n</code></pre>\\n\\n<p>but </p>\\n\\n<pre><code>props&lt;-successes/n\\nsummary(glm(props~x,family=binomial))\\n\\nCall:\\nglm(formula = props ~ x, family = binomial)\\n\\nDeviance Residuals: \\n   Min      1Q  Median      3Q     Max  \\n-0.852  -0.282  -0.105   0.394   0.760  \\n\\nCoefficients:\\n             Estimate Std. Error z value Pr(&gt;|z|)\\n(Intercept) -0.134339   0.403836   -0.33     0.74\\nx            0.000281   0.006941    0.04     0.97\\n\\n(Dispersion parameter for binomial family taken to be 1)\\n\\n    Null deviance: 20.888  on 99  degrees of freedom\\nResidual deviance: 20.887  on 98  degrees of freedom\\nAIC: 141.3\\n\\nNumber of Fisher Scoring iterations: 3\\n\\nWarning message:\\nIn eval(expr, envir, enclos) : non-integer #successes in a binomial glm!\\n</code></pre>\\n',\n",
       " \"<p>Whuber's answer is excellent indeed. \\nAnother way, which is more straightforward/simple is to : </p>\\n\\n<ol>\\n<li><p>Calculate the confidence interval of the promoters and for the detractors. Let us denote a P for promoters and D for detractors </p></li>\\n<li><p>Then calculate : <code>(P + error of p) - (D - error of d)</code> for the highest end of NPS confidence interval and <code>(P - error of p) - (D + error of d)</code> for lowest end of the NPS confidence interval  </p></li>\\n</ol>\\n\\n<p>In other words, to get NPS confidence interval, just add the error of promoters and the error of detractors and NPS +/- sum of both errors will be your confidence interval. </p>\\n\\n<p>Now after you have the confidence interval of your 1st NPS result and the confidence interval of your 2nd NPS result, compare these two to assess if there is a significant difference.</p>\\n\",\n",
       " \"<p>I'm preparing a presentation about parallel statistics. I plan to illustrate the formulas for distributed computation of the mean and variance with examples involving center of gravity and moment of inertia. I'm wondering if there is a physical interpretation of the third or higher central moments that I can use to help illustrate the general formula.</p>\\n\",\n",
       " '<p>There is a lot of literature for testing the change in mean. If it is known that mean does not change, and you need to test the variance, you can convert the problem of testing for change in variance to the one of testing for change in mean with simple transformation. </p>\\n\\n<p>Suppose your initial data is $X_i$, then define $Y_i=(X_i-\\\\\\\\mu)^2$, where $\\\\\\\\mu$ is the mean. Then the change in mean of $Y_i$, $EY_i=E(X_i-\\\\\\\\mu)^2=Var(X_i)$ will be a change of variance in $X_i$.</p>\\n',\n",
       " \"<p>Is it possible to fit an ARIMA under a restriction that bounds the minimum to positive values? I'm using Minitab. </p>\\n\",\n",
       " '<p>How should I elicit prior distributions from experts when fitting a Bayesian model?</p>\\n',\n",
       " '<p>The null hypothesis is that there are no structural breaks. Since your p-value is $0.063$ you can say that data does not disprove null hypothesis at $0.05$ significance level, i.e. there are no structural breaks. I would however hesitate to declare that there are truly no structural breaks without any further investigation.</p>\\n',\n",
       " '<p>Michael Chernick points you in the right direction. I would also look at Ruey Tsay\\'s work as that added to this body of knowledge. See more <a href=\"http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CE4QFjAA&amp;url=http://www.unc.edu/~jbhill/tsay.pdf&amp;ei=NKoGUI2fBoqm6wGMwu3ECA&amp;usg=AFQjCNFyhuiw1FmwWs_jXa3sX6BkWP5B3Q&amp;sig2=yaWLl9Ne5dfPNaz74tbCzw\" rel=\"nofollow\">here</a>.</p>\\n\\n<p>You can\\'t compete against today\\'s automated computer algorithms.  They look at many ways to approach the time series that you haven\\'t considered and often not documented in any paper or book. When one asks how to do an ANOVA, a precise answer can be expected when comparing against different algorithms. When one asks the question how do I do pattern recognition, many answers are possible as heuristics are involved. Your question involves the use of heuristics.</p>\\n\\n<p>The best way to fit an ARIMA model, if outliers exist in the data is to evaluate possible states of nature and to select that approach that is deemed optimal for a particular data set. One possible state of nature is that the ARIMA process is the primary source of explained variation. In this case one would \"tentatively identify\" the ARIMA process via the acf/pacf function and then examine the residuals for possible outliers. Outliers can be Pulses, i.e., one-time events OR seasonal pulses which are evidenced by systematic outliers at some frequency (say, 12 for monthly data). A third type of outlier is where one has a contiguous set of pulses, each having the same sign and magnitude, this is called a step or level shift.  After examining the residuals from the tentative ARIMA process one can then tentatively add the empirically identified deterministic structure to create a tentative combined model. Nor if the primary source of variation is one of the 4 kinds or \"outliers\" then one would be better served by identifying them ab initio (first) and then using the residuals from this \"regression model\" to identify the stochastic (ARIMA) structure. Now these two alternative strategies get a little more complicated when one has a \"problem\" where the ARIMA parameters change over time or the error variance changes over time due to a number of possible causes, possibly the need for weighted least squares or a power transform like logs / reciprocals, etc. Another complication / opportunity is how and when to form the contribution of user-suggested predictor series to form a seamlessly integrated model incorporating memory, causals and empirically identified dummy series. This problem is further exacerbated when one has trending series best modeled with indicator series of the form $0,0,0,0,1,2,3,4,...$, or $1,2,3,4,5,...n$ and combinations of level shift series like $0,0,0,0,0,0,1,1,1,1,1$. You might want to try and write such procedures in R, but life is short. I would be glad to actually solve your problem and demonstrate in this case how the procedure works, please post the data or send it to sales@autobox.com </p>\\n\\n<hr>\\n\\n<p><em>Additional comment after receiving / analyzing the data / daily data for a foreign exchange rate / 18=765 values starting 1/1/2007</em></p>\\n\\n<p><img src=\"http://i.stack.imgur.com/p0dSu.jpg\" alt=\"enter image description here\"> </p>\\n\\n<p>The data had an acf of: </p>\\n\\n<p><img src=\"http://i.stack.imgur.com/ja5Q3.jpg\" alt=\"enter image description here\"></p>\\n\\n<p>Upon identifying an arma model of the form $(1,1,0)(0,0,0)$ and a number of outliers the acf of the residuals indicates randomness since the acf values are very small. AUTOBOX identified a number of outliers: </p>\\n\\n<p><img src=\"http://i.stack.imgur.com/VAr3a.jpg\" alt=\"enter image description here\"></p>\\n\\n<p>The final model:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/BYr19.jpg\" alt=\"enter image description here\"> </p>\\n\\n<p>included the need for a variance stabilization augmentation a la TSAY where variance changes in the residuals were identified and incorporated. The problem that you had with your automatic run was that the procedure you were using, like an accountant, believes the data rather than challenging the data via Intervention Detection (a.k.a., Outlier Detection). I have posted a complete analysis <a href=\"http://www.autobox.com/se/sajeeka.zip\" rel=\"nofollow\">here</a>.  </p>\\n\\n<p><img src=\"http://i.stack.imgur.com/uhCY9.jpg\" alt=\"enter image description here\"></p>\\n',\n",
       " '<p>I would like to use the following type of model and am wondering if someone could let me know if this technique has been studied before (I assume it has) and where to learn more about it?</p>\\n\\n<p>The idea is simple. I have a Markov process with a possibly infinite state space S. I have a partition P of S into a finite number of sets. I would like to compute a \"stationary\" distribution over the \"states\" in P.</p>\\n\\n<p>(Bad example removed)</p>\\n\\n<p><strong>Example</strong> Let S be the set of real numbers in [0,100). My Markov process is to take the current state (a real number) and add a Gaussian variable distributed with mean 50 and variance 25, and taking the result modulo 100. (E.g. -3 ---> 97.) Let P have two states: [0,50) and [50,100). I\\'d like to know the equivalent of the stationary distribution over P: the proportion of the time our state is in (0,50], and the proportion of the time it is in (50,100).</p>\\n\\n<p>Is this problem in general well-defined? (Of course there may be no answer for some examples.) Is there a known way to approach these problems or anywhere I can find out more?</p>\\n\\n<p>Thanks very much!</p>\\n\\n<p>(Edit/PS also let me know if I should crosspost to a different stackexchange.)</p>\\n',\n",
       " '<p>Let $\\\\\\\\textbf{X}$ be an $n \\\\\\\\times p$ matrix with the rows containing observations and the columns containing features. Also assume that the features are centered at $0$. Let $C_k\\\\\\\\subset \\\\\\\\{1, \\\\\\\\dots n \\\\\\\\}$ contain the indices of the observations that belong to class $k$.  Why is an estimate of the within-class covariance matrix $$ \\\\\\\\widehat{\\\\\\\\Sigma_w} = \\\\\\\\frac{1}{n}\\\\\\\\sum_{k=1}^{K} \\\\\\\\sum_{i \\\\\\\\in C_k} (\\\\\\\\textbf{x}_i- \\\\\\\\hat{\\\\\\\\mu}_k)(\\\\\\\\textbf{x}_i-\\\\\\\\hat{\\\\\\\\mu}_{k})^{T}$$</p>\\n\\n<p>For a particular class $k$, are interested in the covariance between observations. So we would have $\\\\\\\\binom{|C_{k}|}{2}$ covariances for $k \\\\\\\\in \\\\\\\\mathbb{N}$. The matrix should be of size $\\\\\\\\binom{|C_{k}|}{2} \\\\\\\\times k$.</p>\\n\\n<p><strong>Added.</strong> $w$ is just an abbreviation for within-class variance.</p>\\n',\n",
       " '<blockquote>\\n  <p><strong>Possible Duplicate:</strong><br>\\n  <a href=\"http://stats.stackexchange.com/questions/3466/best-practice-when-analysing-pre-post-treatment-control-designs\">Best practice when analysing pre-post treatment-control designs</a>  </p>\\n</blockquote>\\n\\n\\n\\n<p>I\\'m doing an action research study with 2 of my classes: 1 is the \\'treatment\\' class and the other is the \\'control\\'. </p>\\n\\n<p>Both classes undertook a pre-test prior to the \\'treatment\\'. </p>\\n\\n<p>Then, after the \\'treatment\\' stage (the control class didn\\'t receive any \\'treatment\\'), both classes did a post-test. </p>\\n\\n<p>What statistics should I use? Paired t-tests? ANOVA with repeated measure? Help!</p>\\n',\n",
       " '<p>For first mode , we can use <a href=\"http://en.wikipedia.org/wiki/Paired_difference_test\" rel=\"nofollow\">Paired comparisons</a>. </p>\\n\\n<p>This can also be done by simulation. Here the concept is that suppose for input 0.1 we have value of 0.5 in one file and 0.52 in other file. So we will shuffle these two outputs. Similarly we will do it for all outputs in both files. Then we will find the difference in the means of both files. And we will simulate this expermient 10000 times , and we will get a bell curve of difference of means. Now we will check how many times we will get the difference greater than the actual difference of means in both files. If this value is greater than some threshold value 0.05(i.e 5% or any value thought by you, according to your experiment), then we will say that the new file is not as good as our control file. You can set threshold value according to your need, how much you can afford for your product to vary from good to bad. </p>\\n\\n<p>For second mode you can make a bell curve and can emit those files which are very much varying from most probable files..</p>\\n',\n",
       " '<p>The random variable taking values in $\\\\\\\\{0,1\\\\\\\\}^n$ is a discrete random variable. Its distribution  is fully described by probabilities \\n$p_{\\\\\\\\mathbf{i}}=P(X=\\\\\\\\mathbf{i})$ with $\\\\\\\\mathbf{i}\\\\\\\\in\\\\\\\\{0,1\\\\\\\\}^n$. The probabilities $p_{i}$ and $p_{ij}$ you give are sums of $p_{\\\\\\\\mathbf{i}}$ for certain indexes $\\\\\\\\mathbf{i}$. </p>\\n\\n<p>Now it seems that you want to describe $p_{\\\\\\\\mathbf{i}}$ by only using $p_i$ and $p_{ij}$. It is not possible without assuming certain properties on $p_{\\\\\\\\mathbf{i}}$. To see that try to derive <a href=\"http://en.wikipedia.org/wiki/Characteristic_function_%28probability_theory%29\" rel=\"nofollow\">characteristic function</a> of $X$. If we take $n=3$ we get</p>\\n\\n<p>\\\\\\\\begin{align}\\nEe^{i(t_1X_1+t_2X_2+t_3X_3)}&amp;=p_{000}+p_{100}e^{it_1}+p_{010}e^{it_2}+p_{001}e^{it_3}\\\\\\\\\\\\\\\\\\n&amp;+p_{110}e^{i(t_1+t_2)}+p_{101}e^{i(t_1+t_3)}+p_{010}e^{i(t_2+t_3)}+p_{111}e^{i(t_1+t_2+t_3)}\\n\\\\\\\\end{align}\\nIt is not possible rearrange this expression so that $p_{\\\\\\\\mathbf{i}}$ dissapear. For the gaussian random variable the characteristic function depends only on mean and covariance parameters. Characteristic functions uniquely define distributions, so this is why Gaussian can be described uniquely by using only mean and covariance. As we see for random variable $X$ this is not the case.</p>\\n',\n",
       " '<h3>Context:</h3>\\n\\n<p>Within the context of structural equation modelling, I have non-normality according to the Mardia test but univariate indices of skewness and kurtosis are less than 2.0.</p>\\n\\n<h3>Questions:</h3>\\n\\n<ul>\\n<li>Should parameter estimates (coefficient estimates) be evaluated using bootstrapping (1000 replicates) with bias-corrected methods? </li>\\n<li>In place of the traditional chi-square test, should the Bollen-Stine bootstrapped version be used?</li>\\n</ul>\\n',\n",
       " '<p>Traditionally P-P plot is done as follows. <strong>On Y axis</strong>, <em>expected cumulative probabilities of the theoretical distribution of your choice</em> are plotted, corresponding to your observed values. That is, for example of normal distribution, CDF.NORMAL(var,m,sd) are plotted, where var is your data, m and sd are the parameters estimated from the data or user-specified. <strong>On X axis</strong>, <em>estimated cumulative proportions of your empirical distribution</em> are plotted. <a href=\"http://stats.stackexchange.com/a/50118/3277\">These are</a> the ranks of your data transformed into proportions by one of the methods: rankit (most universal, and preferable for beta), Blom, Tukey or Van der Waerden.</p>\\n\\n<p>So, if the plotted points lie on X=Y diagonal, that means that the expected theoretical probs and the estimated observed probs coincide which means that your data follows the theoretical distribution.</p>\\n\\n<p>P-P plot addresses basically the same question as Q-Q plot, P-P being somewhat more sensitive to discrepancies in the middle part of the distribution, Q-Q - in tails. Q-Q is generally preferred in the research community.</p>\\n',\n",
       " '<p>I have a simple time series with 5-10 data points per data set at regular intervals.  I am wondering what is the best way to determine whether two data sets are different.  Should i try t-tests on each data point, or look at the area under the curves or is there some kind of multivariate model that would work better?</p>\\n',\n",
       " '<p>I am trying to model data on the number of online sales are made within a fixed sale period of 3 days. Data are generated only when the sale is made. I think for this kind of data I will be using a type of time series model for count data with inflated zero samples. But this dataset adds one more layer of complexity of the time limit. I don’t know any time series statistical model that limits the length of time. I may consider the stochastic process as a (Poisson like) draw of the limited number of trials: if it’s hourly then 72, if it’s by minutes then 4320 trials. By then, if it’s by minutes, the data ends up with too many zero observations, which ruins the effectiveness of the model. Do you have any better idea?</p>\\n\\n<hr>\\n\\n<p>The data is structured like this:</p>\\n\\n<pre><code> 2011-10-31 07:01:55 2011-11-03 06:59:59  2011-11-02 19:15:39        45 \\n 2011-10-31 07:01:55 2011-11-03 06:59:59  2011-11-02 22:32:06        46 \\n 2011-10-31 07:01:55 2011-11-03 06:59:59  2011-11-02 23:39:05        47 \\n 2011-10-31 07:01:55 2011-11-03 06:59:59  2011-11-02 23:20:09        48 \\n 2011-10-31 07:01:55 2011-11-03 06:59:59  2011-11-03 01:32:09        49 \\n 2011-10-31 07:01:55 2011-11-03 06:59:59  2011-11-03 03:33:11        50 \\n 2011-10-31 07:01:55 2011-11-03 06:59:59  2011-11-03 02:47:07        51 \\n 2011-10-31 07:01:55 2011-11-03 06:59:59  2011-11-03 04:05:05        52 \\n 2011-10-31 07:01:55 2011-11-03 06:59:59  2011-11-03 04:01:08        53 \\n 2011-10-31 07:01:55 2011-11-03 06:59:59  2011-11-03 06:53:09        55 \\n 2011-10-31 07:01:55 2011-11-03 06:59:59  2011-11-03 07:00:54        56 \\n</code></pre>\\n\\n<p>The first and second times (1st and 2nd column) are the start and end times of the deal, which is about 3-day apart as the deal lasts for 3 days, and of course fixed for each deal. The third time (in the 3rd column) is increasing as the deal goes by with the cummulative number of items sold on the last columne as a <i>positive integer</i> in the 4th columne. The data is generated only when the sale is made, so the time between succesive sales is <i>irregular</i>, which could be a key term to search for the appropirate model. After further search for the appropriate model, I found integer autoregressive model (INAR) may be used, but I am not sure... Since the sale is recorded by the second, I may think the data series with T=259200 observations having many (too many!) zero observations. I may transform the data by minutes by summing over the sales numbers withine one minute to reduce to T=4320. This will reduce the ratio of zero-total observations smaller, and apply INAR or some kind of zero-inflated stochastic model (truncated poisson or something like that). Could you give me any good ideas?</p>\\n',\n",
       " \"<p>I want to  choose two-pairs for pairtrading. For pairtradings, two pairs need to pass two tests, the Johansen test (for cointegration) and the P.P. test (for stationary).\\nAs I knew, if they related in cointegration, they would be stationary.\\nBut, My data passed Johansen test, but they couldn't pass P.P. test.</p>\\n\\n<p>Why does this result occur?</p>\\n\",\n",
       " '<p>To answer your main question, it would be optimal and more appropiate to scale within the CV. But it will probably not matter much and might not be important in practice at all if your classifier rescales the data, which most do (at least in R). </p>\\n\\n<p>However, selecting feature before cross validating is a BIG NO and will lead to overfitting, since you will select them based on how they perform on the whole data set. The log-transformation is ok to perform outside, since the transformation does not depend on the actual data (more on the type of data) and is not something you would not do if you had only 90% of the data instead of 100% and is not tweaked according to the data.</p>\\n\\n<p>To also answer your comment, obviously whether it will result in overfitting will depend on your manner of feature selection. If you choose them by chance (why would you do that?) or because of a priori theoretical considerations (other literature) it won\\'t matter. But if it depends on your data set it will. Elements of Statistical Learnings has a good explanation. You can freely and legally download a .pdf here <a href=\"http://www-stat.stanford.edu/~tibs/ElemStatLearn/\" rel=\"nofollow\">http://www-stat.stanford.edu/~tibs/ElemStatLearn/</a> </p>\\n\\n<p>The point concerning you is in section 7.10.2 on page 245 of the fifth printing. It is titled \"The Wrong and Right Ways to do Cross-validation\". </p>\\n',\n",
       " '<p>Binning will result in a more complex model, i.e., you will need more terms in the model to predict the outcome as well as a model that treats the predictors as continuous.  Bins also bring a degree of arbitrariness into the model.  Take a look at regression splines as an alternative.  Notes about this may be found at <a href=\"http://biostat.mc.vanderbilt.edu/rms\" rel=\"nofollow\">http://biostat.mc.vanderbilt.edu/rms</a>.  Also make sure that your outcome is truly dichotomous, i.e., that the time until the event is irrelevant and you have no censoring.</p>\\n',\n",
       " '<p>There is a lot of information on this topic at <a href=\"http://glmm.wikidot.com/faq\">http://glmm.wikidot.com/faq</a> . However, in your particular case, I would suggest using</p>\\n\\n<pre><code>library(nlme)\\nm1 &lt;- lme(value~status,random=~1|experiment,data=mydata)\\nanova(m1)\\n</code></pre>\\n\\n<p>because you don\\'t need any of the stuff that <code>lmer</code> offers (higher speed, handling of crossed random effects, GLMMs ...). <code>lme</code> should give you exactly the same coefficient and variance estimates but will also compute df and p-values for you (which <em>do</em> make  sense in a \"classical\" design such as you appear to have). You may also want to consider the random term <code>~status|experiment</code> (allowing for variation of status effects across blocks, or equivalently including a status-by-experiment interaction).  Posters above are also correct that your <code>t</code> statistics are so large that your p-value will definitely be &lt;0.05, but I can imagine you would like \"real\" p-values.</p>\\n',\n",
       " '<p>I have two time-series U and V (approximately 300 samples) having values between 0 and 5. I wanted to calculate the (first order) transfer entropy $I_{uv}$ (of U with the knowledge of V) and $I_{vu}$ (of V with the knowledge of U).\\nHere are the values I got :</p>\\n\\n<p>$I_{uv}$ = 0.1492 and $I_{vu}$ = 0.1342</p>\\n\\n<p>Since these values are close to each other, I started questioning their significance. Thus, I calculated the transfer entropy $I_{u-rv}$ of U with the knowledge of a randomized (in time) version of V, and the transfer entropy $I_{v-ru}$ of V with the knowledge of a randomized (in time) version of U. I calculated 500 values of $I_{u-rv}$ and $I_{v-ru}$, which gave me :</p>\\n\\n<p>mean($I_{u-rv}$ ) = 0.1218 - standard deviation = 0.0198</p>\\n\\n<p>mean($I_{v-ru}$) = 0.1019 - standard deviation = 0.0198</p>\\n\\n<p>The values of $I_{uv}$ and $I_{vu}$ lie close to (mean+std) so I would say there is an influence of one time serie on the other. From these information what can really be said about the two time-series ? (I will upload a picture of these time-series as soon as I figure a way to do it)</p>\\n',\n",
       " \"<p>I am carrying out an evaluation for an intervention using secondary data. Unfortunately the design of the study is weak as participants cannot be randomised. I am looking to see whether the intervention has had any impact on patients quality of life? A measure of QoL has been taken before and after the intervention, i am also interested in a number of variables and their potential effect on reported QoL including: age, gender, pharmacological treatment, disease  duration, mariatal status, and employment status(re literature). These IV's are both continuous and categorical, but can be all be converted to categorical if needs be. I would appreciate your advice concerning which design is best suited to this piece of research. I am considering using a ANCOVA but my experience with violations of assumptions may further weaken the merit of an already compromised design. \\nWarm Regards\\nDara </p>\\n\",\n",
       " '<p>You\\'re not doing anything wrong, the two functions are making different underlying assumptions about the distribution of the data. Your first implementation is assuming multivariate normal, and the 2nd a multivariate t-distribution (see ?cov.trob in package MASS). The effect is easier to see if you pull out one group:</p>\\n\\n<pre><code>#pull out group 1\\npick = group ==1\\np3 &lt;- qplot(data=df[pick,], x=x, y=y)\\ntl = with(df[pick,], \\n     ellipse(cor(x, y),scale=c(sd(x),sd(y)),\\n             centre=c(mean(x),mean(y))))\\np3 &lt;- p3 + geom_path(data=as.data.frame(tl), aes(x=x, y=y))\\np3 &lt;- p3 + stat_ellipse(level=0.95)\\np3 # looks off center\\np3 &lt;- p3 + geom_point(aes(x=mean(x),y=mean(y),size=2,color=\"red\"))\\np3\\n</code></pre>\\n\\n<p>So although it is close to the same center and orientation they are not the same. You can come close to the same size ellipse by using <code>cov.trob()</code> to get the correlation and scale for passing to <code>ellipse()</code>, and using the t argument to set the scaling equal to an f-distribution as <code>stat_ellipse()</code> does.</p>\\n\\n<pre><code>tcv = cov.trob(data[pick,2:3],cor=TRUE)\\ntl = with(df[pick,], \\n          ellipse(tcv$cor[2,1],scale=sqrt(diag(tcv$cov)),\\n                  t=qf(0.95,2,length(x)-1),\\n                  centre=tcv$center))\\np3 &lt;- p3 + geom_path(data=as.data.frame(tl), aes(x=x, y=y,color=\"red\"))\\np3\\n</code></pre>\\n\\n<p>but the correspondence still isn\\'t exact. The difference must be arising between using the cholesky decomposition of the covariance matrix and creating the scaling from the correlation and the standard deviations. I\\'m not enough of a mathematician to see exactly where the difference is. </p>\\n\\n<p>Which one is correct? That\\'s up to you to decide! The <code>stat_ellipse()</code> implementation will be less sensitive to outlying points, while the first will be more conservative. </p>\\n',\n",
       " '<p>In (most of) the analytical chemistry literature, the standard test for detecting outliers in univariate data (e.g. a sequence of measurements of some parameter) is Dixon\\'s Q test. Invariably, all the procedures listed in the textbooks have you compute some quantity from the data to be compared with a tabular value. By hand, this is not much of a concern; however I am planning to write a computer program for Dixon Q, and just caching values strikes me as inelegant. Which brings me to my first question:</p>\\n\\n<ol>\\n<li>How are the tabular values for Dixon Q generated?</li>\\n</ol>\\n\\n<p>Now, I have already looked into this <a href=\"http://pubs.acs.org/doi/pdf/10.1021/ac00002a010\" rel=\"nofollow\">article</a>, but I\\'m of the feeling that this is a bit of cheating, in that the author merely constructs a spline that passes through the tabular values generated by Dixon. I have the feeling that a special function (e.g. error function or incomplete beta/gamma) will be needed somewhere, but at least I have algorithms for those.</p>\\n\\n<p>Now for my second question: ISO seems to be slowly recommending Grubbs\\'s test over Dixon Q nowadays, but judging from the textbooks it has yet to catch on. This on the other hand was relatively easy to implement since it only involves computing the inverse of the CDF of Student t. Now for my second question:</p>\\n\\n<ol>\\n<li>Why would I want to use Grubbs\\'s instead of Dixon\\'s?</li>\\n</ol>\\n\\n<p>On the obvious front in my case, the algorithm is \"neater\", but I suspect there are deeper reasons. Can anyone care to enlighten me?</p>\\n',\n",
       " '<p>Here\\'s something I\\'ve wondered about for a while, but haven\\'t been able to discover the correct terminology. Say you have a relatively complicated density function that you suspect might have a close approximation as a sum of (properly weighted) simpler density functions. Have such things been studied? I\\'m particularly interested in reading about any applications.</p>\\n\\n<p>Here\\'s one example I\\'ve found:</p>\\n\\n<p><a href=\"http://www.soa.org/library/research/transactions-of-society-of-actuaries/1966/january/tsa66v18pt1n5211.pdf\" rel=\"nofollow\">Expansion of probability density functions as a sum of gamma densities with applications in risk theory</a></p>\\n',\n",
       " \"<p>You simply don't have enough information. </p>\\n\\n<p>According to the usual definitions, the $70\\\\\\\\%$ accuracy rate of the classifier is not the average of the probabilities assigned to the correct class. It is the frequency with which the classifier predicts that the correct class has the highest probability. This can mean that the correct category is assigned a probability of $10\\\\\\\\%$ while several other categories are assessed at $8\\\\\\\\%$. You can mix your classifier with a clueless classifier, which assigns every category the same probability regardless of the inputs, and the mixture will have the same accuracy rate as the original, but it will make different probability estimates. </p>\\n\",\n",
       " '<p>I don\\'t know if you can quantify the confidence level of the z-values, but you can certainly give a smaller confidence value for your estimates of the mean.</p>\\n\\n<p>Read the wiki page on <a href=\"http://en.wikipedia.org/wiki/Standard_error\" rel=\"nofollow\">standard error</a>. Standard error differs from standard deviation in that standard error is the amount of variation in the parameter estimates. In this case, the standard error should be calculated as</p>\\n\\n<p>$\\\\\\\\hat \\\\\\\\sigma_\\\\\\\\mu = \\\\\\\\frac{\\\\\\\\hat \\\\\\\\sigma_x}{\\\\\\\\sqrt{N}}$</p>\\n\\n<p>where $N$ is the sample size. As such, your estimates for the mean will have much tighter bounds when you calculate a confidence interval (<em>e.g.</em>, $95\\\\\\\\% CI = \\\\\\\\hat{\\\\\\\\mu} \\\\\\\\pm 1.96\\\\\\\\hat{\\\\\\\\sigma}_\\\\\\\\mu$). As $N$ increases, the bounds will get tighter.</p>\\n\\n<p>Now, if you\\'re dead set on finding an estimate for the z-score, we can find error bounds for it. Assuming you know the population mean $\\\\\\\\mu$ and variance $\\\\\\\\sigma$ of the null distribution, we can define a new random variable, $Z$:</p>\\n\\n<p>$Z = \\\\\\\\frac{X- \\\\\\\\mu}{\\\\\\\\sigma}$</p>\\n\\n<p>We can find the standard error or error depending on which you\\'d prefer. It sounds like you\\'re more interested in finding the standard error, so let\\'s look at</p>\\n\\n<p>$ \\\\\\\\hat{\\\\\\\\mu}_Z = \\\\\\\\frac{ \\\\\\\\hat{\\\\\\\\mu}_X- \\\\\\\\mu}{\\\\\\\\sigma} $</p>\\n\\n<p>$Var(\\\\\\\\hat{\\\\\\\\mu}_Z) \\n= Var(\\\\\\\\frac{\\\\\\\\hat{\\\\\\\\mu}_X- \\\\\\\\mu}{\\\\\\\\sigma})\\n= \\\\\\\\frac{Var(\\\\\\\\hat{\\\\\\\\mu}_X) - Var(\\\\\\\\mu)}{\\\\\\\\sigma^2}\\n= \\\\\\\\frac{Var(\\\\\\\\hat{\\\\\\\\mu}_X)}{\\\\\\\\sigma^2}$</p>\\n\\n<p>Since $Var(\\\\\\\\hat{\\\\\\\\mu}_X) = \\\\\\\\frac{\\\\\\\\hat{\\\\\\\\sigma}_X}{\\\\\\\\sqrt{N}}$, the standard error for a mean z-score is</p>\\n\\n<p>$Var(\\\\\\\\hat{\\\\\\\\mu}_Z) = \\\\\\\\frac{\\\\\\\\hat{\\\\\\\\sigma}_X}{\\\\\\\\sqrt{N}\\\\\\\\sigma}$</p>\\n\\n<p>You can generalize to two samples as well; it all comes down to basic variance calculations. Bear in mind this is from calculating a z-score from a sample mean, you can also derive things from working with Z. Bottom line is divide by sigma for most of these z normalizations.</p>\\n',\n",
       " \"<p>I know that R's <code>rpart</code> function keeps the data it would need to implement multivariate split, but I don't know if it's actually performing multivariate splits.  I've tried researching it online looking at the <code>rpart</code> docs, but I don't see any information that it can do it or is doing it.  Anyone know for sure?</p>\\n\",\n",
       " '<p>How can one obtain standardized (fixed effect) regression weights from a multilevel regression?</p>\\n\\n<p>And, as an \"add-on\": What is the easiest way to obtain these standardized weights from a <code>mer</code>-object (from the <code>lmer</code> function of the <code>lme4</code>package in <code>R</code>)?</p>\\n',\n",
       " \"<p>This is the motivation for my question: Suppose we have $n$ tickets\\nin a bag, and we draw $k$ of them uniformly at random without replacement.\\nNow, repeat the same procedure independently (same $n$ tickets, draw\\n$k$ of them UAR without replacement). I can straightforwardly show\\nthat, if $S$ is the number of overlapping tickets between the two\\nsamples, then $\\\\\\\\mathbb{P}\\\\\\\\left(S=j\\\\\\\\right)={k \\\\\\\\choose j}\\\\\\\\left(\\\\\\\\prod_{i=0}^{j-1}\\\\\\\\frac{k-i}{n-i}\\\\\\\\right)\\\\\\\\left(\\\\\\\\prod_{i=0}^{k-j-1}\\\\\\\\frac{n-k-i}{n-j-i}\\\\\\\\right)$.</p>\\n\\n<p>Now, my question is a generalization of this--essentially, I'm curious\\nabout what happens to this overlap when we don't have the same probability\\nof drawing each ticket. </p>\\n\\n<p>To investigate this, my approach is to assign ''weights'' to tickets\\nby creating duplicates To make this more concrete, suppose we have\\n$g$ groups, each with a different number of tickets $n_{i}$, where\\neach $n_{i}$ corresponds to the ''weight'' we want to assign\\nto a particular ticket type. Now we can draw $k$ tickets uniformly at\\nrandom and without replacement, but instead of looking at the number\\nof individual tickets that are shared between two samples, I am\\ncurious about the number of groups that are shared, which we can denote\\nby $R$. That is, if both samples include tickets from group $i$\\nbut the groups of all the other tickets in sample 1 are different\\nfrom the groups of all the other tickets in sample 2, then we have\\n$R=1$. </p>\\n\\n<p>How can I find $\\\\\\\\mathbb{P}\\\\\\\\left(R=j\\\\\\\\right)$? To tell you the truth\\nI'm having trouble even finding $\\\\\\\\mathbb{P}\\\\\\\\left(R=0\\\\\\\\right)$. Also,\\nif this is a well-known probability question, I would appreciate someone\\ngiving me the name of it, so that I could learn about it some more\\non my own. Thanks!</p>\\n\\n<p>Update: </p>\\n\\n<p>I've written some basic R code that can be used to simulate this kind of problem and check any analytical results people have:</p>\\n\\n<pre><code>nGroups &lt;- 3\\nnPerGroup &lt;- c(4, 2, 5)\\ntickets &lt;- vector(length = sum(nPerGroup))\\ncurPos &lt;- 1\\nfor(i in 1:nGroups){\\n    tickets[curPos:(curPos + nPerGroup[i] - 1)] &lt;- rep(i, nPerGroup[i])\\n    curPos &lt;- curPos + nPerGroup[i]\\n}\\n\\n\\nGroup.Overlap &lt;- function(tickets, k){\\n  sample1 &lt;- sample(tickets, size = k)\\n  sample2 &lt;- sample(tickets, size = k)\\n  sharedGroups &lt;- intersect(unique(sample1), unique(sample2))\\n  return(length(sharedGroups))\\n}\\n</code></pre>\\n\",\n",
       " '<p>What gd047 writes is correct, however the use of RNNs in practice today is restricted to a simpler class of problems: time series/sequential tasks.</p>\\n\\n<p>While feedforward networks are used to learn datasets like $(i, t)$ where $i$ and $t$ are vectors (eg $i \\\\\\\\in \\\\\\\\mathcal{R}^n$, for recurrent networks $i$ will always be a sequence, e.g. $i \\\\\\\\in (\\\\\\\\mathcal{R}^n)^*$.</p>\\n\\n<p>RNNs have been shown to be able to represent any measureable sequence to sequence mapping by Hammer.</p>\\n\\n<p>Thus, RNNs are being used nowadays for  all kinds of sequential tasks: time series prediction, sequence labeling, sequence classification etc. A good overview can be found on <a href=\"http://www.idsia.ch/~juergen/rnn.html\">Schmidhuber\\'s page on RNNs</a>.</p>\\n',\n",
       " '<p>You might consider constructing what is known as an ARMAX Model or a Transfer Function Model in general. Do not assume any differencing or lag structure in your known causals or output series unless you are in solid theoretical grounds. Do perform Intervention Detection to control for unobservable or perhaps simply unkmown deterministic structure. Do allow for parameters to change over tome and do allow for variance changes to be detected and incorporated. Do allow for ARIMA structure to control for unknown ir unspecified stochastic input series. You might review some of the questions/answers , both mine and a few others, to previous time series questions for more enlightenment and support. </p>\\n',\n",
       " '<p>Is there an easy way to apply the trend line formula from a chart to any given X value in Excel?</p>\\n\\n<p>For example, I want to get the Y value for a given X = $2,006.00. I\\'ve already taken the formula and retyped it out be:</p>\\n\\n<pre><code>=-0.000000000008*X^3 - 0.00000001*X^2 + 0.0003*X - 0.0029\\n</code></pre>\\n\\n<p>I am continually making adjustments to the trend line by adding more data, and don\\'t want to retype out the formula every time.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/tgq8c.jpg\" alt=\"enter image description here\"></p>\\n',\n",
       " '<p>If I have multiple CDFs $X_1,X_2,\\\\\\\\cdots,X_n$, (for simplicity I assume 2 : $X,Y$) and if $X$ is discrete and $Y$ is continuous, how would the joint CDF look like?</p>\\n\\n<p>I understand that:</p>\\n\\n<ul>\\n<li><p>If they were both discrete, it would be a staircase in 2 dimensions. Similar to stacking books atop each other on a table.</p></li>\\n<li><p>If they were both continuous, it would be a nice smooth graph coming to 1 at $(\\\\\\\\infty,\\\\\\\\infty)$ </p></li>\\n</ul>\\n',\n",
       " '<p>For a university course, a friend of mine had to test whether there is a relation between two ordinal variables. These variables were opinion about the European Union (positive, neutral, negative) and whether people felt being (a) citizen of their country, (b) primarily citizen of their country, but also European, (c) primarily European, but also citizen of their country and (d) European. The data is available in form of a contingency table.</p>\\n\\n<p>Now, what my friend did was to calculate a chi-squared test. I believe this is wrong, because it does not consider the fact that the groups are ordered, i.e. the variables are ordinal and not categorial. However, chi-squared test seems to be common for this kind of analysis (e.g. <a href=\"http://www.r-tutor.com/elementary-statistics/goodness-fit/chi-squared-test-independence\" rel=\"nofollow\">here</a>).</p>\\n\\n<p>In my opinion, the idea behind this question is whether or not there is a significant correlation between those variables, and since Pearson’s correlation cannot be calculated from ordinal data, Spearman’s Rho should be used. An alternative to that would be Goodman’s/Kruskal’s Gamma test, as suggested <a href=\"http://www.mhhe.com/socscience/sociology/ritchey/student/chap16objectives.htm\" rel=\"nofollow\">here</a>. Perhaps the Kruskal Wallis test would also be an option? But can it be used in this setting?</p>\\n\\n<p>So, my first question is: Is it valid to use the chi-squared test here? What do you think of the alternatives I described?</p>\\n\\n<p>Question two: How can the significance of Spearman’s correlation be calculated in R? It seems to me most commands reqire observations, not contingency tables, as input. </p>\\n',\n",
       " '<p>I would use \"million\", or 10^6, because M stands for molar (concentration in moles per litre) in my discipline.</p>\\n',\n",
       " '<p><a href=\"http://en.wikipedia.org/wiki/Maximum_likelihood\" rel=\"nofollow\">Maximum likelihood</a> and <a href=\"http://en.wikipedia.org/wiki/Likelihood-ratio_test\" rel=\"nofollow\">likelihood ratios</a> are not the same thing. Maximum likelihood is an estimation method, whereas likelihood ratios are used to construct tests. </p>\\n\\n<p>For both of these, you need the likelihood function. In this case it is the probability function of a random variable with a <a href=\"http://en.wikipedia.org/wiki/Hypergeometric_distribution\" rel=\"nofollow\">hypergeometric distribution</a>, since $X$, the number of boys sampled, is hypergeometric. Then</p>\\n\\n<p>$P(X=k)=\\\\\\\\frac{\\\\\\\\binom{m}{k}\\\\\\\\binom{2000-m}{5-k}}{\\\\\\\\binom{2000}{5}}$</p>\\n\\n<p>To obtain the maximum likelihood, maximize this with respect to $m$ for $k=1$. Your claim seems to be that the maximum is given by $m=200$, i.e. when the proportion of boys is 400/2000=1/5=the observered proportion.</p>\\n\\n<p>To obtain the likelihood ratio test, compute $-2\\\\\\\\ln\\\\\\\\frac{P_{H_0}(X=1)}{P_{H_1}(X=1)}$. Your hypotheses should be statements about $m$.</p>\\n',\n",
       " \"<p>After a lot procrastinating, I ended up solving this issue. I thought I'd post my solution so nobody else makes the same (stupid!) error. </p>\\n\\n<p>My issue wasn't categorical variables at all. The issue was that in my independent variables, I had household Income, which was not scaled properly. This meant that both polr and bayespolr fell over when working out the Hessian. </p>\\n\\n<p>So, when having issues like this, just remember first-year stats class: scale your variables. Take logs of things like income. </p>\\n\",\n",
       " '<p>In the card game Pitch using 1 deck with 52 cards and 4 people playing and six cards are dealt to each person and I have a partner and I have a King and wish to bid in the suit the King is in using the King as high what is the percentage my opponents have the Ace in the same suit as my King out of their 12 cards? </p>\\n',\n",
       " '<p>Without trying to give a full primer on PCA, from an optimization standpoint, the primary <strong>objective function</strong> is the <a href=\"http://en.wikipedia.org/wiki/Rayleigh_quotient\">Rayleigh quotient</a>. The matrix that figures in the quotient is (some multiple of) the sample covariance matrix\\n$$\\\\\\\\newcommand{\\\\\\\\m}[1]{\\\\\\\\mathbf{#1}}\\\\\\\\newcommand{\\\\\\\\x}{\\\\\\\\m{x}}\\\\\\\\newcommand{\\\\\\\\S}{\\\\\\\\m{S}}\\\\\\\\newcommand{\\\\\\\\u}{\\\\\\\\m{u}}\\\\\\\\newcommand{\\\\\\\\reals}{\\\\\\\\mathbb{R}}\\\\\\\\newcommand{\\\\\\\\Q}{\\\\\\\\m{Q}}\\\\\\\\newcommand{\\\\\\\\L}{\\\\\\\\boldsymbol{\\\\\\\\Lambda}}\\\\\\\\n\\\\\\\\n\\\\\\\\S = \\\\\\\\frac{1}{n} \\\\\\\\sum_{i=1}^n \\\\\\\\x_i \\\\\\\\x_i^T = \\\\\\\\m{X}^T \\\\\\\\m{X} / n\\\\\\\\n$$\\nwhere each $\\\\\\\\x_i$ is a vector of $p$ features and $\\\\\\\\m{X}$ is the matrix such that the $i$th row is $\\\\\\\\x_i^T$.</p>\\n\\n<p>PCA seeks to solve a <em>sequence</em> of optimization problems. The first in the sequence is the unconstrained problem\\n$$\\\\\\\\n\\\\\\\\begin{array}{ll}\\\\\\\\n\\\\\\\\text{maximize} &amp; \\\\\\\\frac{\\\\\\\\u^T \\\\\\\\S \\\\\\\\u}{\\\\\\\\u^T\\\\\\\\u} \\\\\\\\;, \\\\\\\\u \\\\\\\\in \\\\\\\\reals^p \\\\\\\\&gt; .\\\\\\\\n\\\\\\\\end{array}\\\\\\\\n$$</p>\\n\\n<p>Since $\\\\\\\\u^T \\\\\\\\u = \\\\\\\\|\\\\\\\\u\\\\\\\\|_2^2 = \\\\\\\\|\\\\\\\\u\\\\\\\\| \\\\\\\\|\\\\\\\\u\\\\\\\\|$, the above unconstrained problem is equivalent to the constrained problem\\n$$\\\\\\\\n\\\\\\\\begin{array}{ll}\\\\\\\\n\\\\\\\\text{maximize} &amp; \\\\\\\\u^T \\\\\\\\S \\\\\\\\u \\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\text{subject to} &amp; \\\\\\\\u^T \\\\\\\\u = 1 \\\\\\\\&gt;.\\\\\\\\n\\\\\\\\end{array}\\\\\\\\n$$</p>\\n\\n<p>Here is where the matrix algebra comes in. Since $\\\\\\\\S$ is a symmetric positive semidefinite matrix (by construction!) it has an eigenvalue decomposition of the form\\n$$\\\\\\\\n\\\\\\\\S = \\\\\\\\Q \\\\\\\\L \\\\\\\\Q^T \\\\\\\\&gt;,\\\\\\\\n$$\\nwhere $\\\\\\\\Q$ is an orthogonal matrix (so $\\\\\\\\Q \\\\\\\\Q^T = \\\\\\\\m{I}$) and $\\\\\\\\L$ is a diagonal matrix with nonnegative entries $\\\\\\\\lambda_i$ such that $\\\\\\\\lambda_1 \\\\\\\\geq \\\\\\\\lambda_2 \\\\\\\\geq \\\\\\\\cdots \\\\\\\\geq \\\\\\\\lambda_p \\\\\\\\geq 0$.</p>\\n\\n<p>Hence, $\\\\\\\\u^T \\\\\\\\S \\\\\\\\u = \\\\\\\\u^T \\\\\\\\Q \\\\\\\\L \\\\\\\\Q^T \\\\\\\\u = \\\\\\\\m{w}^T \\\\\\\\L \\\\\\\\m{w} = \\\\\\\\sum_{i=1}^p \\\\\\\\lambda_i w_i^2$. Since $\\\\\\\\u$ is constrained in the problem to have a norm of one, then so is $\\\\\\\\m{w}$ since $\\\\\\\\|\\\\\\\\m{w}\\\\\\\\|_2 = \\\\\\\\|\\\\\\\\Q^T \\\\\\\\u\\\\\\\\|_2 = \\\\\\\\|\\\\\\\\u\\\\\\\\|_2 = 1$, by virtue of $\\\\\\\\Q$ being orthogonal. </p>\\n\\n<p>But, if we want to maximize the quantity $\\\\\\\\sum_{i=1}^p \\\\\\\\lambda_i w_i^2$ under the constraints that $\\\\\\\\sum_{i=1}^p w_i^2 = 1$, then the best we can do is to set $\\\\\\\\m{w} = \\\\\\\\m{e}_1$, that is, $w_1 = 1$ and $w_i = 0$ for $i &gt; 1$.</p>\\n\\n<p>Now, backing out the corresponding $\\\\\\\\u$, which is what we sought in the first place, we get that\\n$$\\\\\\\\n\\\\\\\\u^\\\\\\\\star = \\\\\\\\Q \\\\\\\\m{e}_1 = \\\\\\\\m{q}_1 \\\\\\\\n$$\\nwhere $\\\\\\\\m{q}_1$ denotes the first column of $\\\\\\\\Q$, i.e., the eigenvector corresponding to the largest eigenvalue of $\\\\\\\\S$. The value of the objective function is then also easily seen to be $\\\\\\\\lambda_1$.</p>\\n\\n<hr>\\n\\n<p>The remaining principal component vectors are then found by solving the sequence (indexed by $i$) of optimization problems\\n$$\\\\\\\\n\\\\\\\\begin{array}{ll}\\\\\\\\n\\\\\\\\text{maximize} &amp; \\\\\\\\u_i^T \\\\\\\\S \\\\\\\\u_i \\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\text{subject to} &amp; \\\\\\\\u_i^T \\\\\\\\u_i = 1 \\\\\\\\\\\\\\\\\\\\\\\\n                &amp; \\\\\\\\u_i^T \\\\\\\\u_j = 0 \\\\\\\\quad \\\\\\\\forall 1 \\\\\\\\leq j &lt; i\\\\\\\\&gt;.\\\\\\\\n\\\\\\\\end{array}\\\\\\\\n$$\\nSo, the problem is the same, except that we add the additional constraint that the solution must be orthogonal to <em>all</em> of the previous solutions in the sequence. It is not difficult to extend the argument above inductively to show that the solution of the $i$th problem is, indeed, $\\\\\\\\m{q}_i$, the $i$th eigenvector of $\\\\\\\\S$.</p>\\n\\n<hr>\\n\\n<p>The PCA solution is also often expressed in terms of the <a href=\"http://en.wikipedia.org/wiki/Singular_value_decomposition\">singular value decomposition</a> of $\\\\\\\\m{X}$. To see why, let $\\\\\\\\m{X} = \\\\\\\\m{U} \\\\\\\\m{D} \\\\\\\\m{V}^T$. Then $n \\\\\\\\S = \\\\\\\\m{X}^T \\\\\\\\m{X} = \\\\\\\\m{V} \\\\\\\\m{D}^2 \\\\\\\\m{V}^T$ and so $\\\\\\\\m{V} = \\\\\\\\m{Q}$ (strictly speaking, up to sign flips) and $\\\\\\\\L = \\\\\\\\m{D}^2 / n$.</p>\\n\\n<p>The principal components are found by projecting $\\\\\\\\m{X}$ onto the principal component vectors. From the SVD formulation just given, it\\'s easy to see that\\n$$\\\\\\\\n\\\\\\\\m{X} \\\\\\\\m{Q} = \\\\\\\\m{X} \\\\\\\\m{V} = \\\\\\\\m{U} \\\\\\\\m{D} \\\\\\\\m{V}^T \\\\\\\\m{V} = \\\\\\\\m{U} \\\\\\\\m{D} \\\\\\\\&gt; .\\\\\\\\n$$</p>\\n\\n<p>The simplicity of representation of both the principal component vectors and the principal components themselves in terms of the SVD of the matrix of features is one reason the SVD features so prominently in some treatments of PCA.</p>\\n',\n",
       " '<p>I received this elementary question by email:</p>\\n\\n<blockquote>\\n  <p>In a regression equation am I correct\\n  in thinking that if the beta value is\\n  positive the dependent variable has\\n  increased in response to greater use\\n  of the independent variable, and if\\n  negative the dependent variable has\\n  decreased in response to an increase\\n  in the independent variable - similar\\n  to the way you read correlations?</p>\\n</blockquote>\\n',\n",
       " '<p>Malcom Gladewell analyses the problem in the book Outliers by analyzing Hockey Players.</p>\\n',\n",
       " '<p>This is what sweep and scale are for.</p>\\n\\n<pre><code>sweep(m, 2, colSums(m), FUN=\"/\")\\nscale(m, center=FALSE, scale=colSums(m))\\n</code></pre>\\n\\n<p>Alternatively, you could use recycling, but you have to transpose it twice.</p>\\n\\n<pre><code>t(t(m)/colSums(m))\\n</code></pre>\\n\\n<p>Or you could construct the full matrix you want to divide by, like you did in your question.  Here\\'s another way you might do that.</p>\\n\\n<pre><code>m/colSums(m)[col(m)]\\n</code></pre>\\n\\n<p>And notice also caracal\\'s addition from the comments:</p>\\n\\n<pre><code>m %*% diag(1/colSums(m))\\n</code></pre>\\n',\n",
       " '<p>You are right, this is a significant problem in machine learning/statistical modelling.  Essentially the only way to really solve this problem is to retain an independent test set and keep it held out until the study is complete and use it for final validation.</p>\\n\\n<p>However, inevitably people will look at the results on the test set and then change their model accordingly; however this won\\'t necessarily result in an improvement in generalisation performance as the difference in performance of different models may be largely due to the particular sample of test data that we have.  In this case, in making a choice we are effectively over-fitting the test error.  </p>\\n\\n<p>The way to limit this is to make the variance of the test error as small as possible (i.e. the variability in test error we would see if we used different samples of data as the test set, drawn from the same underlying distribution).  This is most easily achieved using a large test set if that is possible, or e.g. bootstrapping or cross-validation if there isn\\'t much data available.</p>\\n\\n<p>I have found that this sort of over-fitting in model selection is a lot more troublesome than is generally appreciated, especially with regard to performance estimation, see</p>\\n\\n<p>G. C. Cawley and N. L. C. Talbot, Over-fitting in model selection and subsequent selection bias in performance evaluation, Journal of Machine Learning Research, 2010. Research, vol. 11, pp. 2079-2107, July 2010 <a href=\"http://jmlr.csail.mit.edu/papers/v11/cawley10a.html\">(www)</a></p>\\n\\n<p>This sort of problem especially affects the use of benchmark datasets, which have been used in many studies, and each new study is implicitly affected by the results of earlier studies, so the observed performance is likely to be an over-optimistic estimate of the true performance of the method.  The way I try to get around this is to look at many datasets (so the method isn\\'t tuned to one specific dataset) and also use multiple random test/training splits for performance estimation (to reduce the variance of the etsimate).  However the results still need the caveat that these benchmarks have been over-fit.</p>\\n\\n<p>Another example where this ocurrs is in machine learning competitions with a leaderboard based on a validation set.  Inevitably some competitors keep tinkering with their model to get further up the leader board, but then end up towards the bottom of the final rankings.  The reason for this is that their multiple choices have over-fitted the validation set (effectively learning the random variations in the small validation set).</p>\\n\\n<p>If you can\\'t keep a statistically pure test set, then I\\'m afraid the two best options are (i) collect some new data to make a new statistically pure test set or (ii) make the caveat that the new model was based on a choice made after observing the test set error, so the performance estimate is likely to have an optimistic bias.</p>\\n',\n",
       " \"<p>LASSO is non-unique in the case where multiple features have perfect collinearity.  Here's a simple thought experiment to prove it.  </p>\\n\\n<p>Let's say you have three random vectors $y$, $x_1$, $x_2$.  You're trying to predict $y$ from $x_1$, $x_2$.  Now assume $y$ = $x1$ = $x2$.  An optimal LASSO solution would be $\\\\\\\\beta_1 = 1 - P$, $\\\\\\\\beta_2 = 0$, where $P$ is the effect of LASSO penalty.  However, also optimal would be $\\\\\\\\beta_1 = 0$, $\\\\\\\\beta_2 - 1 - P$.</p>\\n\",\n",
       " \"<p>If the data are normal, the sample mean and the sample variance are independent. If the data are not normal, the covariance/correlation between the sample mean and the sample variance are $O( \\\\\\\\kappa_3 n^{-1})$ where $\\\\\\\\kappa_3$ is the (central) skewness of the distribution. If, for any reason, you really need the sample mean and the sample variance to be independent, then you can calculate these statistics on independent subsets of data. However, as other people noted, you will suffer efficiency losses.</p>\\n\\n<p>Cross-validation that you've been thinking of, essentially, fights correlations between different statistics computed on the same data. There are applications where that's a dire necessity. E.g., in regression, the correlation between $\\\\\\\\hat y_i$ and $y_i$ is given by the hat-value $h_{ii}$, the diagonal entry of the projector matrix $X(X&#39;X)^{-1}X&#39;$, and is $O(1)$, so you may need to suppress it when you talk about model selection or residual diagnostics (or at least control for it with degrees of freedom corrections like $n-p = n-\\\\\\\\sum_i h_{ii}$). But there are applications where splitting the sample is a ridiculous overkill.</p>\\n\",\n",
       " '<p>I think the closest you can get to the concept you want to express without risking confusion is</p>\\n\\n<p>$f(x)=\\\\\\\\frac{\\\\\\\\Pr(x&lt;X \\\\\\\\le x+dx)}{dx}$.</p>\\n\\n<p>This is a bit of an abuse of notation unless $\\\\\\\\Pr(x&lt;X \\\\\\\\le x+dx)$ is defined to be equivalent to the <a href=\"http://en.wikipedia.org/wiki/Differential_of_a_function\" rel=\"nofollow\">differential</a> $dF(x)$.</p>\\n',\n",
       " '<p>The $\\\\\\\\rm AR(1)$ model is given as $X_n=  \\\\\\\\rho X_{n-1} + e_n.$ The parameter $\\\\\\\\rho$  is normally estimated by conditional least squares.  If the model is correct the eis have mean $0$ and variance $\\\\\\\\sigma_e^2.$ </p>\\n\\n<p>The parameter $\\\\\\\\rho$ is $$\\\\\\\\rho=\\\\\\\\frac{{\\\\\\\\rm Cov}(X_n, X_{n-1})}{{\\\\\\\\rm Var}(X_n)}.$$</p>\\n\\n<p>When the estimate of $e_n$ is paired with the estimate of $e_{n-1}$ the slope is theoretically $0$ but will not be exactly zero because the estimates of residuals are based on the estimate of $\\\\\\\\rho$. </p>\\n\\n<p>The answer to 2 is yes.</p>\\n',\n",
       " '<p>Let me begin by first confirming that this is indeed the correct place to post this (other ideas I had were math.SE). That said,</p>\\n\\n<hr>\\n\\n<p>Let $X_n$ be a Markov chain on the state space $\\\\\\\\mathcal S$ and for $ y \\\\\\\\in \\\\\\\\mathcal S$ let $T_y = \\\\\\\\min\\\\\\\\{ n \\\\\\\\ge 1 : X_n =y\\\\\\\\}$ be the first return time to $y$. Let $W_y = T_y - 1$ be the time just before the first return to $y$</p>\\n\\n<ul>\\n<li><p>Explain why $W_y$ is not a stopping time </p></li>\\n<li><p>Show that the Strong Markov Property does not apply to $X_n$ at random time $W_y$. </p></li>\\n</ul>\\n\\n<p><strong>My Work</strong></p>\\n\\n<p>When showing that $W_y$ is not a stopping time, is it sufficient to write $$W_y = \\\\\\\\bigcap_{i = 1}^{n-1} \\\\\\\\{X_i \\\\\\\\ne y\\\\\\\\} \\\\\\\\cap X_n = y$$ and claim that since $X_n$ does not belong to the set $\\\\\\\\{X_0, X_1, \\\\\\\\dots, X_{n-1}\\\\\\\\}$, we have that $W_y$ is not a stopping time?</p>\\n\\n<hr>\\n\\n<p>Then, for showing that the Strong Markov Property does not apply, can I write $$\\\\\\\\mathbf{P}(X_n = y \\\\\\\\mid W_y = n-1, X_{n-1} = i, X_{n-2} = x_{n-2}, \\\\\\\\dots, X_0 = y) = 1 \\\\\\\\ne p(i, y)$$ where $p(i,y)$ is the one step transition probability from $i$ to $y$?</p>\\n',\n",
       " \"<p>Normally what you should care about in comparing the two groups at baseline is not so much statistical significance of differences but size of differences:  is any of these differences large enough to matter to the study? Large enough to affect the group comparisons and variable relationships that are the focus of the research? Large enough that adjusting for it (by using it as a covariate) is necessary?  </p>\\n\\n<p>Now, your case is a little bit interesting in that, even with random assignment, you've got 4 out of 24 variables showing differences significant at the .05 level (17% instead of the expected 5%).  That may seem concerning for your randomization process or some other aspect of the study.  But theoretically, if the randomization were done flawlessly and there was no attrition in either group afterwards, a result this extreme or more so should occur 2.4% of the time, based on 24!/(4!(24-4)!) (.05^4) (.95^(24-4)).  That is not really such a rare occurrence after all.  What you have could well be a set of random differences.  I'd stick with judging based on magnitude of differences.</p>\\n\",\n",
       " '<p>Detrending can be done by applying a low pass filter that calculates the trend. The remaining part is your detrended data. Two examples of low pass filters here:</p>\\n\\n<ul>\\n<li><p>apply a <a href=\"http://en.wikipedia.org/wiki/Hodrick%E2%80%93Prescott_filter\" rel=\"nofollow\" title=\"Hodrick Prescott\">Hodrick-Prescott</a> filter. I would advise to use R and the package <strong>mfilter</strong> for instance, but apparentyl you can do it in excel through a <a href=\"http://ideas.repec.org/c/dge/qmrbcd/165.html\" rel=\"nofollow\" title=\"optional\">free plugin</a> (never tried)</p></li>\\n<li><p>perform a <a href=\"http://www.google.com.sg/search?aq=f&amp;sourceid=chrome&amp;ie=UTF-8&amp;q=multi%20resolution%20analysis\" rel=\"nofollow\">multiresolution analysis</a> using a wavelet method. In my experience, this gives very good results. Again, R will do that using package <strong>waveslim</strong> and will give you the best flexibility, but I am sure some people have implemented it in excel.</p></li>\\n</ul>\\n\\n<p>R is free. If you have already worked with Matlab, you will not be too much surprised. It may take you a bit of time to get used to it, but it is worth it. </p>\\n',\n",
       " '<p>I found an omega squared function in somebody\\'s .Rprofile that they made available online:</p>\\n\\n<p><a href=\"http://www.estudiosfonicos.cchs.csic.es/metodolo/1/.Rprofile\" rel=\"nofollow\">http://www.estudiosfonicos.cchs.csic.es/metodolo/1/.Rprofile</a></p>\\n',\n",
       " '<p>LDA finds at most $k - 1$ linear discriminants, where $k$ is the number of classes. In your data you state you have three classes hence only 2 linear discriminants can be resolved.</p>\\n',\n",
       " \"<p>Based on your reaction to my comment:</p>\\n\\n<p>You are looking for prediction. Thus, you should not really rely on (in)significance of the coefficients. You would be better to</p>\\n\\n<ul>\\n<li>Pick a criterion that describes your prediction needs best (e.g.\\nmissclassification rate, AUC of ROC, some form of these with\\nweights,...)</li>\\n<li>For <em>each model of interest</em>, evaluate this criterion. This can be\\ndone e.g.by providing a validation set (if you're lucky or rich),\\nthrough crossvalidation (typically tenfold), or whatever other\\noptions your criterion of interest allows. If possible also find an\\nestimate of the SE of the criterion for each model (e.g. by using the\\nvalues over the different folds in crossvalidation)</li>\\n<li>Now you can pick the model with the best value of the criterion,\\nthough it is typically advised to pick the most parsimoneous model\\n(least variables) that is within one SE of the best value.</li>\\n</ul>\\n\\n<p>Wrt <em>each model of interest</em>: herein lies quite a catch. With 10 potential predictors, that is a truckload of potential models. If you've got the time or the processors for this (or if your data is small enough so that models get fit and evaluated fast enough): have a ball. If not, you can go about this by educated guesses, forward or backward modelling (but using the criterion instead of significance), or better yet: use some algorithm that picks a reasonable set of models. One algorithm that does this, is penalized regression, in particular Lasso regression. If you're using R, just plug in the package glmnet and you're about ready to go.</p>\\n\",\n",
       " '<p>Three options, </p>\\n\\n<ul>\\n<li><a href=\"http://www.yeroon.net/ggplot2/\" rel=\"nofollow\">http://www.yeroon.net/ggplot2/</a> is an online web based gui to ggplot2 that lets you build your plots interactively.</li>\\n<li><a href=\"http://www.deducer.org/pmwiki/pmwiki.php?n=Main.DeducerManual\" rel=\"nofollow\">Deducer</a> is an R gui that has an interactive graph builder</li>\\n<li><a href=\"http://www.rstudio.org/\" rel=\"nofollow\">R Studio</a> has functionality for interactive graph generation.</li>\\n</ul>\\n',\n",
       " '<p>You can back-calculate the standard errors from the CIs. Odds and hazard ratios are typically analyzed on the log scale. So, you will find that on the log scale, you get a symmetric confidence interval around the log of the estimate. For example, for $OR = 1.57$, you get $log(OR) = 0.451$ and the lower and upper CI bounds on the log scale are $0.077$ and $0.820$. Now these CI bounds are calculate with $log(OR) \\\\\\\\pm 1.96 \\\\\\\\times SE$, which implies that $SE = (UB - LB) / (2 \\\\\\\\times 1.96)$. So, we get: $SE = (0.820 - 0.077) / (2 \\\\\\\\times 1.96) = 0.189$. The same things works for the hazard ratio, where you will find that $SE = 0.246$. Note that these standard errors are for the log odds ratio and the log hazad ratio. But this is the scale we work on anyway when combining results from several studies with these outcome measures.</p>\\n',\n",
       " '<p>I find the mean to be a much more useful indicator of central tendency of Likert items than the median.\\nI have elaborated on my argument <a href=\"http://stats.stackexchange.com/questions/97/what-are-good-basic-statistics-to-use-for-ordinal-data/8995#8995\">here on a question asking about whether to use the mean or median for likert items</a>.</p>\\n\\n<p>A recap of some of these reasons:</p>\\n\\n<ul>\\n<li>The mean is more informative; the median is too gross for Likert items. For example, the median of <code>1 1 3 3 3</code> is the same as <code>3 3 3 5 5</code> (i.e., 3) but the mean reflects the difference.</li>\\n<li>Likert items are often phrased in ways where the equal distance between categories assumption is a useful starting point.</li>\\n<li>Even if individual responses are discrete, the group level measurement approaches continuity (with 500 people and a 5 point scale, the value of your mean could take on <code>500 * 4 + 1 = 2001</code> different values)</li>\\n<li>There is little argument that a percentage is a useful summary in yes-no type questions (e.g., voting). This is just the mean where responses have been coded <code>0 and 1</code>. Treating a 5 point likert scale as <code>1 2 3 4 5</code> seems almost as natural to me.</li>\\n<li>Other plausible scalings of the Likert items probably wont change inferences substantively regarding whether differences between means exist (but you can check this).</li>\\n</ul>\\n\\n<p>If you are persuaded that the mean is the appropriate measure of central tendency, then you would want to structure your hypothesis tests so that they test for differences between means.\\nA paired sample t-test would allow for a pair-wise comparison of means, but there would be issues around the accuracy of p-values given the discrete and non-normal error distribution. Nonetheless, adopting a non-parametric approach is not a solution, because it changes the hypothesis.</p>\\n\\n<p>I would expect that the paired sample t-test would be fairly robust at least for typicaly Likert item means that avoid either extreme on the scale, but I don\\'t have any simulation studies on hand.</p>\\n',\n",
       " '<p>It would appear that the random forest is used <a href=\"http://research.microsoft.com/pubs/145347/BodyPartRecognition.pdf\">to predict movements of individual body parts</a>. In this sense, predict does not mean \"predict where the body part will move to\" but \"best guess the 3D location of the body parts\". <a href=\"http://www.i-programmer.info/news/105-artificial-intelligence/2176-kinects-ai-breakthrough-explained.html\">Here</a> is an overview of the method.</p>\\n',\n",
       " \"<p>I practiced some problem (not homework) as below.</p>\\n\\n<p>Suppose that $X_1,X_2,\\\\\\\\cdots$ are independent and identically distributed real-valued random variables with $E|X_1|=\\\\\\\\infty$. Show that $\\\\\\\\sum_{n=1}^\\\\\\\\infty P({|X_n| \\\\\\\\geq a\\\\\\\\cdot n})=\\\\\\\\infty$ for each $a &gt;0$.</p>\\n\\n<p>I obtained that $\\\\\\\\infty = E(|X_n|) = \\\\\\\\int_{0}^\\\\\\\\infty P(|X_n| &gt;x)dx \\\\\\\\leq \\\\\\\\sum_{n=0}^\\\\\\\\infty P(|X_n|&gt;n)$, but I couldn't get the final step $\\\\\\\\sum_{n=1}^\\\\\\\\infty P({|X_n| \\\\\\\\geq an})=\\\\\\\\infty$ for each $a &gt;0$ ( or can I use the statment to imply the result?) Please provide some hints, thanks!</p>\\n\",\n",
       " '<p>I came across the scale parameter used in the logit and probit models. Does any one know what that is and what it is used for? What would go wrong if I did not use it?</p>\\n',\n",
       " '<p>Mixture models arise in attempts to characterize complicated probability distributions, especially those with two or more modes, in terms of distributions with mathematically simple descriptions.</p>\\n\\n<h3>Disambiguation</h3>\\n\\n<p>Do not confuse a \"mixture model\" with a \"<a href=\"http://en.wikipedia.org/wiki/Mixed_model\" rel=\"nofollow\">mixed model</a>\"!  The former concerns distributions, typically multi-modal, that will be analyzed as positive linear combinations of other distributions.  The latter occurs in a regression setting where some of the independent variables are viewed as fixed and others are viewed as realizations of random variables.</p>\\n\\n<p>Note that although the density of a mixture is, by definition, a linear combination of densities, <em>it is not in general the same as the density of a linear combination of random variables.</em>  For example, the average of two normal random variables is normal (and therefore has a single mode), but a 50:50 mixture of two different normal densities often has two modes and is never normal.</p>\\n',\n",
       " '<p>Just for the sake of completeness, if you need this for a class demonstration, I would also mention the <code>manipulate</code> package which comes with <a href=\"http://www.rstudio.org/\" rel=\"nofollow\">RStudio</a>. Note that this package is dependent on RStudio interface, so it won\\'t work outside of it.</p>\\n\\n<p><code>manipulate</code> is quite cool because it allows to quickly create some sliders to manipulate any element in the plot. This would allow to do some easy and <em>real-time</em> demonstration in class.</p>\\n\\n<pre><code>manipulate(\\n  plot(density(1:10, bw)),\\n  bw = slider(0, 10, step = 0.1, initial = 1)) \\n</code></pre>\\n\\n<p>Other examples <a href=\"http://www.rstudio.org/docs/advanced/manipulate\" rel=\"nofollow\">here</a></p>\\n',\n",
       " \"<p>edit: tl;dr: I can coerce a lot of optimization problems to take the form of a non-linear least squares problem, but does it make sense to do so?</p>\\n\\n<p>Suppose we have some empirical data $P=\\\\\\\\{(x_i', y_i')\\\\\\\\}_{i\\\\\\\\in I}$. A non-linear least squares (NLLSQ) problem is one where we want to minimize a function of the form</p>\\n\\n<p>$$S(\\\\\\\\beta)=\\\\\\\\sum_{i\\\\\\\\in I}\\\\\\\\left[y_i-f_\\\\\\\\beta(x_i)\\\\\\\\right]^2$$</p>\\n\\n<p>where $(x_i, y_i) = (x_i', y_i')$ and $f_\\\\\\\\beta$ is a curve with parameters $\\\\\\\\beta$.</p>\\n\\n<p>It seems like a lot of problems which are not naturally NLLSQ problems can be reformulated as such.</p>\\n\\n<ol>\\n<li><p>If $d(x,y)$ is any metric on $\\\\\\\\mathbb{R}$, then</p>\\n\\n<p>$$S_1(\\\\\\\\beta) = \\\\\\\\sum_{i\\\\\\\\in I}\\\\\\\\left[d\\\\\\\\left(y_i',g_\\\\\\\\beta(x_i)\\\\\\\\right)\\\\\\\\right]^2$$</p>\\n\\n<p>is almost of the above form, with $y_i=0$ and\\n$f_\\\\\\\\beta(x_i) = d(y_i',g_\\\\\\\\beta(x_i))$. Unfortunately, $f_\\\\\\\\beta$ is now a function, not only of $x_i$ but also of the index $i$, but if the $x_i$ do not repeat, then we can let $h(x)$ be any function which interpolates the points $(x_i', y_i')$ (like a linear interpolation or spline interpolation), and define</p>\\n\\n<p>$$S_2(\\\\\\\\beta) = \\\\\\\\sum_{i\\\\\\\\in I}\\\\\\\\left[d\\\\\\\\left(h(x_i),g_\\\\\\\\beta(x_i)\\\\\\\\right)\\\\\\\\right]^2$$</p>\\n\\n<p>which is now certainly in the correct form.</p>\\n\\n<p>(This applies if $d$ is any function really, but it seems more interesting in the case of a metric)</p></li>\\n<li><p>The square can also be absorbed into $d(x,y)$ to yield</p>\\n\\n<p>$$S_3(\\\\\\\\beta) = \\\\\\\\sum_{i\\\\\\\\in I} d\\\\\\\\left(h(x_i),g_\\\\\\\\beta(x_i)\\\\\\\\right)$$</p></li>\\n<li><p>Going even further, it is possible to make the sum be indexed over a singleton to yield the following:</p>\\n\\n<p>$$S_4(\\\\\\\\beta) = j_\\\\\\\\beta(P)$$</p>\\n\\n<p>where now $I=\\\\\\\\{0\\\\\\\\}$, $(x_0, y_0) = (0,0)$ and $f_\\\\\\\\beta(x)=\\\\\\\\sqrt{j_\\\\\\\\beta(P)}$ where $j_\\\\\\\\beta$ is constant in terms of x, but can be an arbitrary non-negative function with respect to $(\\\\\\\\beta, P)$.</p></li>\\n</ol>\\n\\n<p>So it seems like we can pose quite general optimization problems as quite degenerate cases of NLLSQ. I have 2 questions.</p>\\n\\n<ul>\\n<li><p>In which of these settings (if any) does it still make sense to apply NLLSQ <em>algorithms</em> to solve the problem. (Thinking in particular of Levenberg-Marquardt).</p></li>\\n<li><p>What other—more general purpose—algorithms are available in the cases where NLLSQ algorithms will perform badly?</p></li>\\n</ul>\\n\",\n",
       " '<p>First, I would like to ask you how do you know linear classifier is the best choice? Intuitively for such a large space (R^10000) it is possible that some other non-linear classifier is a better choice.</p>\\n\\n<p>I suggest you to try several different classifiers and observe the prediction errors (I would try several regularized classification models).</p>\\n\\n<p>If you run out of memory reduce the dimension using <a href=\"http://en.wikipedia.org/wiki/Principal_component_analysis\" rel=\"nofollow\">PCA</a></p>\\n',\n",
       " '<p>Is it possible to specify a variance matrix for the random effects in PROC MIXED with the only restriction that the diagonal entries are equal ? I took a look in the help and I have not found, but this seems strange because such a variance structure is rather natural in some applications. </p>\\n\\n<p>(EDIT) Consider for instance such a dataset:</p>\\n\\n<pre><code>&gt; dat\\n   Subject Dose   y\\n1        1    A  10\\n2        1    A  11\\n3        1    A  12\\n4        1    B  30\\n5        1    B  31\\n6        1    B  32\\n7        1    C 100\\n8        1    C 101\\n9        1    C 102\\n10       2    A  11\\n11       2    A  14\\n12       2    A  13\\n13       2    B  33\\n14       2    B  37\\n15       2    B  36\\n16       2    C 105\\n17       2    C 110\\n18       2    C 109\\n19       3    A   9\\n20       3    A  11\\n21       3    A  12\\n22       3    B  30\\n23       3    B  35\\n24       3    B  32\\n25       3    C 115\\n26       3    C 101\\n27       3    C 102\\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/tKTNV.png\" alt=\"dataplot\"></p>\\n\\n<p>and the following model:</p>\\n\\n<pre><code>PROC MIXED DATA=dat ;\\n    CLASS SUBJECT DOSE ;\\n    MODEL y = DOSE ;\\n    RANDOM DOSE / subject=SUBJECT type=MYMATRIX ;\\nRUN; QUIT;\\n</code></pre>\\n\\n<p>I want a matrix \"MYMATRIX\" with the same variance for each level of the DOSE factor, but not a compound symmetry matrix because the correlation between the means of the levels are different.</p>\\n\\n<p>(EDIT2) The mathematical meaning of this model is the following one. Denoting by $i$ the index for the dose level and by $j$ the index for the subject, one has $$(y_{ijk} | \\\\\\\\mu_{ij}) \\\\\\\\sim_{\\\\\\\\text{iid}} {\\\\\\\\cal N}(\\\\\\\\mu_{ij}, \\\\\\\\sigma^2_w), \\\\\\\\quad k=1, \\\\\\\\ldots, 3 \\\\\\\\quad \\\\\\\\text{ for all } i,j$$ and $$ \\\\\\\\begin{pmatrix} \\n\\\\\\\\mu_{1j} \\\\\\\\\\\\\\\\\\n\\\\\\\\mu_{2j} \\\\\\\\\\\\\\\\\\n\\\\\\\\mu_{3j}  \\n \\\\\\\\end{pmatrix}\\n \\\\\\\\sim_{\\\\\\\\text{iid}} \\n {\\\\\\\\cal N}_3\\n \\\\\\\\left( \\n \\\\\\\\begin{pmatrix} \\n\\\\\\\\mu_1 \\\\\\\\\\\\\\\\\\n\\\\\\\\mu_2 \\\\\\\\\\\\\\\\ \\n\\\\\\\\mu_3 \\n \\\\\\\\end{pmatrix}, \\n G\\n \\\\\\\\right), \\\\\\\\quad j=1, \\\\\\\\ldots, 3$$\\nThe diagonal entries of the $G$ matrix are the between variances for each level of the dose. I want $G$ to be of the form $$G=\\\\\\\\begin{pmatrix} \\\\\\\\sigma^2_b &amp; \\\\\\\\sigma_{12} &amp; \\\\\\\\sigma_{13} \\\\\\\\\\\\\\\\  \\\\\\\\sigma_{12} &amp; \\\\\\\\sigma^2_b &amp; \\\\\\\\sigma_{23} \\\\\\\\\\\\\\\\ \\\\\\\\sigma_{13} &amp;   \\\\\\\\sigma_{23} &amp; \\\\\\\\sigma^2_b \\\\\\\\end{pmatrix}$$</p>\\n',\n",
       " \"<p>I have 31 observations for 9 subjects undergoing therapy (4 session for 4 subjects, and 3 session for the rest).  I have implemented a mixed model to assess relationships between the observations using Fisher-transformed correlation coefficients.  </p>\\n\\n<p>For example, to assess the effect of finger tapping on correct responses, I correlated the number of finger taps with the percentage of correct responses for each subject (3 pairs of observations for some subjects, and 4 for others).  I then perform a Fisher-transform on the 9 resulting correlation coefficients to get 9 z-scores.  Then it's a simple matter of testing whether or not those z-scores differ from 0.</p>\\n\\n<p>What I would like to do is control for subjects' age.  However, I'm not clear on the best way to do this.  It's easy to calculate the covariance between the z-scores and subject age, but how do I get the effect of finger taps on correct responses, controlling for age?  </p>\\n\\n<p>(As a side note, I've implemented this as a MIXED model in SPSS, but it fails to converge, so I can't trust the results).  </p>\\n\",\n",
       " '<p>Pooling can be still useful since it reduces variance but in this case it can also introduce bias (due to the endogeneity you\\'ve mentioned). One way to get around that is to include the number of visits/observations as one of the covariates in your model (so pooling will \"shrink\" the model parameters toward the sample mean while controlling for the effect you\\'ve mentioned)</p>\\n',\n",
       " '<p>It is a general and maybe stupid question, but it will help me to avoid a mistake. To test the dependence of continuous variable on nominal with parametric test (t-test, ANOVA), \"the data has to be normally distributed\" (distribution have to be not much violated from normal). </p>\\n\\n<p>But I don\\'t clearly understand: I have to test the normality of the continuous data together (whole sample) or the normality of data in every single group? </p>\\n\\n<p>And if my groups have small sizes (n=4-6), and therefore I fail to reject null, does this indicate a permission to use parametric methods?</p>\\n\\n<p>Can anyone provide me a link to some <em>good</em> examples of statistical (biomedical) hypothesis testing with R (google did not helped)?</p>\\n',\n",
       " '<p>Simulation variance is brought about by the fact that estimating expectations (integrals that are part of likelihood functions) by averaging a bunch of realizations of the respective random variables is not precise for a finite bunch. It gets smaller with the number of simulations and not related to the sample size, N.</p>\\n\\n<p>Economic models may not have less simulation variance, they assume integrals to be computed exactly, so it is a separate issue.</p>\\n',\n",
       " \"<p>I'm trying to fit a multilevel model for a repeated mesures design with three levels:\\nSujects - conditions - trials. Each subject pass the test in two conditions, and there are 21 trials in each condition. Each trial is expected to be correlated in some manner with previous and following trial. </p>\\n\\n<p>My problem is that I don't know which covariance type for repeated measures should I select to the model... Anyone can tell me how can I find out which is the correct? (By the way, I'm using SPSS and not familiar with other softwares...)</p>\\n\\n<p>Thanks!</p>\\n\",\n",
       " '<p>Perhaps you want to arrive at the <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Confidence_interval\" rel=\"nofollow\">confidence interval</a> for the probability $\\\\\\\\theta$ that the predicted variable is the same as the observed one. That will give you the an estimate (mid-point of the interval) of how good the predictions are, and how confident (significance level) you can be that the accuracy lies in a certain range (interval width).</p>\\n',\n",
       " \"<p>Instead of looking for statistical solutions that directly solve this problem, I would look for solutions that improve the diagnosis.  </p>\\n\\n<p>First, I'd compare the different samples used in the different studies.</p>\\n\\n<p>Then, if you have the data, I'd look at the correlation patterns among the variables in the different samples.  (You may be able to get these from other authors).</p>\\n\",\n",
       " '<p>My colleague and I are fitting a range of linear and nonlinear mixed effect models in R. We are asked to perform cross-validation on the fitted models so that one can verify that the effects observed are relatively generalizable. This is normally a trivial task, but in our case, we have to split the whole data into a training part and a testing part (for CV purposes) that share no common levels. For example,</p>\\n\\n<p>The training data may be based on Groups 1,2,3,4;\\nThe fitted model is then cross-validated on Group 5.</p>\\n\\n<p>So this creates a problem since the group-based random effects estimated on the training data do not apply to the testing data. Thus, we cannot CV the model.</p>\\n\\n<p>Is there a relatively straightforward solution to this? Or has anyone written a package to tackle this problem yet? Any hint is welcome!</p>\\n\\n<p>Thanks!</p>\\n',\n",
       " '<p>I wrote a post compiling links of  Practice Questions for Statistics in Psychology (Undergraduate Level).\\n<a href=\"http://jeromyanglim.blogspot.com/2009/12/practice-questions-for-statistics-in.html\" rel=\"nofollow\">http://jeromyanglim.blogspot.com/2009/12/practice-questions-for-statistics-in.html</a></p>\\n\\n<p>The questions would fall into the introductory category.</p>\\n',\n",
       " '<p>Weka is very lacking and slow in clustering.</p>\\n\\n<p>KNIME and RapidMiner are mostly frontends to Weka when it comes to algorithms.</p>\\n\\n<p>You should have a look at ELKI, it has a lot more in clustering and outlier detection than Weka, and usually is a lot faster. There are quite some algorithms that I\\'m not aware of other implementations being publicly available except in ELKI. Just compare OPTICS in Weka and ELKI and you\\'ll see the difference in the results and runtime.</p>\\n\\n<p>The only thing is: technically it\\'s not a library. It\\'s more of a standalone application, designed to develop new algorithms in the frameworks. But it\\'s not a \"feed a double[][] array in, execute magic(), get magic output result out\" type of library. Data mining and clustering is more of an explorative process than something you can automate - it\\'s the machine learning side that you can automate to some extend (most as in: once you have a good classifier, it will classify new data, too). But say, you analyze a data set one day, you find some structure. Over the next year, you data set grew to twice the size and the structure has disappeared, until you change some parameters, or even the algorithm. If you then find the same structure again, then you probably are on to some real structure in your data, at least, and not some random data artifact. :-)</p>\\n\\n<p>Mahout has some of the standard stuff, their focus is on scalability, not so much on flexibility. Some people use it on small data sets, too, but that doesn\\'t make a lot of sense IMHO. It also is coming more from a classification side, like Weka, in my opinion. Data exploration in big data also is a lot harder. With machine learning, more data usually just makes your predictions better (if you can still process it all, which is where Hadoop and Mahout come in). But say you run a cluster analysis on a big data set, and you get out 1000 clusters - what are you going to do with them? How do you analyze them if they make any sense? You cannot really measure the quality of a clustering result without either comparing it with previous knowledge (at which point you did not gain anything except a shiny benchmark number, because you already know the \"truth\") or by manually inspecting the resulting cluster.</p>\\n',\n",
       " '<p>I would like to add one remark - knn is sensitive to let say number of observation on the boundery of given class to the total number of observation in that class. If you have three classes with the same number of observations from the same distribution but with different means and second class is visiably cloud between two others - its expected value is between two others, then there is more missclassfications in the class number two. But something like this hold for every classifier.</p>\\n',\n",
       " \"<p>When doing multivariate regression, it's often the case that some predictors often have many zero values - dichotomous inputs, dummy coding of polychotomous inputs, interval coding, etc.  The fraction of nonzero observations for a predictor is especially decreased when interacting these variables.</p>\\n\\n<p>It seems obvious that only the observations of a predictor for which it is nonzero (after any transformations) will affect the estimation of its coefficient.</p>\\n\\n<p>When doing multivariate regression I like to use weakly informative priors (like Gaussian or Cauchy) for regularization.  I've noticed that sometimes, predictors which are usually zero still take on implausible regression coefficients, like ~6 for a logistic regression model.  This seems to be because a variable that is nonzero for only a handful of observations is still overcoming the prior because it's getting full credit for all of its zero observations, and thus getting a huge coefficient, which is in many cases a prior obviously overfitting, and hurts generalization performance for prediction.</p>\\n\\n<p>Multiplying the prior's scale by the fraction of observations of a predictor that are nonzero fixes this and greatly improves generalization performance, makes coefficients plausible, and seems to be common sense.</p>\\n\\n<p>So - why don't more people do this?  I've tried a cursory search in common textbooks (various machine learning books, applied and theoretical Bayesian statistics), R packages on CRAN, some machine learning packages, and I don't see evidence of anyone else doing this...  isn't this just common sense?  Or is there something wrong with this approach that I'm missing?</p>\\n\",\n",
       " '<p>I recommend <a href=\"http://www.sciviews.org/Tinn-R/\" rel=\"nofollow\">Tinn-R</a> (Which is the acronym for Tinn is not Notepad)</p>\\n',\n",
       " '<p>In PCA eigenvalues determine the order of components. In ICA I am using kurtosis to obtain the ordering. What are some accepted methods to assess the number, (given I have the order) of components that are singificant apart from prior knowledge about the signal?</p>\\n',\n",
       " '<p>The simple version is that any two variables that tend to change in one direction over time will appear to be correlated, whether there\\'s any connection between them or not.  Consider the following variables:</p>\\n\\n<pre><code>set.seed(1)\\ntime = seq(from=1, to=100, by=1)\\nx  = .5 + .3*time +        rnorm(100)\\ny1 =  3 + .3*time +        rnorm(100)\\ny2 =  7 + .1*time + .8*x + rnorm(100)\\n</code></pre>\\n\\n<p>$x$ is just a function of time, as is $y1$.  $y2$ is a function of both time and $x$.  The point is to recognize from the code that there really is a relationship between $x$ and $y2$, and that there is no relationship between $x$ and $y1$.  Now look at the following figure, all three lines look awfully similar, don\\'t they?</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/gmvYM.jpg\" alt=\"enter image description here\"></p>\\n\\n<p>In fact, the $R^2$ value for the relationship between $x$ and $y1$ is 98%, and the $R^2$ for $x$ and $y2$ is 99%.  But we know that there\\'s no real relationship between $x$ and $y1$, whereas there is between $x$ and $y2$, so how do we differentiate the real from mere appearance?  That\\'s where differencing comes in.  For any two of the variables, since they both tend to go up over time, that\\'s not very informative, but given that one goes up by some specific amount, does that tell us how much the other goes up?  Differencing allows us to answer that question.  Note the following two figures, scatterplots I made after differencing all three variables.  </p>\\n\\n<p><img src=\"http://i.stack.imgur.com/SCpCd.jpg\" alt=\"enter image description here\"></p>\\n\\n<p><img src=\"http://i.stack.imgur.com/etJAY.jpg\" alt=\"enter image description here\"></p>\\n\\n<p>Here, we clearly see that knowing something about <em>how much</em> $x$ went up tells us something about how much $y2$ goes up ($R^2=.43$), but that this is not the case for $x$ and $y1$ ($R^2=.07$).  So the answer to your question is that you should ignore the correlations amongst your original variables and look at the differenced variables.  Given that your $R^2$ is .004, I would say there\\'s no actual relationship.  </p>\\n\\n<p>Some other points:  In the figures, I make a point of noting that these are simultaneous changes.  There\\'s nothing wrong with that, and it follows from the way I set up the problem, but usually people are interested in effects at some lag.  (That is, change in one thing at one point in time leads to change in something else later.)  Second, you mention taking the log of one of your series.  Taking the log simply switches your data from levels to rates.  And thus, when you difference, you are looking at changes in rates rather than changes in levels.  That\\'s very common, but I didn\\'t include that element in my demonstration; it\\'s orthogonal to the issues I discussed.  Lastly, I want to acknowledge that time series data are often more complicated than my demonstration lets on.  A comprehensive overview would require a book length treatment, but @Charlie\\'s answer does a good job of succinctly pointing out some of the complexities that I left out.</p>\\n',\n",
       " '<p>I have used various algorithms, including Bayesian approaches (and, I am sorry to confess, even Excel many years ago), to fit mixtures.  When there is not a clear visual indication of the two (or more components) in the histogram, you can expect the likelihood function to be extremely flat--almost parabolic--near its peak.  This is because the visual impression translates mathematically into an ability to trade off some proportion of one mixture with an equivalent proportion of the other (adjusting the parameters of the components to keep a good fit) while making only a minor change to the likelihood.  In many cases it\\'s difficult to pin down the maximum likelihood.  (This is evidenced by regime-switching in the Markov chains, for instance: a chain will pursue an area where one component predominates and after longish periods switch to an area where another component predominates, never really settling down to a single optimum.)  In any event <em>you also want to assess uncertainty.</em>  This is reflected by how much change is needed in the mixture parameters to reduce the likelihood by some threshold amount.  The near-parabolic flatness near the optimum delineates a long \"ridge\" of near-optimum values, resulting in a long elliptical confidence region for the mixture.  Usually the major axis of that ellipse corresponds to the mixture proportions.  Thus, you might conclude that your data are $p$ percent of component A and $1-p$ percent of component B, but $p$ might be anywhere from 0 to 70%.  (Yes, there are boundary value problems with mixtures, too.)  It can take an extraordinary amount of data to reduce these wide confidence intervals if you can even reliably find them.</p>\\n\\n<p>These problems are exacerbated when only the tails of the data provide most of the information needed to disentangle the distributions.  This would often be the case for unimodal data.</p>\\n',\n",
       " '<p>Your approach seems way too complicated to me. Let\\'s start with some data:</p>\\n\\n<pre><code>## make up some data\\nstatus &lt;- factor(rbinom(1000, 1, 0.3), labels = c(\"single\", \"married\"))\\nage &lt;- sample(20:50, 1000, replace = TRUE)\\ndf &lt;- data.frame(status, age)\\nhead(df)\\n</code></pre>\\n\\n<p>Print the first six cases:</p>\\n\\n<pre><code>&gt; head(df)\\n   status age\\n1 married  21\\n2  single  50\\n3  single  43\\n4  single  28\\n5 married  28\\n6  single  40\\n</code></pre>\\n\\n<p>Next, we need to calculate row wise percentages; even if I doubt that this makes sense (it refers to your statement: \"What i need to do is to count number of people that were never married at each age and divide it by the total number of never married people to get a percentage.\").</p>\\n\\n<pre><code>## calculate row wise percentages (is that what you are looking for?)\\n(tab &lt;- prop.table(table(df), 1)*100)\\n</code></pre>\\n\\n<p>The resulting table looks like this:</p>\\n\\n<pre><code>&gt; (tab &lt;- prop.table(table(df), 1)*100)\\n         age\\nstatus          20       21       22       23       24       25       26\\n  single  1.857143 3.142857 3.428571 2.285714 2.142857 2.857143 3.428571\\n  married 2.333333 2.333333 5.666667 1.333333 3.333333 5.333333 2.000000\\n         age\\nstatus          27       28       29       30       31       32       33\\n  single  2.857143 3.142857 3.428571 3.285714 2.714286 3.714286 3.571429\\n  married 5.000000 4.333333 2.666667 4.000000 1.666667 4.666667 3.000000\\n         age\\nstatus          34       35       36       37       38       39       40\\n  single  3.000000 2.857143 5.000000 3.571429 2.857143 3.571429 3.000000\\n  married 3.333333 4.000000 4.000000 2.333333 2.000000 2.000000 2.000000\\n         age\\nstatus          41       42       43       44       45       46       47\\n  single  4.285714 3.000000 3.714286 3.857143 2.857143 3.714286 1.714286\\n  married 2.333333 3.333333 2.000000 4.333333 3.666667 5.333333 2.666667\\n         age\\nstatus          48       49       50\\n  single  2.857143 3.428571 4.857143\\n  married 2.333333 3.000000 3.666667\\n</code></pre>\\n\\n<p>That is, if you sum up row wise, it gives 100%</p>\\n\\n<pre><code>&gt; sum(tab[1,])\\n[1] 100\\n</code></pre>\\n\\n<p>Finally, plot it. </p>\\n\\n<pre><code>## plot it\\nplot(as.numeric(dimnames(tab)$age), tab[1,], \\n     xlab = \"Age\", ylab = \"Single [%]\")\\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/wlZEV.png\" alt=\"enter image description here\"></p>\\n',\n",
       " '<p>The answer to your monotonicity conjecture is <strong>affirmative</strong>. It admits a somewhat sneaky proof and allows us to conclude something about the Poisson distribution in the\\nprocess. This is what we explore below.</p>\\n\\n<p><strong>The picture</strong></p>\\n\\n<p>The question asks whether the cdfs decrease pointwise as $n$ increases\\nfor each $x$ to the left of the vertical line $x=1$.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/22ZBd.png\" alt=\"Gamma cdfs\"></p>\\n\\n<p><strong>The math</strong></p>\\n\\n<p>Let\\'s start with a restatement of the question. First, if $\\\\\\\\renewcommand{\\\\\\\\Pr}{\\\\\\\\mathbb P}\\\\\\\\newcommand{\\\\\\\\d}{\\\\\\\\mathrm d}T \\\\\\\\sim\\n\\\\\\\\mathrm{Exp}(1)$, then it\\'s easy to see that $T/n \\\\\\\\sim\\n\\\\\\\\mathrm{Exp}(n)$. So, we can rewrite a <a href=\"http://en.wikipedia.org/wiki/Gamma_distribution#Characterization_using_shape_.CE.B1_and_rate_.CE.B2\" rel=\"nofollow\">$\\\\\\\\Gamma(n,n)$ random variable</a>\\n$S_n$ as $S_n = (T_1 + \\\\\\\\cdots + T_n)/n$ where $T_i$ is a sequence of\\niid $\\\\\\\\mathrm{Exp}(1)$ random variables. Instead of asking whether the\\ncdf decreases pointwise as a function of $n$ for each $0 &lt; x &lt; 1$, we\\ncan ask whether the survival distribution <em>increases</em>\\npointwise. Hence, we have the following equivalent question.</p>\\n\\n<blockquote>\\n  <p><strong>Equivalent question</strong>: Let $T_1, T_2,\\\\\\\\ldots$ be an iid sequence of   $\\\\\\\\mathrm{Exp}(1)$ random variables and let $S_n = T_1 + \\\\\\\\dots +  T_n$. Then, is it true that, for all fixed $x \\\\\\\\in (0,1)$, $$ \\\\\\\\Pr( S_{n+1} &gt; (n+1) x ) \\\\\\\\geq \\\\\\\\Pr(S_n &gt; n x) \\\\\\\\,? $$</p>\\n</blockquote>\\n\\n<p><em>Proof</em>. The key idea here is to <strong>divide and conquer</strong>. We need to somehow compare the events $\\\\\\\\{S_{n+1} &gt; (n+1)x\\\\\\\\}$ and $\\\\\\\\{S_n &gt; nx\\\\\\\\}$. To that end, notice that\\n$$\\n\\\\\\\\Pr(S_{n+1} &gt; (n+1)x) = \\\\\\\\Pr(S_{n+1} &gt; n x) - \\\\\\\\Pr(nx \\\\\\\\leq S_{n+1} \\\\\\\\leq (n+1)x) \\\\\\\\&gt;.\\n$$\\nWe are halfway there. To deal with the remaining part, observe that\\n$$\\n\\\\\\\\{S_{n+1} &gt; n x\\\\\\\\} = \\\\\\\\{S_n &gt; n x\\\\\\\\} \\\\\\\\cup \\\\\\\\{S_n \\\\\\\\leq n x, T_{n+1} &gt; nx - S_n \\\\\\\\} \\\\\\\\&gt;,\\n$$\\nand the two events on the right-hand size are disjoint. So,\\n$$\\n\\\\\\\\Pr(S_{n+1} &gt; (n+1)x) = \\\\\\\\Pr(S_n &gt; n x) + \\\\\\\\Pr(S_n \\\\\\\\leq nx, T_{n+1} &gt; nx - S_n) - \\\\\\\\Pr(nx \\\\\\\\leq S_{n+1} \\\\\\\\leq (n+1)x) \\\\\\\\&gt;.\\n$$</p>\\n\\n<p>If we can show the second probability is greater than the third, we are done. To do this, we\\'ll first find an explicit value for the second probability and then find an upper bound for the third.</p>\\n\\n<p>(<em>Second term</em>.) $S_n$ and $T_{n+1}$ are independent, so to find $\\\\\\\\Pr(S_n \\\\\\\\leq nx, T_{n+1} &gt; nx - S_n)$, we need only integrate over the\\nregion of interest. This is\\n$$\\n\\\\\\\\Pr(S_n \\\\\\\\leq nx, T_{n+1} &gt; nx - S_n) = \\\\\\\\int_0^{nx} \\\\\\\\int_{nx-s}^\\\\\\\\infty\\ne^{-t} \\\\\\\\frac{s^{n-1}e^{-s}}{(n-1)!} \\\\\\\\, \\\\\\\\d t \\\\\\\\, \\\\\\\\d s  =\\n\\\\\\\\frac{e^{-nx}(nx)^n}{n!} \\\\\\\\&gt;.\\n$$</p>\\n\\n<p>(<em>Third term</em>.) This step is more involved, but still employs only elementary tools. Using the density of $S_{n+1}$, we have\\n$$\\\\\\\\newcommand{\\\\\\\\d}{\\\\\\\\mathrm d}\\n\\\\\\\\Pr(nx \\\\\\\\leq S_{n+1} \\\\\\\\leq (n+1)x )= \\\\\\\\int_{nx}^{(n+1)x} \\\\\\\\frac{u^n\\ne^{-u}}{n!} \\\\\\\\,d u = \\\\\\\\frac{e^{-nx} (nx)^n}{n!} x \\\\\\\\int_0^1 (1+v/n)^n\\ne^{-xv} \\\\\\\\d v\\n$$\\nwhere we\\'ve obtained the integral on the far right by using the substitution $v =\\n(u-nx)/x$. Now $(1+v/n)^n &lt; e^v$ for all $n$ and so\\n$$\\n\\\\\\\\int_0^1 (1+v/n)^n e^{-xv} \\\\\\\\d v &lt; \\\\\\\\int_0^1 e^{(1-x) v} \\\\\\\\d v =\\n\\\\\\\\frac{e^{1-x} - 1}{1-x} \\\\\\\\leq \\\\\\\\frac{1}{x} \\\\\\\\&gt;,\\n$$\\nwhere the last step follows from the fact that $e^{1-x} &lt; x^{-1}$ for\\nall $x \\\\\\\\in (0,1)$. So,\\n$$\\n\\\\\\\\Pr(nx \\\\\\\\leq S_{n+1} \\\\\\\\leq (n+1)x ) &lt; \\\\\\\\frac{e^{-nx}(nx)^n}{n!} \\\\\\\\&gt;. \\n$$</p>\\n\\n<p>But, this last quantity is the same as the second term! Putting the three pieces together, we get\\n$$\\n\\\\\\\\Pr(S_{n+1} &gt; (n+1)x ) &gt; \\\\\\\\Pr(S_n &gt; n x) \\\\\\\\&gt;,\\n$$\\nfor each $x \\\\\\\\in (0,1)$, which is what we set out to prove.</p>\\n\\n<p><strong>A consequence</strong></p>\\n\\n<p>The appearance of the term $\\\\\\\\frac{e^{-nx}(nx)^n}{n!}$ strongly hints\\nthat there is a relation to the Poisson hiding in here. Indeed, the\\nproof implies the following.</p>\\n\\n<blockquote>\\n  <p><strong>Proposition</strong>: Let $X_{n\\\\\\\\lambda} \\\\\\\\sim \\\\\\\\mathrm{Poisson}(n\\\\\\\\lambda)$   for $\\\\\\\\lambda \\\\\\\\in (0,1)$. Then $$ \\\\\\\\Pr(X_{n\\\\\\\\lambda} &lt; n) \\\\\\\\geq e^{-\\\\\\\\lambda} \\\\\\\\&gt;. $$</p>\\n</blockquote>\\n\\n<p>Indeed, this was the original context in which I recently gave an equivalent\\nproof <a href=\"http://mathoverflow.net/questions/104413/tail-bound-for-poisson-random-variable/104591#104591\" rel=\"nofollow\">in this MathOverflow answer</a>.</p>\\n',\n",
       " '<p>A few quick points:</p>\\n\\n<ul>\\n<li>Variance can be arbitrarily increased or decreased by adopting a different scale for your variable. Multiplying a scale by a constant greater than one would increase the variance, but not change the predictive power of the variable. </li>\\n<li>You may be confusing variance with reliability. All else being equal (and assuming that there is at least some true score prediction), increasing the reliability with which you measure a construct should increase its predictive power. Check out this discussion of <a href=\"http://en.wikipedia.org/wiki/Correction_for_attenuation\">correction for attenuation</a>.</li>\\n<li>Assuming that both scales were made up of twenty 5-point items, and thus had total scores that ranged from 20 to 100, then the version with the greater variance would also be more reliable (at least in terms of internal consistency).</li>\\n<li>Internal consistency reliability is not the only standard by which to judge a psychological test, and it is not the only factor that distinguishes the predictive power of one scale versus another for a given construct.</li>\\n</ul>\\n',\n",
       " '<p>How about making a solution based on the Simplex Method. Since the premise for using the Simplex method isn\\'t fulfilled we need to modify the method slightly. I call the modified version \"Walk the line\".</p>\\n\\n<p>Method:</p>\\n\\n<p>You are able to measure the uncertainty of each match. Do it! Calculate the uncertainty of each match with a single or double bet (for a triple bet there is no uncertainty).\\nWhen adding a double or triple bet, always choose the one that reduces uncertainty the most.</p>\\n\\n<ol>\\n<li>Start at maximum number of triple bets. Calculate total uncertainty.</li>\\n<li>Remove one triple bet. Add one or two double bets, keeping under maximum cost. Calculate total uncertainty.</li>\\n<li>Repeat step 2 until you have the maximum number of double bets.</li>\\n</ol>\\n\\n<p>Pick the bet with the lowest total uncertainty.</p>\\n',\n",
       " '<p>Yes, it is possible in SPSS.</p>\\n\\n<p>The data set must have all the counts in one column and the identifying information in two other columns.</p>\\n\\n<p>The variable that contains the counts needs to be identified by choosing \"Weight Cases\" from the \"Data\" menu. </p>\\n\\n<p>Click \"Weight cases by frequency variable\" and move over the variable containing the counts.</p>\\n\\n<p>Then use \"Crosstabs\" to get the results.</p>\\n',\n",
       " '<p>In circular statistics, the expectation value of a random variable $Z$ with values on the circle $S$ is defined as\\n$$\\nm_1(Z)=\\\\\\\\int_S z P^Z(\\\\\\\\theta)\\\\\\\\textrm{d}\\\\\\\\theta\\n$$\\n(see <a href=\"http://en.wikipedia.org/wiki/Circular_statistics#Moments\" rel=\"nofollow\">wikipedia</a>).\\nThis is a very natural definition, as is the definition of the variance\\n$$\\n\\\\\\\\mathrm{Var}(Z)=1-|m_1(Z)|.\\n$$\\nSo we didn\\'t need a second moment in order to define the variance!</p>\\n\\n<p>Nonetheless, we define the higher moments\\n$$\\nm_n(Z)=\\\\\\\\int_S z^n P^Z(\\\\\\\\theta)\\\\\\\\textrm{d}\\\\\\\\theta.\\n$$\\nI admit that this looks rather natural as well at first sight, and very similar to the definition in linear statistics. But still I feel a little bit uncomfortable, and have the following</p>\\n\\n<p><strong>Questions:</strong></p>\\n\\n<p>1.\\n<strong>What is measured by the higher moments</strong> defined above (intuitively)? Which properties of the distribution can be characterized by their moments?</p>\\n\\n<p>2.\\nIn the computation of the higher moments we use multiplication of complex numbers, although we think of the values of our random variables merely as vectors in the  plane or as angles. I know that complex multiplication is essentially addition of angles in this case, but still:\\n<strong>Why is complex multiplication a meaningful operation for circular data?</strong></p>\\n',\n",
       " \"<p>I don't see any cause for concern here -- No assumptions are obviously violated.  But this is often difficult to confirm with so few data points.  I think you are ok.</p>\\n\",\n",
       " \"<p>Given a set of extracted data from different sources with different accuracies, how can I combine the accuracy of those who give the same output?</p>\\n\\n<p>Example :</p>\\n\\n<pre><code>Data from source A are 80% correct\\nData from source B are 85% correct\\nData from source C are 90% correct\\n</code></pre>\\n\\n<p>If two of the sources give the same result (ResultA) and the third disagrees (ResultB) what's the probability of (A) being correct? This is not a homework question. I am a software developer and I don't have a clue about statistics and probability.</p>\\n\\n<p><strong>Update :</strong></p>\\n\\n<p>I've done an experiment using a random number generator</p>\\n\\n<p>Test 1 - 2 Possible outcomes (0/1) three methods (Acc: 0.5, 0.3, 0.1)</p>\\n\\n<pre><code>Samples      : 100000000\\nMethod A     : 0,49993692\\nMethod B     : 0,30023622\\nMethod C     : 0,09994145\\nMethod A+B   : 0,794567779569577\\nMethod B+C   : 0,0455372643070089\\nMethod C+A   : 0,205615801945512\\nMethod A+B+C : 0,0455215295368209\\n</code></pre>\\n\\n<p>Test 2 - 2 Possible outcomes (0/1) three methods (Acc: 0.8, 0.85, 0.9)</p>\\n\\n<pre><code>Samples      : 100000000\\nMethod A     : 0,80003639\\nMethod B     : 0,8500426\\nMethod C     : 0,90005791\\nMethod A+B   : 0,715942797491352\\nMethod B+C   : 0,927408281972288\\nMethod C+A   : 0,864147967527417\\nMethod A+B+C : 0,995137034088319\\n</code></pre>\\n\\n<p>That's the numbers I am looking for but I don't know how to calculate them...</p>\\n\",\n",
       " '<p>I hope you can help me because at the moment I am confused and helpless. I am final year PhD student, and my supervisors asked me to find out statistical methods other than exploratory (percentage of findings) to present my data. </p>\\n\\n<p>Here are my research data where we want to present consumer attitude and reasons towards disposal, with individual responses (N=70) collected via a questionnaire with the following items:</p>\\n\\n<ol>\\n<li>Product is functional? yes/no/not answered</li>\\n<li>If it is working can you pinpoint the failure point? yes/no/not answered \\ngive the failure point</li>\\n<li>If it is working, did you buy new product? yes/no/not answered</li>\\n<li>If it is not working did you buy new products? yes/no/not answered?</li>\\n<li>Are you aware of other options? yes/no/not answered</li>\\n</ol>\\n\\n<p>I want to know how can I measure reliability, and what other statistics analysis I can present: GLM, ANOVA , MANOVA, etc.</p>\\n',\n",
       " '<p>While \"The Elements of Statistical Learning\" is a brilliant book, it requires a relatively high level of knowledge to get the most from it. There are many other resources on the web to help you to understand the topics in the book. </p>\\n\\n<p>Lets take a very simple example of linear discriminant analysis where you want to group a set of two dimensional data points into K = 2 groups. The drop in dimensions will be only be K-1 = 2-1 = 1. As @deinst explained, the drop in dimensions can be explained with elementary geometry. </p>\\n\\n<p>Two points in any dimension can be joined by a line, and a line is one dimensional. This is an example of a K-1 = 2-1 = 1 dimensional subspace.</p>\\n\\n<p>Now, in this simple example, the set of data points will be scattered in two-dimensional space. The points will be represented by (x,y), so for example you could have data points such as (1,2), (2,1), (9,10), (13,13). Now, using linear discriminant analysis to create two groups A and B will result in the data points being classified as belonging to group A or to group B such that certain properties are satisfied. Linear discriminant analysis attempts to maximize the variance between the groups compared to the variance within the groups. </p>\\n\\n<p>In other words, groups A and B will be far apart and contain data points that are close together. In this simple example, it is clear that the points will be grouped as follows. Group A = {(1,2), (2,1)} and Group B = {(9,10), (13,13)}.</p>\\n\\n<p>Now, the centroids are calculated as the centroids of the groups of data points so</p>\\n\\n<pre><code>Centroid of group A = ((1+2)/2, (2+1)/2) = (1.5,1.5), (11,11.5)\\n\\nCentroid of group B = ((9+13)/2, (10+13)/2)\\n</code></pre>\\n\\n<p>The Centroids are simply 2 points and they span a 1-dimensional line which joins them together.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/7oWlH.png\" alt=\"Figure 1\"></p>\\n\\n<p>You can think of linear discriminant analysis as a projection of the data points on a line so that the two groups of data points are as \"separated as possible\"</p>\\n\\n<p>If you had three groups (and say three dimensional data points) then you would get three centroids, simply three points, and three points in 3D space define a two dimensional plane. Again the rule K-1 = 3-1 = 2 dimensions.</p>\\n\\n<p>I suggest you search the web for resources that will help explain and expand on the simple introduction I have given; for example <a href=\"http://www.music.mcgill.ca/~ich/classes/mumt611_07/classifiers/lda_theory.pdf\">http://www.music.mcgill.ca/~ich/classes/mumt611_07/classifiers/lda_theory.pdf</a></p>\\n',\n",
       " \"<p>Given that the tasting ability is ordinal, you could consider ordinal logistic regression, if you have learned that yet (since you tagged your question with homework). There are also tests of ordinal differences, such as Jonckheere-Terpstra, that are more similar to the spirit of $\\\\\\\\chi^2$. But those don't mark one variable as dependent and the other as independent. </p>\\n\",\n",
       " '<p>In a slightly more general context with $Y$ an $n$-dimensional vector of $y$-observations (the responses, or dependent variables), $X$ an $n \\\\\\\\times p$ matrix of $x$-observations (covariates, or dependent variables) and $\\\\\\\\theta = (\\\\\\\\beta_1, \\\\\\\\beta_2, \\\\\\\\sigma)$ the parameters such that $Y \\\\\\\\sim N(X\\\\\\\\beta_1, \\\\\\\\Sigma(\\\\\\\\beta_2, \\\\\\\\sigma))$ then the minus-log-likelihood is \\n$$l(\\\\\\\\beta_1, \\\\\\\\beta_2, \\\\\\\\sigma) = \\\\\\\\frac{1}{2}(Y-X\\\\\\\\beta_1)^T \\\\\\\\Sigma(\\\\\\\\beta_2, \\\\\\\\sigma)^{-1} (Y-X\\\\\\\\beta_1) + \\\\\\\\frac{1}{2}\\\\\\\\log |\\\\\\\\Sigma(\\\\\\\\beta_2, \\\\\\\\sigma)|$$\\nIn the OP\\'s question, $\\\\\\\\Sigma(\\\\\\\\beta_2, \\\\\\\\sigma)$ is diagonal with \\n$$\\\\\\\\Sigma(\\\\\\\\beta_2, \\\\\\\\sigma)_{ii} = \\\\\\\\sigma^2 g(z_i^T \\\\\\\\beta_2)^2$$\\nso the determinant becomes $\\\\\\\\sigma^{2n} \\\\\\\\prod_{i=1}^n g(z_i^T \\\\\\\\beta_2)^2$ and the resulting minus-log-likelihood becomes\\n$$\\\\\\\\frac{1}{2\\\\\\\\sigma^2} \\\\\\\\sum_{i=1}^n \\\\\\\\frac{(y_i-x_i^T\\\\\\\\beta_1)^2}{ g(z_i^T \\\\\\\\beta_2)^2} + n \\\\\\\\log \\\\\\\\sigma + \\\\\\\\sum_{i=1}^n  \\\\\\\\log g(z_i^T \\\\\\\\beta_2)$$\\nThere are several ways to approach the minimization of this function (assuming the three parameters are variation independent). </p>\\n\\n<ul>\\n<li>You can try to minimize the function by a standard optimization algorithm remembering the constraint that $\\\\\\\\sigma &gt; 0$. </li>\\n<li>You can compute the profile minus-log-likelihood of $(\\\\\\\\beta_1, \\\\\\\\beta_2)$ by minimizing over $\\\\\\\\sigma$ for fixed $(\\\\\\\\beta_1, \\\\\\\\beta_2)$, and then plug the resulting function into a standard unconstrained optimization algorithm.</li>\\n<li>You can alternate between optimizing over each of the three parameters separately. Optimizing over $\\\\\\\\sigma$ can be done analytically, optimizing over $\\\\\\\\beta_1$ is a weighted least squares regression problem, and optimizing over $\\\\\\\\beta_2$ is equivalent to fitting a gamma generalized linear model with $g^2$ the inverse link. </li>\\n</ul>\\n\\n<p>The last suggestion appeals to me because it builds on solutions that I already know well. In addition, the first iteration is something I would consider doing anyway. That is, first compute an initial estimate of $\\\\\\\\beta_1$ by ordinary least squares ignoring the potential heteroskedasticity, and then fit a gamma glm to the squared residuals to get an initial estimate of $\\\\\\\\beta_2$ $-$ just to check if the more complicated model seems worthwhile. Iterations incorporating the heteroskedasticity into the least squares solution as weights might then improve upon the estimate. </p>\\n\\n<p>Regarding the second part of the question, I would probably consider computing a confidence interval for the linear combination $w_1^T\\\\\\\\beta_1 + w_2^T\\\\\\\\beta_2$ either by using standard MLE asymptotics (checking with simulations that the asymptotics works) or by bootstrapping.</p>\\n\\n<p><strong>Edit:</strong> By <em>standard MLE asymptotics</em> I mean using the multivariate normal approximation to the distribution of the MLE with covariance matrix the inverse Fisher information. The Fisher information is by definition the covariance matrix of the gradient of $l$. It depends in general on the parameters. If you can find an analytic expression for this quantity you can try plugging in the MLE. In the alternative, you can estimate the Fisher information by the <em>observed</em> Fisher information, which is the Hessian of $l$ in the MLE. Your parameter of interest is a linear combination of the parameters in the two $\\\\\\\\beta$-vectors, hence from the approximating multivariate normal of the MLE you can find a normal approximation of the estimators distribution as described <a href=\"http://en.wikipedia.org/wiki/Multivariate_normal_distribution#Affine_transformation\" rel=\"nofollow\">here</a>. This gives you an approximate standard error and you can compute confidence intervals. It\\'s well described in many (mathematical) statistics books, but a reasonably accessible presentation I can recommend is <a href=\"http://rads.stackoverflow.com/amzn/click/0198507658\" rel=\"nofollow\">In All Likelihood</a> by Yudi Pawitan. Anyway, the formal derivation of the asymptotic theory is fairly complicated and rely on a number of regularity conditions, and it only gives valid <em>asymptotic</em> distributions. Hence, if in doubt I would always do some simulations with a new model to check if I can trust the results for realistic parameters and sample sizes. Simple, non-parametric bootstrapping where you sample the triples $(y_i,x_i,z_i)$ from the observed data set with replacement can be a useful alternative if the fitting procedure is not too time consuming. </p>\\n',\n",
       " '<p>If your question is whether it is possible to separate without errors a linearly separable set of points by using polynomial kernel $k(x, z) = (\\\\\\\\langle x, z \\\\\\\\rangle + 1)^d$, $d &gt; 1$, then the answer is <strong>yes</strong>, it is possible to do that. One of the feature spaces $H$ for the polynomial kernel $k(x, z) = (\\\\\\\\langle x, z \\\\\\\\rangle + 1)^d$ defined for $x, z \\\\\\\\in R^n$ contains all monomials of variables\\n$x_1, x_2, ..., x_n$ of degree not higher than $d$. Therefore it contains a subspace of variables $x_1, ..., x_n$. If your dataset is linearly separable in the space of $x_1, ..., x_n$ then it is linearly separable in $H$, which means that there exist an SVM with kernel $k(x, z)$ that separates your dataset in $R^n$ (without errors). It does not mean, however, that your SVM training algorithm of choice will find that hyperplane for a given $d$.</p>\\n',\n",
       " '<p>I conducted listening experiment in which 16 participants had to rate the audio stimuli along 5 scales representing an emotion (sad, tender, neutral, happy and aggressive). Each audio stimulus was synthesized in order to represent a particular emotion. Participants had to move 5 sliders each of which corresponded to one of the 5 emotions.</p>\\n\\n<p>The sliders range was [0,10] but participants were only informed about the extremities of the sliders (not at all - very much).\\nThere was not a force choice, therefore potentially each audio stimulus could be rated with all the scales (e.g. sad = 0.1, tender = 2.5, neutral = 2., happy = 8.3, aggressive = 1.7).</p>\\n\\n<p>There were 40 stimuli, each stimulus was repeated twice, for a total of 80 trials.</p>\\n\\n<p>To analyze the data I want to use a MANOVA with repeated measures (in R).\\nFor this purpose I use the audio stimulus as independent variable having 40 levels, while the 5 responses as dependent variables. Since each individual has been measured twice, I include a within-subjects factor for trial number. </p>\\n\\n<p>I want to demonstrate that the created stimuli were actually correctly classified in the corresponding emotion. For example I expect that happy sounds result in happy ratings by participants and that these happy ratings are greater than the other 4 responses.</p>\\n\\n<p>I studied the R documentation and examples, but I encountered the following error while adapting those examples to my case:</p>\\n\\n<pre><code>model.emotions&lt;-lm(cbind(Sad,Tender,Neutral,Happy,Aggressive) ~ Trial,data=scrd)\\nidata&lt;-data.frame(scrd$Trial_number)\\naov.emotions&lt;-anova(model.emotions,idata=idata, idesign=~Trial, type=\"III\")\\n\\n&gt; aov.emotions&lt;-anova(model.emotions,idata=idata, idesign=~Trial, type=\"III\")\\nError in cbind(M, X) : number of rows of matrices must match (see arg 2)\\n</code></pre>\\n\\n<p>I am not fully sure of the above formula, since I am not an expert in R. Can you please correct them showing the correct R code?</p>\\n\\n<p>The .csv table with all the data can be downloaded here: <a href=\"https://dl.dropbox.com/u/3288659/Results_data_listening_Test.csv\" rel=\"nofollow\">https://dl.dropbox.com/u/3288659/Results_data_listening_Test.csv</a></p>\\n\\n<p>In addition I was not able to find any post hoc test to apply to the result of the MANOVA in case of a significant main effect. Any suggestion?</p>\\n\\n<p>Thanks in advance</p>\\n',\n",
       " \"<p>is anybody familiar with the <code>tobit()</code> command using the package AER? I'm searching for a command to compute the marginal effects for y (not for the latent variable y*). It seems to be $\\\\\\\\phi(x\\\\\\\\beta/\\\\\\\\sigma)\\\\\\\\beta$, where $\\\\\\\\phi$ is the std.normal cumulative distribution function. But how can I compute those effects with R?</p>\\n\",\n",
       " \"<p>I have an experiment with any possible (reasonable) number of parameters (independent variables). I run the experiment several times for each possible combination of my variables. The data I get will be generally numeric. </p>\\n\\n<p>However I know nothing (and any assumptions are difficult) about the distribution of my data.</p>\\n\\n<p>What I am interested in is a measure of how well do my parameters predict the data I get. Which statistic should I use? How do I calculate it (by hand, a link to a tutorial would be very sweet)?</p>\\n\\n<h3>Edit</h3>\\n\\n<p>I am trying to solve this as generally as possible (hence the slightly non-specific description) for a piece of software I'm working on. To make it a bit more clear a bit of an example:</p>\\n\\n<p>I have these parameters:</p>\\n\\n<pre><code>decay: 0.1 | 0.2 | 0.3\\nparticles: 10 | 100\\nvelocity: 30 | 70\\n</code></pre>\\n\\n<p>This gives 12 combinations (3 * 2 * 2) and I'll measure my dependent variable (say temperature) five times for each combinations. </p>\\n\\n<p>Thus my final dataset will have 60 measurements of temperature.</p>\\n\\n<p>Now suppose that temperature was in fact given by:</p>\\n\\n<p>$t = K(0.3v + 0.6d + \\\\\\\\varepsilon)$\\r</p>\\n\\n<p>where $t$ is temperature, $K$ is some constant, $v$ is velocity, $d$ is decay and $\\\\\\\\varepsilon$ is some sort of random effect. Particles is completely unrelated to the measured temperature.</p>\\n\\n<p>Now I'd like to perform a test that would tell me that velocity has a ~0.3 effect, decay ~0.6 and particles ~0 effect.</p>\\n\\n<p>However I may have more or less variables and more or less measurements.</p>\\n\",\n",
       " '<p>Could someone please explain what \"$J$\" consists of in this <a href=\"http://www.google.de/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDgQFjAA&amp;url=http%3A%2F%2Fwww.utgjiu.ro%2Fmath%2Fsma%2Fv04%2Fp08.pdf&amp;ei=RhX0UNLcCMim4ASy-IDgBQ&amp;usg=AFQjCNE8atO375vGfPifjzcpbDdyBwa4_Q&amp;bvm=bv.1357700187,d.Yms\" rel=\"nofollow\">paper</a>, equation 1.5.\\n$$\\nJ \\\\\\\\sim N(\\\\\\\\beta, \\\\\\\\sigma^2 I/k)\\n$$\\nWhat\\'s $\\\\\\\\beta$ here? What\\'s $N$?</p>\\n\\n<p>Also, why are they putting that much effort in deriving $k$ with complicated formulas instead of just saying \"use CV to find the best $k$\"?</p>\\n',\n",
       " '<p>The question suggests a comparison of three related models.  To make the comparison clear, let $Y$ be the dependent variable, let $X \\\\\\\\in \\\\\\\\{1,2,3\\\\\\\\}$ be the current community code, and define $X_1$ and $X_2$ to be indicators of communities 1 and 2, respectively.  (This means that $X_1=1$ for community 1 and $X_1=0$ for communities 2 and 3; $X_2=1$ for community 2 and $X_2=0$ for communities 1 and 3.)</p>\\n\\n<p>The current analysis may be one of the following: either</p>\\n\\n<p>$$Y = \\\\\\\\alpha + \\\\\\\\beta X + \\\\\\\\varepsilon\\\\\\\\quad\\\\\\\\text{(first model)}$$</p>\\n\\n<p>or</p>\\n\\n<p>$$Y = \\\\\\\\alpha + \\\\\\\\beta_1 X_1 + \\\\\\\\beta_2 X_2 + \\\\\\\\varepsilon\\\\\\\\quad\\\\\\\\text{(second model)}.$$</p>\\n\\n<p>In both cases $\\\\\\\\varepsilon$ represents a set of identically distributed independent random variables with zero expectation.  The second model likely is the one intended, but the first model is the one that will be fit with the coding that is described in the question.</p>\\n\\n<p>The output of the OLS regression is a set of fitted parameters (indicated with \"hats\" on their symbols) <em>together with</em> an estimate of the common variance of the errors.  In the first model there is one t-test to compare $\\\\\\\\hat{\\\\\\\\beta}$ to $0$.  In the second model there are <em>two</em> t-tests: one to compare $\\\\\\\\hat{\\\\\\\\beta_1}$ to $0$ and another to compare $\\\\\\\\hat{\\\\\\\\beta_2}$ to $0$.  Because the question reports only one t-test, let\\'s start by examining the first model.</p>\\n\\n<p>Having concluded that $\\\\\\\\hat{\\\\\\\\beta}$ is significantly different from $0$, we can make an estimate of $Y$ = $\\\\\\\\mathbb{E}[\\\\\\\\alpha + \\\\\\\\beta X + \\\\\\\\varepsilon]$ = $\\\\\\\\alpha + \\\\\\\\beta X$ for any community: </p>\\n\\n<p>for community 1, $X=1$ and the estimate equals $\\\\\\\\alpha+\\\\\\\\beta$; </p>\\n\\n<p>for community 2, $X=2$ and the estimate equals $\\\\\\\\alpha+2\\\\\\\\beta$; and </p>\\n\\n<p>for community 3, $X=3$ and the estimate equals $\\\\\\\\alpha+3\\\\\\\\beta$.  </p>\\n\\n<p>In particular, <strong>the first model forces the community effects to be in arithmetic progression.</strong> If the community coding is intended as just an arbitrary way of differentiating among communities, <em>this built-in restriction is equally arbitrary and likely wrong.</em></p>\\n\\n<p>It is instructive to perform the same detailed analysis of the second model\\'s predictions:</p>\\n\\n<p>For community 1, where $X_1=1$ and $X_2=0$, the predicted value of $Y$ equals $\\\\\\\\alpha + \\\\\\\\beta_1$.  Specifically,</p>\\n\\n<p>$$Y(\\\\\\\\text{community 1}) = \\\\\\\\alpha + \\\\\\\\beta_1 + \\\\\\\\varepsilon.$$</p>\\n\\n<p>For community 2, where $X_1=0$ and $X_2=1$, the predicted value of $Y$ equals $\\\\\\\\alpha+\\\\\\\\beta_2$.  Specifically,</p>\\n\\n<p>$$Y(\\\\\\\\text{community 2}) = \\\\\\\\alpha + \\\\\\\\beta_2 + \\\\\\\\varepsilon.$$</p>\\n\\n<p>For community 3, where $X_1=X_2=0$, the predicted value of $Y$ equals $\\\\\\\\alpha$. Specifically,</p>\\n\\n<p>$$Y(\\\\\\\\text{community 3}) = \\\\\\\\alpha + \\\\\\\\varepsilon.$$</p>\\n\\n<p><strong>The three parameters effectively give the second model full freedom to estimate the three expected values of $Y$ separately.</strong>  The t-tests assess whether (1) $\\\\\\\\beta_1=0$; that is, whether there is a difference between communities 1 and 3; and (2) $\\\\\\\\beta_2=0$; that is, whether there is a difference between communities 2 and 3.  In addition, one can test the \"contrast\" $\\\\\\\\beta_2-\\\\\\\\beta_1$ with a t-test to see whether communities 2 and 1 differ: this works because their difference is $(\\\\\\\\alpha + \\\\\\\\beta_2) - (\\\\\\\\alpha + \\\\\\\\beta_1)$ = $\\\\\\\\beta_2-\\\\\\\\beta_1$.</p>\\n\\n<p>Now we can assess the effect of three separate regressions.  They would be</p>\\n\\n<p>$$Y(\\\\\\\\text{community 1}) = \\\\\\\\alpha_1 + \\\\\\\\varepsilon_1,$$</p>\\n\\n<p>$$Y(\\\\\\\\text{community 2}) = \\\\\\\\alpha_2 + \\\\\\\\varepsilon_2,$$</p>\\n\\n<p>$$Y(\\\\\\\\text{community 3}) = \\\\\\\\alpha_3 + \\\\\\\\varepsilon_3.$$</p>\\n\\n<p>Comparing this to the second model, we see that $\\\\\\\\alpha_1$ should agree with $\\\\\\\\alpha+\\\\\\\\beta_1$, $\\\\\\\\alpha_2$ should agree with $\\\\\\\\alpha+\\\\\\\\beta_2$, and $\\\\\\\\alpha_3$ should agree with $\\\\\\\\alpha$.  So, in terms of flexibility of fitting parameters, both models are equally good. However, the assumptions in this model about the error terms are weaker.  All the $\\\\\\\\varepsilon_1$ must be independent and identically distributed (iid); all the $\\\\\\\\varepsilon_2$ must be iid, and all the $\\\\\\\\varepsilon_3$ must be iid, <em>but nothing is assumed about statistical relations among the separate regressions.</em>  <strong>Separate regressions therefore allow for additional flexibility:</strong></p>\\n\\n<ul>\\n<li><p>Most importantly, the distribution of the $\\\\\\\\varepsilon_1$ can differ from that of the $\\\\\\\\varepsilon_2$ which can differ from that of the $\\\\\\\\varepsilon_3$.</p></li>\\n<li><p>In some situations, the $\\\\\\\\varepsilon_i$ may be correlated with the $\\\\\\\\varepsilon_j$.  None of these models explicitly handles this, but the third model (separate regressions) at least won\\'t be adversely affected by it.</p></li>\\n</ul>\\n\\n<p>This additional flexibility means that the t-test results for the parameters will likely differ between the second and third model.  (It should not result in different parameter estimates, though.)</p>\\n\\n<p><strong>To see whether separate regressions are needed</strong>, do the following:</p>\\n\\n<p>Fit the second model.  Plot the residuals against community, for example as a set of side-by-side boxplots or a trio of histograms or even as three probability plots.  Look for evidence of different distributional shapes and especially of appreciably different variances.  If that evidence is absent, the second model should be ok.  If it\\'s present, separate regressions are warranted.</p>\\n\\n<p>When the models are multivariate--that is, they include other factors--a similar analysis is possible, with similar (but more complicated) conclusions.  In general, <strong>performing separate regressions is tantamount to including all possible two-way interactions with the community variable (coded as in the second model, not the first) and allowing for different error distributions for each community.</strong></p>\\n',\n",
       " \"<p>If the best linear approximation (using least squares) of my data points is the line $y=mx+b$, how can I calculate the approximation error? If I compute standard deviation of differences between observations and predictions $e_i=real(x_i)-(mx_i+b)$, can I later say that a real (but not observed) value $y_r=real(x_0)$ belongs to the interval $[y_p-\\\\\\\\sigma, y_p+\\\\\\\\sigma]$ ($y_p=mx_0+b$) with probability ~68%, assuming normal distribution?</p>\\n\\n<p>To clarify:</p>\\n\\n<p>I made observations regarding a function $f(x)$ by evaluating it a some points $x_i$. I fit these observations to a line $l(x)=mx+b$. For $x_0$ that I did not observe, I'd like to know how big can $f(x_0)-l(x_0)$  be. Using the method above, is it correct to say that $f(x_0) \\\\\\\\in [l(x_0)-\\\\\\\\sigma, l(x_0)+\\\\\\\\sigma]$ with prob. ~68%?</p>\\n\",\n",
       " '<p>My question is, how to get effect sizes for a linear mixed model?</p>\\n\\n<p>I am using the following model in SPSS:</p>\\n\\n<pre><code>MIXED\\n  transfer  BY distance training rotation sequence  WITH pretest\\n  /CRITERIA = CIN(95) MXITER(100) MXSTEP(5) SCORING(1) SINGULAR(0.000000000001) \\n     HCONVERGE(0, ABSOLUTE) LCONVERGE(0, ABSOLUTE) PCONVERGE(0.000001, ABSOLUTE)\\n  /FIXED = pretest rotation sequence distance training distance*training  | SSTYPE(3)\\n  /METHOD = REML\\n  /REPEATED = distance | SUBJECT(ResponseID) COVTYPE(CS) .\\n</code></pre>\\n\\n<p>I\\'ve also done it in R, where it looks like this:</p>\\n\\n<pre><code>library( nlme )\\noptions(contrasts=c(\"contr.sum\",\"contr.poly\"))\\nlm1 &lt;- lme(transfer ~ training * distance + rotation + sequence + pretest, \\n           random=~1|ResponseID, method=\"REML\", data=wide.data )\\nfit &lt;- anova.lme(lm1,type=\\'marginal\\')\\nprint( fit )\\n</code></pre>\\n\\n<p>The dv, transfer, measures pretest to posttest improvement for a given level of distance. Distance is a categorical within-subjects variable with either 2 or 3 levels depending on the dataset. Training is a categorical between-subjects variable with either 2 or 4 levels depending on the dataset. Rotation and sequence are binary categorical between-subjects variables. Pretest is a covariate with different values for each level of distance.</p>\\n\\n<p>I\\'ve already submitted the results of the analysis for publication, and got a reviewer comment asking for odds ratios. Ideally I\\'d like to get odds ratios for each effect in the model. If I can\\'t get odds ratios, some other measure of effect sizes might be OK. Most important, I need to get it for the training * distance interaction and the covariate (pretest) because these are the only significant effects.</p>\\n\\n<p>Ideally I\\'d get it straight from SPSS but as far as I can see, SPSS can\\'t do it. I couldn\\'t figure out how to do it in R either. Second best would be to put the output of one of these into some other software that could calculate it for me. I found some free software that could compute eta-squared for regular mixed ANOVA but not for linear mixed model.</p>\\n\\n<p>Other questions about effect sizes seem to relate to other models, not LMMs. I did see another similar question about LMMs which is currently unanswered.</p>\\n\\n<p>Any ideas?</p>\\n',\n",
       " '<p>I was thinking about this today so I decided to ask it here.  I know the rule of adding probabilities.  As I was taught in grade school \"OR\" typically means add and \"AND\" typically means multiply.</p>\\n\\n<p>Suppose there is a jar with a red, blue, green, and yellow ball.  The probability of picking a red ball is 25%.  The probability of picking a red ball, putting it back, AND then picking a green ball is 12.5% (25% * 25%).  The probability of picking either a red OR a green ball on the first try is 50% (25% + 25%).</p>\\n\\n<p>These are independent events.  Now lets take another set of independent events.</p>\\n\\n<ol>\\n<li><p>The weather man said there is an 80% chance of rain.  This means it should rain 80% of the time I assume.  If you looked back at the past 100 days with an 80% rain chance it should rain on ~80 of them.</p></li>\\n<li><p>A girl hooked up with 5 of our 10 mutual friends.  I have a 50% chance of hooking up with her too.</p></li>\\n<li><p>Rolling an even number on a dice carries a 50% probability.</p></li>\\n</ol>\\n\\n<p>If it rains OR I hook up with the girl OR I roll an even number on the dice then I win.  If any one of these things happen I win, just like if I pick a red or a green ball, it doesn\\'t matter if its red or green.  With that being said the probability that it rains or I hook up or I roll an even number is 180%.  This is not true because it is remotely possible that none of those things happen.</p>\\n',\n",
       " '<p>One strategy would be to see your design as containing 5 groups, which we could label:</p>\\n\\n<pre><code>C, M2050, M2100, E2050, E2100\\n</code></pre>\\n\\n<p>You could then set up various planned contrasts that examine questions of interest. </p>\\n\\n<p>Here are some example contrast weights for testing various research questions:</p>\\n\\n<pre><code>C    M2050   M2100  E2050  E2100  Comparison\\n+4   -1      -1     -1     -1     Control versus other\\n0    +1      +1     -1     -1     Moderate versus extreme\\n0    +1      -1     +1     -1     2050 versus 2100\\n</code></pre>\\n\\n<p>You could achieve something similar, by first testing whether control is different to the average of the other four groups and then performing the $2\\\\\\\\times2$ ANOVA omitting the control group. The default tests in the $2\\\\\\\\times2$ are likely to correspond to many of the planned comparisons you would do anyway (i.e., M versus E, 2050 versus 2010, and the interaction between <code>amount</code> and <code>duration</code>). \\nHowever, the contrast approach might be slightly more powerful because your error variance may be smaller (its based on deviations from the four non-control group means rather than one overall non-control mean). </p>\\n\\n<p>In previous questions, you\\'ve asked about SPSS, so here\\'s an example of testing a contrast using <a href=\"http://www.ats.ucla.edu/stat/spss/faq/contrast.htm\" rel=\"nofollow\"><code>GLM</code> in SPSS</a>. And here\\'s a <a href=\"http://faculty.smu.edu/kyler/courses/7311/planned_4up.pdf\" rel=\"nofollow\">lecture on how to do it using R</a>.</p>\\n',\n",
       " '<p>You should probably take a look at this:</p>\\n\\n<p><a href=\"http://stackoverflow.com/questions/1181025/goodness-of-fit-functions-in-r\">http://stackoverflow.com/questions/1181025/goodness-of-fit-functions-in-r</a></p>\\n',\n",
       " '<p>Let me state up front that I don\\'t have answers for all of your questions.  I\\'m not as strong on competing risks as simpler applications of survival analysis.  So, I will just throw out a couple of pieces of information here that may be helpful.  I suspect KM curves are more common because they are older and conceptually easier to understand (for both the researcher and the consumer of research).  If the competing risks are truly independent, then I believe the KM estimates should be unbiased.  That is, a plausible reason why people may prefer KM curves is that many people already understand them, and if those patients who had died due to other causes would have followed the same path as everyone else if they hadn\\'t, the KM curves usefully illustrate what was learned from the study.  </p>\\n\\n<p>Regarding the question of whether there is over-estimation in the literature, one relevant fact, distinct from these issues, is that for practical purposes \\'significance\\' is often required for publication.  This guarantees that the literature is biased (specifically over-estimated), an issue known as <a href=\"http://en.wikipedia.org/wiki/Publication_bias\" rel=\"nofollow\">the file drawer problem</a>.</p>\\n',\n",
       " '<p>I have a mixed ANOVA with one between and one within factor:</p>\\n\\n<p>between factor: control versus treament </p>\\n\\n<p>within factor: ad1, ad2 (measures click rates on 2 ads)</p>\\n\\n<pre><code>aov(repdat~(within*between)+Error(subjcts/(withincontrasts))+(between),data.rep)\\n</code></pre>\\n\\n<p>However my outcome repdat has a Poisson distribution and 0s (since sometimes users just dont click). How do I model this in R?</p>\\n\\n<p>I considered:</p>\\n\\n<p>1.) using Zelig package with model Poisson, but I dont know how to properly write the mixed model formula here. Is it the same as for the mixed ANOVA?</p>\\n\\n<p>2.) using lme4 package with model Poisson. Can I use both a between and within factor or even treat \"ads\" as a random variable?</p>\\n\\n<p>It would be great if someone could quickly show me example code for both 1. and 2.</p>\\n\\n<p>thanks</p>\\n',\n",
       " '<p>The answer to your question is in my experience \"no\", SVMs are not definitely superior, and which works best depends on the nature of the dataset at hand and on the relative skill of the operator with each set of tools.  In general SVMs are good because the training algorithm is efficient, and it has a regularisation parameter, which forces you to think about regularisation and over-fitting.  However, there are datasets where MLPs give much better performance than SVMs (as they are allowed to decide their own internal representation, rather than having it pre-specified by the kernel function).  A good implementation of MLPs (e.g. NETLAB) and regularisation or early stopping or architecture selection (or better still all three) can often give very good results and be reproducible (at least in terms of performance).</p>\\n\\n<p>Model selection is the major problem with SVMs, choosing the kernel and optimising the kernel and regularisation parameters can often lead to severe over-fitting if you over-optimise the model selection criterion.  While the theory under-pinning the SVM is a comfort, most of it only applies for a fixed kernel, so as soon as you try to optimise the kernel parameters it no longer applies (for instance the optimisation problem to be solved in tuning the kernel is generally non-convex and may well have local minima).</p>\\n',\n",
       " '<p>You can use the <a href=\"http://www.cs.princeton.edu/~blei/lda-c/\" rel=\"nofollow\">original C++ code</a> of the LDA inventors Blei et al. </p>\\n\\n<p>Also quite fast is <a href=\"http://gibbslda.sourceforge.net/\" rel=\"nofollow\">GibbsLDA</a> (written in C and C++).</p>\\n\\n<p>If you want to parallelize it you might want to check out <a href=\"http://code.google.com/p/plda/\" rel=\"nofollow\">plda</a> (C++). </p>\\n',\n",
       " \"<p>Generally, you should focus on the task, not on a method.</p>\\n\\n<p>There are two approaches to create a recommendation engine:</p>\\n\\n<ol>\\n<li>user-based</li>\\n<li>item-based</li>\\n</ol>\\n\\n<p>In the former, you look for similar users (for example you group them in clusters based on demographic/sociographic attributes) and then recommend 'an item' to the specific user.\\n[search terms: clustering]</p>\\n\\n<p>In the latter, some patterns in combination on bought items have to be found (e.g. 90% of users who bought house bought also furnitures). Basing on that information you recommend new item (e.g. if user bought house then you should offer him/her some furnitures).\\n[search terms: association rules]</p>\\n\",\n",
       " '<p>I suspect 30--60 is about the best you\\'ll get. The standard approach is the <em>leaps-and-bounds</em> algorithm which doesn\\'t require fitting every possible model. In $R$, the <strong><a href=\"http://cran.r-project.org/web/packages/leaps/index.html\">leaps</a></strong> package is one implementation.</p>\\n\\n<p>The documentation for the <code>regsubsets</code> function in the <strong>leaps</strong> package states that it will handle up to 50 variables without complaining. It can be \"forced\" to do more than 50 by setting the appropriate boolean flag.</p>\\n\\n<p>You might do a bit better with some parallelization technique, but the number of total models you can consider will (almost undoubtedly) only scale linearly with the number of CPU cores available to you. So, if 50 variables is the upper limit for a single core, and you have 1000 cores at your disposal, you could bump that to about 60 variables.</p>\\n',\n",
       " '<p>Final edit with all resources updated:</p>\\n\\n<p>For a project, I am applying machine learning algorithms for classification.</p>\\n\\n<p><strong>Challenge:</strong>\\nQuite limited labeled data and much more unlabeled data.</p>\\n\\n<p><strong>Goals:</strong></p>\\n\\n<ol>\\n<li>Apply semi-supervised classification</li>\\n<li>Apply a somehow semi-supervised labeling process (known as active learning)</li>\\n</ol>\\n\\n<p>I\\'ve found a lot of information from research papers, like applying EM, Transductive SVM or S3VM (Semi Supervised SVM), or somehow using LDA, etc. Even there are few books on this topic.</p>\\n\\n<p><strong>Question:</strong>\\nWhere are the implementations and practical sources?</p>\\n\\n<hr>\\n\\n<p><strong>Final update (based on helps provided by mpiktas, bayer, and Dikran Marsupial)</strong></p>\\n\\n<p><strong>Semi-supervised learning:</strong></p>\\n\\n<ul>\\n<li>TSVM: in <a href=\"http://svmlight.joachims.org/\">SVMligth</a> and <a href=\"http://vikas.sindhwani.org/svmlin.html\">SVMlin</a>. </li>\\n<li><a href=\"http://www.mblondel.org/journal/2010/06/21/semi-supervised-naive-bayes-in-python/\">EM Naive Bayes in Python</a></li>\\n<li><a href=\"http://alias-i.com/lingpipe/demos/tutorial/em/read-me.html\">EM in LinePipe project</a></li>\\n</ul>\\n\\n<p><strong>Active learning:</strong></p>\\n\\n<ul>\\n<li><a href=\"http://code.google.com/p/dualist/\"><strong>Dualist</strong></a>: an implementation of active learning with source code on text classification</li>\\n<li>This <a href=\"http://active-learning.net/\">webpage</a> serves a wonderful overview of active learning.</li>\\n<li>An experimental Design workshop: <a href=\"http://jmlr.csail.mit.edu/proceedings/papers/v16/\">here</a>.</li>\\n</ul>\\n\\n<p><strong>Deep learning:</strong></p>\\n\\n<ul>\\n<li>Introductory video at <a href=\"http://www.youtube.com/watch?v=ZmNOAtZIgIk\">here</a>.</li>\\n<li><a href=\"http://deeplearning.net/\">General site</a>.</li>\\n<li>Stanford Unsupervised Feature Learning and Deep Learning <a href=\"http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial\">tutorial</a>.</li>\\n</ul>\\n',\n",
       " '<p>I have a data set that has 600 observations divided in two groups.  I am going to compare the central tendencies (e.g., the means) of these two groups.  However, there are violations of classical assumptions present, such as normality and equality of variances.  </p>\\n\\n<ul>\\n<li>Can I use a parametric approach (specifically, the t-test), since the sample sizes are large (based on the Central Limit theorem), or do I have to use a non-parametric approach?  </li>\\n<li>If I should use a non-parametric approach, which test (Mann-Whitney, Median or Kolmogorov-Smirnov) is most appropriate?</li>\\n</ul>\\n',\n",
       " '<p>I designed a field experiment with 4 independent factors but data is not normal and heteroscedastic.  Friedman test (agricolae package) from R only fits for rbd.  Can anybody suggest how to analyze my data please?</p>\\n\\n<p>Many thanks.</p>\\n',\n",
       " '<p>I suspect the answer to this question is something along the lines of \"there is no free lunch.\" Perhaps the reason statisticians, computer scientists, and electrical engineers have developed different algorithms is that they\\'re interest in solving different sorts of problems. </p>\\n',\n",
       " '<p>I understand the rules for combining experimental errors in sums, differences and ratios (<a href=\"http://www.rod.beavon.clara.net/err_comb.htm\" rel=\"nofollow\">as explained here</a>), but what happens to an experimental error when you average it?</p>\\n\\n<p>Say, ruler measurements of the length of beetles that are all values like 12.3 cm $\\\\\\\\pm$ 2mm.<br>\\nI can find plenty of explanation on the web about how to add and multiply values like that together, but what happens to the error when you take an average of (say) 10 measurements?</p>\\n',\n",
       " \"<p>I typically like to work with normalized data.  Imagine I have data on height of Americans (in cm) and I have predictor variables Weight (in kg), and Age (in years).  If I don't standardize the data, I might get a regression such as age Height (in cm) = 2 * Weight(in kg) + 3*Age(in years).  The physical units don't make sense to me.  How do I take a combination of kg and years to create cm?  A physicist would complain.  </p>\\n\\n<p>By standardizing the data, which typically refers to subtracting the mean and dividing by the standard deviation, we are able to remove the confusing units.  If weight is in kg, its standard deviation will also be in kg.  Dividing some number in kg with another number in kg yields a number with no units.  Doing this is essentially the same as working with each variable's Z score.  Now, in the end I might see </p>\\n\\n<p>Unitless measurement for Height = Unitless measurement for Weight + 0.5 * Unitless measurement for Age</p>\\n\\n<p>Later on if you decide to use other techniques for modeling like PCA, it will become very important to work with normalized data.  In the PCA case, it is important to have mean 0 data, but having standard deviation 1 is not so important.</p>\\n\",\n",
       " '<p>I dont understand how the lecturer solved this problem. The question is: </p>\\n\\n<blockquote>\\n  <p>You are working with a dataset that contains descriptions of toxic and\\n  non-toxic substances. The dataset, which consists of 1000 samples\\n  from each of the two classes, is described in terms of a class label\\n  and a number of attributes. The dataset is sorted so that the 1000\\n  toxic samples come first, followed by the 1000 non-toxic samples.\\n  Someone tells you that they have confirmed that, for this data set,\\n  the conditional probability that is gained from knowledge about\\n  attribute X is not different from the prior class probability.\\n  Assuming that they are correct, which of the following statements\\n  could be correct and which could not be correct?</p>\\n  \\n  <ol>\\n  <li>All samples have the same value for attribute X.</li>\\n  <li>All toxic samples have the same value for attribute X while each of the non-toxic samples has its own random value for attribute X.</li>\\n  </ol>\\n</blockquote>\\n\\n<p>My question is how can either of these answers to this question be correct when the information given is very limited?</p>\\n\\n<p>The book we use for data mining is Witten and Frank, <a href=\"http://www.cs.waikato.ac.nz/ml/weka/book.html\" rel=\"nofollow\"><em>Data Mining: Practical Machine Learning Tools and Techniques</em></a>, Morgan Kaufmann, 2011 (3rd ed.).</p>\\n',\n",
       " '<p>I am trying to convert results I am finding in scientific articles into relative risks so that I can compare studies in a meta analysis (if there is enough homogeneity in the studies). </p>\\n\\n<p>I came across a study entitled \"Intervention to improve practices of prescribing appropriate medication before an operation; pre-and-post-intervention study\". </p>\\n\\n<p>In the pre-intervention group, 16 out of 233 patients (so 217 with correct prescription) had an incorrect prescription and post intervention 3 out of 137 patients (134 with correct) had an incorrect prescription. </p>\\n\\n<p>In the paper, a p-value of 0.046 was given, using Fisher\\'s exact test. I checked it using a hand calculation and got 0.049. When I put this result into an RR calculator, I get a RR of 0.32 [0.09, 1.07]. Now I am a relative stats rookie but I thought the 95% CI should not cross 1 if my p-value for comparing two proportions is below 0.05.</p>\\n',\n",
       " '<p>The only similarity in the two models is the general type of models they belong to, otherwise they are not similar in general as pointed out by M. Tibbits.</p>\\n\\n<p>Both these models belong the class of hierarchical models with varying slope (cf <a href=\"http://rads.stackoverflow.com/amzn/click/052168689X\" rel=\"nofollow\">Gelman and Hill 2006</a> for detailed treatment)</p>\\n\\n<p>The answer for \"why not\" are many and one of them is pointed out by M. Tibbits. A few more are:</p>\\n\\n<ul>\\n<li><p>In the model 2 the slope on an average is 6/3, where as it is 0 for model 1.(This description can be made much better if we had data but it is \"almost\" accurate given the limited description)</p></li>\\n<li><p>You would expect the data in model 1 to be distributed randomly along a approximately horizontal line where as in model 2 you would expect data to be randomly distributed along a line of slope approx. beta_0 and <em>zero</em> intercept.</p></li>\\n</ul>\\n\\n<p>These answers can be best varified if you similated the data based on your hierarchical setting and look at the results of the analysis.</p>\\n\\n<p>Thanks,</p>\\n\\n<p>S.  </p>\\n',\n",
       " '<p>Your dataset clearly is not normal.  (With this much data, any goodness of fit test will tell you that.)  But you can read much more than that from the normal probability plot:</p>\\n\\n<ul>\\n<li><p>The generally smooth curvature does not hint at a mixture structure.</p></li>\\n<li><p>The upper tail is too stretched out (values too high compared to the reference distribution).</p></li>\\n<li><p>The lower tail is too compressed (values also too high).</p></li>\\n</ul>\\n\\n<p>This suggests that a mild <a href=\"http://en.wikipedia.org/wiki/Power_transform\" rel=\"nofollow\">Box-Cox transformation</a> will produce nearly-normal, or at least symmetric, data.   To find it, consider some key values on this plot: the median, found above the x-value of 0, is about 0.90; +2 standard deviations is about 0.99; and -2 standard deviations is about 0.825.  The nonlinearity is apparent from the simple calculations 0.99 - 0.90 = 0.09 whereas 0.90 - 0.825 = 0.075: the rise from the median to the upper tail is greater than the rise from the lower tail to the median.  We can equalize the slopes by trying out some simple re-expressions <em>of these three values only.</em>  For example, taking the reciprocals of the three key data values (Box-Cox power of -1) gives</p>\\n\\n<pre><code>1/0.825 = 1.21\\n1/0.90  = 1.11; 1.21 - 1.11 = 0.10 (new slope is 0.050 per SD)\\n1/0.99  = 1.01; 1.11 - 1.01 = 0.10 (0.050 per SD)\\n</code></pre>\\n\\n<p>Because the slopes <em>of the re-expressed values</em> are now equal, we know the plot of <em>reciprocals</em> of the data will be approximately linear between -2 and +2 SDs.  As a check, let\\'s pick more points further out into the tails and see what the reciprocal does to them.  I estimate that the value in the plot at -3 SD from the mean is around 0.79 and the value +3 SD from the mean is 1.05.  The two slopes in question equal 0.053 and 0.052 per SD: close enough to each other <em>and</em> to the slopes found between -2 and +2 SD.</p>\\n\\n<p>My estimates--based on the plot as shown on a monitor--are crude, so you will want to repeat these (simple, quick) calculations with the actual data.  Nevertheless, there is considerable evidence that your data when suitably re-expressed with a simple transformation will be close to normally distributed.</p>\\n',\n",
       " '<p>If you want to have at least a definite number of predictors with some range of values defined by the literature, why choose the pure-LASSO approach to begin with? As @probabilityislogic suggested, you should be using some informative priors on those variables where you have some knowledge about. If you want to retain some of the LASSO properties for the rest of the predictors, maybe you could use a prior with a double exponential distribution for each other input, i.e., use a density of the form\\n$$p(\\\\\\\\beta_i)=\\\\\\\\frac{\\\\\\\\lambda}{2}\\\\\\\\text{exp}\\\\\\\\left(-\\\\\\\\lambda|\\\\\\\\beta_i|\\\\\\\\right),$$\\nwhere $\\\\\\\\lambda$ is the lagrange multiplier corresponding to the pure-LASSO solution. This last statement comes from the fact that, in the absense of the variables with the informative priors, this is another way of deriving the LASSO (by maximizing the posterior mode given normality assumptions for the residuals).</p>\\n',\n",
       " '<p>This problem calls for Friedman test. It is multivariate extension of Wilcoxon test (or: nonparametric repeated-mesaures ANOVA).</p>\\n\\n<p>You can still use Wilcoxon test in a multiple comparison manner pairwise, analogically to post-hoc tests in ANOVA: it will show you whether one symptom is more severe than the other.</p>\\n\\n<p>Be sure to penalize it for the multiple-comparison effects; since your data are correlated, the Bonferroni, Holms and other corrections not taking this fact into consideration will give you too conservative test.</p>\\n\\n<p>You can (always) make multiple comparison using bootstraps (see: <a href=\"http://books.google.pl/books/about/Resampling_Based_Multiple_Testing.html?id=nuQXORVGI1QC&amp;redir_esc=y\" rel=\"nofollow\">Westfall P.G,Young S.S - Resampling-Based Multiple Testing Examples and Methods for p-Value Adjustment(1993)</a>)</p>\\n',\n",
       " '<p>In my view, Python is a good choice for building the machine learning part (you don\\'t say anything about the rest of your application, so I can\\'t comment of that).</p>\\n\\n<p><a href=\"http://numpy.scipy.org/\" rel=\"nofollow\">NumPy</a> is powerful and mature, and has lots of numerical packages built on top of it.</p>\\n\\n<p>For example, <a href=\"http://projects.scipy.org/scikits\" rel=\"nofollow\">SciKits</a> is a suite of such packages. It incorporates <a href=\"http://scikit-learn.org/stable/\" rel=\"nofollow\"><code>scikit-learn</code></a>, which is</p>\\n\\n<blockquote>\\n  <p>a Python module integrating classic machine learning algorithms in the tightly-knit scientific Python world (numpy, scipy, matplotlib). It aims to provide simple and efficient solutions to learning problems, accessible to everybody and reusable in various contexts: machine-learning as a versatile tool for science and engineering</p>\\n</blockquote>\\n\\n<p>With regards to performance, native NumPy operations are on par with their <a href=\"http://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms\" rel=\"nofollow\">BLAS</a> counterparts (they basically are wrappers around BLAS). Thus, NumPy code that can be expressed in terms of vector/matrix operations tends to be as fast as comparable C/Fortran code.</p>\\n\\n<p>On the flip side, code expressed as Python loops can be slow. Additionally, it is <a href=\"http://en.wikipedia.org/wiki/Global_Interpreter_Lock\" rel=\"nofollow\">hard to speed things up</a> by using multiple threads. However, there are ways around both of these shortcomings: using <a href=\"http://docs.python.org/library/multiprocessing.html\" rel=\"nofollow\"><code>multiprocessing</code></a> instead of threading, <a href=\"http://code.google.com/p/numexpr/\" rel=\"nofollow\"><code>numexpr</code></a>, <a href=\"http://cython.org/\" rel=\"nofollow\">Cython</a> and so on.</p>\\n',\n",
       " '<p>Jeffrey Wooldridge in his <em>Econometric Analysis of Cross Section and Panel Data</em> (page 357) says that the empirical Hessian \"is not guaranteed to be positive definite, or even positive semidefinite, for the particular sample we are working with.\".</p>\\n\\n<p>This seems wrong to me as (numerical problems apart) the Hessian must be positive semidefinite as a result of the definition of the M-estimator as the value of the parameter which minimizes the objective function for the given sample and the well-known fact that at a (local) minimum the Hessian is positive semidefinite.</p>\\n\\n<p>Is my argument right?</p>\\n\\n<p><strong>[EDIT: The statement has been removed in the 2nd ed. of the book. See comment.]</strong></p>\\n\\n<p>BACKGROUND\\nSuppose that $\\\\\\\\widehat \\\\\\\\theta_N$ is an estimator obtained by minimizing\\n$${1 \\\\\\\\over N}\\\\\\\\sum_{i=1}^N q(w_i,\\\\\\\\theta),$$\\nwhere $w_i$ denotes the $i$-th observation.</p>\\n\\n<p>Let\\'s denote the Hessian of $q$ by $H$,\\n$$H(q,\\\\\\\\theta)_{ij}=\\\\\\\\frac{\\\\\\\\partial^2 q}{\\\\\\\\partial \\\\\\\\theta_i \\\\\\\\partial \\\\\\\\theta_j}$$</p>\\n\\n<p>The asymptotic covariance of $\\\\\\\\widehat \\\\\\\\theta_n$ involves $E[H(q,\\\\\\\\theta_0)]$ where $\\\\\\\\theta_0$ is the true parameter value. One way to estimate it is to use the empirical Hesssian</p>\\n\\n<p>$$\\\\\\\\widehat H=\\\\\\\\frac{1}{N}\\\\\\\\sum_{i=1}^N H(w_i,\\\\\\\\widehat \\\\\\\\theta_n)$$</p>\\n\\n<p>It is the definiteness of $\\\\\\\\widehat H$ which is in question.</p>\\n',\n",
       " \"<p>Did your reading suggest that the liklihood ratio test statistic had problems? or that the chi-squared approximation was not very good?</p>\\n\\n<p>I expect that most of the problems are more likely the latter, the test statistic is fine, but we don't know the distribution of it under the null hypothesis.  With modern computers we can estimate the distribution fairly easily and compare to that (R works great for this).</p>\\n\\n<p>Just generate data that looks like yours but assuming the null hypothesis to be true, compute the LR statistic (or other statistic) for each simulated dataset.  Now compare the same statistic for your real data to the set of simulated statistics.</p>\\n\\n<p>Another option, depending on the nature of your data and question, is to use permutation tests where the null hypothesis is that there is no difference between your groups, they are just samples from the same population.  So you mix them together and recreate the samples over and over again and compare the test statistic of interest from the original samples to the permuted samples.  Fisher's exact test is actualy a permutation test.</p>\\n\",\n",
       " \"<p>They are the same, aren't they?</p>\\n\\n<p>Let's take the first model, coefficient is -1.3 and variable is in original scale (0-1). So if the variable increases by 0.01, say from 0.08 to 0.09, then the log odds are down by 1.3*0.01 = 0.013. </p>\\n\\n<p>Now the second model, coefficient is -0.013 and variable is multiplied by 100 (0 - 100). So if the variable increases by 1 from 8 to 9 (which is actually from 0.08 to 0.09), then the log odds are down by 0.013*1=0.013.</p>\\n\",\n",
       " '<p>Lorenz curve is also known under the name of \"<a href=\"http://mrvar.fdv.uni-lj.si/pub/mz/mz3.1/vuk.pdf\" rel=\"nofollow\">lift curve</a>\" when applied to classification/ranking. For a given range of predicted probability values, the lift represents a multiplicative increase in the positive class\\'s rate (due to a given predictive model) over a random guess.</p>\\n\\n<p><a href=\"http://rocr.bioinf.mpi-sb.mpg.de/ROCR.pdf\" rel=\"nofollow\">rocr package</a> can calculate lift values/curves (the manual also has a concise definition of the lift). The Gini index can be calculated from the area under the lift curve (I typically use cumulative lift value at a given predicted probability threshold instead since it is easier to relate to business metrics) </p>\\n',\n",
       " '<p>I take the following interpretation of your question:</p>\\n\\n<p>The observations are $(X_i, Y_i)$ for $i = 1, \\\\\\\\dots, n$, with $X_i$ drawn without replacement in  $\\\\\\\\{1, \\\\\\\\dots, N \\\\\\\\}$ and $y_i$ drawn <em>independently</em> from a Bernoulli $\\\\\\\\mathcal B(p)$. You’re asking for the distribution of \\n$$M = \\\\\\\\min \\\\\\\\{X_i \\\\\\\\&gt;:\\\\\\\\&gt; Y_i = 1 \\\\\\\\}.$$\\nNote that $M$ can be equal to $+\\\\\\\\infty$ as all $Y_i$ can be equal to 0.</p>\\n\\n<p>Let us denote $X_{(i)}$ the $i$-th order statistic of the sample $X_1,\\\\\\\\dots,X_n$ and $Y_{(i)}$ the $Y_i$’s reordered as the $X_i$’s (ie $Y_{(i)} = Y_j$ if $X_{(i)} = X_j$).</p>\\n\\n<p>\\\\\\\\begin{align*}\\n\\\\\\\\mathbb P(M = k) =&amp; \\\\\\\\mathbb P( X_{(1)}=k,\\\\\\\\ Y_{(1)} = 1) +  \\\\\\\\mathbb P( X_{(2)}=k,\\\\\\\\ Y_{(1)} = 0, Y_{(2)} = 1 )  + \\\\\\\\cdots \\\\\\\\cr\\n&amp;\\\\\\\\cdots + \\\\\\\\mathbb P( X_{(n)}=k,\\\\\\\\ Y_{(1)} = 0, Y_{(2)} = 0, \\\\\\\\dots, Y_{(n)} = 1) \\\\\\\\cr\\n=&amp; p \\\\\\\\cdot \\\\\\\\mathbb P(X_{(1)}=k) + (1-p)p\\\\\\\\cdot \\\\\\\\mathbb P(X_{(2)}=k) + \\\\\\\\cdots\\\\\\\\cr\\n&amp;\\\\\\\\cdots + (1-p)^{n-1} p \\\\\\\\mathbb P(X_{(n)}=k) \\\\\\\\cr\\n=&amp; p\\\\\\\\cdot\\\\\\\\sum_i (1-p)^{i-1} \\\\\\\\mathbb P(X_{(i)} = k) \\\\\\\\cr\\n\\\\\\\\mathbb P(M = +\\\\\\\\infty) =&amp; (1-p)^n\\n\\\\\\\\end{align*}</p>\\n\\n<p>On the other hand, $\\\\\\\\mathbb P(X_{(i)} = k)$ can be found just by counting the favorable events. There are $N \\\\\\\\choose n$ ways to draw $n$ elements among $N$. If the $i$-th element is $k$, the $i-1$ first have to be drawn among $k-1$ elements, and the $n-i$ remaining among $N-k$ elements. Hence\\n$$ \\\\\\\\mathbb P(X_{(i)} = k) = { {k-1 \\\\\\\\choose i-1} {N-k \\\\\\\\choose n-i} \\\\\\\\over {N\\\\\\\\choose n}} \\\\\\\\hspace{1cm} (\\\\\\\\text{for } i \\\\\\\\le k \\\\\\\\le i + N -n),$$\\nand we are done. </p>\\n',\n",
       " '<p>In Bayesian statistics, it is often mentioned that the posterior distribution is intractable and thus approximate inference must be applied. What are the factors that cause this intractability? </p>\\n',\n",
       " '<p>What is the best out-of-the-box 2-class classifier? Yes, I guess that\\'s the million dollar question, and yes, I\\'m aware of the <a href=\"http://www.no-free-lunch.org/\">no free lunch theorem</a>, and I\\'ve also  read the previous questions:</p>\\n\\n<ul>\\n<li><a href=\"http://stats.stackexchange.com/questions/258/poll-what-is-the-best-out-of-the-box-2-class-classifier-for-your-application\">Poll: What is the best out-of-the-box 2-class classifier for your application?</a></li>\\n<li>and <a href=\"http://stats.stackexchange.com/questions/5987/worst-classifier\">Worst classifier</a></li>\\n</ul>\\n\\n<p>Still, I\\'m interested in reading more on the subject.</p>\\n\\n<p>What is a good source of information that includes a general comparison of the characteristics, advantage, and features of  different classifiers?</p>\\n',\n",
       " '<p>There\\'s a book called \"A Handbook of Small Datasets\" by D.J. Hand, F. Daly, A.D. Lunn, K.J. McConway and E. Ostrowski.  The Statistics department at NCSU have electronically posted the datasets from this book <a href=\"http://www.stat.ncsu.edu/sas/sicl/data/\">here</a>. </p>\\n\\n<p>The website above gives only the data; you would need to read the book to get the story behind the numbers, that is, any story beyond what you can glean from the data set\\'s title.  But, they are <em>small</em>, and they are <em>real</em>.</p>\\n',\n",
       " \"<p>I can see Julia replacing Matlab, which would be a huge service for humanity.</p>\\n\\n<p>To replace R, you'd need to consider all of the things that Neil G, Harlan, and others have mentioned, plus one big factor that I don't believe has been addressed: easy installation of the application and its libraries.</p>\\n\\n<p>Right now, you can download a binary of R for Mac, Windows, or Linux. It works out of the box with a large selection of statistical methods. If you want to download a package, it's a simple command or mouse click. It just works.</p>\\n\\n<p>I went to download Julia and it's not simple. Even if you download the binary, you have to have gfortran installed in order to get the proper libraries. I downloaded the source and tried to <code>make</code> and it failed with no really useful message. I have an undergraduate and a graduate degree in computer science, so I could poke around and get it to work if I was so inclined. (I'm not.) Will Joe Statistician do that?</p>\\n\\n<p>R not only has a huge selection of packages, it has a fairly sophisticated system that makes binaries of the application and almost all packages, automatically. If, for some reason, you need to compile a package from source, that's not really any more difficult (as long as you have an appropriate compiler, etc, installed on your system). You can't ignore this infrastructure, do everything via github, and expect wide adoption.</p>\\n\\n<p>EDIT: I wanted to fool around with Julia -- it looks exciting. Two problems:</p>\\n\\n<p>1) When I tried installing additional packages (forget what they're called in Julia), it failed with obscure errors. Evidently my Mac doesn't have a make-like tool that they expected. Not only does it fail, but it leaves stuff lying around that I have to manually delete or other installs will fail.</p>\\n\\n<p>2) They force certain spacing in a line of code. I don't have the details in front of me, but it has to do with macros and not having a space between the macro and the parenthesis opening its arguments. That kind of restriction really bugs me, since I've developed my code formatting over many years and languages and I do actually put a space between a function/macro name and the opening parenthesis. Some code formatting restrictions I understand, but whitespace within a line?</p>\\n\",\n",
       " \"<p>It's the residuals that should be normally distributed, not the marginal distribution of your response variable.</p>\\n\\n<p>I would try using transformations, do the ANOVA, and check the residuals. If they look noticeably non-normal regardless of what transformation you use, I would switch to a non-parametric test such as the Friedman test.</p>\\n\",\n",
       " '<p>I am very new in PLS and I try to understand the output of the R function plsr(). Let us simulate data and run the PLS:</p>\\n\\n<pre><code>library(pls)\\nn &lt;- 50\\nx1 &lt;- rnorm(n); xx1 &lt;- scale(x1) \\nx2 &lt;- rnorm(n); xx2 &lt;- scale(x2)\\ny &lt;- x1 + x2 + rnorm(n,0,0.1); yy &lt;- scale(y)\\np &lt;- plsr(yy ~ xx1+xx2, ncomp=1)\\n</code></pre>\\n\\n<p>I was expecting that the following numbers $a$ and $b$</p>\\n\\n<pre><code>&gt; ( w &lt;- loading.weights(p) )\\n\\nLoadings:\\n    Comp 1\\nxx1 0.723 \\nxx2 0.690 \\n\\n               Comp 1\\nSS loadings       1.0\\nProportion Var    0.5\\n&gt; a &lt;- w[\"xx1\",]\\n&gt; b &lt;- w[\"xx2\",]\\n&gt; a^2+b^2\\n[1] 1\\n</code></pre>\\n\\n<p>are calculated in order to maximize </p>\\n\\n<pre><code>&gt; cor(y, a*xx1+b*xx2)\\n          [,1]\\n[1,] 0.9981291\\n</code></pre>\\n\\n<p>but this is not exactly the case:</p>\\n\\n<pre><code>&gt; f &lt;- function(ab){\\n+ a &lt;- ab[1]; b &lt;- ab[2]\\n+ cor(y, a*xx1+b*xx2)\\n+ }\\n&gt; optim(c(0.7,0.6), f, control=list(fnscale=-1))\\n$par\\n[1] 0.7128259 0.6672870\\n\\n$value\\n[1] 0.9981618\\n</code></pre>\\n\\n<p>Is it a numerical error, or do I misunderstand the nature of $a$ and $b$ ? </p>\\n\\n<p>I would also like to know what are these coefficients: </p>\\n\\n<pre><code>&gt; p$coef\\n, , 1 comps\\n\\n           yy\\nxx1 0.6672848\\nxx2 0.6368604 \\n</code></pre>\\n\\n<p><strong>EDIT</strong>: Now I see what is p$coef :</p>\\n\\n<pre><code>&gt; x &lt;- a*xx1+b*xx2\\n&gt; coef(lm(yy~0+x))\\n        x \\n0.9224208 \\n&gt; coef(lm(yy~0+x))*a\\n        x \\n0.6672848 \\n&gt; coef(lm(yy~0+x))*b\\n        x \\n0.6368604 \\n</code></pre>\\n\\n<p>So I think I\\'m right about the nature of $a$ and $b$.</p>\\n\\n<p>EDIT: In view of the comments given by chl I feel my question is not clear enough, so let me provide more details.  In my example there is a vector $Y$ of responses and a two-columns matrix $X$ of predictors and I use the normalized version $\\\\\\\\tilde Y$ of $Y$ and the normalized version $\\\\\\\\tilde X$ of $X$ (centered and divided by standard deviations). The definition of the first PLS component $t_1$ is $t_1 = a \\\\\\\\tilde X_1 + b \\\\\\\\tilde X_2$ with $a$ and $b$ chosen in order to have a maximal value of the inner product $\\\\\\\\langle t_1, \\\\\\\\tilde Y \\\\\\\\rangle$. Hence it is equivalent to maximize the correlation between $t_1$ and $Y$, isn\\'t it ?</p>\\n',\n",
       " '<p>Let\\'s say that I have twenty candidates in a political race and that I want to run tests on the success of each candidate based on a variety of <a href=\"http://en.wikipedia.org/wiki/Voting_system#Multiple-winner_methods\" rel=\"nofollow\">voting systems</a>. Is there a software that allowed me to do this, i.e. enter the candidates, pick voting system, run user tests, get results?</p>\\n',\n",
       " '<p>Normality is not needed to fit the regression line, the normality is used when making inference about it (hypothesis tests and confidence intervals).  Without normality the regression equation will still predict the mean (assuming the correct model), but a prediction interval based on normality and that mean will also be meaningless.  However, in highly skewed cases the mean may not be the most meaningful statistic, which is why things like robust regression, quantile regression, and others are often suggested.  With a y value between 0 and 1 you might consider Beta regression.</p>\\n\\n<p>Most of this comes down to figuring out what question you are trying to answer and what makes sense relative to the question and the science (does $y^7$ have any scientific meaning? if not I would avoid using it).</p>\\n',\n",
       " '<p>you can try for example the glm function with the family=binomial and the logit link (or the probit) doing a logistic regression, which outputs are probabilitys.. Here you have a link\\n<a href=\"http://nlp.stanford.edu/~manning/courses/ling289/logistic.pdf\" rel=\"nofollow\">logistic regression R</a></p>\\n\\n<p>You also can try with trees or with \"ensembles\" of trees , boosting, bagging or random forest wuth packages like rpart ,randomForest etc If your dependent variable is 1/0 you will have outcomes with estimated proportions.</p>\\n\\n<p><a href=\"http://www.statmethods.net/advstats/cart.html\" rel=\"nofollow\">CART trees R</a> </p>\\n\\n<p><a href=\"http://cran.r-project.org/web/packages/randomForest/randomForest.pdf\" rel=\"nofollow\">randomForest CRAN</a>\\n.</p>\\n',\n",
       " '<p>For a university paper, I have to compare whether two program versions are statistically significantly different. The comparison in terms of runtime performance is straightforward -- I determine whether the confidence intervals for the two sets of measurements overlap. However, in the case of memory footprint, I have no sets of measurements, because the memory footprint does not vary from measurement to measurement (e.g., version 1 has a constant footprint of 920 kb and version 2 consumes 1040 kb). Is there any way to compare both values in a statistically sound manner?</p>\\n',\n",
       " '<p>Clustering the results and defining a scale might be a solution.</p>\\n\\n<p>Make a category variable like so (or differently):</p>\\n\\n<ol>\\n<li>High sensitivity</li>\\n<li>Normal sensitivity</li>\\n<li>Low sensitivity</li>\\n<li>Insensitive (the ones that are off the scale in your case)</li>\\n</ol>\\n\\n<p>You could use this variable to do the analysis, but whether the results are meaningful depends on how well you define the categories.</p>\\n',\n",
       " '<p>I will leave the statistical demonstration to those who are better suited than me for it... but intuitively say that event A generates a process X that contributes to the generation of event C. Then A is correlated to C (through X). B, on the other hand generates Y, that also shapes C. Therefore A is correlated to C, B is correlated to C but A and B are not correlated.</p>\\n',\n",
       " '<p>Mutual information measures the independence between two random variables and is maximal when these random variables covary [T. M. Mitchell, Machine Learning. McGraw-Hill Science/Engineering/Math](Available: <a href=\"http://www.worldcat.org/isbn/0070428077\" rel=\"nofollow\">http://www.worldcat.org/isbn/0070428077</a>).</p>\\n\\n<p>I am however not sure whether I understand your question correctly.  Is n the total number of clusters? (You used the term number of documents).</p>\\n\\n<p>If there are many clusters and items that you are comparing do not covary in many/most of them, then mutual information will be low in most of them and by using the mean (average) you might consequently get small values as results.</p>\\n\\n<p>Possibly changing the way that this problem is represented or using a different measure might allow you to analyse your data phenomenon better. </p>\\n',\n",
       " '<p>For completeness, econometricians also like the Kiefer and Salmon test from their 1983 paper in Economics Letters -- it sums \\'normalized\\' expressions of skewness and kurtosis which is then chi-square distributed.  I have an old C++ version I wrote during grad school I could translate into R.</p>\\n\\n<p><em>Edit:</em> And <a href=\"http://econ.la.psu.edu/~hbierens/NORMTEST.PDF\" rel=\"nofollow\">here</a> is recent paper by Bierens (re-)deriving Jarque-Bera and Kiefer-Salmon.</p>\\n\\n<p><em>Edit 2:</em> I looked over the old code, and it seems that it really is the same test between Jarque-Bera and Kiefer-Salmon. </p>\\n',\n",
       " '<p>I think R is doing perfectly what you want it to do.</p>\\n\\n<p>You are plotting:</p>\\n\\n<blockquote>\\n  <p>x = qnbinom(ppoints(data),\\n  size=2.3539444, mu=50.7752809)</p>\\n</blockquote>\\n\\n<p>which is:</p>\\n\\n<blockquote>\\n  <p>[1]   3   5   7   9  10  11  12  13 \\n  14  15  16  17  18  19  20 [16]  21 \\n  21  22  23  24  25  25  26  27  28  28\\n  29  30  31  31 [31]  32  33  34  35 \\n  35  36  37  38  39  39  40  41  42  43\\n  44 [46]  45  45  46  47  48  49  50 \\n  51  52  53  54  55  56  57  59 [61] \\n  60  61  62  63  65  66  68  69  71  72\\n  74  76  77  79  81 [76]  84  86  89 \\n  91  94  97 101 105 110 116 123 132 146\\n  175</p>\\n</blockquote>\\n\\n<p>with respect to </p>\\n\\n<blockquote>\\n  <p>y = sort(data)</p>\\n</blockquote>\\n\\n<p>which is:</p>\\n\\n<blockquote>\\n  <p>[1]   6   7   7   8   9  14  15  15 \\n  16  16  17  17  17  18  19 [16]  19 \\n  22  22  23  23  24  25  25  25  26  26\\n  27  28  28  31 [31]  31  31  31  31 \\n  32  33  33  33  34  36  37  38  38  38\\n  38 [46]  39  40  43  44  48  48  54 \\n  57  57  58  58  58  62  63  63 [61] \\n  64  65  65  67  67  68  69  70  72  75\\n  77  78  81  81  86 [76]  87  89  91 \\n  93  93  95 103 103 108 117 117 137 150\\n  170</p>\\n</blockquote>\\n\\n<p>Therefore, you have 100+ values on both the axis. If you want to plot quantiles, you need to tell R to do so by doing this:</p>\\n\\n<blockquote>\\n  <p>plot(pnbinom(sort(data), size=2.3539444, mu=50.7752809), ppoints(data))</p>\\n</blockquote>\\n',\n",
       " '<p>Another <a href=\"http://stats.stackexchange.com/questions/527/what-ways-are-there-to-show-two-analytical-methods-are-equivalent/\">question</a> seems to have covered similar ground, at least in regards to similarity of discrete values.  There I suggest regressing regression would be usable in theory for the GPS position, though I imagine a solution that respects the two dimensionality of the data would be preferable.  @chi has a better answer on that same question (that includes citations). Similarity of distributions seems to be a question for a K-S test (I wonder if there is a multivariate version).  For the categorization data it seems like Baaysean approach could be useful.  Given categorization A at time slice 1 using Method 1... what is the Pr(Cat B) @ time slice 1 using Method 1?  You might also do a lagged version of this to look at whether one method is picking up categorization changes before or after the other.</p>\\n',\n",
       " '<p>Recent deep learning methods using Restricted Boltzmann Machine have shown nice features on several data types (audio, images, text).</p>\\n\\n<p>Since these methods create a generative model, you can often generate really nice samples from the model.</p>\\n\\n<p>Check out Hinton\\'s publications.\\n<a href=\"http://www.cs.toronto.edu/~hinton/\">http://www.cs.toronto.edu/~hinton/</a></p>\\n\\n<p>These methods aren\\'t totally general (run same code on every data), but the underlying model is usually similar.</p>\\n',\n",
       " '<p>Cross validate the out-of-sample accuracy for each regression, and compare them.  The better model will likely be more accurate out-of-sample.</p>\\n',\n",
       " \"<p>I'm not sure that I've titled this question correctly, but here is my query. </p>\\n\\n<p>Suppose you are given a set of measurements and the uncertainty (variance) associated with each. The task is to statistically figure out how many different objects were likely measured and finally, to combine measurements into a single estimate for each. </p>\\n\\n<p>The second part is easy enough - an uncertainty-weighted mean would do it - but I am having difficulty understanding how to sort out how many objects were measured. If there were just two objects, ANOVA would work. But what if there are an unknown number of objects? </p>\\n\\n<p>As an aside, I'm aware of a Baysian technique in which one considers each measurement in turn, building a set of hypotheses for each measurement that doesn't fall within the confidence interval of an existing hypothesis and combining it into the hypothesis when they do. But I think this method is dependent on the order the measurements are considered and therefore imparts a kind of time dependence on measurements that have none. </p>\\n\\n<p>I feel like this is something that's commonly done and I should know how to do, but I'm stumped so any thoughts you all might have would be much appreciated.</p>\\n\\n<p>Thanks,\\nVal </p>\\n\",\n",
       " '<p>First of all, I think you are conflating Gibbs-sampling-based simulated annealing (an optimization procedure that uses Gibbs sampling to draw the updates of the algorithm) and pure Gibbs sampling (a means to sample from a distribution). </p>\\n\\n<p>Your question applies to simulated annealing that uses Gibbs sampling, but it does not make sense for general Gibbs sampling. </p>\\n\\n<p>As <a href=\"http://www.umiacs.umd.edu/~raghuram/ENEE731/MRF/GemanPAMI84.pdf\" rel=\"nofollow\">Geman and Geman proved in their paper that introduced the widespread use of Gibbs sampling</a>, a simulated annealing procedure based on a Metropolis/Gibbs type proposal distribution will converge to a stationary distribution that samples uniformly from all of the local modes, as long as the cooling schedule is slow enough. </p>\\n\\n<p>So if you have an objective function that is the mixture of 3 Gaussians, say, with 3 distinct modes, then a simulated annealing process with a slow enough cooling schedule will converge to the uniform distribution on those three modes.</p>\\n\\n<p>The cooling has to be inverse-logarithmic in the relative score of the objective function, which turns out to be way way too slow for practice. In practice, you give up the guarantee that you will converge to the distribution that samples all the modes (and hence would let you see the global minimum by simple inspection), and in return you get faster cooling schedules and approximate solutions with non-zero probability of getting stuck in a local mode that is not globally optimal.  </p>\\n',\n",
       " '<p>Hypergeometric distribution to the rescue.</p>\\n\\n<p>The complementary of finding at least one error in 2350 samples knowing that the occurence rate is 6% after 26 trials is 0.80. Checking just 48 makes you reach the 95% probability.</p>\\n\\n<p>Some simple R code plotting the cumulative distribution follows:</p>\\n\\n<pre><code>&gt; pe &lt;- 0.06\\n&gt; barplot(sapply(seq(55), function(i) 1-phyper(0,2350*pe, 2350*(1-pe), i)))\\n</code></pre>\\n\\n<p>Of course that 6% does not apply anymore because if your system is any good the rate of occurrence should be lower on the remaining 2350 items but then I let you modify the variable pe accordingly.</p>\\n',\n",
       " '<p>Here are some hints.  I am sure you would have been taught these somewhere.  It is much simpler than the methods you are considering.</p>\\n\\n<p>First, if W=X-2Y, what do you get if you plug the means of X and Y into that equation?</p>\\n\\n<p>Secondly, if you multiply a random variable by a constant, the variance of the new random variable is going to be the original variance multiplied by that constant, squared.  </p>\\n\\n<p>Thirdly, if you then take two independent random variables and add them together, the the variance of the sums is the sum of the variance.</p>\\n',\n",
       " '<p>My research model has two levels of dependent variable, at the organizational level and a predictor variable at individual level. I have 500 answers from 5 middle managers (in each firm) in 100 firms at an individual level and 200 from 2 top managers (in each firm) from the same 100 firms. </p>\\n\\n<p>My question is whether aggregation of each data set and using mean score in regression is the best option? If yes, what are the aggregation terms? If not what other option or software is the best to fit this model? </p>\\n',\n",
       " '<p>If you have growth curves from each feed concentration, with time on the x-axis, you\\'ll first want to estimate the growth from each curve. % change at the end points may be sufficient for your purposes, but to use all of your data consider looking to fitting some kind of model to each growth curve with a regression. Then you can estimate the parameters of the model and do a statistical comparison on those. Your institution might have access to graph pad, (can\\'t link b/c not enough rep but google), and it is very intuitive.</p>\\n\\n<p>This approach has some benefits. For example, one concentration might lead to faster growth initially but not peak as high, for whatever reason--maybe it becomes slightly toxic with time. This analysis could potentially detect that. </p>\\n\\n<p>For more help we\\'d need to know the type of growth you expect from your specimen. If it\\'s bacteria there are a ton of ways to do growth curves, see e.g. the classic paper by Zwietering et al here: <a href=\"http://aem.asm.org/cgi/reprint/56/6/1875.pdf\" rel=\"nofollow\">http://aem.asm.org/cgi/reprint/56/6/1875.pdf</a>. </p>\\n',\n",
       " '<ul>\\n<li><p>The standard tools, at least in psychology, in your situation would be exploratory and confirmatory factor analysis to assess the convergence of the inter-item correlation matrix with some proposed model of the relationship between factors and items. The way that you have phrased your question suggests that you might not be familiar with this literature.\\nFor example, here are my notes on the <a href=\"http://jeromyanglim.blogspot.com/2009/10/exploratory-factor-analysis-and-scale.html\" rel=\"nofollow\">scale construction and factor analysis</a> and here is a <a href=\"http://www.statmethods.net/advstats/factor.html\" rel=\"nofollow\">tutorial in R on factor analysis form Quick-R</a>.\\nThus, while it\\'s worth answering your specific question, I think that your broader aims will be better served by examining factor analytic approaches to evaluating multi-item, multi-factor scales.</p></li>\\n<li><p>Another standard strategy would be to calculate total scores for each group of variables (what I would call a \"scale\") and correlate the scales.</p></li>\\n<li><p>Many reliability analysis tools will report average inter-item correlation.</p></li>\\n<li><p>If you created the 50 by 50 matrix of correlations between items, you could write a function in R that averaged subsets based on combinations of groups of variables. You might not get what you want if you have a mixture of positive and negative items, as the negative correlations might cancel out the positive correlations.</p></li>\\n</ul>\\n',\n",
       " \"<p>The <em>p</em>-value itself cannot tell you how strong the relationship is, because the <em>p</em>-value is so influenced by sample size, among other things.  But assuming your N is something on the order of 100-150, I'd say there's a reasonably strong effect involving Size whereby as Size increases, the log of the odds of Y being 1 is notably different from the log of the odds of Y being 0.  As you indicate, the same cannot be said of the comparison of Y values of -1 and 0.</p>\\n\\n<p>You are right in viewing all of this as somewhat invalidated by the overall nonsignificance of Size (depending on your <em>alpha</em>, or criterion for significance).  You wouldn't get too many arguments if you simply declared Size a nonfactor due to its high <em>p</em>-value.  But then again, if your N is sufficiently small--perhaps below 80 or 100--then your design affords low power for detecting effects, and you might make a case for taking seriously the specific effect that managed to show up anyway.  </p>\\n\\n<p>A way around the problem of relying on <em>p</em>-values involves two steps.  First, decide what range of odds ratios would constitute an effect worth bothering with, or worth calling substantial.  (The trick there is in being facile enough with odds to recognize what they mean for the more intuitive metric of probability.)  Then construct a confidence interval for the odds ratio associated with each coefficient and consider it in light of your hypothetical  range.  Regardless of statistical significance, does the effect have practical significance?    </p>\\n\",\n",
       " '<p>Great answers!</p>\\n\\n<p>I would add that by default, calling <code>formula</code> on a <code>data.frame</code> creates an additive formula to regress the first column onto the others.</p>\\n\\n<p>So in the case of the answer of @danas.zuokas you can even do</p>\\n\\n<pre><code>lm(df)\\n</code></pre>\\n\\n<p>which is interpreted correctly.</p>\\n',\n",
       " '<p>I\\'ll give you the perspective from the view of Likelihood Theory which originated with <a href=\"http://en.wikipedia.org/wiki/Ronald_Fisher\">Fisher</a> -- and is the basis for the statistical definition in the cited Wikipedia article.  </p>\\n\\n<p>Suppose you have random variates $X$ which arise from a parameterized distribution $F(X; \\\\\\\\theta)$, where $\\\\\\\\theta$ is the parameter characterizing $F$.  Then the probability of $X = x$ would be: $P(X = x) = F(x; \\\\\\\\theta)$, with known $\\\\\\\\theta$.  </p>\\n\\n<p>More often, you have data $X$ and $\\\\\\\\theta$ is unknown.  Given the assumed model $F$, the likelihood is defined as the probability of observed data as a function of $\\\\\\\\theta$: $L(\\\\\\\\theta) = P(\\\\\\\\theta; X = x)$.  Note that $X$ is known, but $\\\\\\\\theta$ is unknown; in fact the motivation for defining the likelihood is to determine the parameter of the distribution.</p>\\n\\n<p>Although it seems like we have simply re-written the probability function, a key consequences of this is that the likelihood function does <em>not</em> obey the laws of probability (for example, it\\'s not bound to the [0, 1] interval).  However, the likelihood function is proportional to the probabiilty of the observed data.  </p>\\n\\n<p>This concept of likelihood actually leads to a different school of thought, \"likelihoodists\" (distinct from frequentist and bayesian) and you can google to search for all the various historical debates.  The cornerstone is the <a href=\"http://en.wikipedia.org/wiki/Likelihood_principle\">Likelihood Principle</a> which essentially says that we can perform inference directly from the likelihood function (neither Bayesians nor frequentists accept this since it is not probability based inference).  These days a lot of what is taught as \"frequentist\" in schools is actually an amalgam of frequentist and likelihood thinking.  </p>\\n\\n<p>For deeper insight, a nice start and historical reference is Edwards\\' <a href=\"http://rads.stackoverflow.com/amzn/click/0801844436\">Likelihood</a>.  For a modern take, I\\'d recommend Richard Royall\\'s wonderful monograph, <a href=\"http://rads.stackoverflow.com/amzn/click/0412044110\">Statistical Evidence: A Likelihood Paradigm</a>.  </p>\\n',\n",
       " '<p>I am working on an algorithm to minimize a function by iterative improvement, and I am plotting the function\\'s value for each iteration (pretty straightforward plot). I am feeding GnuPlot or PgfPlots an input file like:</p>\\n\\n<pre><code>\"Solution 1\"\\n2.22419e+007\\n2.22418e+007\\n2.22418e+007\\n...\\n</code></pre>\\n\\n<p>GnuPlot script:</p>\\n\\n<pre><code>set key autotitle columnheader\\nplot \\'data.txt\\' with lines\\n</code></pre>\\n\\n<p>PgfPlots script:</p>\\n\\n<pre><code>\\\\\\\\begin{tikzpicture}\\n\\\\\\\\begin{axis}[xlabel={Iteration}, ylabel={Solutions values}]\\n\\n\\\\\\\\addplot [blue] table[x expr=\\\\\\\\coordindex, y index=0] {data.txt};\\n\\n\\\\\\\\end{axis}\\n\\\\\\\\end{tikzpicture}\\n</code></pre>\\n\\n<p>In my algorithm sometimes an event happens, namely I switch to another method for few iterations, or I have to relax some constraints because I can\\'t get an improvement. I would like to show these events in the plot. I am looking for something like this (black vertical lines), but I am open to different alternatives as long as the meaning is the same:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/MSs3q.png\" alt=\"Ideal plot\"></p>\\n\\n<p>What is the best way to show events in this kind of plots? How to do it with GnuPlot and PgfPlots?</p>\\n',\n",
       " '<p>First you have to think over if the missing data are missing completely at random (MCAR), missing at random (MAR) or missing not at random (MNAR) as deletion (in other words complete-case analysis) may lead to biased results. Alternatives are inverse probability weighting, multiple imputation, the full-likelihood method and doubly-robust methods. Multiple imputation with chained equations (MICE) if often the easiest way to go.</p>\\n',\n",
       " \"<p>Expanding on Mark T Patterson's answer...</p>\\n\\n<pre><code>&gt; dummy_cardgame &lt;- function(params) as.integer(runif(1) &lt; params)\\n&gt; replicate(10, dummy_cardgame(0.25))\\n [1] 0 1 0 0 0 1 0 1 0 0\\n</code></pre>\\n\",\n",
       " '<p>Before you run the analysis you have to assign an output file in the third tab under \"Predicted Value.\" Then you have to go to utilities and select scoring wizard, choose the model you just saved and it will automatically assign the predicted values to the rest of the data set.</p>\\n',\n",
       " \"<p>Most statistics packages have ways of saving residuals from your model. Using <code>GLM - UNIVARIATE</code> in SPSS you can save residuals. This will add a variable to your data file representing the residual for each observation. </p>\\n\\n<p>Once you have your residuals you can then examine them to see whether they are normally distributed, homoscedastic, and so on. For example, you could use a formal normality test on your residual variable or perhaps more appropriately, you could plot the residuals to check for any major departures from normality. If you want to examine homoscedasticity, you could get a plot that looked at the residuals by group.</p>\\n\\n<p>For a basic between subjects factorial ANOVA, where homogeneity of variance holds, normality within cells means normality of residuals because your model in ANOVA is to predict group means. Thus, the residual is just the difference between group means and observed data.</p>\\n\\n<h3>Response to comments below:</h3>\\n\\n<ul>\\n<li>Residuals are defined relative to your model predictions. In this case your model predictions are your cell means. It is a more generalisable way of thinking about assumption testing if you focus on plotting the residuals rather than plotting individual cell means, even if in this particular case, they are basically the same. For example, if you add a covariate (ANCOVA), residuals would be more appropriate to examine than distributions within cells.</li>\\n<li>For purposes of examining normality, standardised and unstandardised residuals will provide the same answer. Standardised residuals can be useful when you are trying to identify data that is poorly modelled by the data (i.e., an outlier). </li>\\n<li>Homogeneity of variance and homoscedasticity mean the same thing as far as I'm aware. Once again, it is common to examine this assumption by comparing the variances across groups/cells. In your case, whether you calculate variance in residuals for each cell or based on the raw data in each cell, you will get the same values. However, you can also plot residuals on the y-axis and predicted values on the x-axis. This is a more generalisable approach as it is also applicable to other situations such as where you add covariates or you are doing multiple regression.</li>\\n<li>A point was raised below that when you have heteroscedasticity (i.e., within cell variance varies between cells in the population) and normally distributed residuals within cells, the resulting distribution of all residuals would be non-normal. The result would be a mixture distribution of variables with mean of zero and different variances with proportions relative to cell sizes. The resulting distribution will have no zero skew, but would presumably have some amount of kurtosis. If you divide residuals by their corresponding within-cell standard deviation, then you could remove the effect heteroscedasticity; plotting the residuals that result would provide an overall test of whether residuals are normally distributed independent of any heteroscedasticity.</li>\\n</ul>\\n\",\n",
       " '<p>The mean and variance of a Gaussian are the unknown parameters that specify that distribution in that case. Likewise, in topic modeling, you attempt to learn the unknown parameters of $K$ topics, where each topic is a multinomial distribution over words in the vocabulary. Thus, it is the parameters of each multinomial distribution (each topic) that you seek to infer.</p>\\n\\n<p>Note that your learning algorithm will also output another set of multinomial paramaters that represent the distribution of topics for each document.  </p>\\n',\n",
       " '<p>You don\\'t need normality. All you need is that \\n$$s^2 = \\\\\\\\frac{1}{n-1} \\\\\\\\sum_{i=1}^n(x_i - \\\\\\\\bar{x})^2$$\\n<strong>is</strong> an unbiased estimator of the variance $\\\\\\\\sigma^2$. Then use that the square root function is <em>strictly concave</em> such that (by a strong form of <a href=\"http://en.wikipedia.org/wiki/Jensen%27s_inequality#Proofs\">Jensen\\'s inequality</a>)<br>\\n$$E(\\\\\\\\sqrt{s^2}) &lt; \\\\\\\\sqrt{E(s^2)} = \\\\\\\\sigma$$\\nunless the distribution of $s^2$ is degenerate at $\\\\\\\\sigma^2$. </p>\\n',\n",
       " '<p>I am not very familiar with this literature but I believe some physicists use much lower thresholds in statistical tests but they talk about it a little differently. For example, if a measure is three standard deviations from the theoretical prediction, it is described as a “three sigma” deviation. Basically, this means that the parameter of interest is statistically different from the predicted value in a z test with α = .01. Two sigma is roughly equivalent to α = .05 (in fact it would be 1.96 σ). If I am not mistaken, the standard error level in physics is 5 sigma, which would be α = 5*10^-7</p>\\n\\n<p>Also, in neuroscience or epidemiology, it seems increasingly common to routinely perform some correction for multiple comparisons. The error level for each individual test can therefore be lower than p &lt; .01</p>\\n',\n",
       " '<p>There is no ready to use robust counterpart to arima function in R <a href=\"http://r-forge.r-project.org/projects/robust-ts/\" rel=\"nofollow\">(yet)</a>; should one appear, it will be listed <a href=\"http://cran.r-project.org/web/views/Robust.html\" rel=\"nofollow\">here</a>. Maybe an alternative is to down-weight those observations that are outlying with respect to a simple univariate outlier detection rule, but I don\\'t see ready to use packages to run weighted ARMA regression either. Another possible alternative would then be to Winsorize the outlying points: </p>\\n\\n<pre><code>#parameters\\npara     &lt;- list(ar=c(0.6,-0.48), ma=c(-0.22,0.24))\\n#original series\\ny1 &lt;- y0 &lt;- arima.sim(n=100, para, sd=sqrt(0.1796)) \\n#outliers\\nout      &lt;- sample(1:100, 20)               \\n#contaminated series\\ny1[out]  &lt;- rnorm(20, 10, 1)                \\nplot( y1, type=\"l\")\\nlines(y0, col=\"red\")\\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/JyYq7.png\" alt=\"example of random contamination\"></p>\\n\\n<pre><code>#winsorized series\\ny2      &lt;- rep(NA, length(y1))\\na1      &lt;- (y1-median(y1)) / mad(y1)\\na2      &lt;- which(abs(a1)&gt;3)\\ny2[-a2] &lt;- y1[-a2]\\nfor(i in 2:length(y2)){\\n   if(is.na(y2[i])){ y2[i] &lt;- y2[i-1] }\\n}       \\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/FSEhE.png\" alt=\"cleaned series\">                </p>\\n',\n",
       " '<p>At first I thought I\\'d lean to your second option too.  It seems a slightly more orderly approach to the data and makes the most of the fact that subjects may have several pairs of close events.  </p>\\n\\n<p>However, the argument for the first option would be that it better represents your research question, which as I understand it is about the characteristics of <em>subjects</em> that lead to them experiencing events in close succession.  A natural way of going about this is to identify the subjects who experience such an issue and look at their characteristics - which is your option 1.  An extension of option 1 would be to actually produce a count for each subject \"how many times has this subject experienced a second event close after a first event?\".  This count variable would then be the response in a generalized linear model with a poisson response.  Once I\\'d thought of this, I decided I prefer option 1.</p>\\n\\n<p>If you go for option 2, whether you can collapse your outcomes 2 and 3 together depends a bit on your research question and the underlying theory you want to test.  How arbitrary is the three days limit for example?  If you <em>don\\'t</em> collapse them however you don\\'t want a multinomial logit model, you want ordinal logistic regression.  A multinomial model would be if you have three outcomes with no real ordering to them - in your case, clearly there is a natural order between \"another event within three days\", \"another event before observations finished\" and \"no other event\".</p>\\n\\n<p>One argument in favour of collapsing your outcomes 2 and 3 will be that if you <em>don\\'t</em> you have a problem similar to many survival studies.  That is, the chances of getting the second outcome rather than the third depend on how late in the study the first event happened, purely due to the fact that you observe the post-event environments earlier in the experiment for a longer time.  Collapsing outcomes 2 and 3 fixes that problem at least.</p>\\n\\n<p>To control for subjects under option 2 you <em>must</em> introduce a level of randomness for subject.  Otherwise you certainly stuff up your research question (and would be better off with option 1). You can do this with a generalized linear mixed effects model, for which there are implementations in various software applications.</p>\\n',\n",
       " '`ordinal` refers to data that have an order but not necessarily equal spacing between levels. It can also refer to ordinal logistic regression.',\n",
       " '<p>I\\'ll ty to answer the question in terms of <a href=\"http://en.wikipedia.org/wiki/Kolmogorov_complexity\" rel=\"nofollow\">Kolmogorov complexity</a>, which is the length of the smallest description of a finite sequence, given a fixed description language. A sequence is called Kolmogorov random, if the Kolmogorov complexity is at least as big as the length of the sequence (i.e., the sequence is incompressible).</p>\\n\\n<p>For your problem, we use the set of programs on a fixed universal Turing machine as description language. Without loss of generality, we may assume that the universal Turing machine uses a binary alphabet.\\nFor each natural number $n&gt;0$, there exists a Kolmogorov random sequence. The proof is simple: There are $2^n$ sequences of length $n$, but only $2^{n-1}$ programs of length less than $n$. So by the pigeonhole principle, there must be some sequences -- in fact, at least $2^n - 2^{n-1} = 2^{n-1}$ sequences -- of length $n$ that are incompressible.</p>\\n\\n<p>This proof is of course not constructive. Also, it is not computable if a given sequence is Kolmogorov random.</p>\\n\\n<p><strong>Update</strong></p>\\n\\n<p>To make a connection to statistical redundancy:\\nIf you generate a sequence by some random process, the Kolmogorov complexity divided by the length of the sequence converges to the entropy of the generating process (as the length of the sequence goes to infinity). Thus, a sequence generated by a Bernoulli process with p=0.5 will be \"Kolmogorov random in the limit\".</p>\\n',\n",
       " \"<p>I have a question about Scientific Data Mining.</p>\\n\\n<ol>\\n<li><p>Do you know successful case studies of applying Data Mining / Machine Learning techniques in hydrodynamics?</p></li>\\n<li><p>In general, does it make actually sense to try to apply DM/ML techniques to such deterministic systems as gas/fluid flows which are described by Navier-Stokes equations?</p></li>\\n</ol>\\n\\n<p>I guess that answer is 'no' in the case of very simple flows. But maybe it makes sense if we have a deal with complex data from turbulent/multiphase/... flows? </p>\\n\\n<ol>\\n<li>What could be the problem formulation in this area?</li>\\n</ol>\\n\\n<p>I will appreciate to your opinion, links to the papers and web-pages.</p>\\n\",\n",
       " \"<p>Okay, I think I might have solved the problem. Generally the differences in the gradients are &lt; 1e-4, though I do have at least one which is 6e-4. Does anyone know if this is still acceptable?</p>\\n\\n<p>To get this result, I rewrote the code and without tying the weight matrices (I'm not sure if doing so will always cause the derivative check to fail). I've also included biases, as they didn't complicate things too badly.</p>\\n\\n<p>Something else I realized when debugging is that it's <em>really</em> easy to make a mistake in the code. For example, it took me a while to catch:</p>\\n\\n<pre><code>grad_W1 = error_h*X';\\n</code></pre>\\n\\n<p>instead of:</p>\\n\\n<pre><code>grad_W1  = X*error_h';\\n</code></pre>\\n\\n<p>While the difference between these two lines is just the transpose of grad_W1, because of the requirement of packing/unpacking the parameters into a single vector, there's no way for Matlab to complain about grad_W1 being the wrong dimensions.</p>\\n\\n<p>I've also included my own derivative check which gives slightly different answers than minFunc's (my deriviate check gives differences that are all below 1e-4).</p>\\n\\n<p><strong>fwdprop.m:</strong></p>\\n\\n<pre><code>function [ hidden, output ] = fwdprop(W1, bias1, W2, bias2, X)\\n  hidden = sigmoid(bsxfun(@plus, W1'*X, bias1));\\n  output = sigmoid(bsxfun(@plus, W2'*hidden, bias2));\\n end\\n</code></pre>\\n\\n<p><strong>calcLoss.m:</strong></p>\\n\\n<pre><code>function [ loss, grad ] = calcLoss(theta, X, nHidden)\\n  [nVars, nInstances] = size(X);\\n  [W1, bias1, W2, bias2] = unpackParams(theta, nVars, nHidden);\\n  [hidden, output] = fwdprop(W1, bias1, W2, bias2, X);\\n  err = output - X;\\n  delta_o = err .* output .* (1.0 - output);\\n  delta_h = W2*delta_o .* hidden .* (1.0 - hidden);\\n\\n  grad_W1 = X*delta_h';\\n  grad_bias1 = sum(delta_h, 2);\\n  grad_W2 = hidden*delta_o';\\n  grad_bias2 = sum(delta_o, 2);\\n\\n  loss = 0.5*sum(err(:).^2);\\n  grad = packParams(grad_W1, grad_bias1, grad_W2, grad_bias2);\\nend\\n</code></pre>\\n\\n<p><strong>unpackParams.m:</strong></p>\\n\\n<pre><code>function [ W1, bias1, W2, bias2 ] = unpackParams(params, nVisible, nHidden)\\n  mSize = nVisible*nHidden;\\n\\n  W1 = reshape(params(1:mSize), nVisible, nHidden);\\n  offset = mSize;    \\n\\n  bias1 = params(offset+1:offset+nHidden);\\n  offset = offset + nHidden;\\n\\n  W2 = reshape(params(offset+1:offset+mSize), nHidden, nVisible);\\n  offset = offset + mSize;\\n\\n  bias2 = params(offset+1:end);\\nend\\n</code></pre>\\n\\n<p><strong>packParams.m</strong></p>\\n\\n<pre><code>function [ params ] = packParams(W1, bias1, W2, bias2)\\n  params = [W1(:); bias1; W2(:); bias2(:)];\\nend\\n</code></pre>\\n\\n<p><strong>checkDeriv.m:</strong></p>\\n\\n<pre><code>function [check] = checkDeriv(X, theta, nHidden, epsilon)\\n  [nVars, nInstances] = size(X);\\n\\n  [W1, bias1, W2, bias2] = unpackParams(theta, nVars, nHidden);\\n  [hidden, output] = fwdprop(W1, bias1, W2, bias2, X);\\n  err = output - X;\\n  delta_o = err .* output .* (1.0 - output);\\n  delta_h = W2*delta_o .* hidden .* (1.0 - hidden);\\n\\n  grad_W1 = X*delta_h';\\n  grad_bias1 = sum(delta_h, 2);\\n  grad_W2 = hidden*delta_o';\\n  grad_bias2 = sum(delta_o, 2);\\n\\n  check = zeros(size(theta, 1), 2);\\n  grad = packParams(grad_W1, grad_bias1, grad_W2, grad_bias2);\\n  for i = 1:size(theta, 1)\\n      Jplus = calcHalfDeriv(X, theta(:), i, nHidden, epsilon);\\n      Jminus = calcHalfDeriv(X, theta(:), i, nHidden, -epsilon);\\n\\n      calcGrad = (Jplus - Jminus)/(2*epsilon);\\n      check(i, :) = [calcGrad grad(i)];\\n  end\\nend\\n</code></pre>\\n\\n<p><strong>checkHalfDeriv.m:</strong></p>\\n\\n<pre><code>function [ loss ] = calcHalfDeriv(X, theta, i, nHidden, epsilon)\\n  theta(i) = theta(i) + epsilon;\\n\\n  [nVisible, nInstances] = size(X);\\n  [W1, bias1, W2, bias2] = unpackParams(theta, nVisible, nHidden);\\n  [hidden, output] = fwdprop(W1, bias1, W2, bias2, X);\\n\\n  err = output - X;\\n  loss = 0.5*sum(err(:).^2);\\nend\\n</code></pre>\\n\",\n",
       " '<p>It\\'s been a long since I didn\\'t play with Mathematica, and I just had a quick look on Google, but can\\'t you just use (here with some fake data)</p>\\n\\n<pre><code>x = Table[Sin[x] + 0.2 RandomReal[], {x, -4, 4, .1}];\\nListPlot[x, DataRange -&gt; {-4, 4}]\\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/ckN4G.png\" alt=\"enter image description here\"></p>\\n\\n<p>the function <a href=\"http://reference.wolfram.com/mathematica/ref/ListCorrelate.html\" rel=\"nofollow\">ListCorrelate</a>?</p>\\n\\n<pre><code>acf = ListCorrelate[x, x, {1, 1}, 0]\\nListPlot[acf, Filling -&gt; Axis]\\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/bzHkD.png\" alt=\"enter image description here\"></p>\\n',\n",
       " '<p>Specifically on color and perception, I liked the papers below by Bergman, Rogowitz and Treinish.  </p>\\n\\n<ul>\\n<li><a href=\"http://www.research.ibm.com/people/l/lloydt/color/color.HTM\" rel=\"nofollow\">Why Should Engineers and Scientists Be Worried About Color?</a></li>\\n<li><a href=\"http://www.research.ibm.com/dx/proceedings/pravda/index.htm\" rel=\"nofollow\">A Rule-based Tool for Assisting Colormap Selection</a></li>\\n<li><a href=\"http://www.research.ibm.com/dx/proceedings/pravda/truevis.htm\" rel=\"nofollow\">How NOT to Lie with Visualization</a></li>\\n<li><a href=\"http://www.research.ibm.com/people/l/lloydt/\" rel=\"nofollow\">Lloyd Treinish\\'s home page</a>  (for links to related work)</li>\\n</ul>\\n',\n",
       " '<p>I would be very glad if someone can point me out to statistics and econometrics distance learning courses like <a href=\"http://www2.statistics.com\" rel=\"nofollow\">http://www2.statistics.com</a>.\\nThanks in advance</p>\\n',\n",
       " \"<p>Though @Macro's answer is nice, it does require an assumption about the (in)dependence of the statistics. Another approach would be to use bootstrapping. The idea would be to keep one one variable fixed and shuffle the other variable, calculate the correlation for each of your samples, and take their difference. Repeat many times to get a distribution and use this distribution to test the hypothesis that the correlations are the same. The structure of your data set isn't that clear to me, so it's hard to provide more details.</p>\\n\",\n",
       " \"<p>I have a situation in which I have $n$ observations, each with $p$ independent variables and $q$ dependent variables. I would like to build a model or series of models to obtain predictions of the $q$ dependent variables for a new observation.</p>\\n\\n<p>One way is to build multiple models, each one predicting a single dependent variable. An alternative approach is to build a single model to predict all the dependent variables at one go (multivariate regression or PLS etc).</p>\\n\\n<p>My question is: does taking into account multiple DV's simultaneously lead to a more robust/accurate/reliable model? Given the fact that some of the $q$ dependent variables might be correlated with each other, does this fact hamper or help a single model approach? Are there references that I could look up on this topic?</p>\\n\",\n",
       " '<p>I want to talk about some data that is only relevant when looked at as a whole, to see the trends, but each individual piece of data is suspect, similar to web traffic data.</p>\\n\\n<p>What word should I use in this blank space?</p>\\n\\n<p>\"This data is not supposed to be analyzed as <strong><em>_</em></strong>, it is only good for showing trends and getting a total statistical picture of the whole data set.\"</p>\\n',\n",
       " '<p>The following provides an inaccurate approximation, although the inaccuracy will depend on the distribution of the input data.  It is an online algorithm, but only approximates the absolute deviance.  It is based on a <a href=\"http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation/\" rel=\"nofollow\">well known algorithm</a> for calculating variance online, described by <a href=\"http://www.jstor.org/stable/1266577\" rel=\"nofollow\">Welford</a> in the 1960s.  His algorithm, translated into R, looks like:</p>\\n\\n<pre><code>M2 &lt;- 0\\nmean &lt;- 0\\nn &lt;- 0\\n\\nvar.online &lt;- function(x){\\n    n &lt;&lt;- n + 1\\n    diff &lt;- x - mean\\n    mean &lt;&lt;- mean + diff / n\\n    M2 &lt;&lt;- M2 + diff * (x - mean)\\n    variance &lt;- M2 / (n - 1)\\n    return(variance)\\n}\\n</code></pre>\\n\\n<p>It performs very similarly to R\\'s builtin variance function:</p>\\n\\n<pre><code>set.seed(2099)\\nn.testitems &lt;- 1000\\nn.tests &lt;- 100\\ndifferences &lt;- rep(NA, n.tests)\\nfor (i in 1:n.tests){\\n        # Reset counters\\n        M2 &lt;- 0\\n        mean &lt;- 0\\n        n &lt;- 0\\n\\n        xs &lt;- rnorm(n.testitems)\\n        for (j in 1:n.testitems){\\n                v &lt;- var.online(xs[j])\\n        }\\n\\n        differences[i] &lt;- abs(v - var(xs))\\n\\n}\\nsummary(differences)\\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \\n0.000e+00 2.220e-16 4.996e-16 6.595e-16 9.992e-16 1.887e-15 \\n</code></pre>\\n\\n<p>Modifying the algorithm to calculate absolute deviation simply involves an additional <code>sqrt</code> call.  However, the <code>sqrt</code> introduces inaccuracies that are reflected in the result:</p>\\n\\n<pre><code>absolute.deviance.online &lt;- function(x){\\n    n &lt;&lt;- n + 1\\n    diff &lt;- x - mean\\n    mean &lt;&lt;- mean + diff / n\\n    a.dev &lt;&lt;- a.dev + sqrt(diff * (x - mean))\\n    return(a.dev)\\n}\\n</code></pre>\\n\\n<p>The errors, calculated as above, are much greater than for the variance calculation:</p>\\n\\n<pre><code>    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \\n0.005126 0.364600 0.808000 0.958800 1.360000 3.312000 \\n</code></pre>\\n\\n<p>However, depending on your use case, this magnitude of error might be acceptable.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/bLZr0.png\" alt=\"historgram of differences\"></p>\\n',\n",
       " '<p>This is a quick example in R for taking a 5x5 positive definite matrix and turning it into an approximation of the first matrix.</p>\\n\\n<pre><code>#Test Matrix\\nA &lt;- matrix(1,nrow=5,ncol=5)\\ndiag(A) &lt;- 1:5\\n\\nevec &lt;- eigen(A)$vectors\\neval &lt;- eigen(A)$values\\n\\n# we can get the original matrix back through\\n# spectral decomposition\\nevec %*% diag(eval) %*% t(evec)\\n\\n# since R and most other software automatically sorts\\n# the eigenvalue/vector pairs by absolute magnitude we\\n# can just do the following subsets to approximate A\\n\\nevec[,1:3] %*% diag(eval)[1:3,1:3] %*% t(evec[,1:3])\\n</code></pre>\\n\\n<p>The difference you are seeing might be from your code. In this case I would check if you could generalize your function to reconstruct the matrix for any subset of the eigenvalues/eigenvector pairs and test it for the case where you include all the pairs.  This case should match the original matrix.</p>\\n\\n<p>The other cause for the large change is that the sum of the deleted eigenvalues is reasonably large.  Recall from the spectral decomposition of a positive definite matrix we can write the matrix as follows $A = \\\\\\\\lambda_1 v_1 v_1^T + ... + \\\\\\\\lambda_n v_n v_n^T $.  So by subsetting the deleted values and reconstructing this portion of A, you will get what was lost by your approximation.</p>\\n',\n",
       " 'MANOVA is the acronym for Multivariate Analysis of Variance. This is a generalization of ANOVA for the case where there are multiple dependent variables. ',\n",
       " \"<p>Having a score board</p>\\n\\n<pre><code>        Points\\nDay 1   1000\\nDay 2   1050\\nDay 3   1010\\nDay 4   1010\\nDay 5   1030\\n</code></pre>\\n\\n<p>How to calculate the average variation points? i.e. How many points, on average, a person gains or loses per day.</p>\\n\\n<p>This must be something really simple. Yet, I can't figure out.</p>\\n\",\n",
       " '<p>I have a lin-log regression model like </p>\\n\\n<p>$$Y = b_0 + b_1 \\\\\\\\log(x_1 + 1) + e.$$</p>\\n\\n<p>The distribution of $x_1$ is very skewed, thus I use the natural logarithm to get a more Gaussian like distribution. Because 3 out of 100 values have zero as entry I add a constant c, in my case plus 1, to avoid -Inf. </p>\\n\\n<p>The resulting estimation of $b_1$ is about -0.14. </p>\\n\\n<p>Without the constant the interpretation is clear: a 1% change in $x$ results in a $0.01\\\\\\\\cdot b_1$ change in $y$.</p>\\n\\n<p>I struggle with the constant. How can I account for it in my interpretation? If I change the value of c I get, of course, other estimates. I have chosen + 1 because this results in positive log values (the values of $x_1$ are originally positive too).</p>\\n\\n<p>Or should I add a small value just to the three 0s?</p>\\n\\n<p>Many thanks in advance and, please, a non mathematical answer ;-)\\nMarco</p>\\n',\n",
       " \"<p>Here is the problem (not homework),</p>\\n\\n<p>Let $U_1,\\\\\\\\cdots,U_n$ be i.i.d. uniform$(-n,n)$ random variables. \\nFor $-n&lt;a&lt;b&lt;n$, we set $1_{U_i}(a,b)$ be the indicator function such that $1_{U_i}=1$ if $U_i\\\\\\\\in(a,b)$ and 0 otherwise. What is approximate distribution as n large of $U_1+,\\\\\\\\cdots,+U_n$.</p>\\n\\n<p>I computed the characteristic function of $U_1+,\\\\\\\\cdots,+U_n$, i.e.,\\n   $\\\\\\\\phi(t) = \\\\\\\\left(\\\\\\\\frac{sin(nt)}{nt}\\\\\\\\right)^n$,\\nbut I don't know how to get the final result. By the way, I have no idea to use the hint in the problem. Please provide me some hints or references. Thanks!</p>\\n\",\n",
       " '<p>In case you are using LaTeX for your report writing, the package pgfplots can read in data files and plot single or double logarithmic axis. In case you need to do calculations you can escape to gnuplot. </p>\\n\\n<p>It just looks this tiny bit better if your text font matches your axis labels font.</p>\\n',\n",
       " '<p>Throw $x$ dice, each of which has $z$ sides, keep the $y$ highest values rolled, and find their sum $s$. In roleplaying games, this is called a \"roll and keep dice mechanic\" and notated \"$x$d$z$k$y$\" or perhaps just \"$x$k$y$\" if the number of sides $z$ is understood. Let (capital) $S$ be the random variable for the sum of the $y$ highest dice, and let (lower case) $s$ be the observed sum of the $y$ highest dice.</p>\\n\\n<p>A combinatorial formula for the probability mass funtion $f$ for this distribution was derived by a user called \"techmologist\" over at PhysicsForums: <a href=\"http://www.physicsforums.com/showthread.php?p=2813034\" rel=\"nofollow\">Puzzling \"roll X dice, choose Y highest\" problem</a>.</p>\\n\\n<p>The pmf is</p>\\n\\n<p>$ f(s) = $\\n$$\\\\\\\\sum_{r=1}^{z}\\\\\\\\sum_{i=0}^{x-y}\\\\\\\\sum_{j=x-y-i+1}^{x-i}{x \\\\\\\\choose i,j,x-i-j}(r-1)^i N(s+i+j-x-ry; x-i-j, z-r-1) $$</p>\\n\\n<p>where $N$ is the number of ways to distribute $B$ indistinguishable balls among $C$ cells so that each cell has between 0 and $D$ balls, namely,</p>\\n\\n<p>$$ N(B;C,D) = \\\\\\\\sum_{k=0}^{\\\\\\\\lfloor B/(D+1) \\\\\\\\rfloor} (-1)^k \\\\\\\\binom{C}{k} \\\\\\\\binom{B-k(D+1) + C-1}{C-1}  $$</p>\\n\\n<p>My intuition is that there might be a simpler way to express this mechanic using order statistics. Let $Z_1,Z_2,\\\\\\\\ldots,Z_x$ be the unordered random variables for the $x$ iid dice throws, which are each discrete uniform distributions on the integer values $\\\\\\\\{1, 2, \\\\\\\\ldots, z \\\\\\\\}$. And let $1 \\\\\\\\leq Z_{1:x} \\\\\\\\leq Z_{2:x} \\\\\\\\leq \\\\\\\\ldots \\\\\\\\leq Z_{x:x} \\\\\\\\leq z$ be the order statistics, so that for example, $Z_{x:x}$ is the r.v. representing maximum roll of the $x$ dice.</p>\\n\\n<p>In support of my intuition, it can be shown that </p>\\n\\n<p>$ E(S) = E(Z_{(x-y+1):x} + Z_{(x-y+2):x} + \\\\\\\\ldots + Z_{(x-1):x} + Z_{x:x}) $</p>\\n\\n<p>For instance, for the 4d6k3 distribution, the mean is 15869/1296 (or 12.2446). And this can be verified (quite easily in Mathematica) by using either techmologist\\'s cominatorial formula or by using the expected value of the sum of the three highest order statistics shown above or by enumerating all outcomes or by simulating a large number of dice throws.</p>\\n\\n<p>However,</p>\\n\\n<p>$ S \\\\\\\\not= Z_{(x-y+1):x} + Z_{(x-y+2):x} + \\\\\\\\ldots + Z_{(x-1):x} + Z_{x:x} $ (WRONG)</p>\\n\\n<p><strong>EDIT:</strong> Thanks to Douglas Zare for explaining that the LHS and RHS are in fact equal... This helped me realize that my real confusion was that the pmf of S cannot be calculated easily by taking the convolution of the pmfs of the second through fourth order statistics, because the order statistics are not independent RVs. And that leads to a follow-up question: In a case like this, how would you find the convolution of the pmfs of the order statistics on the RHS, given that order statistics are not independent RVs? I will think about a better way to phrase this question and post it separately. Thanks again!</p>\\n\\n<p>The question is whether my intuition is correct: <strong>Is there some way to express the pmf $f$ of $S$ in terms of the pmfs of the order statistics? And if not, what\\'s wrong with my intuition that there should be some easy way to calculate the pmf $f$ from the pmf of the  order statistics?</strong></p>\\n\\n<p>BTW, this isn\\'t homework. I\\'m just a roleplayer, not a statistician, trying to satisfy my curiosity whether this dice mechanic can be analyzed and expressed more simply using order statistics. Thanks.</p>\\n',\n",
       " '<p>I have a brief question. I am having trouble finding a good reference that explains what crisp logic is. </p>\\n\\n<p>What I think: </p>\\n\\n<p>I have two classification models, a decision tree and a ruleset, which I think are crisp models. Since they say that an instance is either class A or not. </p>\\n\\n<p>I have another classification model, a logistic regression, that is not crisp, since it gives the probability that an instance belongs to this class. </p>\\n\\n<p>I want to mention this in a presentation that I have later today. I tried looking up this subject, but it would be great if somebody could just confirm how I interpreted it. I could also use a good reference to use in my paper. </p>\\n\\n<p>Thanks, </p>\\n',\n",
       " \"<p>The vast majority of GPUs in circulation only support single precision floating point.</p>\\n\\n<p>As far as the title question, you need to look at the data you'll be handling to determine if single precision is enough for you.  Often, you'll find that singles are perfectly acceptable for >90% of the data you handle, but will fail spectacularly for that last 10%; unless you have an easy way of determining whether your particular data set will fail or not, you're stuck using double precision for everything.</p>\\n\",\n",
       " '<p>I have data on tariff rates and a proxy for tariff evasion (that is common in the literature). The data spans a couple of years before the country I\\'m studying implements a tariff reform and lowers its tariffs. The data also spans many years after the implementation of this tariff reform. Now I\\'m looking for a suitable way of analyzing the effect this tariff reduction has had on evasion (given it\\'s a good proxy). The hypothesis, perhaps obviously, is that a lowering of the tariff rate will lead to a decrease in evasion. I\\'ve read about the difference-in-difference method but here I don\\'t have a \"control group\". I\\'m quite a new practitioner to econometrics so any suggestions will do. I\\'m thinking that even though the simple set up (sorry for the sloppy notation):  evasion(i) = intercept + B1(tariff(i)) + error(i) were to be run on some year(s) before and after the tariff reform and a reduced effect on tariff is found, there are possibly a number of things that could have influenced this change in evasion other than the tariff reduction. What are the the problems and possibilities with this data-set?  Any suggestions would be much appreciated. </p>\\n\\n<p>Thanks! /Oscar</p>\\n',\n",
       " '<p>I\\'d start with:</p>\\n\\n<p>Casella, George; George, Edward I. (1992). \"<a href=\"http://www.jstor.org/stable/2685208\" rel=\"nofollow\">Explaining the Gibbs sampler</a>\". <em>The American Statistician</em> <strong>46</strong> (3): 167–174. (<a href=\"http://biostat.jhsph.edu/~mmccall/articles/casella_1992.pdf\" rel=\"nofollow\">FREE PDF</a>)</p>\\n\\n<blockquote>\\n  <p><strong>Abstract</strong>: Computer-intensive algorithms, such as the Gibbs sampler, have become increasingly popular statistical tools, both in applied and theoretical work. The properties of such algorithms, however, may sometimes not be obvious. Here we give a simple explanation of how and why the Gibbs sampler works. We analytically establish its properties in a simple case and provide insight for more complicated cases. There are also a number of examples.</p>\\n</blockquote>\\n\\n<p><em>The American Statistician</em> is often a good source for short(ish) introductory articles that don\\'t assume any prior knowledge of the topic, though they do assume you have the background in probability and statistics that could reasonably be expected of a member of the <a href=\"http://www.amstat.org\" rel=\"nofollow\">American Statistical Association</a>.</p>\\n',\n",
       " '<p>I am confused between what types of problems these three models capture, and their applications:</p>\\n\\n<ul>\\n<li><a href=\"http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\" rel=\"nofollow\">Latent Dirichlet Allocation (LDA)</a></li>\\n<li><a href=\"http://en.wikipedia.org/wiki/Dirichlet_process\" rel=\"nofollow\">Dirichlet Processes</a> and <a href=\"http://en.wikipedia.org/wiki/Pitman%E2%80%93Yor_process\" rel=\"nofollow\">Pitman-Yor processes</a></li>\\n<li><a href=\"http://en.wikipedia.org/wiki/Hierarchical_Dirichlet_process\" rel=\"nofollow\">Hierarchical Dirichlet Process</a> (HDP) &amp; Hierarhical Pitman-Yor Processes</li>\\n</ul>\\n\\n<p>More than formal definitions (which I can find on Wikipedia), I am looking for the <strong>intuition</strong> behind each of them ( how they build on each other).</p>\\n\\n<p>Perhaps, more specifically:</p>\\n\\n<ul>\\n<li>Is the difference between LDA and HDP that LDA is parameteric (i.e. I need to pre-specify the number of topics) whereas HDP is non-parametric? (and therefore I don\\'t need to know how many topics I have)</li>\\n<li>What is the difference between a Dirichlet Processes and Pitman-Yor Processes? </li>\\n<li>What is the difference between a non-parametric process (e.g. DP) and a hierarhical non-parametric process (e.g. HDP)?</li>\\n</ul>\\n',\n",
       " '<p>This is not a direct answer to your question (\\'Which type of averaging to choose\\'), but rather a recommendation to avoid calculating averages at all:</p>\\n\\n<p>Your scenario seems to look like a case for <a href=\"http://statcomp.ats.ucla.edu/mlm/default.htm\" rel=\"nofollow\">hierarchical/ multilevel models</a> (MLM), as data are perfectly nested. You have three levels of random effects: pixels (Level 1) nested in cells (L2), nested in fields (L3), nested in wells (L4). Treatments should be treated as fixed effects. </p>\\n\\n<p>You are only interested in the effect of treatment; the MLM method takes care of the different variances of each level and gives you also an estimate of how much variance is explained by which level. So you do not \\'lose\\' any variance by treating an averaged value as \\'the measurement\\', but you estimate the model on the level of raw data.</p>\\n\\n<p>This method, however, calls for a sufficient number of groups for each random effect (i.e., enough pixels, enough cells, enough fields, enough wells). As you are not interested in cross level interactions, general recommendations say something like 10 to 30 units minimum (of course, depending on the specific scenario, etc.; see, e.g., <a href=\"http://books.google.com/books/about/Multilevel_analysis.html?id=LEfj0Nu2D8UC\" rel=\"nofollow\">here</a>).</p>\\n',\n",
       " '<p>How can one easily and quickly grasp the idea of Bayesian statistics and modelling? I can understand the Bayesian theorem on conditional probabilities, I understand how frequentist statistics works (test statistics, p-values, hypothesis testing...), I understand <a href=\"http://stats.stackexchange.com/questions/2356/are-there-any-examples-where-bayesian-credible-intervals-are-obviously-inferior-t/6431#6431\">some ideas of Bayesian statistics</a>, but how does Bayesian inference work (technically) - that\\'s still a little mystery to me.</p>\\n',\n",
       " '<p>How to use / interpret the coefficients from a regression model with categorical variables to get predicted variables depends on how your variables are coded.  There are many different coding schemes (see <a href=\"http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm\" rel=\"nofollow\">here</a> for a good overview).  It sounds like you used \\'reference cell coding\\', which most people call \\'dummy coding\\'.  I gather your race1 category is the reference category.  In this case, the intercept is the mean of the race1 group.  To compute the predicted value, you would solve the equation using whatever values for other variables apply and omitting the coefficients for the other categories (i.e., race2 &amp; race3).  There is some good, relevant info <a href=\"http://stats.stackexchange.com/questions/24242/how-to-apply-coefficient-term-for-factors-and-interactive-terms-in-a-linear-equa\">here</a>, and <a href=\"http://stats.stackexchange.com/questions/21282/regression-based-for-example-on-days-of-week\">here</a>.  </p>\\n\\n<p><strong>edit:</strong> The way the question is phrased made me think about situations in which there is only <em>one</em> factor in the model, however, @Michelle raises the question of the more general case.  To keep this relatively simple, imagine a case with just two factors, e.g. race and sex, plus some continuous covariates.  Using reference cell coding, we will create a dummy for male.  Now, solving the regression equation without including any of the factor coefficients (i.e., just the intercept + continuous covariates) yields the predicted mean of the reference cell, which in this case is the <em>race1 female</em> group.  Should you want to know the value for race1 males, you would solve as above, but also include the coefficient for male.  If you wanted to ignore sex, or make a prediction for a mixed-sex group, you would calculate a weighted average of the above two predictions.  Obviously, this will get more complicated as the number of factors, $J$, increases, but the pattern should be clear enough.  </p>\\n',\n",
       " '<p>I want to run a binary logistic regression to model the presence or absence of conflict (dependent variable) from a set of independent variables over a 10 year period (1997-2006), with each year having 107 observations. My independents are: </p>\\n\\n<ul>\\n<li>land degradation (categorical for 2 types of degradation); </li>\\n<li>population increase (0- no; 1-yes); </li>\\n<li>livelihood type (0 - type one; 1 - type two); </li>\\n<li>population density (three levels of density); </li>\\n<li><a href=\"http://en.wikipedia.org/wiki/NDVI\">NDVI</a> continuous (max. veg productivity); </li>\\n<li>NDVI$_{t-1}$ (decline in veg from the previous year - 0 - no; 1 -yes) and</li>\\n<li>and NDVI$_{t-2}$ (decline in veg from two years past - 0- no; 1- yes). </li>\\n</ul>\\n\\n<p>I am fairly new to it all - this is a project my lecturer has given me - and so I would be grateful of some advice or guidance. I have tested for multicolliniarity already.</p>\\n\\n<p>Essentially my data is split up into 107 units of observation (spatial regions) covering 10 years (1070 in total) and for every unit of observation it gives be a \\'snapshot\\' value of conditions of the independent variables at that time within that unit (region). I want to know how to set up my logistic regression (or table) to recognize the 107 values of each year separately so that the temporal NDVI changes between different unit years can be assessed?</p>\\n',\n",
       " '<p>A convex loss function that has a minimum guarantees that there is a minimum loss that can be reached by going \"downhill\" from whatever point you\\'re at.  So your supervised learning algorithm, which is typically trying to minimize some loss function, will (unless something goes wrong) be able to find the \"best\" result, where \"best\" is relative to the specified loss function.  Without convexity, you might have multiple local minima, and your algorithm can find one which is, local minimum though it may be, nonetheless very poor relative to the global minimum.</p>\\n\\n<p>Edit: As mbq points out in comments, some algorithms will still be able to find global minima (under certain conditions), but many algorithms are local optimizers that use convexity assumptions because it makes finding a minimum a lot simpler.</p>\\n\\n<p>Nonlinear classification is important because it greatly expands the possibilities for good classification rules.  Linear rules imply that everything on one side of a straight line (plane in multiple dimensions) gets one classification, everything on the other gets the other (if there are two classes.)  Nonlinear rules allow much more flexibility, e.g., everything inside a circle gets one classification, everything outside gets another.  Of course, you can mimic a nonlinear classification rule by transforming your inputs to a linear classification rule appropriately, but what is appropriate?  Much better to have an algorithm in effect figure that out for you, more or less, than have to guess, and guess, and guess again - unless of course you already have a good idea.  </p>\\n',\n",
       " '<p>I have finally been able to wrap my head around the mechanics of how to initialize and train a multivariate Gaussian mixture model using expectation maximization algorithm. So I wonder how difficult this GMM and EM task is in comparison to all other <strong>common</strong> algorithms and models in machine learning. I appreciate any feedback. Thank you.</p>\\n',\n",
       " '<p>I have 108 counts taken from 36 field points. I took three counts from each point at different times, also, the 36 field points are divided in a altitude factor of three levels (12 points at each level). I have also six numerical independent variables measured at each count. I want to make a generalized linear model to see which of my independent variables contribute significantly to the counts observed (time is a factor of three levels).</p>\\n\\n<p>I know that if I had only factors I would use a split-plot design to see differences among levels, where I know how to nest, but I have six numerical independent variables. Transform data to do a lm is not working, that´s why I am trying to use a <code>glm()</code>.</p>\\n\\n<p>My question is, how can I nest my data in a <code>glm()</code> function?</p>\\n',\n",
       " \"<p>I've got a particular MCMC algorithm which I would like to port to C/C++. Much of the expensive computation is in C already via Cython, but I want to have the whole sampler written in a compiled language so that I can just write wrappers for Python/R/Matlab/whatever.</p>\\n\\n<p>After poking around I'm leaning towards C++. A couple of relevant libraries I know of are Armadillo (http://arma.sourceforge.net/) and Scythe (http://scythe.wustl.edu/). Both try to emulate some aspects of R/Matlab to ease the learning curve, which I like a lot. Scythe squares a little better with what I want to do I think. In particular, its RNG includes a lot of distributions where Armadillo only has uniform/normal, which is inconvenient. Armadillo seems to be under pretty active development while Scythe saw its last release in 2007.</p>\\n\\n<p>So what I'm wondering is if anyone has experience with these libraries -- or others I have almost surely missed -- and if so, whether there is anything to recommend one over the others for a statistician very familiar with Python/R/Matlab but less so with compiled languages (not completely ignorant, but not exactly proficient...).</p>\\n\",\n",
       " \"<p>This isn't possible unless the mean of epsilon depends on X (or$\\\\\\\\beta=0$) and I think it most likely there is a mistake in the paper.  </p>\\n\\n<p>X is described as the systemic risk factor and I think is meant to be non random so that simplifies things somewhat.</p>\\n\\n<p>The mean of $\\\\\\\\varepsilon_{PD,t}$ must vary depending on PD and t, as it must exactly cancel out the structural part of the model (ie $E(\\\\\\\\varepsilon_{PD,t})=-\\\\\\\\beta_{PD}*X_t$. This seems so odd that I can't imagine it is what they mean, although it does literally meet their definition.</p>\\n\\n<p>If you want to simulate this in R rnorm() can take a vector of variable means to create the epsilons (with variance one and mean defined as above); but I doubt this would be worthwhile.</p>\\n\\n<pre><code>&gt; Bpd&lt;-0.4; Xt&lt;-rnorm(100000,0,1)\\n&gt; \\n&gt; E.pd&lt;-rnorm(100000,-Bpd*Xt,1)\\n&gt; \\n&gt; Pt = (Bpd)*Xt + E.pd \\n&gt; c(mean(Pt), var(Pt))\\n[1] 0.004140578 0.997591190\\n</code></pre>\\n\",\n",
       " '<p>I am working on <a href=\"http://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test#Assumptions\" rel=\"nofollow\">the assumption page of the Wilcoxon signed-rank test</a> (the wilcox \"paired test\") in Wikipedia.</p>\\n\\n<p>I was able to locate a reference for the assumption that I wrote there, which are:</p>\\n\\n<p>Let $Z_i=X_i – Y_i$ for $i = 1, \\\\\\\\ldots , n$.</p>\\n\\n<ol>\\n<li>The differences $Z_i$ are assumed to be independent. </li>\\n<li>Each $Z_i$ comes from\\nthe same continuous population, and is symmetric about a common\\nmedian θ .</li>\\n<li>The values which $X_i$ and $Y_i$ represent are ordered (at\\nleast the ordinal level of measurement), so the comparisons\\n\"greater than\", \"less than\", and \"equal to\" are useful.</li>\\n</ol>\\n\\n<p>However, I get this nagging feeling that I am missing something here.  Can anyone correct/expand on this?</p>\\n\\n<p>Thanks.</p>\\n',\n",
       " '<p>Build two forests using the same input data and different response variables, one forest for A and one for B.</p>\\n\\n<p>You can then combine the output of the two trees to create a prediction {A,B}.</p>\\n',\n",
       " '<p>I´m modeling the effect of pregnancy on the outcome of a disease (dead-alive). Approx 40% of the patients did become pregnant after the time of diagnosis-but at different points in time. So far I´ve done KM plots showing a clear protective effect of pregnancy on survival and also a regular Cox model-however these have been modeled using only a dichotomised pregnancy variable and assuming the effect is present from the time of diagnosis which is clearly unrealistic since the median time to pregnancy is 4 years from diagnosis.</p>\\n\\n<p>What kind of model would absorb the effect of multiple pregnancies at different time points after diagnosis? Would it be correct to model the pregnancies interacting with time (which would require some serious data reconstruction-any automated software that could help with this?) or is there another preferred modeling strategy for these problems? Also what is the preferred plotting strategy for these problems?</p>\\n',\n",
       " '<p>Khan Academy has some nice <strong>introductory/beginner</strong> videos on statistics: <a href=\"http://www.khanacademy.org/#statistics\">http://www.khanacademy.org/#statistics</a></p>\\n',\n",
       " '<p>Turns out I was not using the ddof argument properly - Warren Weckesser explained in a stackoverflow reply that: \"ddof is the <em>change</em> to make to the default degrees of freedom. The default is one less than the length. So you do not have to specify ddof at all\". Indeed, if passing no ddof (or ddof=0) I get:</p>\\n\\n<blockquote>\\n  <p>print scipy.stats.chisquare(np.array(observed), np.array(expected), ddof=0)\\n  (10.112318133864241, <strong>0.18229735660914098</strong>)</p>\\n</blockquote>\\n',\n",
       " \"<p>I assume that A through D are different symptoms, say, and 1 and 2 are the two raters. As you tagged this in Stata, I will build a Stata example. Let us first simulate some data: we have a bunch of subjects with two uncorrelated traits, and a battery of questions, tapping upon these traits. The two raters have different sensitivities to each of the traits: the first rater is a tad more likely than the second rater to give a positive answer on question A, but slightly less likely to give a positive answer on question B, etc.</p>\\n\\n<pre><code>    clear\\n    set seed 10101\\n    set obs 200\\n\\n    * generate orthogonal individual traits\\n    generate trait1 = rnormal()\\n    generate trait2 = rnormal()\\n\\n    * raters' interecepts for individual questions\\n    local q1list 0.3 0.7 -0.2 -0.4\\n    local q2list 0.5 0.5 0 -0.5\\n\\n    * prefixes\\n    local letters a b c d\\n\\n    forvalues k = 1/4 {\\n      local thisletter : word `k' of `letters'\\n      local rater1     : word `k' of `q1list'\\n      local rater2     : word `k' of `q2list'\\n      generate byte `thisletter'1 = ( `k'/3*trait1 + (3-`k')/5*trait2 + 0.3*rnormal() &gt; `rater1' ) \\n      generate byte `thisletter'2 = ( `k'/3*trait1 + (3-`k')/5*trait2 + 0.3*rnormal() &gt; `rater2' ) \\n    }\\n</code></pre>\\n\\n<p>This should produce something like</p>\\n\\n<pre><code>      . list a1-d2 in 1/5, noobs\\n\\n       +---------------------------------------+\\n      | a1   a2   b1   b2   c1   c2   d1   d2 |\\n      |---------------------------------------|\\n      |  1    1    0    0    1    0    1    1 |\\n      |  0    0    0    0    0    1    1    1 |\\n      |  0    0    0    0    0    0    0    0 |\\n      |  1    0    0    1    1    1    1    1 |\\n      |  0    0    0    1    1    1    1    1 |\\n      +---------------------------------------+\\n</code></pre>\\n\\n<p>which I hope resembles your data, at least in terms of the existing variables.</p>\\n\\n<p>A fully non-parametric summary of the inter-rater agreement can be constructed by converting the binary representation into a decimal representation. The outcome a1=0, b1=0, c1=0, c4=0 is 0000b=0; the outcome in the first observation is 1011b = 11, etc. Let us produce this encoding:</p>\\n\\n<pre><code>      generate int pattern1 = 0\\n      generate int pattern2 = 0\\n      forvalues k = 1/4 {\\n        local thisletter : word `k' of `letters'\\n        replace pattern1 = pattern1 + `thisletter'1 * 2^(4-`k')\\n        replace pattern2 = pattern2 + `thisletter'2 * 2^(4-`k')\\n        tab pattern*\\n      }\\n</code></pre>\\n\\n<p>This should produce something like</p>\\n\\n<pre><code>      . list  a1- d2 pat* in 1/5, noobs\\n\\n        +-------------------------------------------------------------+\\n        | a1   a2   b1   b2   c1   c2   d1   d2   pattern1   pattern2 |\\n        |-------------------------------------------------------------|\\n        |  1    1    0    0    1    0    1    1         11          9 |\\n        |  0    0    0    0    0    1    1    1          1          3 |\\n        |  0    0    0    0    0    0    0    0          0          0 |\\n        |  1    0    0    1    1    1    1    1         11          7 |\\n        |  0    0    0    1    1    1    1    1          3          7 |\\n        +-------------------------------------------------------------+\\n</code></pre>\\n\\n<p>Now, these patterns are perfectly comparable using <code>kap</code>:</p>\\n\\n<pre><code>        . kap pattern1 pattern2\\n\\n\\n        Agreement   Exp.Agrmt    Kappa     Std. Err.      Z        Prob&gt;Z\\n        -----------------------------------------------------------------\\n        54.00%        17.91%     0.4396     0.0308      14.25      0.0000\\n</code></pre>\\n\\n<p>You can play with the sample size or with the differences between raters to produce a non-significant answer :). This kappa suffers from a serious drawback: it does not reflect the fact of having some common items: the patterns 0001 and 0000, even though they match by 75%, would be considered non-matches within this approach. So it is an extremely conservative measure of the inter-rater agreement.</p>\\n\\n<p>To get fair estimates of all the ICCs, you would need to run a cross-classified mixed model. Let us first <code>reshape</code> the data to make it possible:</p>\\n\\n<pre><code>        generate long id = _n\\n\\n        * reshape the raters\\n        reshape long a b c d , i(id) j(rater 1 2)\\n\\n        * reshape the items\\n        forvalues k = 1/4 {\\n          local thisletter : word `k' of `letters'\\n          rename `thisletter' q`k'\\n        }\\n        reshape long q , i(id rater) j(item 1 2 3 4)\\n</code></pre>\\n\\n<p>Now, we can run <code>xtmelogit</code> (or <code>gllamm</code> if you like it better) on this data:</p>\\n\\n<pre><code>        . xtmelogit q || _all : R.rater || _all: R.item || _all: R.id, nolog\\n\\n        Note: factor variables specified; option laplace assumed\\n\\n        Mixed-effects logistic regression               Number of obs      =      1600\\n        Group variable: _all                            Number of groups   =         1\\n\\n                                                        Obs per group:\\n        min =      1600\\n        avg =    1600.0\\n        max =      1600\\n\\n        Integration points =   1                        Wald chi2(0)       =         .\\n        Log likelihood = -697.55526                     Prob &gt; chi2        =         .\\n\\n        ------------------------------------------------------------------------------\\n        q            |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\\n        -------------+----------------------------------------------------------------\\n        _cons        |  -.7795316   .9384147    -0.83   0.406    -2.618791    1.059727\\n        ------------------------------------------------------------------------------\\n\\n        ------------------------------------------------------------------------------\\n        Random-effects Parameters    |   Estimate   Std. Err.     [95% Conf. Interval]\\n        -----------------------------+------------------------------------------------\\n        _all: Identity               |\\n        sd(R.rater)                  |   .1407056   .1627763      .0145745    1.358408\\n        -----------------------------+------------------------------------------------\\n        _all: Identity               |\\n        sd(R.item)                   |   1.797133   .6461083      .8882897    3.635847\\n        -----------------------------+------------------------------------------------\\n        _all: Identity               |\\n        sd(R.id)                     |    3.18933   .2673165      2.706171    3.758751\\n        ------------------------------------------------------------------------------\\n        LR test vs. logistic regression:     chi2(3) =   793.71   Prob &gt; chi2 = 0.0000\\n\\n        Note: LR test is conservative and provided only for reference.\\n        Note: log-likelihood calculations are based on the Laplacian approximation.\\n</code></pre>\\n\\n<p>This is a cross-classified model with three random effects: subjects, raters and items, assuming that they are uncorrelated (which is wrong for this data; see below). Let us now estimate the ICCs:</p>\\n\\n<pre><code>        . local Vrater ( exp(2*_b[lns1_1_1:_cons]) )\\n        . local Vitem ( exp(2*_b[lns1_2_1:_cons]) )\\n        . local Vid ( exp(2*_b[lns1_3_1:_cons]) )\\n        . nlcom `Vrater' / (`Vrater' + `Vitem' + `Vid' + _pi*_pi/3 )\\n\\n        -----------------------------------------------------------------------\\n        q     |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\\n        ------+----------------------------------------------------------------\\n        _nl_1 |   .0011847   .0027384     0.43   0.665    -.0041824    .0065519\\n        -----------------------------------------------------------------------\\n\\n        . nlcom `Vid' / (`Vrater' + `Vitem' + `Vid' + _pi*_pi/3 )\\n\\n        ------------------------------------------------------------------------\\n        q     |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\\n        ------+----------------------------------------------------------------\\n        _nl_1 |   .6086839   .0903816     6.73   0.000     .4315393    .7858285\\n        ------------------------------------------------------------------------\\n\\n        . nlcom `Vitem' / (`Vrater' + `Vitem' + `Vid' + _pi*_pi/3 )\\n\\n        -----------------------------------------------------------------------\\n        q     |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\\n        ------+----------------------------------------------------------------\\n        _nl_1 |    .193265   .1121376     1.72   0.085    -.0265206    .4130506\\n        -----------------------------------------------------------------------\\n</code></pre>\\n\\n<p>(<em>Hint</em>: I figured out the names of the parameters by <code>matrix list e(b)</code>.)</p>\\n\\n<p>These are ICCs corresponding to raters, subjects and items, respectively. The zero ICC of the raters actually makes sense in the context of how the data were generated: there is no systematic effect in the sense that one rater consistently rates the condition better or worse than the other rater. There is an interaction between rater and item, but the model does not reflect it. True to life would be something like </p>\\n\\n<pre><code> xtmelogit q ibn.item##ibn.rater, nocons || id:\\n</code></pre>\\n\\n<p>With this specification, you would have to get ICCs by an even more complicated mix of the variance components and the point estimates from the fixed effects part of the model.</p>\\n\\n<p>If you have the patience (or a powerful computer), you can specify <code>intp(7)</code> or something like that to get an approximation more accurate than the Laplace approximation (a single point at the mode of the distribution of the random effects).</p>\\n\",\n",
       " '<p>I have a dataset, which contains measurments from many different conditions. Since my hypothesis suggested a large difference for the measurements in each condition, in order to clean the data, I analyzed all conditions independently.</p>\\n\\n<p>I.e. I grouped all measurments from one conditions into quartiles, calculated the interquartile range and then removed any datapoint, which was further than 1.5 times the size of the interquartile range from the median.</p>\\n\\n<p>Now someone looked over the numbers and remarked, that the rate of removed data was much higher than usual. I re-checked my calculations several times, and arrived at such a high ratio of outliers each time.</p>\\n\\n<p>Now I was thinking, that if I had analyzed all measurements from each condition together, most likely much less measurements would have been removed, due to the differences between the conditions. However as far as I understand these cleaning methods, they are meant to be used on single (gaußian) distributions, and not the sum of two or more distributions. So which method for data cleaning would actually be the correct one: Cleaning all measurements together or cleaning each condition separately?</p>\\n',\n",
       " '<p>I am unaware of a name for this distribution, but the probabilities are straightforward to work out.  Not by computing all possible draws, though!  (There are far too many.)</p>\\n\\n<p>The technique is quite general, so <strong>let\\'s broaden the problem statement</strong>.  Suppose we have $n_i \\\\\\\\gt 0$ marbles of color $j$, $j=1,2,\\\\\\\\ldots,l$, for a total of $n=n_1+\\\\\\\\cdots +n_l$ marbles.  For any subset of these colors $J = \\\\\\\\{j_1,\\\\\\\\ldots,j_s\\\\\\\\}$, what fraction of all <a href=\"http://en.wikipedia.org/wiki/Simple_random_sample\" rel=\"nofollow\">simple random samples</a> of size $k$ contain exactly these colors?  The answer is immediate: it\\'s the number of all simple random samples of size $k$ from a bag containing only marbles whose colors are in $J$, divided by the number of all simple random samples of size $k$ from the original bag:</p>\\n\\n<p>$$f(J, k) = \\\\\\\\binom{n_{j_1}+n_{j_2}+\\\\\\\\cdots+n_{j_s}}{k} / \\\\\\\\binom{n}{k}$$</p>\\n\\n<p>By the <a href=\"http://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle\" rel=\"nofollow\">Principle of Inclusion-Exclusion</a>, the fraction of simple random samples of size $k$ without all $l$ colors equals</p>\\n\\n<p>$$\\\\\\\\sum_{\\\\\\\\varnothing \\\\\\\\ne J \\\\\\\\subset \\\\\\\\{1,\\\\\\\\ldots,l\\\\\\\\}}(-1)^{l-\\\\\\\\vert J\\\\\\\\vert}f(J,k).$$</p>\\n\\n<p>The computational effort, properly coded, is $O(2^l)$--fine for small numbers of distinct colors--and just $O(l)$ when all the $n_i$ are equal, because all terms for $J$ of a given size will be equal and only sizes $1$ through $l$ need be considered.  A <em>Mathematica</em> program that directly expresses the general formula is</p>\\n\\n<pre><code>h[x_, k_Integer] /; Total[x] &gt;= k &gt; 0 := \\nWith[{n = Total[x], l = Length[x]},\\n  Sum[(-1)^(l - j) Binomial[m, k]/Binomial[n, k], {j, 1, l}, {m, Total /@ Subsets[x, {j}]}]\\n]\\n</code></pre>\\n\\n<p>Using it, it takes 0.015 seconds (on one core) to find <code>h[{1000,1000,1000,1000,1000}, k]</code> for <code>k</code> from 5 through 34.  Rounded, the values are</p>\\n\\n<pre><code>0.038, 0.115, 0.215, 0.323, 0.428,     0.523, 0.607, 0.679, 0.739, 0.789,\\n0.83,  0.863, 0.89,  0.912, 0.929,     0.943, 0.955, 0.964, 0.971, 0.977, \\n0.981, 0.985, 0.988, 0.991, 0.992,     0.994, 0.995, 0.996, 0.997, 0.998 \\n</code></pre>\\n\\n<p>Here is a plot of these values:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/VCXvc.png\" alt=\"Discrete plot\"></p>\\n\\n<p>For instance, the chance that all five colors are obtained in the first five draws is $.038 = 3.8\\\\\\\\%$ (more precisely, it\\'s $0.038476...$).  As a check, if the sampling were <em>with</em> replacement this chance wouldn\\'t change much and it would be computed as $1 \\\\\\\\times \\\\\\\\frac{4}{5} \\\\\\\\times \\\\\\\\frac{3}{5} \\\\\\\\times \\\\\\\\frac{2}{5} \\\\\\\\times \\\\\\\\frac{1}{5} = 0.0384$.</p>\\n\\n<p>By inspection, <strong>the answer to the question</strong> (concerning the 50%/70%/90% points) is $10, 13, 18$.  For instance, $k=17$ draws have less than a 90% chance of exhibiting all five colors (the chance is $0.89$) while $k=18$ draws have at least a 90% chance (it\\'s actually $0.912$). </p>\\n',\n",
       " '<p>Speaking from the perspective of a psychologist with only slight statistical sophistication: When you introduce the method, also introduce the tools. If you tell most researchers in my field a long story about a great new method, they\\'re going to spend the whole time worried that the punchline is \"and all you have to do is brush up on your differential calculus and then take a two week training course!\" (or \"and buy a $2000 stats package!\" or \"and adapt 5000 lines of Python and R code!\"). Whereas if there\\'s an implementation of the method available in the stats package they already use, or in a piece of free software with a comprehensible GUI, and they can get up to speed on it in a day or two, they might be willing to give it a try. </p>\\n\\n<p>I\\'m aware that this approach can seem venal and unscientific, but it\\'s easy for people to fall into when they\\'re worried about grants and publications, and don\\'t see learning huge amounts of math as likely to help them keep their jobs. </p>\\n',\n",
       " \"<p>Pearson's R Value shows the correlation of two arrays of real numbers. I would like to calculate an correlation value for text which is associated with numbers then a P value to judge the statistical significance of this relationship. Is this possible with non-parametric statistics?\\nThank you</p>\\n\",\n",
       " '<p>I have been given a project to implement an <a href=\"http://en.wikipedia.org/wiki/Epidemic_model#The_SIRS_Model\" rel=\"nofollow\">SIRS model</a>. While searching how to do it, I found this site and a <a href=\"http://stats.stackexchange.com/q/16437/930\">question related to epidemic model</a>. It is very much related to my project and is quite helpful. However, since I\\'m new to this topic, can you please help me on how to start implementing SIRS model. I have to implement one simulator in Java. I don\\'t have any idea on how to start the implementation. I will be really grateful if you help me. </p>\\n',\n",
       " '<p>There are a range of possibilities described by J.W. Gillard in <a href=\"http://www.cardiff.ac.uk/maths/resources/Gillard_Tech_Report.pdf\">An Historical Overview\\nof Linear Regression with Errors in both Variables</a> </p>\\n\\n<p>If you are not interested in details or reasons for choosing one method over another, just go with the simplest, which is to draw the line through the centroid $(\\\\\\\\bar{x},\\\\\\\\bar{y})$ with slope $\\\\\\\\hat{\\\\\\\\beta}=s_y/s_x$, i.e. the ratio of the observed standard deviations (making the sign of the slope the same as the sign of the covariance of $x$ and $y$); as you can probably work out, this gives an intercept on the $y$-axis of $\\\\\\\\hat{\\\\\\\\alpha}=\\\\\\\\bar{y}-\\\\\\\\hat{\\\\\\\\beta}\\\\\\\\bar{x}.$  </p>\\n\\n<p>The merits of this particular approach are </p>\\n\\n<ol>\\n<li>it gives the same line comparing $x$ against $y$ as $y$ against $x$, </li>\\n<li>it is scale-invariant so you do not need to worry about units,</li>\\n<li>it lies between the two ordinary linear regression lines </li>\\n<li>it crosses them where they cross each other at the centroid of the observations, and </li>\\n<li>it is very easy to calculate.</li>\\n</ol>\\n\\n<p>The slope is the geometric mean of the slopes of the two ordinary linear regression slopes. It is also what you would get if you standardised the $x$ and $y$ observations, drew a line at 45&deg; (or 135&deg; if there is negative correlation) and then de-standardised the line.  It could also be seen as equivalent to making an implicit assumption that the variances of the two sets of errors are proportional to the variances of the two sets of observations; as far as I can tell, you claim not to know which way this is wrong. </p>\\n\\n<p>Here is some R code to illustrate: the red line in the chart is OLS regression of $Y$ on $X$, the blue line is OLS regression of $X$ on $Y$, and the green line is this simple method. Note that the slope should be about 5.</p>\\n\\n<pre><code>X0 &lt;- 1600:3600\\nY0 &lt;- 5*X0 + 700\\nX1 &lt;- X0 + 400*rnorm(2001)\\nY1 &lt;- Y0 + 2000*rnorm(2001)\\nslopeOLSXY  &lt;- lm(Y1 ~ X1)$coefficients[2]     #OLS slope of Y on X\\nslopeOLSYX  &lt;- 1/lm(X1 ~ Y1)$coefficients[2]   #Inverse of OLS slope of X on Y\\nslopesimple &lt;- sd(Y1)/sd(X1) *sign(cov(X1,Y1)) #Simple slope\\nc(slopeOLSXY, slopeOLSYX, slopesimple)         #Show the three slopes\\nplot(Y1~X1)\\nabline(mean(Y1) - slopeOLSXY  * mean(X1), slopeOLSXY,  col=\"red\")\\nabline(mean(Y1) - slopeOLSYX  * mean(X1), slopeOLSYX,  col=\"blue\")\\nabline(mean(Y1) - slopesimple * mean(X1), slopesimple, col=\"green\")\\n</code></pre>\\n',\n",
       " '<p>I have an ascii dataset which consists of three columns, but only the last two are actual data. Now I want to create a dotchart of the data by using <code>read.csv(file = \"result1\", sep= \" \")</code>. R reads all three columns. How do I avoid this?</p>\\n',\n",
       " '<p>I am building a Box-Jenkins model in Excel using solver. The model is AR(2).\\nThe data that I have contains trend and seasonality both. </p>\\n\\n<p>I know how to remove seasonality using seasonal indexes and add it back to the forecast. \\nBut, how do I handle trend? If I remove trend from the data, how should I add it back to the forecast?</p>\\n\\n<p>Also, is the excel solver best way to find the AR parameters?</p>\\n',\n",
       " '<p>The t-test with two groups assumes that each group is normally distributed with the same variance (although the means may differ under the alternative hypothesis). That is equivalent to a regression with a dummy variable as the regression allows the mean of each group to differ but not the variance. Hence the residuals (equal to the data with the group means subtracted) have the same distribution --- that is, they are normally distributed with zero mean.</p>\\n\\n<p>A t-test with unequal variances is not equivalent to a one-way ANOVA.</p>\\n',\n",
       " '<p>Why not do a more conventional thing and do one regression with gender as a covariate?  Then a statistically significant gender effect would indicate that gender is important and more specifically include a gender interaction term with your IV.  If the interaction is statistically significant that indicates that the male and female slopes differ. Make the test one sided and you have your answer.</p>\\n',\n",
       " '<p>Here is an ANOVA model with one between-subject factor (condition; 4 levels) and one within-subject factor (trial_seq; 20 levels).</p>\\n\\n<pre><code>amod = aov(decision_quality ~ condition*trial_seq +\\n       Error(user_id/trial_seq), data = d.task1)\\nsummary(amod)\\n</code></pre>\\n\\n<p>I want to do  a pairwise analysis for both the between-subject factor and the within-subject factor. I found that it is not straightforward to use a function like TukeyHSD() for a within-subject factor.  I researched this problem and found some suggestions in <a href=\"http://stats.stackexchange.com/questions/575/post-hocs-for-within-subjects-tests\">another thread</a>.</p>\\n\\n<p>However, my lack of statistical background prevented me from fully understanding vignettes coming with the multcomp package. Anyway, I tried some random statements like:</p>\\n\\n<pre><code>summary(glht(amod, linfct = mcp(trial_seq = \"Tukey\")))\\n</code></pre>\\n\\n<p>However, it generates the following error:</p>\\n\\n<pre><code>Error in model.matrix.aovlist(model) : \\n  ‘glht’ does not support objects of class ‘aovlist’\\nError in summary(glht(amod, linfct = mcp(trial_seq = \"Tukey\"))) : \\n  error in evaluating the argument \\'object\\' in selecting a method for     \\nfunction \\'summary\\': Error in factor_contrasts(model) : \\n  no ‘model.matrix’ method for ‘model’ found!\\n</code></pre>\\n\\n<ul>\\n<li>What did I do wrong?</li>\\n</ul>\\n',\n",
       " '<p>Your first SE formula is correct. The second SE formula which concerns sensitivity should have the total number of positive cases in the denominator:\\n  $$SE_\\\\\\\\text{sensitivity} = \\\\\\\\sqrt{ \\\\\\\\frac{SENS(1-SENS)}{TP+FN}} $$</p>\\n\\n<p>The logic is that sensitivity = $\\\\\\\\frac{TP}{TP+FN}$, and the denominator in the SE formula is the same.</p>\\n\\n<p>As @onestop pointed out in their comment <a href=\"http://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval\">methods of calculating a binomial proportion confidence interval</a> can be used here. The method you follow is the normal approximation, however unless you have really large counts other methods like the Wilson interval will be more accurate.</p>\\n',\n",
       " '<p>This is a typical linear programming problem, where both objective and constraints are linear. R has many functions to solve this, for example optim() or package BB. However your decision variables are binary and N is probably big. So you may want to try to solve the dual problem, instead of solving the primal directly.</p>\\n',\n",
       " '<p>I tried to specify a partial proportional odds regression in STATA using the <a href=\"http://www.stata-journal.com/article.html?article=st0097\" rel=\"nofollow\"><code>gologit2</code> command.</a> </p>\\n\\n<p>However, <code>gologit2</code> runs <a href=\"https://www3.nd.edu/~rwilliam/gologit2/tsfaq.html\" rel=\"nofollow\">extraordinarily slow</a> in my dataset (108k observations of 9 vars). For example,</p>\\n\\n<pre><code>ologit PfSt age if bund == 1\\n</code></pre>\\n\\n<p>runs in less than 1 sec, while</p>\\n\\n<pre><code>gologit2 PfSt age if bund == 1, pl\\n</code></pre>\\n\\n<p>which is <a href=\"http://fmwww.bc.edu/repec/bocode/g/gologit2.html\" rel=\"nofollow\">essentially the same</a>, as the <code>pl</code> option enforces parallel lines in all vars, i.e., the <code>ologit</code> specification, takes about a minute on my computer.</p>\\n\\n<p>Calculating the fully specified model is possible with <code>ologit</code> and <code>mlogit</code> (~ 1 min), but not with <code>gologit2</code>, even if I specify for which variables parallel lines should be assumed. \\nThe regression command is</p>\\n\\n<pre><code>ologit PfSt age age_sq gender i.bundesland tt tt2 [fw=size], cluster(bundesland)\\n</code></pre>\\n\\n<p>where <code>PfSt</code> ranges from 0 to 7 and <code>tt</code> capture time trends (<code>tt = year − 1996</code> and <code>tt2 = -1/tt +1</code>, a “phasing out” trend)</p>\\n\\n<p>Now, considering that relaxing the parallel lines assumption for some variables would be identical to adding interaction terms to my model specification, I could change my regression command to</p>\\n\\n<p><code>ologit PfSt age* gender i.bundesland#i.PfSt tt*, cluster(bundesland)</code> would yield me different coefficients per level of <code>PfSt</code> for the one variable for which I would relax the parallel lines assumption. <strong>Is that correct?</strong></p>\\n\\n<p><em>However</em>, the estimation process <strong>fails to converge</strong>, as now (1200th iteration) negative log pseudoelikelihood ratios rise (i.e., drop in absolute values, now &lt;&lt;0.0001), but they are <code>(non concave)</code>. I wonder <strong>why is this the case?</strong>, as both <code>ologit</code> and <code>mlogit</code> did produce results after a few iterations.</p>\\n',\n",
       " '<p>Recently I got a global longitudinal data from several countries, and each county has one outcome variable and two predictors from 1995 to 2008. I found one of the predictors is always missing in each country in 1995, 1997, 1999, and 2001 because that variable was collected every two years before 2001. This situation seriously affects another complete predictor when using a complete case analysis because too many useful information is lost in that four years. Also, it obviously affects the fitting of a time smoothing function. </p>\\n\\n<p>I am pretty wondering whether this situation is appropriate to use the multiple imputation method to generate data. As I know, this case is not a missing at random mechanism, so the multiple imputation may not be a perfect way to deal with my case. I am looking for any advice here to find out a solution to deal with those missing data. Any suggestion is appreciated. </p>\\n',\n",
       " \"<p>This question contains a relatively long prelude, since I want to explain as clearly as possible the motivation for the question. It may well be the case that I am asking the wrong question (i.e. there is a better way of solving the problem than what I am attempting) and I would welcome any good answers along those lines.</p>\\n\\n<p>I have implemented a real time map for a physical quantity over a large region (a continent) that combines many hundreds of data points and uses a Kalman Filter to jointly estimate the coefficients of a linear combination of basis functions (the map) plus a large number of instrumental biases and other nuisance parameters.</p>\\n\\n<p>A problem with this approach is that the data points are distributed in a very uneven way; some regions have a lot of closely spaced data points and other regions are much more sparse. Since the map is represented by a global set of basis functions, there is no real happy medium in choosing the best order of expansion for the generating function. I either have a too high order for the sparse regions leading to spurious artefacts, or too low order for the data rich regions, smoothing out real structure that is supported by the data.</p>\\n\\n<p>In order to be more adaptive to local data density, I am thinking of using some kind of radial basis function (RBF) approach utilising unsupervised clustering (e.g. K-means) to distribute a set number of basis function origins to match the local data density. I still need to use some kind of filtering to estimate the nuisance parameters along with the map, but this RBF approach maintains the linearity of the system so the Kalman Filter can be used. I'd 'just' need to experiment with different forms of the basis functions and their parameters, the number of basis functions to use etc.</p>\\n\\n<p>However, it would be ideal to also incorporate a smoothness constraint, as in the case of a thin plate spline (TPS). This can be expressed as an additional set of linear constraints on the map values at each basis origin and the standard TPS formulation has a closed form solution. What I would like to do is incorporate these linear constraints into the filtering mechanism, as is required due to the noisiness of some parts of the system, rather than the one off minimisation of the TPS. I can vaguely conceive of how this might be done, but my linear algebra skills are not up to the task of fully deriving this (or even demonstrating that it can be done).</p>\\n\\n<p>So my question is whether this approach is feasible and if so what the form of the resulting filter looks like. Is there a specific piece of terminology that succinctly describes what I am trying to do? Is there a standard result out there for this situation?</p>\\n\",\n",
       " '<p>One more method, which has been suggested in other topics, is to just use the arima function with xregs.  Arima seems to be able to make forecasts from a new set of xregs just fine.</p>\\n',\n",
       " '<p>This is a classical example in the field of <a href=\"http://en.wikipedia.org/wiki/Graphical_model\" rel=\"nofollow\">Probabilistic Graphical Models</a> (PGMs). PGMS are widely used in several areas, from text mining to bioinformatics, for problems such as inference and naturally belong within Machine Learning/pattern recognition/artificial intelligence.</p>\\n\\n<p>As you have correctly suggested, the Bayes rule play a major role here. In essence, you can think of PGMs as a simplified representation of a very large joint distribution over many variables (simplified due to independence of variables), and some of the methods consist of repeatedly applying the Bayes rule.</p>\\n\\n<p>A simple example of a PGM is a <a href=\"http://en.wikipedia.org/wiki/Hidden_Markov_model\" rel=\"nofollow\">Hidden Markov Model</a> (HMM), which is especially relevant to your example. Lucky for you, there is very good material for studying PGMs and even more on HMMs:</p>\\n\\n<p>Prof. Daphne Koller from Stanford gives a free PGM course online, and I also recommend her <a href=\"http://rads.stackoverflow.com/amzn/click/0262013193\" rel=\"nofollow\">book</a> if you want a very rigorous (CS/math) treatment of the subject. For HMMs you can just google to find dozens of tutorials and select the one that fits your background the most.</p>\\n\\n<p>Finally, PGMs are just one way of doing this kind of inference and if you want to expand your knowledge of such predictive methods you should look into the more general field of Machine Learning which I think you will find rich, deep and applicable (but maybe that\\'s just me).</p>\\n',\n",
       " '<p>Textbooks typically have nice example plots of of the basis for uniform splines when they\\'re explaining the topic.  Something like a row of little triangles for a linear spline, or a row of little humps for a cubic spline.</p>\\n\\n<p>This is a typical example:</p>\\n\\n<p><a href=\"http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_introcom_a0000000525.htm\" rel=\"nofollow\">http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_introcom_a0000000525.htm</a></p>\\n\\n<p>I\\'m wondering if there is an easy way to generate a plot of the spline basis using standard R functions (like bs or ns).  I guess there\\'s some simple piece of matrix arithmetic combined with a trivial R program which will spit out a pretty plots of a spline basis in an elegant way. I just can\\'t think of it!</p>\\n',\n",
       " '<p>This is one of the classic problems for the \\'Iris\\' data set. <a href=\"http://www2.warwick.ac.uk/fac/sci/moac/people/students/peter_cock/r/iris_plots/\">This is a link</a> to a whole set of plotting projects based on that data set with R code, which you may be able to adapt to your problem.</p>\\n\\n<p>Here is an approach that uses with base R rather than an add-on package.</p>\\n\\n<pre><code>plot(iris$Petal.Length, iris$Petal.Width, pch=21, \\n     bg=c(\"red\",\"green3\",\"blue\")[unclass(iris$Species)], \\n     main=\"Edgar Anderson\\'s Iris Data\")\\n</code></pre>\\n\\n<p>which produces this figure:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/Y2iOi.png\" alt=\"enter image description here\"></p>\\n\\n<p>From there, depending on your plot, you can start messing about with alpha/transparency levels to allow for overplotting, etc. but I would build up from a very basic graph first.</p>\\n\\n<p>While there are many reasons to stick with base R, other packages simplify plotting. Separating out data by a distinguishing feature is one of the strengths of the <a href=\"http://cran.r-project.org/web/packages/ggplot2/index.html\">ggplot2</a> and <a href=\"http://cran.r-project.org/web/packages/lattice/index.html\">lattice</a> packages. ggplot2 makes particularly visually appealing plots. Both packages are <a href=\"http://stats.stackexchange.com/a/30800/2750\">demonstrated in the answer</a> by @cbeleites.</p>\\n',\n",
       " '<p>There is also the <a href=\"http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\" rel=\"nofollow\">Kullback-Leibler</a> divergence, which is related to the Hellinger Distance you mention above.</p>\\n',\n",
       " '<ol>\\n<li><p><strong>Have an objective:</strong> With 53 variables, each of which can be reported on singly or in analyses that combine two or more variables, there is an infinite range of analysis you can do. While there is an enormous range of possible objectives, I\\'ll invent a simplistic example. Let\\'s say you are consultant to a hip local clothing company building its first online retail presence, and they asked you to help them understand their likely target customers and design a marketing plan to bring traffic to the website and ultimately sell something. </p></li>\\n<li><p><strong>Break down your objective into components:</strong> The objective as stated as too big to connect directly to the data. You\\'ve got to break it down.</p>\\n\\n<ol>\\n<li>Understand target customer: a hip clothing company can probably tell you the age, gender, and income range of their current customers. You can tell them what percentage of the population surveyed meets those criteria, and the other demographics associated with that target customer.</li>\\n<li>Bring traffic to the website: you can tell them how frequently those target customers use different categories of online media (email, jobs, etc.) which will help them come up with an initial display advertising plan. </li>\\n<li>Sell them something: you can tell them the 5 most important and least important things to those target customers who shop online.</li>\\n</ol></li>\\n<li><p><strong>Analyze the data:</strong> </p>\\n\\n<ol>\\n<li>Understand target customer: There are a few ways you can do that in SPSS; the quickest and dirtiest way if you have the Custom Tables utility is to create a nested table with each of those three variables (drag Age, Gender, and Income out into the table, and arrange them however you like), and set the summary statistic to Table % (it might be called something else, I don\\'t have it in front of me). Then you can say females from age 35-45 making $40,000 a year or more make up 6% of the surveyed population.</li>\\n<li>Bring traffic to the website: you can tell them how frequently those target customers use different categories of online media (email, jobs, etc.) which will help them come up with an initial display advertising plan. In SPSS, you can filter the data (go to Select Cases and set an If condition matching your target customer), and then create a table with just the Frequency of Use questions. Since I\\'m guessing frequency is given on a range, and it will be the same for all questions, you can put the questions in the columns and have the response choices shown in the rows, for a nice readable table. The summary statistic here will be column %.</li>\\n<li>Sell them something: You\\'ve got an awful lot of variables about what\\'s important and not important for online shoppers and non-shoppers, and I\\'m not sure I understand how it\\'s all set up. But there is probably some sort of way to come up with a score - if they have used a rating scale of some sort, then you can use the Mean as a summary statistic, or summarize by grouping response options into groups (like 0-3, 4-6, 7-10) and reporting on some group, like the percentage that rated it 7-10. You can do this grouping in the Categories dialog inside Tables. When you have that score for each variable, you can put them in a ranked list and say \"these are the 5 most important things\", and \"these are the 5 least important things\". </li>\\n</ol></li>\\n</ol>\\n\\n<p>The key always goes back to starting with a good objective. Analyzing the data without an end in mind is a fruitless exercise.</p>\\n',\n",
       " \"<p>I've been testing PCA via SVD to decompose a simple time series data matrix, $X$.  I have two signals $x_1(t)$ and $x_2(t)$ in a data matrix where $M$ rows represents each timepoint sample and each column represents $x_1$ and $x_2$.</p>\\n\\n<p>The mean signal, $\\\\\\\\hat{x}$, is defined as the mean along the row axis (average of $x_1$ and $x_2$ along each timepoint).  I normalize each column of $X$ by subtracting its mean and dividing by the standard deviation.</p>\\n\\n<p>When I use [U S V] = svd(Xz) in matlab, regardless of how the variables are distributed (whether they are correlated or uncorrelated), one of the columns of the right singular matrix, V, always points in the same direction (to a multiplicative constant) as the mean vector $(1/2, 1/2)$.  But when add an additional third time series, this is never the case (where the mean vector is $(1/3, 1/3, 1/3)$.</p>\\n\\n<p>Because I normalize the standard deviation for each vector, it does make sense that the direction of most variance given by PCA would be the diagonal 45 degree line.  But if both variables $x_1$ and $x_2$ are independent gaussians, couldn't the PCA direction be any direction since the distribution is radially symmetric?</p>\\n\\n<p>MATLAB Code:</p>\\n\\n<pre><code>s = RandStream('mcg16807', 'Seed', 0);\\nRandStream.setDefaultStream(s);\\n\\nG = zeros(1000,2);\\nG(:,1) = 40*randn(1000,1)-100;\\nG(:,2) = tan(G(:,1)) + randn(1000,1);\\nX = G - repmat(mean(G),[size(G,1), 1]);\\nXz = X./repmat(std(G),[size(G,1), 1]);\\n[U S V] = svd(Xz);\\nG_mean = mean(Xz,2);\\ncorrcoef(G_mean, U(:,1))\\ncorrcoef(G_mean, U(:,2))\\n</code></pre>\\n\",\n",
       " \"<p>If BPM is staying the same over many samples (or changing infinitesimally in a way you aren't concerned about) you can truncate your data to a significant digit that you actually care about and then do Run Length Encoding.  </p>\\n\\n<p>For example, in R this data:</p>\\n\\n<pre><code>0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\\n</code></pre>\\n\\n<p>has this output</p>\\n\\n<pre><code>rle(data)\\nRun Length Encoding\\n  lengths: int [1:3] 10 15 15\\n  values : num [1:3] 0 1 2\\n</code></pre>\\n\",\n",
       " '<p>Lets say you want to use Accuracy (or % correct) to evaluate \"optimal,\" and you have time to look at 25 values for k.  The following R code will answer your question using 15 repeats of 10-fold cross-validation.  It will also take a long time to run.</p>\\n\\n<pre><code>library(caret)\\nmodel &lt;- train(\\n    Species~., \\n    data=iris, \\n    method=\\'knn\\',\\n    tuneGrid=expand.grid(.k=1:25),\\n    metric=\\'Accuracy\\',\\n    trControl=trainControl(\\n        method=\\'repeatedcv\\', \\n        number=10, \\n        repeats=15))\\n\\nmodel\\nplot(model)\\n&gt; confusionMatrix(model)\\nCross-Validated (10 fold, repeated 15 times) Confusion Matrix \\n\\n(entries are percentages of table totals)\\n\\n            Reference\\nPrediction   setosa versicolor virginica\\n  setosa       33.3        0.0       0.0\\n  versicolor    0.0       31.9       1.2\\n  virginica     0.0        1.4      32.1\\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/JAPq7.png\" alt=\"Accuracy\"></p>\\n\\n<p>So, by this criteria, I get an answer of 17, but it looks like the \"true\" value could lie anywhere between 5 and 20.  You can substitute \"Kappa\" or some other metric if you want, and add more cv-folds as well.  You can also try different methods of cross validation, such as leave-one-out, or bootstrap re-sampling.</p>\\n\\n<p>/Edit: in response for your request for variety, I wrote this function to calculate a variety of metrics for multi-class problems:</p>\\n\\n<pre><code>#Multi-Class Summary Function\\n#Based on caret:::twoClassSummary\\nrequire(compiler)\\nmultiClassSummary &lt;- cmpfun(function (data, lev = NULL, model = NULL){\\n\\n  #Load Libraries\\n  require(Metrics)\\n  require(caret)\\n\\n  #Check data\\n  if (!all(levels(data[, \"pred\"]) == levels(data[, \"obs\"]))) \\n    stop(\"levels of observed and predicted data do not match\")\\n\\n  #Calculate custom one-vs-all stats for each class\\n  prob_stats &lt;- lapply(levels(data[, \"pred\"]), function(class){\\n\\n    #Grab one-vs-all data for the class\\n    pred &lt;- ifelse(data[, \"pred\"] == class, 1, 0)\\n    obs  &lt;- ifelse(data[,  \"obs\"] == class, 1, 0)\\n    prob &lt;- data[,class]\\n\\n    #Calculate one-vs-all AUC and logLoss and return\\n    cap_prob &lt;- pmin(pmax(prob, .000001), .999999)\\n    prob_stats &lt;- c(auc(obs, prob), logLoss(obs, cap_prob))\\n    names(prob_stats) &lt;- c(\\'ROC\\', \\'logLoss\\')\\n    return(prob_stats) \\n  })\\n  prob_stats &lt;- do.call(rbind, prob_stats)\\n  rownames(prob_stats) &lt;- paste(\\'Class:\\', levels(data[, \"pred\"]))\\n\\n  #Calculate confusion matrix-based statistics\\n  CM &lt;- confusionMatrix(data[, \"pred\"], data[, \"obs\"])\\n\\n  #Aggregate and average class-wise stats\\n  #Todo: add weights\\n  class_stats &lt;- cbind(CM$byClass, prob_stats)\\n  class_stats &lt;- colMeans(class_stats)\\n\\n  #Aggregate overall stats\\n  overall_stats &lt;- c(CM$overall)\\n\\n  #Combine overall with class-wise stats and remove some stats we don\\'t want \\n  stats &lt;- c(overall_stats, class_stats)\\n  stats &lt;- stats[! names(stats) %in% c(\\'AccuracyNull\\', \\'Prevalence\\', \\'Detection Prevalence\\')]\\n\\n  #Clean names and return\\n  names(stats) &lt;- gsub(\\'[[:blank:]]+\\', \\'_\\', names(stats))\\n  return(stats)\\n})\\n</code></pre>\\n\\n<p>It\\'s a doozy of a function, so it\\'s going to slow down caret a bit, but I\\'d be very happy if you posted the results of your 1000 repeats of 10-fold CV (I have neither the time not the computational capacity to attempt this at present).  Here\\'s my code for 15 repeats of 10-fold CV.  Note that you can easily modify this code to try other re-sampling methods, such as bootstrap sampling:</p>\\n\\n<pre><code>library(caret)\\nset.seed(19556)\\nmodel &lt;- train(\\n  Species~., \\n  data=iris, \\n  method=\\'knn\\',\\n  tuneGrid=expand.grid(.k=1:30),\\n  metric=\\'Accuracy\\',\\n  trControl=trainControl(\\n    method=\\'repeatedcv\\', \\n    number=10, \\n    repeats=15,\\n    classProbs=TRUE,\\n    summaryFunction=multiClassSummary))\\n</code></pre>\\n\\n<p>Both <a href=\"http://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_curve\">ROC</a> and LogLoss seem to peak around 8:\\n<img src=\"http://i.stack.imgur.com/1z3vF.png\" alt=\"ROC\">\\n<img src=\"http://i.stack.imgur.com/wAczt.png\" alt=\"logLoss\"></p>\\n\\n<p>While <a href=\"http://en.wikipedia.org/wiki/Specificity_%28statistics%29\">sensitivity and specificity</a> seem to peak around 15:\\n<img src=\"http://i.stack.imgur.com/zO068.png\" alt=\"Sens\"> \\n<img src=\"http://i.stack.imgur.com/Au38S.png\" alt=\"Spec\"></p>\\n\\n<p>Here\\'s some code to output all the plots as a pdf:</p>\\n\\n<pre><code>dev.off()\\npdf(\\'plots.pdf\\')\\nfor(stat in c(\\'Accuracy\\', \\'Kappa\\', \\'AccuracyLower\\', \\'AccuracyUpper\\', \\'AccuracyPValue\\', \\n              \\'Sensitivity\\', \\'Specificity\\', \\'Pos_Pred_Value\\', \\n              \\'Neg_Pred_Value\\', \\'Detection_Rate\\', \\'ROC\\', \\'logLoss\\')) {\\n\\n  print(plot(model, metric=stat))\\n}\\ndev.off()\\n</code></pre>\\n\\n<p>If you put a gun to my head, I\\'d probably say 8...</p>\\n',\n",
       " '<p>There were already some useful comments, that are probably waiting for some updates in the question, so I will just drop some general online references:</p>\\n\\n<ul>\\n<li><a href=\"http://magnuson.psy.uconn.edu/mirman/R/Baayen.pdf?bcsi_scan_D99544420D78AF92=0&amp;bcsi_scan_filename=Baayen.pdf\" rel=\"nofollow\">Practical Data Analysis for the Language Sciences with R</a>, Baayen (2008)</li>\\n<li><a href=\"http://nlp.stanford.edu/~manning/courses/ling289/Jaeger07catdata.pdf\" rel=\"nofollow\">Categorical data analysis: Away from ANOVAs (transformation or not) and towards logit mixed models</a>, Jaeger (J. Mem. Language 2008 59(4))</li>\\n</ul>\\n\\n<p>Examples using R may be found on Doug Bates\\' <a href=\"http://lme4.r-forge.r-project.org/\" rel=\"nofollow\">lme4 - Mixed-effects models project</a>.</p>\\n',\n",
       " '<p>I have a data set of 100,000. If I want to pick a subset (sample), how can I ensure that it\\'s a \"good\" sample? I know that I can make it unbiased by picking at random, but how do I know how many to pick to account for all the variance in the dataset?</p>\\n',\n",
       " '<p>Your instinct about the R and L legs\\' results being correlated seems right, but even so, I think repeated measures anova is appropriate.  You have no between subject factors, just 2 within-subject factors.  These are R vs. L Leg (2 levels) and what you\\'ve been hitherto calling \"conditions\" but which I would rename (5 levels).  These make 2*5=10 within-subject conditions.</p>\\n',\n",
       " '<p>Consider the first order autoregressive process\\n$X_t= aX_{t-1} + e_t$  where $e_t$ is white nose.  The model can be expressed as $X_t-aX_{t-1} = e_t$.</p>\\n\\n<p>Using the backshift operator $BX_t = X_{t-1}$ we can reexpress the model as $X_t-aBX_t =e_t$ or $X_t(1-aB) = e_t$.  So the characteristic polynomial is $1-aB$.  This has a unit root at $a=1$.  Then for $|a|&lt;1$ we have a stationary $AR(1)$ process, for $|a|&gt;1$ we have an explosive nonstationary $AR(1)$ process.  </p>\\n\\n<p>For $a=1$ we have a random walk which is nonstationary.  So the unit roots form the boundary between stationarity and nonstationarity and the $AR(1)$ model is the simplest to illustrate it.</p>\\n',\n",
       " \"<p>Classic Torgerson metric MDS is actually done by transforming distances into similarities and performing PCA (eigen-decomposition or singular-value-decomposition) on those. [The other name of this procedure (distances between objects -> similarities between them -> PCA, whereby loadings are the sought-for coordinates) is Principal Coordinate Analysis.] So, PCA might be called the algorithm of the simplest MDS.</p>\\n\\n<p>Non-metric MDS is based on iterative ALSCAL algorithm (or algorithm similar to it) which is a more versatile mapping technique than PCA and can be applied to metric MDS as well. While PCA <em>retains</em> <strong>m</strong> important dimensions for you, ALSCAL <em>fits</em> configuration to <strong>m</strong> dimensions (you pre-define <strong>m</strong>).</p>\\n\\n<p>Thus, MDS and PCA are not at the same level to be in line or opposite to each other. PCA is just a method while MDS is a class of analysis. As mapping, PCA is a particular case of MDS. On the other hand, PCA is a particular case of Factor analysis which, being a data reduction, is more than only a mapping, while MDS is only a mapping.</p>\\n\\n<p>As for your question about metric MDS vs non-metric MDS there's little to comment because the answer is straightforward. If I believe my input dissimilarities are so close to be euclidean distances that a linear transform will suffice to map them in m-dimensional space, I will prefer metric MDS. If I don't believe, then monotonic transform is necessary, implying use of non-metric MDS.</p>\\n\",\n",
       " '<p>There is a free introductory textbook at <a href=\"http://www.openintro.org/stat/index.php\" rel=\"nofollow\">openintro</a> that also has sample datasets and a library of homework/quiz questions.  That would be a place to start.  I don\\'t know if it will get into as advanced topics as what you want.</p>\\n',\n",
       " '<p>I applied Gaussian Mixture Model on my data and train the model in MATLAB. How I can test my model or use it to cluster new data?</p>\\n\\n<p>Thanks for any answer or comment.</p>\\n',\n",
       " '<p>Consider a multinomial $2\\\\\\\\times 2$ table $\\\\\\\\begin{pmatrix} x_{11} &amp; x_{12} \\\\\\\\\\\\\\\\ x_{21} &amp; x_{22} \\\\\\\\end{pmatrix}$ with theoretical probabilities: $$\\\\\\\\begin{pmatrix} \\\\\\\\theta_{11} &amp; \\\\\\\\theta_{12} \\\\\\\\\\\\\\\\  \\\\\\\\theta_{21} &amp; \\\\\\\\theta_{22} \\\\\\\\end{pmatrix}.$$ \\nThe conditional distribution of $x_{11}$ given $x_{11}+x_{12}=n$ is binomial with size $n$ and proportion $p=\\\\\\\\dfrac{\\\\\\\\theta_{11}}{\\\\\\\\theta_{11}+\\\\\\\\theta_{12}}$. Therefore we have a straightforward way to perform statistical inference on $p$ by conditioning on $x_{11}+x_{12}$. Is it possible to do an unconditional inference on $p$ ?</p>\\n\\n<p>Parenthesis : I have noted that adopting the Bayesian approach with the Jeffreys (Dirichlet) prior of the unconditional multinomial model is equivalent to adopt the Bayesian approach with the Jeffreys (Beta) prior of the conditional binomial model.</p>\\n',\n",
       " \"<p>How best to do this is going to vary <em>tremendously</em> depending on the task you're performing, so it's impossible to say what will be best in a task-independent way.</p>\\n\\n<p>There are two easy things to try if your levels are ordinal:</p>\\n\\n<ol>\\n<li>Bin them. E.g., 0 = (0 250), 1 = (251 500), etc. You may want to select the limits so each bin has an equal number of items.</li>\\n<li>You can also take a log transform of the levels. This will squish the range down.</li>\\n</ol>\\n\\n<p>If the levels are not ordinal you can cluster the levels based on other features/variables in your dataset and substitute the cluster ids for the previous levels. There are as many ways to do this as there are clustering algorithms, so the field is wide open. As I read it, this is what <code>combine.levels()</code> is doing. You could do similarly using <code>kmeans()</code> or <code>prcomp()</code>. (You could/should subsequently train a classifier to predict the clusters for new datapoints.)</p>\\n\",\n",
       " '<p>The music is usually described using <a href=\"http://mpeg.chiariglione.org/standards/mpeg-7/mpeg-7.htm#E12E41\">MPEG7 descriptors</a> with some additional stuff like <a href=\"http://en.wikipedia.org/wiki/Mel-frequency_cepstral_coefficient\">MFCCs</a> calculated on the chunks of piece made by some moving window approach (i.e. you have some window size and hop, start with the window placed on the beginning of the sound, calculate the descriptors on the window, then move it by hop and repeat until the end is reached).<br>\\nThis way a piece is transformed into a table; in your case it can be used to apply some clustering on the chunks and so detect those \"parts\".</p>\\n',\n",
       " \"<p>Why is the average of the highest value from 100 draws from a normal distribution different from the 98% percentile of the normal distribution?  It seems that by definition that they should be the same.  But...</p>\\n\\n<p>Code in R:</p>\\n\\n<pre><code>NSIM &lt;- 10000\\nx &lt;- rep(NA,NSIM)\\nfor (i in 1:NSIM)\\n{\\n    x[i] &lt;- max(rnorm(100))\\n}\\nqnorm(.98)\\nqnorm(.99)\\nmean(x)\\nmedian(x)\\nhist(x)\\n</code></pre>\\n\\n<p>I imagine that I'm misunderstanding something about what the maximum of a 100 draws from the normal distribution should be.  As is demonstrated by an unexpectedly asymetrical distribution of maximum values.</p>\\n\",\n",
       " '<p>Let us consider a comparison of two machine learning algorithms (A and B) on some dataset. Results (root mean squared error) of both algorithms depend on randomly generated initial approximation (parameters).</p>\\n\\n<p>Questions:</p>\\n\\n<ol>\\n<li>When I use the same parameters for both algorithms, \"usually\" A slightly outperforms B. How many different experiments (<em>with different parameters /</em> updated <em>/</em>) have I to perform to make \"sure\" that A is better than B?</li>\\n<li>How to measure significance of my results? (To what extent I am \"sure\"?)</li>\\n</ol>\\n\\n<p>Relevant links are welcome!</p>\\n\\n<p>PS. I\\'ve seen papers in which authors use t-test and p-value; but i\\'m not sure if it is ok to use them in a such situation.</p>\\n\\n<p><strong>UPDATE.</strong>\\nThe problem is that A (almost) always outperforms B if initial params and learning/validation/testing sets are the same; but it doesn\\'t neccessarily hold if they differ.</p>\\n\\n<p>I see the following approaches here:</p>\\n\\n<ul>\\n<li><p>split data into disjoint sets D_1, D_2, ...; generate parameters params_1; compare A(params_1, D_2, ...,) and B(params_1, D_2, ...,) on D_1;\\n generate  params_2; compare A(params_2, D_1, D_3,...) and B(params_2, D_1, D_3,...) on D_2 and so on.\\n Remember how often A outperforms B.</p></li>\\n<li><p>split data into disjoint sets D_1, D_2, ...; generate parameters params_1a and params_1b; compare A(params_1a, D_2, ...,) and B(params_1b, D_2, ...,) on D_1; ....\\n Remember how often A outperforms B.</p></li>\\n<li><p>first, do cross-validation for A. Then, independently, for B. Compare results.</p></li>\\n</ul>\\n\\n<p>Which approach is better? How to find significance of the result in this best case?</p>\\n',\n",
       " '<p>The <strong>correlation</strong> is a bivariate statistic that would involve only x and y and would not have an intercept. None of what you\\'ve given is a correlation.</p>\\n\\n<p>But, given that there is an interaction with other variables, the correlation, while mathematically correct, will be misleading. </p>\\n\\n<p>In one of your replies you write:</p>\\n\\n<blockquote>\\n  <p>For example, is it correct to say \"among all categories c, the\\n  average correlation of x with y is b_1\", and if so, which b_1 should I\\n  use here? The one in model 1, 2, or 3?</p>\\n</blockquote>\\n\\n<p>If you want to make a statement of this type, you can get a weighted average of the correlations (not the regression parameters) in the different categories of c. But I don\\'t think this is a good approach.</p>\\n\\n<p>Given this goal</p>\\n\\n<blockquote>\\n  <p>What I\\'m interested in is a (justified) general statement about the\\n  correlation I can make to someone who knows very little statistics.</p>\\n</blockquote>\\n\\n<p>What I would do is present a graph of the variables x and y, with the dots on the scatterplot colored differently for each level of c, and with regression lines (or possibly loess lines) superimposed on top.</p>\\n\\n<p>This wouldn\\'t be a single simple statement, but sometimes there is no good single simple statement. </p>\\n',\n",
       " '<p>Although you could in principle directly apply a DBN to your data to learn new feature representations, a more promising approach in my view would be to look for models tailored more closely to your problem. Hugo Larochelle, for example, has used and extended RBMs to model both documents and classification problems. Maybe you can find something useful on his website:</p>\\n\\n<p><a href=\"http://www.dmi.usherb.ca/~larocheh/projects_classrbm.html\" rel=\"nofollow\">http://www.dmi.usherb.ca/~larocheh/projects_classrbm.html</a></p>\\n',\n",
       " \"<p>In my opinion, if you don't code yourself the test, you are prone to errors and misunderstandings of the results.</p>\\n\\n<p>I think that you should recommend them to hire a statistician that has computer skills. </p>\\n\\n<p>If it is to do always the same thing, then indeed you can use a small tool (blackbox) that will do the stuff. But I am not sure this is still called data exploration.</p>\\n\",\n",
       " \"<p>This is perhaps basic but I couldn't find a suitable reference. </p>\\n\\n<p>I have a regression model with a rather complicated link function. \\nSo $\\\\\\\\vec{x}$ is a vector of continuous predictors, and $z$ is a binary variable such \\nthat according to the model: $Pr(z=1)  = f(\\\\\\\\vec{x})$ for some (known) function $f$. </p>\\n\\n<p>I observe data of the form $(\\\\\\\\vec{x}^{(1)}, z^{(1)}), (\\\\\\\\vec{x}^{(2)}, z^{(2)}), (\\\\\\\\vec{x}^{(n)}, z^{(n)})$ and want to test the null hypothesis that the above model is the one generating the data - that is compute a statistic and reject the model if the statistic is too extreme. What would be a good goodness-of-fit test for this case? is there a 'standard' way to test for this? </p>\\n\\n<p>One possibility is binning the data points by the value of  $f(\\\\\\\\vec{x})$, (say to $10$ bins: $([0,0.1], ..[0.9,1])$ and performing a chi-square test for expected vs. observed proportion of $z$'s in each bin. Another is to bin the multidimensional space of the $\\\\\\\\vec{x}$'s (say if $\\\\\\\\vec{x}$ is two-dimensional, we can divide $R^2$ to $100$ squares and compute a chi-square for observed vs. expected for each square). Yet another one is not binning at all but just computing $\\\\\\\\sum_i (z^{(i)} - f(\\\\\\\\vec{x}^{(i)}))^2/f(\\\\\\\\vec{x}^{(i)})$\\nbut this seems to cause numerical issues since sometimes $f(\\\\\\\\vec{x}^{(i)})$ is very small.\\nAre there other known approaches? which test would be the most appropriate?</p>\\n\",\n",
       " \"<p>Perhaps.  Though my take could easily be construed as a bit too anal retentive:</p>\\n\\n<p>I tend to use the term seasonality as a metaphor for the 'seasons' of the year: i.e. Spring, Summer, Fall, Winter (or 'Almost Winter', Winter, 'Still Winter', and 'Construction' if you live in Pennsylvania...).  In other words, I would expect a seasonal trend to have a <em>periodicity</em> of roughly 365 days.</p>\\n\\n<p>I tend to use the term 'cyclicality' to refer to a response, which when decomposed in frequency space has a single dominant peak. Or, a bit more generally, much as one could stare at an engine, 'cyclicality' implies a dominant <em>cycle</em> -- the piston moves up, and then it moves down, and then it moves up again. Numerically, I would expect low, high, low, high, low, high, etc. So two things: (1) magnitude &amp;/or sign switches from a low to high and (2) these switches occur with a predictable frequency. This rigor naturally evaporates when talking about business cycles -- however, I often find that a dominant frequency remains, e.g. <em>every business quarter</em>, or <em>every year</em>, things are slow for the first few weeks and high pressure the last few weeks...  So there is a dominant period, but it could be very different from 'seasonality' which to me implies a year.</p>\\n\\n<p>Lastly, I tend to use 'periodicity' when referring to the frequency of collecting measurements.  Differing from cyclicality, the term 'periodicity' for me implies no expectation for the magnitude or sign of the data collected.</p>\\n\\n<p>But this is just my $0.02.  And I'm just a stat student -- take from this what you will.</p>\\n\",\n",
       " \"<p>I understand the logic of standardising raw data that is based on different scales into z scores so that they can compared, for example comparing a score of 75 out of 100 in one test versus 65 out of 120 in another test.</p>\\n\\n<ul>\\n<li>Can I run the Spearman's correlation test on the z scores then?  (i.e. forget about the raw data). </li>\\n<li>How do I then relate the result back to the raw data?</li>\\n</ul>\\n\",\n",
       " '<p>I\\'m using R to provide bootstrap (percentile and t methods) of estimated population totals, using data from a complex survey.  It is a stratified survey of tourists expenditure that is weighted to population (the known number of tourists from customs information).  I want a confidence interval for total expenditure of various combinations of tourist types (eg \"Australian business travellers\").  Those tourist characteristics are part of the poststratification weighting scheme, but not the original stratification.  Stratification is by departing flight where the interview took place.  The weights are very complex and definitely not independent of the variable of interest.</p>\\n\\n<p>My procedure is to produce a stratified resample; take the cases in this sample\\'s weights from the original sample and scale them up/down so they add again to the correct population totals; and calculate my statistic.  Repeat r times.  If I don\\'t do the reweighting procedure in the middle my bootstrapped estimates are on average significantly higher than the estimates from the original sample and hence useless.  My reweighting procedure does not exactly duplicate the original poststratification weighting (it is a simpler version) but is callibrated to produce the same marginal population totals for combinations of tourists\\' country of residence, purpose of visit, and age.</p>\\n\\n<p>I have written code in R to do this but am interested in if there is an existing function I can compare my results with.  I\\'ve looked at both the boot and survey packages and can\\'t find anything.  While boot allows bootstrapping from a stratified sample, I can\\'t see a way to perform the reweighting to population of each replicate resample.  My <strong>first question</strong> is - any pointers to a prebuilt function in R that does this reweighting to population as part of its bootstrap?</p>\\n\\n<p>I also haven\\'t found anything on exactly this problem in my cursory examination of the literature.  However, it must be a common challenge for surveys that have been weighted to populations.  My <strong>second question</strong> is - any pointers to discussion in the literature (not just on R) about the merits or otherwise of this reweighting procedure in the middle of a bootstrap?</p>\\n',\n",
       " \"<p>I'm using the <code>logistf</code> package in R to perform Firth logistic regression on an unbalanced dataset. I have a logistf object:</p>\\n\\n<pre><code>fit = logistf(a~b)\\n</code></pre>\\n\\n<p>Is there a <code>predict()</code> function like on that's used in the <code>lm</code> class to predict probabilities for future data points? Or do I have to manually input the estimated parameters from the Firth regression.</p>\\n\\n<p>Thanks!</p>\\n\",\n",
       " '<p>Some idea might be to generate something like the <a href=\"http://archive.ics.uci.edu/ml/datasets/Madelon\" rel=\"nofollow\">Madelon set</a> from NIPS 2003 challenge; it fits your requirements pretty well. </p>\\n\\n<p>You can generate a set like this starting with <code>mlbench.xor</code> (or <code>mlbench.hypercube</code>, might be easier) form mlbench package, then you combine classes it generated into two groups to make the dichotomous response and add new attributes to increase dimensionality -- some being random linear combinations of the original ones, some being just random noise.</p>\\n',\n",
       " \"<p>Given two independent variables with Beta distribution, $X \\\\\\\\sim \\\\\\\\text{Be}(a_1, b_1)$ and $Y \\\\\\\\sim \\\\\\\\text{Be}(a_2, b_2)$, how do you find the probability that the value of X is greater than the value of Y for a given observation?</p>\\n\\n<p>Does this probability have a name that I'm just blanking on?</p>\\n\",\n",
       " '<p>You can improve the acceptance rate using <strong><em>delayed rejection</em></strong> as described in <a href=\"http://scholar.google.fr/scholar?cluster=8248856686379090018\" rel=\"nofollow\">Tierney, Mira (1999)</a>. It is based on a second proposal function and <strong>a second acceptance probability</strong>, which guarantees the Markov chain is still reversible with the same invariant distribution: you have to be cautious since \"<em>it is easy to construct adaptive methods that might seem to work but in fact sample from the wrong distribution</em>\".</p>\\n',\n",
       " '<p>Standard programming practice:\\n-when debugging run the simulation with fixed sources of randomness (i.e. same seed) so that any changes are due to code changes and not different random numbers\\n-try your code on a model (or several models) where the answer IS known\\n-adopt good programming habits so that you introduce fewer bugs \\n-think very hard &amp; long about the answers you do get, whether they make sense, etc.</p>\\n\\n<p>I wish you good luck, and plenty of coffee!</p>\\n',\n",
       " \"<p>The first question is whether the Weibull distribution is still a good model of wind speed when it is a monthly average.  I'm not familiar with this field, but from what you say it sounds as though average hourly wind speed is often modelled as having a Weibull distribution.  If you take the average of 700 or so such random variables (24*30) the distribution will be very nearly normally distributed because of the central limit theorem, even with the autocorrelation of the underlying hourly observations.</p>\\n\\n<p>I'd suggest looking at the actual distribution of your 60 data points and comparing it to a normal distribution, using something like the qqnorm() function in R to draw a plot being the obvious starting point.</p>\\n\\n<p>But basically, I doubt the probability distribution of your variables is your main problem here.  The challenge with modelling your data will be more related to the need for you to estimate and control for seasonal effects from quite a small data set (by time series standards).  Exactly what sort of challenge that is though depends on what your research question is (for example, do you need to see if one year's wind speed was different from others? or are you looking for a trend? or what?).</p>\\n\",\n",
       " '<p>You might want to spend $1.20 printing out Matthias Vallentin\\'s <a href=\"http://www.cs.berkeley.edu/~mavam/dl/probstat.pdf\" rel=\"nofollow\">probability and statistics cheat sheet</a>.</p>\\n',\n",
       " '<p>Where I can find comparison of variances (or design effect) of different <a href=\"https://en.wikipedia.org/wiki/Sampling_methods#Sampling_methods\" rel=\"nofollow\">sampling methods</a>?</p>\\n\\n<p>Same population, same sample size: I want to know which one has smaller variance than other; in simple random sampling, systematic sampling, stratified sampling, cluster sampling and so on.</p>\\n',\n",
       " '<p>Note that <code>rbind(a, b)</code> creates a single data frame, so that\\'s not it. The unexpected behavior of <code>unstack(a)</code> results from the fact that you only have one observation (<code>count</code>) per factor level (<code>state</code>). To see what\\'s going on, you have to look at the <code>unstack()</code> function.</p>\\n\\n<pre><code># list source code for unstack()\\'s method for a data frame\\n&gt; getS3method(\"unstack\", \"data.frame\")\\nfunction (x, form, ...) \\n{\\n    form &lt;- if (missing(form)) \\n        stats::formula(x)\\n    else stats::as.formula(form)\\n    if (length(form) &lt; 3) \\n        stop(\"\\'form\\' must be a two-sided formula\")\\n    res &lt;- c(tapply(eval(form[[2L]], x), eval(form[[3L]], x), \\n        as.vector))\\n    if (length(res) &lt; 2L || any(diff(unlist(lapply(res, length))) != \\n        0L)) \\n        return(res)\\n    data.frame(res)\\n}\\n&lt;environment: namespace:utils&gt;\\n</code></pre>\\n\\n<p>The relevant bits are <code>c(tapply(eval(form[[2L]], x), eval(form[[3L]], x), as.vector))</code> and <code>data.frame(res)</code>. The first expression here is equivalent to</p>\\n\\n<pre><code>&gt; c(tapply(a$count, a$state, as.vector))\\n  RSTO   RSTR     S1     S2     S3     SF \\n199665   4147  31274      1   2522 118009\\n</code></pre>\\n\\n<p>The output of <code>tapply()</code> is a 1-dimensional array if the provided function only returns one value when applied to the data in each level of the factor. <code>c()</code> strips the class, thus returning a numeric vector. <code>data.frame(&lt;vector&gt;)</code> creates a data frame with one variable equal to that vector. Compare this to what happens with two observations per condition:</p>\\n\\n<pre><code>&gt; ab &lt;- rbind(a, b)\\n&gt; c(tapply(ab$count, ab$state, as.vector))\\n$RSTO\\n[1] 199665  31956\\n\\n$RSTR\\n[1]  4147 11689\\n\\n$S1\\n[1] 31274  6702\\n\\n$S2\\n[1]    1 2838\\n\\n$S3\\n[1] 2522 6268\\n\\n$SF\\n[1] 118009 672561\\n</code></pre>\\n\\n<p>Here, the function provided for <code>tapply()</code> returns a vector with two elements for each level of the factor. The result then is a list with as many components as you have factor levels, each component containing the result of the supplied function for that level. All components have equal length, hence <code>data.frame(&lt;list&gt;)</code> returns a data frame where each component becomes a variable (data frames are special lists with components of equal length).</p>\\n\\n<p>Now, another question would be if the behavior of <code>unstack()</code> is sensible in a case like yours. If this is of practical importance, you can add an id-variable and use <code>reshape()</code> instead.</p>\\n\\n<pre><code>&gt; a$id &lt;- 1\\n&gt; reshape(a, direction=\"wide\", v.names=\"count\", timevar=\"state\", idvar=\"id\")\\n  id count.RSTO count.RSTR count.S1 count.S2 count.S3 count.SF\\n1  1     199665       4147    31274        1     2522   118009\\n\\n&gt; ab$id &lt;- rep(1:2, each=nrow(a))\\n&gt; reshape(ab, direction=\"wide\", v.names=\"count\", timevar=\"state\", idvar=\"id\")\\n   id count.RSTO count.RSTR count.S1 count.S2 count.S3 count.SF\\n1   1     199665       4147    31274        1     2522   118009\\n11  2      31956      11689     6702     2838     6268   672561\\n</code></pre>\\n',\n",
       " '<p>I found ANOVA which gives a significance that replaces the t test</p>\\n',\n",
       " '<p>It seems that most authorities agree that dark or otherwise prominent gridlines in plots are \"chartjunk\" by any reasonable definition and distract the viewer from the message in the main body of the chart.  So I won\\'t bother to give references on that point.</p>\\n\\n<p>Equally, we can all agree that there will be times that <em>pale</em> gridlines to create a reference for viewers <em>will</em> be necessary.  Tufte argued the need for(and used) gridlines occasionally, as pointed out for example in <a href=\"http://www.forbes.com/sites/naomirobbins/2012/02/22/are-grid-lines-useful-or-chartjunk/2/\">this article</a>.  And I agree with the Hadley Wickham\\'s approach in ggplot2 to make such gridlines white on a pale grey background, when you need to use them.</p>\\n\\n<p>What I am uncertain about however is whether such gridlines and grey background should be the <em>default</em>, as they are in ggplot2.  For example, there seems to be no reason for the grey background other than to case the white gridlines into relief - which further begs the question of whether either is needed.  I have recently started using ggplot2 for most of my graphics needs and think it is awesome, but it has challenged my \"no box, no background, no gridlines\" approach to graphics that I used before.  I used to think that gridelines=OFF should be my default unless there is a particular reason for adding them in - basically the approach recommended in <a href=\"http://www.perceptualedge.com/articles/dmreview/grid_lines.pdf\">this article</a>, for example.  </p>\\n\\n<p>Of course, it is straightforward to define a theme in ggplot2 to avoid the gridlines and background shading (and in fact we have done this at my work), but ggplot2\\'s approach is so awesome and generally its default aesthetic choices are good that I wonder if I am missing something.</p>\\n\\n<p>So - I would be grateful for any references on this point.  I am sure it has been well thought through (for example by Hadley Wickham in setting the ggplot2 defaults) and I am very open to being pointed in the right direction.  The best I\\'ve been able to find is <a href=\"http://groups.google.com/group/ggplot2/browse_thread/thread/568e23b7aae6f5d2\">a couple of links</a> on the ggplot2 google group but the most helpful looking reference by Cleveland isn\\'t available at the link given.</p>\\n',\n",
       " '<p>There\\'s a very neat way of computing the combinations or probabilities in a spreadsheet (such as excel) that computes the convolutions directly. </p>\\n\\n<p>I\\'ll do it in terms of probabilities and illustrate it for six sided dice but you can do it for dice with any number of sides (including adding different ones).</p>\\n\\n<p>(btw it\\'s also easy in something like R or matlab that will do convolutions)</p>\\n\\n<p>Start with a clean sheet, in a few columns, and move down a bunch of rows from the top (more than 6).</p>\\n\\n<ol>\\n<li><p>put the value 1 in a cell. That\\'s the probabilities associated with 0 dice. put a 0 to its left; that\\'s the value column - continue down from there with 1,2,3 down as far as you need.</p></li>\\n<li><p>move one column to the right and down a row from the \\'1\\'. enter the formula \"=sum(\" then left-arrow up-arrow (to highlight the cell with 1 in it), hit \":\" (to start entering a range) and then up-arrow 5 times, followed by \")/6\" and press Enter - so you end up with a formula like <code>=sum(c4:c9)/6</code>  (where here <code>C9</code> is the cell with the 1 in it).</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/Fhvr4.png\" alt=\"enter image description here\"></p>\\n\\n<p>Then copy the formula and paste it to the 5 cells below it. They should each contain 0.16667 (ish).</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/VB27Z.png\" alt=\"enter image description here\"></p>\\n\\n<p>Don\\'t type anything into the empty cells these formulas refer to!</p></li>\\n<li><p>move down 1 and to the right 1 from the top of that column of values and paste ...</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/0jzx9.png\" alt=\"enter image description here\"></p>\\n\\n<p>... a total of another 11 values. These will be the probabilities for two dice. </p>\\n\\n<p><img src=\"http://i.stack.imgur.com/fU1oM.png\" alt=\"enter image description here\"></p>\\n\\n<p>It doesn\\'t matter if you paste a few too many, you\\'ll just get zeroes.</p></li>\\n<li><p>repeat step 3 for the next column for three dice, and again for four, five, etc dice.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/OEJyX.png\" alt=\"enter image description here\"></p>\\n\\n<p>We see here that the probability of rolling $12$ on 4d6 is 0.096451  (if you multiply by $4^6$ you\\'ll be able to write it as an exact fraction).</p></li>\\n</ol>\\n\\n<p>If you\\'re adept with Excel - things like copying a formula from a cell and pasting into many cells in a column, you can generate all tables up to say 10d6 in about a minute or so (possibly faster if you\\'ve done it a few times).</p>\\n\\n<hr>\\n\\n<p>If you want combination counts instead of probabilities, don\\'t divide by 6.</p>\\n\\n<p>If you want dice with different numbers of faces, you can sum $k$ (rather than 6) cells and then divide by $k$. You can mix dice across columns (e.g. do a column for d6 and one for d8 to get the probability function for d6+d8):</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/mnJlr.png\" alt=\"enter image description here\"></p>\\n',\n",
       " '<p><strong>Context</strong></p>\\n\\n<p>I have a set of data that was collected from several inertial measurement units (orientation and acceleration data). I want to determine to what extent an inference method degrades when the data becomes noisy (or, should I say \"noisier\").</p>\\n\\n<p><strong>Questions</strong></p>\\n\\n<p>How do I determine what type of noise to use? Is there a way in which I can find this out from the data itself?</p>\\n\\n<p>What range of noise levels would be appropriate for testing the inference method\\'s robustness? My guess is that it would depend on the answer to the question: \"how noisy would you expect the environment/system to get\"? Again, given only the collected data, what is the best approach in determining these levels?</p>\\n',\n",
       " \"<p>Suppose that, in a study of 15 subjects, the response variable (res) is modeled with two explanatory variables, one (level) is categorical with 5 levels and the other (response time: RT) is continuous.  With lmer in the lme4 package of R, I have:</p>\\n\\n<pre><code>fm1 &lt;- lmer(res ~ level * RT + (level-1 | subject), data=mydata)\\nanova(fm1)\\n\\n             Df  Sum Sq Mean Sq  F value\\nlevel        4  3974.9   993.7   9.2181\\nRT           1  1953.5  1953.5  18.1209\\nlevel:RT     4  5191.4  1297.9  12.0393\\n</code></pre>\\n\\n<p>If I change the order of the two variables, I get slightly different results for the main effects:</p>\\n\\n<pre><code>fm2 &lt;- lmer(res ~ RT * level + (level-1 | subject), data=mydata)\\nanova(fm2)\\n\\n             Df  Sum Sq Mean Sq  F value\\nRT           1  1671.8  1671.8  15.5077\\nlevel        4  4256.7  1064.2   9.8715\\nRT:level     4  5191.4  1297.9  12.0393\\n</code></pre>\\n\\n<p>Does such a difference come from the sequential (instead of marginal) approach in lme4 in accounting for data variability? In this case, the variable order change does not lead to a big difference, but previously I've seen dramatic differences. What does such a big difference mean? Does it mean that the model needs more tuning until big difference disappears?</p>\\n\\n<p>My second question is that, if I want to know which variable among the two (RT and level) accounts for more data variability, what would be a reasonable approach? Based on the relative magnitude of Sum Sq (or Mean Sq) of the two variables? Any statistical testing method to compare variability among explanatory variables?</p>\\n\",\n",
       " '<p>Not answering to your exact question, but the followings could be interesting by visualizing one possible pitfall of linear correlations based on an <a href=\"http://stackoverflow.com/questions/4666590/remove-outliers-from-correlation-coefficient-calculation/4668720#4668720\">answer</a> from <a href=\"http://stackoverflow.com/q/4666590/564164\">stackoveflow</a>:</p>\\n\\n<pre><code>par(mfrow=c(2,1))\\n\\nset.seed(1)\\nx &lt;- rnorm(1000)\\ny &lt;- rnorm(1000)\\nplot(y~x, ylab = \"\", main=paste(\\'1000 random values (r=\\', round(cor(x,y), 4), \\')\\',  sep=\\'\\'))\\nabline(lm(y~x), col = 2, lwd = 2)\\n\\nx &lt;- c(x, 500)\\ny &lt;- c(y, 500)\\ncor(x,y)\\nplot(y~x, ylab = \"\", main=paste(\\'1000 random values and (500, 500) (r=\\', round(cor(x,y), 4), \\')\\',  sep=\\'\\'))\\nabline(lm(y~x), col = 2, lwd = 2)\\n</code></pre>\\n\\n<p><img src=\"http://i.stack.imgur.com/LcRHP.png\" alt=\"alt text\"></p>\\n\\n<p><a href=\"http://stackoverflow.com/questions/4666590/remove-outliers-from-correlation-coefficient-calculation/4667300#4667300\">@Gavin Simpson\\'s</a> and <a href=\"http://stackoverflow.com/questions/4666590/remove-outliers-from-correlation-coefficient-calculation/4675675#4675675\">@bill_080\\'s answer</a> also includes nice plots of correlation in the same topic.</p>\\n',\n",
       " '<p>With clustered data, you have 500 degrees of freedom, anyway. It does not matter that your nominal sample size may be 1005 or 1320 or whatever the number will be. The sampling variance of your estimates will generally improve only to the extent that you increase the number of clusters. So I would not see the random sample size as an issue.</p>\\n\\n<p>I have written cluster bootstrap code in Stata, see <a href=\"http://www.stata-journal.com/article.html?article=st0187\" rel=\"nofollow\">http://www.stata-journal.com/article.html?article=st0187</a>.</p>\\n',\n",
       " \"<p>In OLS regression, I find the semipartial correlation (a.k.a. part correlation) to be a very useful indicator.  When squared, it shows each predictor's unique contribution to explained variance in the outcome.  If one tries several models, entering predictors in various orders, the semipartial correlation will always turn out the same by the final model.  This indicator can also be a useful basis for creating Venn diagrams showing the relative importance of different predictors.  Given all this, why is it cited so seldom?  For example, after 6,300 questions on this site, it has never come up, nor do I see it cited in journal publications.</p>\\n\",\n",
       " \"<p>I have 4 data groups:\\n- 3 groups from a 5-point Likert scale (1 ~ 5)\\n- 1 group is binary (Incorrect/Correct -- converted to 0, 1)</p>\\n\\n<p>I have performed Pearson's r and Spearman's rho with the Likert responses easily enough and they appear to be related to a scatterplot. However, the binary data is giving some strange numbers that don't appear to be showing a true relationship (I'm new to using SPSS so this may be the problem).</p>\\n\\n<p>An alternative I want advice on in the comparison of means. I am aware of paired t-tests but I don't know if they are suitable or if something else is better suited. \\nI'm especially interested in the relationship between, e.g. a) Perception of x= M:3.12, b) Realisation of x= M:3.95</p>\\n\",\n",
       " '<p>Michael, I did not say that when you exclude a group from randomization you can still get unbiased estimates, and I would never argue in favor of this idea; this is certainly not true. What is true is that you don\\'t have to have the same sampling rates, and that\\'s the imbalance that you can correct with weights. You illustrate this point with your Lady-Tasting-Tea example, which works unless one of the sample sizes is zero.</p>\\n\\n<p><a href=\"http://stats.stackexchange.com/questions/37422/formal-definition-of-random-assignment/37426#comment73474_37435\">The example I gave</a> is aimed at a different subtle point. It uses all units in the population, but it\\'s not a full design (which would be ${5 \\\\\\\\choose 2}=10$ possible assignments); yet every unit is assigned to treatment twice, and to control, three times. So with respect to randomization to these assignments, the difference of the treatment group means is an unbiased estimator of the mean population difference (in potential outcomes). This example shows that you don\\'t have to fully randomize, in the sense of tossing a coin independently for every unit (which may put you to an awkward situation of having no controls or no treatments with probability 1/16 for sample this small, by the way). However, for this example to work out, you need to have a sample size that was fixed in advance, and you had to toss a five-sided coin in advance before the very first unit enters the study. So each approach has its pros and cons; I doubt that the balanced sampling example that I gave is hugely practical, although <a href=\"http://www.jstor.org/stable/20441151\" rel=\"nofollow\">cube sampling</a> appears to be picking up in survey statistics.</p>\\n\\n<p>I am not a causal inference statistician, nor am I terribly good with experimental designs. But I know my randomization inference from survey statistics, so the way I\\'ve been viewing this problem is from the perspective of Horvitz-Thompson estimator.</p>\\n',\n",
       " '<p>Rob Hyndman is doing some active research on forecasting with nueral nets.  He recently added the <code>nnetar()</code> function to the <code>forecast</code> package that utilizes the <code>nnet</code> package you reference to fit to time series data.</p>\\n\\n<p><a href=\"http://cran.r-project.org/web/packages/forecast/index.html\" rel=\"nofollow\">http://cran.r-project.org/web/packages/forecast/index.html</a></p>\\n\\n<p>The example from the help docs:</p>\\n\\n<pre><code>fit &lt;- nnetar(lynx)\\nfcast &lt;- forecast(fit)\\nplot(fcast)\\n</code></pre>\\n\\n<p>Rob gives more context in this specific section of his online text: <a href=\"https://www.otexts.org/fpp/9/3\" rel=\"nofollow\">Forecasting: principles and practice</a>.</p>\\n\\n<p>(And a big thanks to Rob obviously.)</p>\\n',\n",
       " '<p>I assume you can\\'t just ask them if they prefer a laptop or a tablet; or you want to check what they think they prefer with what you think they should prefer...</p>\\n\\n<p>There are a number of ways to do this.  This is in fact a version of the very common real life problem of evaluating job applicants, or tenders for contracting work - you need to decide on criteria, weight them, and rate the candidates against the criteria.  You have emphasised the problem of weighting the criteria, but the rating of candidates (laptop and tablet) against the criteria is crucial, as was the choice of the six criteria in the first place.  These are largely judgement rather than statistical questions.</p>\\n\\n<p>There are two steps necessary: combine the information in the two questions to give you weightings for the criteria; and the compare the importance given to the six qualities to the performance of the two products against those six qualities.</p>\\n\\n<p>Your first problem is that you have two questions that are apparently (see my comment) getting at basically the same underlying factor and that respondents will inevitably not be completely consistent in their answers (although hopefully not as much as in your example, where storage capacity is the lowest priority but \"very important\"!)</p>\\n\\n<p>One approach to combining these two is to convert the ranking to a rating on the same scale as the second question and then take an average.  You could do this for example by $rate_{new}=\\\\\\\\frac{rate+rank*\\\\\\\\frac{4}{5}+0.2}{2}$.  </p>\\n\\n<p>This is a bit crude, but the fact is there is no really satisfactory way of combining the two without drawbacks of some sort.  Converting rankings to ratings and vice versa is a problem however you do it, and some kind of rule of thumb is needed to deal with ties in the ratings (if you want to turn them into rankings) or unknown range behind the rankings (if you want to turn them into ratings ie the user has been forced to rank from one to 6, but really might think they are all really important - or unimportant...).</p>\\n\\n<p>The next crudity is you will need to score the products against the six qualities.  Often subjects would have been asked to do this, but in this case it looks like you have to do it yourself.  You will produce a matrix like:</p>\\n\\n<pre><code>                        Tablet   Laptop\\nStorage capacity         4          2\\nPortability              1          2\\nTouch interface          1          4\\nKeyboard                 5          1\\nLong battery life        3          2\\nEntertainment on the go  1          3\\n</code></pre>\\n\\n<p>I\\'ve kept to the convention you have of low scores being good.</p>\\n\\n<p>Then you just multiply and sum your importance ratings by these quality scores and you get a score for tablet and one for laptop.  The one with the lowest score is the preference - you don\\'t need a threshold, just to compare the two scores.</p>\\n\\n<p>Note that how you score the two products against the six qualities will be crucial in this - probably more important than how you generated the weightings.  So you\\'d want to try a range of different scores and see which ones give plausible results.  There\\'s no statistical way of getting the \"right\" scores, with the information you\\'ve got.  If you knew people\\'s actual laptop/table preferences, you could perhaps generate a set of scores that produced those preferences, but then the whole exercise would be a different one.</p>\\n\\n<p>See below for some R code and output that implements this and suggests that your somewhat confused subject might actually want a laptop:</p>\\n\\n<pre><code>&gt; r1 &lt;- c(6,5,1,4,2,3)\\n&gt; r2 &lt;- c(1,3,1,1,2,4)\\n&gt; newrate &lt;- (r2+r1*4/5+.2)/2\\n&gt; products &lt;- as.matrix(data.frame(Tablet=c(4,1,1,5,3,1), Laptop=c(2,2,4,1,2,3)))\\n&gt; cbind(products, newrate)\\n     Tablet Laptop newrate\\n[1,]      4      2     3.0\\n[2,]      1      2     3.6\\n[3,]      1      4     1.0\\n[4,]      5      1     2.2\\n[5,]      3      2     1.9\\n[6,]      1      3     3.3\\n&gt; newrate%*%products\\n     Tablet Laptop\\n[1,]   36.6   33.1\\n</code></pre>\\n',\n",
       " \"<p>There are some methods like singular value decomposition (SVD), principal component analysis (PCA), factorial analysis and many more that are used to reduce a high-dimensional dataset into fewer dimensions while retaining important information. What would happen if for instance the problem was not about finding those very informative features, but synthetizing the data in a way to assure that correlation between the resulting dimensions is minimized regardless of whether the resulting number of dimensions is low enough and capable of retaining most of the variability.</p>\\n\\n<p>With traditional dimensionality reduction techniques such as SVD, PCA or FA you don't really assure that the resulting dimensions will be uncorrelated, I wonder if there is any solution to this problem.</p>\\n\",\n",
       " '<p>I am a database developer working for a sales and manufacturing business.  I am mostly ignorant about statistics.  We need useful metrics.  Our managers are tuned to accounting, and the vagaries of production tend to confound us.  We do very little measuring of our production, and what we have is poorly formed.  I should note we are a \"job shop\", not a \"flow shop\" -- we do a lot of engineer-to-order work, so the usual MRP standards are often hard to apply.  </p>\\n\\n<p>Some tradition business metrics are known to us; for example \"inventory turn-over rate.\"  But we are unable to convert those to useful information.  I believe our inability to qualify data statistically is a big reason why.  </p>\\n\\n<p>Of course we perform averaging all the time.  Rolling averages (smoothing data over time using a 3-week rolling average, for example) is a helpful extension.  Recently I discovered how to apply standard deviation to labor costing with wonderful benefits.  </p>\\n\\n<p>Now that I understand (A) averaging, (B) rolling averages, and (C) standard deviation, what are the next useful functions or functions I should seek to learn?</p>\\n\\n<p>I would love to have your insights on \"business intelligence,\" I mean defining and using metrics.  But it\\'s the use of stats to get from raw data to usable information that I\\'m really after.  No matter, give me whatever you\\'ve got.  </p>\\n',\n",
       " '<p>We are trying to predict values of variable A having N other variables. What we do, we calculate Pearson correlation between A and each of the other N variables, for last M values, using fixed M. We use variable with largest correlation coefficient as predictor.</p>\\n\\n<p>This scheme works fine when analyzing N variables during 24 month and decreasing  between month 25-36.<br>\\nWhat could come as improvement of Pearson correlation in this context?</p>\\n\\n<p>Variables are bi-valued.</p>\\n',\n",
       " \"<p>Let's take football (soccer) for example. There are 3 possible outcomes, home win, draw, away win. I took a random game from bet365<br></p>\\n\\n<pre><code>Turkey vs Ukraine\\nhwin, draw, awin\\n2.20  3.40  3.20\\n</code></pre>\\n\\n<p>So for investment of 100\\\\\\\\$ on given result, you either loose 100\\\\\\\\$ or win: 220\\\\\\\\$, 340\\\\\\\\$ or 320\\\\\\\\$. Their probability assessment doesn't add up to 100%, they take extra 5%-12%, but how did they come to these numbers (2.20, 3.40, 3.20)? Is it the betting patterns of people that bet, for example, if 90% of people put money on Turkey, <code>hwin</code> coefficient would be lower, or is it some kind of a calculation? <br>\\nThe problem with calculations is that the sample is very poor, national teams play very few games in long periods of time, between whole range of teams of different strengths, many outside parameters contribute, like injuries, current form and motivation of individual players, etc.<br>\\nIs their strategy for national championships any different, you could find more regularity as the games are more frequently played, though 4 national league games per month isn't that much (and are also played home/away, which are two very different things).<br>\\nSo basically, the question is on what do they rely the most, how do they come up to these numbers, is it the calculation, betting patterns of other players, combinations etc?<br><br>\\nOne sub question, if other gamblers have a strong influence how coefficients are put, it seems to me that such assessments would be with significant error. I don't know if you can tell the difference between 65% and 70% for a given outcome, but that difference to me is indistinguishable. To be clear, I believe that Turkey in given example is a favorite, mostly because they play at home, but are their chances for a win 45% or 55% is too abstract, if they played against Monaco national team, then I'd give you a probability for the win with much more confidence.</p>\\n\",\n",
       " '<p>I am sampling from a parameter with unknown distribution. I would like to calculate a 95% CI for the standard deviation of the sample. </p>\\n\\n<p>@cardinal provides a nice general solution for calculating a CI in his [answer] to my previous question,  <a href=\"http://stats.stackexchange.com/q/7004/2750\">Calculating required sample size, precision of variance estimate?</a>. And @erik-p provides an estimate of the standard deviation of the variance of the sample.</p>\\n\\n<p>However, in order to calculate the 95%CI for the sample variance, it seems that I need to know the distribution of the sample variance. Is it possible to calculate such an estimate without knowing the distribution from which the sample was taken?</p>\\n\\n<p>A related question is <a href=\"http://stats.stackexchange.com/q/29905/2750\">Reference for $Var[s^2]=\\\\\\\\sigma^4 \\\\\\\\left(\\\\\\\\frac{2}{n-1} + \\\\\\\\frac{\\\\\\\\kappa}{n}\\\\\\\\right)$?</a></p>\\n',\n",
       " '<p>Below is code which implements my solution and Paramonov\\'s solution (a slight edit: I have changed <code>dlmFilter(u,mod)$a</code> in the orginally posted answer by \\n<code>dlmFilter(u,mod)$m</code>).</p>\\n\\n<pre><code>library(dlm)\\nset.seed(1234)\\nreps      &lt;- 100\\nMyEstimates &lt;- YourEstimates &lt;- matrix(0,reps,2)\\nfor (i in (1:reps) ) {\\nX &lt;- r &lt;- rnorm(100)\\nu &lt;- -1*r + 0.5*rnorm(100)\\n#\\nfit &lt;- dlmMLE(u, parm = c(1, sd(u)),\\n              build = function(x)\\n                dlmModReg(r, FALSE, dV = x[2]^2,\\n                          m0 = x[1], C0 = matrix(0)))\\nYourEstimates[i,] &lt;- fit$par\\n#\\nMyModel &lt;- function(x)  dlmModReg(X, FALSE, dV = x[1]^2)\\nfit &lt;- dlmMLE(u, parm = c(0.3), build = MyModel)\\nmod &lt;- MyModel(fit$par)\\nMyEstimates[i,] &lt;- c(dlmFilter(u,mod)$m[101],fit$par[1])\\n}\\n</code></pre>\\n\\n<p>When I run the above code, this is what I get:</p>\\n\\n<pre><code>&gt; summary(YourEstimates)\\n       V1                V2         \\n Min.   :-9.5284   Min.   :-0.5747  \\n 1st Qu.:-1.4280   1st Qu.: 0.4710  \\n Median :-0.9795   Median : 0.4937  \\n Mean   :-0.9737   Mean   : 0.4369  \\n 3rd Qu.:-0.5636   3rd Qu.: 0.5215  \\n Max.   : 4.5222   Max.   : 0.5980  \\n&gt; summary(MyEstimates)\\n       V1                V2         \\n Min.   :-1.1099   Min.   :-0.6010  \\n 1st Qu.:-1.0266   1st Qu.: 0.4736  \\n Median :-0.9974   Median : 0.4961  \\n Mean   :-0.9938   Mean   : 0.4469  \\n 3rd Qu.:-0.9635   3rd Qu.: 0.5158  \\n Max.   :-0.8390   Max.   : 0.5776  \\n</code></pre>\\n\\n<p>While the first set of estimates gives similar estimates for the second parameter, it occasionally gives values well off the mark for the first. I think the reason is that \"tying\" the state to its initial value with </p>\\n\\n<pre><code>C0=matrix(0)\\n</code></pre>\\n\\n<p>leads to numerical instability, but I am not sure. In any case, you may want to look at the issue.</p>\\n',\n",
       " '<p>Write a script that aligns short DNA fragments (25-100 basepairs) to an existing genome. </p>\\n',\n",
       " '<p>One simple test that at once come to my mind is <strong>marginal homogeneity</strong> test (it is available in SPSS: it seems to me, by the looks of your chart, that you were using SPSS). This is a repeated-mesures test (I understood from your comment that you have a total sample of 50 patients X 3 variables (times), but it will compare you only 2 variables at a time, not all 3 at once. It will tell you whether the frequency distribution of 5 categories is the same in both times. Thus, it is not category-by-category test but the simultaneous test for all the categories. Missing data is the problem: you should either delete it (then your sample will be 41 instead of 50) or fill in randomly.</p>\\n\\n<p>Of course, other approaches and more complex analyses are possible with your data.</p>\\n',\n",
       " '<p>In my opinion selecting the appropriate number of lags is no different than selecting the number of input series in a stepwise forward regression procedure. The incremental importance of lags or a specific input series is the basis for the tentative model specification.</p>\\n\\n<p>Since you have asserted that the acf/pacf is the only basis for Box-Jenkins model selection, let me tell you what some experience has taught me. If a series exhibits an acf that doesn\\'t decay, the Box-Jenkins approach (circa 1965) suggests differencing the data. But if a series has a level shift, like the <a href=\"http://support.sas.com/rnd/app/da/new/802ce/ets/chap3/sect8.htm\" rel=\"nofollow\">Nile data</a>, then the \"visually apparent\" non-stationarity is a symptom of needed structure but differencing is not the remedy. This Nile dataset can be modeled without differencing by simply identifying the need for a level shift first. In a similar vein we are taught using 1960 concepts that if the acf exhibits a seasonal structure (<em>i.e.</em> significant values at lags of s,2s,3s,...) then we should incorporate a seasonal ARIMA component. For discussion purposes, consider a series that is stationary around a mean and at fixed intervals, say every June there is a \"high value\".  This series is properly treated by incorporating an \"old-fashioned\" dummy series of 0 and 1\\'s (at June) in order to treat the seasonal structure. A seasonal ARIMA model would incorrectly use memory instead of an unspecified but waiting-to-be-found X variable. These two concepts of identifying/incorporating unspecified deterministic structure are direct applications of the work of I. Chang, William Bell, George Tiao, <a href=\"http://www.unc.edu/~jbhill/tsay.pdf\" rel=\"nofollow\">R.Tsay</a>, Chen <em>et al</em> (starting in 1978) under the general concept of Intervention Detection.</p>\\n\\n<p>Even today some analysts are mindlessly performing memory maximization strategies, calling them Automatic ARIMA, without recognizing that \"mindless memory modeling\" assumes that deterministic structure such as pulses, level shifts, seasonal pulses and local time trends are non-existent or worse yet play no role in model identification. This is akin to putting one\\'s head in the sand, IMHO.</p>\\n',\n",
       " \"<p>I think meta-analyses are a great way of exploring a hypothesis when available evidence is heterogeneous. Usually however when conducting a meta-analysis one puts aggregated data in a model, possibly losing information. </p>\\n\\n<p>Therefore I am considering gathering data from published research from the authors in order to be able to conduct analysis on 'raw data' rather then aggregated data (my field is neuroscience/psychology/psychiatry). </p>\\n\\n<h3>Questions</h3>\\n\\n<ul>\\n<li>Where can I learn more about conducting such analyses?</li>\\n<li>Are researchers typically willing to share their data?</li>\\n<li>Would ethical guidelines preclude such data sharing?</li>\\n</ul>\\n\",\n",
       " '<p>Can someone tell me how the type of distribution is related to an empirical coverage probability? (for some distributions the Central-limit-theorem seems to be more easily satisfied)</p>\\n',\n",
       " '<p>Neural network may be to slow for a large number of documents (also this is now pretty much obsolete).<br>\\nAnd you may also check Random Forest among classifiers; it is quite fast, scales nice and does not need complex tuning.</p>\\n',\n",
       " '<p>Difference in differences is analogous to a within-units (patients, firms, etc.) analysis, but is applied to both units that receive a treatment and units that did not.  The analysis then, is of the difference between the two differences.  If the treatment influences the response variable, the differences should differ, but not otherwise.  </p>\\n\\n<p>Using this technique to infer causality with observational data requires the assumption that the change in the treated group would be equivalent to the change in the untreated group, if the treatment were unrelated to changes in the response variable.  This assumption might be violated, for example, if the two groups were not trending together prior to the <em>before</em> observation, or if an unobserved event that does affect the response happens to one of the groups within the same interval.</p>\\n',\n",
       " '<p>My favorite: <a href=\"http://www.evernote.com/\">Evernote</a>. You can tag entries (e.g., \\'analysis\\', \\'idea\\', etc.), you can paste pictures and graphics, and you can share notebooks with collaborators. And: it\\'s basically free (well, freemium). But the free edition is absolutely sufficient for me.</p>\\n',\n",
       " '<p>I have used them both, and have a few points to make.</p>\\n\\n<ul>\\n<li>Rmse is useful because it is simple to explain. Everybody knows what it is.</li>\\n<li>Rmse does not show relative values. If $rmse=0.2$, you must specifically know  the range $\\\\\\\\alpha &lt;y_x&lt; \\\\\\\\beta$. If $\\\\\\\\alpha=1, \\\\\\\\beta=1000$, then 0.2 is a good value. If $\\\\\\\\alpha=0, \\\\\\\\beta=1$, it does not seem not so good anymore.</li>\\n<li>Inline with the previous approach, rmse is a good way to hide the fact that the people you surveyed, or the measurements you took are mostly uniform (everybody rated the product with 3 stars), and your results look good because data helped you. If data was a bit random, you would find your model orbiting Jupiter.</li>\\n<li>Use adjusted coefficient of determination, rather than the ordinary $R^2$</li>\\n<li>Coefficient of determination is difficult to explain. Even people from the field needs a footnote tip like \\\\\\\\footnote{The adjusted coefficient of determination is the proportion of variability in a data set that can be explained by the statistical model. This value shows how well future outcomes can be predicted by the model. $R^2$ can take 0 as minimum, and 1 as maximum.}</li>\\n<li>Coefficient of determination is however very precise in telling how well your model explains a phenomena. if $R^2=0.2$, regardless of $y_x$ values, your model is bad. I believe cut off point for a good model starts from 0.6, and if you have something around 0.7-0.8, your model is a very good one. </li>\\n<li>To recap, $R^2=0.7$ says that, with your model, you can explain 70% of what is going on in the real data. The rest, 30%, is something you do not know and you cannot explain. It is probably because there are confounding factors, or you made some mistakes in constructing the model.</li>\\n<li>In computer science, almost everybody uses rmse. Social sciences use $R^2$ more often.</li>\\n<li>If you do not need to justify the parameters in your model, just use rmse. However, if you need to put in, remove or change your parameters while building your model, you need to use $R^2$ to show that these parameters can explain the data best.</li>\\n<li>If you will use $R^2$, code in the R language. It has libraries, and you just give it the data to have all results.</li>\\n</ul>\\n\\n<p>For an aspiring computer scientist, it was thrilling to write about statistics . Yours truly.</p>\\n',\n",
       " '<p>I had good success with the tree-based learners in <a href=\"http://luispedro.org/software/milk\" rel=\"nofollow\">Milk: Machine Learning Toolkit for Python</a>.  It seems to be under active development, but the documentation was a bit sparse when I was using it.  The test suite (github.com/luispedro/milk/blob/master/tests/test_adaboost.py) contains a \"boosted stump\" though, which could get you going pretty quickly:</p>\\n\\n<pre><code>import numpy as np\\nimport milk.supervised.tree\\nimport milk.supervised.adaboost\\n\\ndef test_learner():\\n    from milksets import wine\\n    learner = milk.supervised.adaboost.boost_learner(milk.supervised.tree.stump_learner())\\n    features, labels = wine.load()\\n    features = features[labels &lt; 2]\\n    labels = labels[labels &lt; 2] == 0\\n    labels = labels.astype(int)\\n    model = learner.train(features, labels)\\n    train_out = np.array(map(model.apply, features))\\n    assert (train_out == labels).mean() &gt; .9\\n</code></pre>\\n',\n",
       " \"<p>You are calculating the mean of a variable that is 0 if no event and 1 if there is an event. The sum of $N$ such (independent) binomial random variables has a variance $N\\\\\\\\times p(1-p)$. The mean has a variance $p(1-p)/N$. We can use a two-sample difference in means test to see whether the difference in proportions between the groups is significant. Calculate:</p>\\n\\n<p>$$\\\\\\\\begin{equation*}\\\\\\\\frac{p - q}{\\\\\\\\sqrt{p(1-p)/N + q(1-q)/M}}\\\\\\\\end{equation*}$$</p>\\n\\n<p>where $p$ is the proportion from group 1, which has $N$ observations, and $q$ is the proportion from group 2 with $M$ observations. If this number is large in absolute value (bigger than 1.96 is a typical norm, giving a hypothesis test with a significance level of 5%), then you can reject the claim that the two groups have the same proportion of events. </p>\\n\\n<p>This assumes that each person in group 1 has the same probability of having an event and each person in group 2 has the same probability of event, but these probabilities can differ across groups. Since you are randomly assigning people to the groups (e.g., they aren't self-selecting into them), this is a reasonably good assumption.</p>\\n\\n<p>Unfortunately, I can't help with you PHP coding, but I hope that this gets you started.</p>\\n\",\n",
       " '<p>Australia is currently having an election and understandably the media reports new political poll results daily. In a country of 22 million what percentage of the population would need to be sampled to get a statistically valid result?</p>\\n\\n<p>Is it possible that using too large a sample could affect the results, or does statistical validity monotonically increase with sample size?</p>\\n',\n",
       " '<p>I read wiki article about Apriori. I have the trouble in understanding the prune and Join step. Can anyone explain me how Apriori algorithm works in simple terms(such that Novice like me can understand easily)? </p>\\n\\n<p>It will be good if someone explains the step by step process involved in it. </p>\\n',\n",
       " '<p>There\\'s a nice paper on visualization techniques you might use by Michael Friendly:</p>\\n\\n<ul>\\n<li><a href=\"http://www.math.yorku.ca/SCS/vcd/vcdstory.pdf\" rel=\"nofollow\">Visualizing Categorical Data: Data, Stories, and Pictures</a></li>\\n</ul>\\n\\n<p>(Actually, there\\'s a whole <a href=\"http://books.google.com/books?id=eG0phz62f1cC&amp;lpg=PP1&amp;dq=michael%20friendly%20visualizing%20categorical%20data&amp;pg=PP1#v=onepage&amp;q&amp;f=false\" rel=\"nofollow\">book</a> devoted to this by the same author.)  The <a href=\"http://cran.r-project.org/web/packages/vcd/index.html\" rel=\"nofollow\">vcd</a> package in R implements many of these techniques.</p>\\n',\n",
       " '<p>Here are 3 options; hard brackets need to be filled in, while braces contain optional subcommands:</p>\\n\\n<pre><code>cross [varlist] by age {/cells count col row}.\\n\\nmeans [varlist] by age {/stat anova}.\\n</code></pre>\\n\\n<p>summarize command - best obtained through the menus via Analyze...Reports...Case Summaries.  Then you may need to double-click the resulting pivot table and right-click ... Pivoting Trays to rearrange.</p>\\n',\n",
       " '<p>We could argue forever about foundations of inference to defend both approaches, but let me propose something different. A $\\\\\\\\textit{practical}$ reason to favor a Bayesian analysis over a classical one is shown clearly by how both approaches deal with prediction. Suppose that we have the usual conditionally i.i.d. case. Classically, a predictive density is defined plugging the value $\\\\\\\\hat{\\\\\\\\theta} = \\\\\\\\hat{\\\\\\\\theta}(x_1,\\\\\\\\dots,x_n)$ of an estimate of the parameter $\\\\\\\\Theta$ into the conditional density $f_{X_{n+1}\\\\\\\\mid\\\\\\\\Theta}(x_{n+1}\\\\\\\\mid\\\\\\\\theta)$. This classical predictive density $f_{X_{n+1}\\\\\\\\mid\\\\\\\\Theta}(x_{n+1}\\\\\\\\mid\\\\\\\\hat{\\\\\\\\theta})$ does not account for the uncertainty of the estimate $\\\\\\\\hat{\\\\\\\\theta}$: two equal point estimates with totally different confidence intervals give you the same predictive density. On the other hand, the Bayesian predictive density takes into account the uncertainty about the parameter, given the information in a sample of observations, automatically, since\\n$$\\n  f_{X_{n+1}\\\\\\\\mid X_1,\\\\\\\\dots,X_m}(x_{n+1}\\\\\\\\mid x_1,\\\\\\\\dots,x_n) = \\\\\\\\int f_{X_{n+1}\\\\\\\\mid\\\\\\\\Theta}(x_{n+1}\\\\\\\\mid\\\\\\\\theta) \\\\\\\\, \\\\\\\\pi(\\\\\\\\theta\\\\\\\\mid x_1,\\\\\\\\dots,x_n) \\\\\\\\, d\\\\\\\\theta \\\\\\\\, .\\n$$ </p>\\n',\n",
       " '<p>I am trying to use a function from the <a href=\"http://cran.r-project.org/web/packages/forecast/index.html\" rel=\"nofollow\">forecast</a> package, <code>seasonaldummy()</code>, which requires a time series object as input, but I have an <a href=\"http://cran.r-project.org/web/packages/xts/index.html\" rel=\"nofollow\">xts</a> object, call it <code>x</code>. So first, is it correct that <a href=\"http://cran.r-project.org/web/packages/xts/index.html\" rel=\"nofollow\">xts</a> objects can not be directly passed to functions that require a <code>ts</code> object?</p>\\n\\n<p>I then tried to coerce it to a <code>ts</code> object. I tried this by <code>as.ts(x)</code>. The function still produces an error.</p>\\n\\n<p>I looked at the <a href=\"http://cran.r-project.org/web/packages/xts/index.html\" rel=\"nofollow\">xts</a> vignette, and in particular the <code>reclass</code> function, but all I could understand from that was how to coerce non xts objects to xts. It appears I have to do the opposite, and that <code>as.ts(x)</code> does not work.</p>\\n\\n<p>I have put a MWE below, where the last two lines generate an error.</p>\\n\\n<pre><code>&gt; a = rnorm(20)    \\n&gt; dt = seq(as.POSIXct(\"2010-03-24\"),by=\"days\",len=20)\\n&gt; library(xts)   \\n&gt; a = xts(a,dt)   \\n&gt; library(forecast)   \\n&gt; seasonaldummy(a)\\n\\nError in seasonaldummy(a) : Not a time series\\n\\n&gt; seasonaldummy(as.ts(a))\\n\\nError in seasonaldummy(as.ts(a)) : subscript out of bounds\\n</code></pre>\\n',\n",
       " '<p>I do not know anything about NLMEFITSA but quite a lot about mixed effects modelling. The covariance matrix for beta (psi) is a measure of how the fixed effects parameters are spread over the population whereas covb is a measure of the uncertainty of the parameters.</p>\\n\\n<p>Consider a one-parameter model for simplicity. If the single parameter b is assumed to be normally distributed over the population beta is its estimated mean and psi its variance (over the population). covb on the other hand is a measure of the uncertainty of the estimated value beta itself, thus an uncertatinty measure of the mean. I\\'m not sure how the program works, but also the estimated value of the covariance parameter psi is of course uncertain and covb might also inlude an estimate of the uncertainty of psi (making covb a 2x2 matrix in the one-parameter case). The matrix covb is usually computed by using Fisher information theory.</p>\\n\\n<p>Google \"fisher information matrix\" AND covariance, or something similar, for more information about parameter uncertainty in likelihood methods.</p>\\n',\n",
       " '<p>I am a software engineer by trade doing stats in my free time.  I am playing around with an implementation of <a href=\"http://research.microsoft.com/en-us/projects/trueskill/\" rel=\"nofollow\">Microsoft\\'s TrueSkill</a> rating system for ranking players and openings in from a data set of ~700,000 Dominion games.  I want to measure the log loss of predictions that the system gives for multiplayer games so that I can optimize hyperparameters of the system as well as explicitly model the inherent turn order advantage in the game.</p>\\n\\n<p>In general, the TrueSkill builds a Bayesian Graphical Model, assuming a normal distribution on player skills as well as a normal distribution for player luck per game, and produces a mean/variance for the independent performance of players.  For a two player game, the probability of one beating the other is encoded in the difference of the performance distributions.</p>\\n\\n<p>However, predicting the probability of a player winning a 3 player game seems to requires estimating the probability that given player will exceed the maximum performance of the other two players.  Is the maximum of two Gaussian also Gaussian?</p>\\n\\n<p>I\\'ve read <a href=\"http://ee162.caltech.edu/notes/lect8.pdf\" rel=\"nofollow\">these notes</a> which seem to solve a more general version of problem that I want on page 27 with equation (8-47).   </p>\\n\\n<p>Basically it says that for random variables X and Y, then if </p>\\n\\n<p>$Z = \\\\\\\\max(X, Y)$</p>\\n\\n<p>With X and Y independent,</p>\\n\\n<p>(1) $F_z(z) = F_x(x) F_y(y)$</p>\\n\\n<p>and hence</p>\\n\\n<p>(2) $f_z(z) = F_x(z)f_y(z) + f_x(z)F_y(z)$</p>\\n\\n<p>Should I then plug in the formulas for the cdf/pdfs and pray that I get something useful?</p>\\n',\n",
       " 'Refers to the AutoRegressive Integrated Moving Average model used in time series modeling both for data description and for forecasting. This model generalizes the ARMA model by including a term for differencing, which is useful for removing trends and handling some types of non-stationarity. ',\n",
       " '<p>If the goal of the standard deviation is to summarise the spread of a symmetrical data set (i.e. in general how far each datum is from the mean), then we need a good method of defining how to measure that spread.</p>\\n\\n<p>The benefits of squaring include:</p>\\n\\n<ul>\\n<li>Squaring always gives a positive\\nvalue, so the sum will not be zero.</li>\\n<li>Squaring emphasizes larger differences - a feature that turns out to be both good and bad (think of the effect outliers have).</li>\\n</ul>\\n\\n<p>Squaring however does have a problem as a measure of spread and that is that the units are all squared, where as we\\'d might prefer the spread to be in the same units as the original data (think of squared pounds or squared dollars or squared apples). Hence the square root allows us to return to the original units.</p>\\n\\n<p>I suppose you could say that absolute difference assigns equal weight to the spread of data where as squaring emphasises the extremes. Technically though, as others have pointed out, squaring makes the algebra much easier to work with and offers properties that the absolute method does not (for example, the variance is equal to the expected value of the square of the distribution minus the square of the mean of the distribution)</p>\\n\\n<p><em><strong>It\\'s important to note</em></strong> <em><strong>however</em></strong> that there\\'s no reason you couldn\\'t take the absolute difference if that is your preference on how you wish to view \\'spread\\' (sort of how some people see 5% as some magical thresh hold for p-values, when in fact it\\'s situation dependent). Indeed, there are in fact several competing methods for measuring spread.</p>\\n\\n<p>My view is to use the squared values because I like to think of how it relates to the Pythagorean Theorem of Statistics: c = sqrt(a^2 + b^2)  ...this also helps me remember that when working with independent random variables, variances add, standard deviations don\\'t. But that\\'s just my personal subjective preference.</p>\\n\\n<p>An much more indepth analysis can be read <a href=\"http://www.leeds.ac.uk/educol/documents/00003759.htm\">here</a>.</p>\\n',\n",
       " \"<p>I have registry data for which I have data for individuals for each hospital attendance over a 10 year period.  I wish to determine the effect of chronic antibiotic use upon acquisition of specific infections (outcome).  However, often the choice of antibiotic may differ over time and so I want to be able to summarise individuals' antibiotic use so that eventually I can group individuals with similar exposures.</p>\\n\\n<p>In order to do this I have generated an indicator variable that records each hospital visit as first, second visit, etc.  However I cannot work out how I may then use this to summarise individuals' exposure.  Any ideas?</p>\\n\",\n",
       " \"<p>I'm not entirely sure how to answer your question, so I'll just throw out a couple of thoughts here, and maybe something will help.  It occurs to me that it depends where estimates came from.  For example, people will often use a basic multiple regression model to calculate predictions for some outcome variable based on a set of known factors.  At the end of this process, people properly want to know how well the model's predictions do.  </p>\\n\\n<ul>\\n<li>A basic, common approach is to simply correlate the predictions with the observed values.  Obviously, you want as strong a positive correlation as possible.  Moreover, if you square the correlation, that gives you the common metric $R^2$, a measure of the model's informativeness.  </li>\\n<li>As for the correlation between the predictions and the difference\\nbetween the prediction and the observed value, you want it to be 0,\\nand it should be when coming from a multiple regression model.  Should it not be 0, that implies a bias in the process that generated the predictions.  </li>\\n<li>Your sample size (~200) should be adequate so long as you don't have\\ntoo many predictor variables.  In experiments (where observations are assigned to levels of the factors such that they are orthogonal) a typical recommendation is to have at least 10 observations per factor.  With observational research, you'll want more, depending on the level of inter-correlation amongst the predictor variables.  </li>\\n<li>As for the idea of predictions being 'significantly different' from responses, that depends on the model.  In linear regression, it implies that the model is misspecified (e.g. fitting curvilinear data with a straight line).  </li>\\n<li>One last note, it is known that the variance of a time increases with\\nmean time.  In other words, although something takes usually an hour to finish,\\nsometimes it will take more or less.  Likewise for a task that\\ntypically takes two hours, but in the latter case, the more or less\\nwill be spread out further than in the former case.  You should check\\nfor this in your data.  If you find it, you may want to transform\\nyour completion times by taking the logarithm of the values, and then recalculate your predictions.  </li>\\n</ul>\\n\\n<p>I can't tell if the situation you face is the same as a common multiple regression situation, but ideas taken from there may be of help.  Let me know if the information you need is different from this.</p>\\n\",\n",
       " '<p>Let $\\\\\\\\Phi(x) = P[X\\\\\\\\leq x]$ when X is normal with mean m and standard deviation s.  Then</p>\\n\\n<p>$$ \\\\\\\\Phi^n(x)= P[\\\\\\\\max (X_1,X_2,..., X_n) \\\\\\\\leq x].  $$</p>\\n\\n<p>So $E[\\\\\\\\max (X_1,X_2,..., X_n)] = \\\\\\\\int_{-\\\\\\\\infty}^{\\\\\\\\infty} x n \\\\\\\\Phi^{n-1}(x) \\\\\\\\Phi^{\\\\\\\\prime}(x)dx$.</p>\\n',\n",
       " '<p>My friend and I are working on a project on distributed datastructures. We were wondering how much is nearest neighbor information used in modern recommendation systems and whether it would be worthwhile to work on a distributed datastructure (say a kd-tree) for that purpose.</p>\\n\\n<p>Thanks</p>\\n',\n",
       " '<p>If I understand correctly, then you can calculate your statistic $v$ multiple times. In that case you are looking for a one-sample test for evaluating whether the $v_i$ values that are generated by using your implementation many times, follow a $\\\\\\\\chi^2_{k-1}$ distribution. So you consider $v_i$ your data, not the $x_{ki}$. The <a href=\"http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\" rel=\"nofollow\">Kolmogorov-Smirnov test</a> can be used for this. It looks at the largest vertical deviation between the observed cumulative distribution function and the theoretical one.</p>\\n',\n",
       " \"<p>I recently read an approach which is used to find the effect of changing an independent variable.</p>\\n\\n<p>They are doing a classification problem, so each data row (or record) is associated with an outcome of YES or NO.</p>\\n\\n<p>They take one data row (i.e., test row), and then build a model using the other n-1 rows (more specifically, they use k nearest neighbour to build the model). They use the model to compute the probability of the outcome of that one particular test row. They get a value of, for example, 51% chance that the outcome for this data row is YES.</p>\\n\\n<p>They then modify the value of one of the independent variable in that test row by X, and the use the model to examine the outcome again. They get a value of 55%, for example.</p>\\n\\n<p>After the steps, they say that changing the independent variable by X can increase the probability of this test row being YES by 4% (55-51). </p>\\n\\n<p>Since I am new to the field of statistical analysis and data mining...I am not sure whether this analysis approach is sound. I tried to google for other references for this approach, but I could't find any...</p>\\n\\n<p>Could someone please help me explain whether this technique is valid or point me to some references? Thank you very much in advance!</p>\\n\",\n",
       " '<p>+1 to @StatsStudent; your basic issue here is that you have few data.  However, it might help to talk a little about what those plots are there for.  Of course, you can get many things from looking at a plot, but those are the standard <code>lm()</code> diagnostic plots in R, so I will mention a conventional use for each.  </p>\\n\\n<p><strong>Residuals vs Fitted:</strong><br>\\nThis plot can be used to assess model misspecification.  For example, if you have only one covariate, you can use this to detect if the wrong functional form has been used.  Imagine if the residuals formed a curve, with the residuals below the dotted gray line on the sides, and above the line in the center, that would suggest you need to add a squared term to capture a curvilinear relationship.  </p>\\n\\n<p>R has helpfully laid a <a href=\"http://en.wikipedia.org/wiki/Local_regression\">loess</a> line over the residuals to make it easier to see whatever structure there may be in the residuals.  When the smoothing bandwidth parameter, $\\\\\\\\alpha$, is small, the line will bounce around much more, whereas when it\\'s large, the loess fit will tend to be fairly straight no matter how curvy the data are.  In your case, you have few data, and $\\\\\\\\alpha$ is too small, so the line zig-zags from one point to the next, but this only means that the default setting for the loess line is miscalibrated for very small datasets.  You don\\'t see the kind of systematic deviations I described above.</p>\\n\\n<p><strong>Normal Q-Q:</strong><br>\\nYour next plot is a <a href=\"http://en.wikipedia.org/wiki/Qq_plot\">qq-plot</a>.  This is the plot you should primarily focus on to determine if your residuals are roughly normally distributed.  (Note that only the <a href=\"http://stats.stackexchange.com/questions/12262/what-if-residuals-are-normally-distributed-but-y-is-not\">residuals</a> need to be normally distributed.)  Here we see that your data track the dotted gray line very well, so there is no indication that your residuals deviate from normality.  (There is one potentially interesting point, #29, that deviates from the line, we\\'ll come back to that in a moment.)  </p>\\n\\n<p><strong>Scale-Location:</strong><br>\\nThe scale-location plot can help you determine if there is substantial <a href=\"http://en.wikipedia.org/wiki/Homoscedasticity\">heteroskedasticity</a>.  What you are looking for here is typically if the plot is fan-shaped, with one side more spread out than the other.  You don\\'t have that.  (Once again, the loess fit goes up in the center, but you have more data there, so they ought to spread further, and $\\\\\\\\alpha$ is again too small for your $N$.)  </p>\\n\\n<p><strong>Residuals vs Leverage:</strong><br>\\nThis graph helps you determine if some of your data are driving your results.  That is, you don\\'t want to draw the conclusion that is based on just a couple of data points, where the rest of your data don\\'t suggest that conclusion.  This is a question of <a href=\"http://en.wikipedia.org/wiki/Leverage_%28statistics%29\">leverage</a>.  Just because a datum has a large residual value doesn\\'t mean it exerts much influence on the estimated slope, it depends on where that datum lies along the x-axis.  Data near the mean of x exert much less leverage even if their residual values are large, whereas smaller residuals can exert substantial leverage if they are far enough away from the mean of x.  What we see here is that all your data have <a href=\"http://en.wikipedia.org/wiki/Cook%27s_distance\">Cook\\'s distance</a> values less than .5 (including point #29), so you don\\'t seem to have a problem with that, either.  </p>\\n\\n<p>In sum, these plots give you reason to have confidence in your model.  </p>\\n',\n",
       " '<p>It seems that you have a predictor, the answer to \"How long are you employed as a dentist ?\", which could be discrete or continuous depending on what types of answer you allowed; &amp; a response, the total number of points, which can likely be treated as continuous.  Perform  analysis of variance or linear regression as appropriate (&amp; check the assumptions). </p>\\n',\n",
       " '<p>No, $O(n\\\\\\\\log(n))$ is the lower theoretical bound (see (1)) for selecting the $k^{th}$ element among all $\\\\\\\\frac{n(n-1)}{2}$ possible $|x_i - x_j|: 1 \\\\\\\\leq i \\\\\\\\lt j \\\\\\\\leq n$. </p>\\n\\n<p>You can get $O(1)$ space, but only by naively checking all combinations of $x_i-x_j$ in time $O(n^2)$.</p>\\n\\n<p>The good news is that you can use the $\\\\\\\\tau$ estimator of scale (see (2) and (3) for an improved version and some timing comparisons), implemented in the function \\n<strong>scaleTau2()</strong> in the <strong>R</strong> package <a href=\"http://cran.r-project.org/web/packages/robustbase/index.html3.\" rel=\"nofollow\"><strong>robustbase</strong></a>. The univariate $\\\\\\\\tau$ estimator is an two-step (i.e. re-weighted) estimator of scale. It has 95 percent Gaussian efficiency, 50 percent breakdown point, and complexity of $O(n)$ time and $O(1)$ space (plus it can easily be made \\'online\\', shaving off half the computational costs in repeated use -- although you will have to dig into the <strong>R</strong> code to implement this option, it is rather straightforward to do).</p>\\n\\n<ol>\\n<li>The complexity of selection and ranking in X + Y and matrices with sorted columns\\nG. N. Frederickson and D. B. Johnson, Journal of Computer and System Sciences\\nVolume 24, Issue 2, April 1982, Pages 197-208.</li>\\n<li>Yohai, V. and Zamar, R. (1988). High breakdown point estimates of regression by means of the minimization of an efficient scale. Journal of the American Statistical Association 83 406–413.</li>\\n<li>Maronna, R. and Zamar, R. (2002). Robust estimates of location and dispersion for high-\\ndimensional data sets. Technometrics 44 307–317</li>\\n</ol>\\n\\n<p><strong>Edit</strong> To use this </p>\\n\\n<ol>\\n<li>Fire up R (its free and can be downloaded from <a href=\"http://www.freestatistics.org/cran/2.\" rel=\"nofollow\">here</a>)</li>\\n<li><p>install the package by tipping </p>\\n\\n<p>install.packages(\"robustbase\")</p></li>\\n<li><p>load the package by tipping </p>\\n\\n<p>library(\"robustbase\")</p></li>\\n<li><p>load your data file and run the function:</p>\\n\\n<p>mydatavector&lt;-read.table(\"adress to my file in text format\",header=T)</p>\\n\\n<p>scaleTau2(mydatavector)</p></li>\\n</ol>\\n',\n",
       " '<blockquote>\\n  <p><strong>Possible Duplicate:</strong><br>\\n  <a href=\"http://stats.stackexchange.com/questions/28656/likelihood-ratio-testing-in-multinomial-logistic-regression-using-spss\">Likelihood Ratio Testing in Multinomial logistic regression using SPSS</a>  </p>\\n</blockquote>\\n\\n\\n\\n<p>I am doing multinomial logistic regression.</p>\\n\\n<p>Dependent variable is categorical with 6 categories.\\n15 Independent variables are measured on an interval scale (included as covariates in SPSS)</p>\\n\\n<p>Now I am watching the LR test. None of the independent variables have a chi-square value and no degrees of freedom are shown, so significance is not specified. (except for the constant, which is significant and has 5 df\\'s).</p>\\n\\n<p>When I omit one IV in the model, there are chisquare values, df\\'s and significance values for the independent variables. Was the first model over-parametrised or something?</p>\\n\\n<p>what is the explanation for this situation? And is it valid to omit one independent variable in the model?</p>\\n',\n",
       " \"<p>Note that since the denominator only depends on the margins, not on the association, the exact permutation p-value is the same for any of the statistics that has $N_c-N_d$ on the numerator (Kendall's tau, tau-b, tau-c, Somer's-D etc). </p>\\n\\n<p>There are algorithms for computing p-values of all these measures of association for ordinal data efficiently, by taking account of the ordering of the permutations induced by some simple form of the test statistic, and only considering the ones more extreme than the test statistic (or considering as few additional ones as possible), generally arising from ideas based on Mehta and Patel's networking algorithm, though there have been developments and ideas from many authors.</p>\\n\\n<p>Some stats packages implement such algorithms. It might be worth checking whether some of the exact-test related packages already have an efficient version of it for some version of the Kendall statistic.</p>\\n\\n<p>For example, I believe SPSS Exact Tests has this implemented for the Kendall tau. </p>\\n\\n<p>(Out of curiosity, why would it matter for you if you were only able to get a probabilistic <em>bound</em> on the p-value using resampling? How does (say) knowing the p-value is almost certainly less that $3.2 \\\\\\\\times 10^{-5}$ rather than computing it to be exactly $1.6245 \\\\\\\\times 10^{-6}$? What additional information does that give you?)</p>\\n\",\n",
       " \"<p>Think about the trapezoid rule <code>integrate.xy()</code> uses. For the normal distribution, it will <em>underestimate</em> the area under the curve in the interval (-1,1) where the density is concave (and hence the linear interpolation is below the true density), and <em>overestimate</em> it elsewhere (as the linear interpolation goes on top of the true density). Since the latter region is larger (in Lesbegue measure, if you like), the trapezoid rule tends to overestimate the integral. Now, as you move to smaller bandwidths, pretty much all of your estimate is piecewise convex, with a lot of narrow spikes corresponding to the data points, and valleys between them. That's where the trapezoid rule breaks down especially badly.</p>\\n\",\n",
       " '<p>If I have a population and want to draw samples from there, what are the steps to determine the number of samples to draw from the population? In other words, if I have population N and want to draw k samples from them so that the samples are representative of the entire population, how can I determine k?</p>\\n\\n<p>I am having some insights from <a href=\"http://www.isixsigma.com/tools-templates/sampling-data/how-determine-sample-size-determining-sample-size/\" rel=\"nofollow\">here</a> but in the end of this document I find- </p>\\n\\n<blockquote>\\n  <p>You can still use this formula if you don’t know your population standard deviation</p>\\n</blockquote>\\n\\n<p>which is my case (i.e., I do not know the population variance). How can I use that formula if I don\\'t know the population standard deviation/ variance? </p>\\n\\n<p>n.b. I posted this question to that forum but still no luck.</p>\\n',\n",
       " \"<p>We have created a questionnaire. In this questionnaire there are different dimensions with different answering scales. </p>\\n\\n<p>Because of our rightly skewed data we log transformed our data. But here is the thing: </p>\\n\\n<p>Because of the different answering scales (some are Likert 5, some Likert 7, and even a dichotomous scale) they have suggested to transform our data into z-scores. This would be applicable if the data wasn't log transformed, but is it usefull (possible) to transform the log transformed scores into z-scores? </p>\\n\\n<p>P.s.) We eventually want to perform a Bivariate Correlation and Linear regression (with SPSS 16). </p>\\n\",\n",
       " '<p>What are some of the best ranking algorithms with inputs as up and down votes?</p>\\n',\n",
       " \"<p>If I have daily data set and it's a non-stationary series, then what is the lag I have to consider for the first difference 1 or 7?</p>\\n\",\n",
       " '<h3>General Advice</h3>\\n\\n<ul>\\n<li>Start analysing data and reading the analyses of other researchers.\\nThis should assist you in mapping statistical techniques onto data analytic problems.</li>\\n<li>Read some applied statistics textbooks related to a domain that you are interested in.</li>\\n<li>If you have specific questions (e.g., when would you report the SD versus the variance? or when would you use the median rather than the mean), do a search and see if a question already exists on the site. If no such question exists, ask it here.</li>\\n</ul>\\n\\n<h3>Mean, SD, Var, CI, Median</h3>\\n\\n<p>I\\'d broadly classify the five things you mentioned into</p>\\n\\n<ul>\\n<li>Measures of central tendency: Mean, Median\\n<ul>\\n<li>There are questions on this site that discuss when to use mean versus median <a href=\"http://stats.stackexchange.com/questions/2547/why-is-median-age-a-better-statistic-than-mean-age\">such as this one</a>.</li>\\n</ul></li>\\n<li><p>Measures of spread: SD, Var</p></li>\\n<li><p>Measures of confidence in parameter estimation: CI</p></li>\\n</ul>\\n',\n",
       " '<p>The main difficulty in practice is not the statistical uncertainty that a fluke streak of luck would have given one candidate more votes.  The main difficulty, by an order of magnitude or more, is that <strong>the ballots which have been opened are almost never an unbiased sample of the votes cast.</strong> If you ignore this effect, you get the famous error <a href=\"http://en.wikipedia.org/wiki/Dewey_Defeats_Truman\">\"Dewey Defeats Truman,\"</a> which occurred with a large biased sample.</p>\\n\\n<p>In practice, voters who favor one candidate versus another are not equally distributed by region, by whether they work during the day, or by whether they would be deployed overseas hence would vote by absentee ballots. These are not small differences.</p>\\n\\n<p>I think what news organizations do now is to break the population into groups and use the results to estimate how each group voted (including turnout). These may be based on models and prior assumptions based on previous elections, not just the data from this election. These may not take into account oddities such as the <a href=\"http://www.asktog.com/columns/042ButterflyBallot.html\">butterfly ballots of Palm Beach.</a></p>\\n',\n",
       " \"<p>If you were still in school then you might have a couple points taken off for saying that you accept the null hypothesis.  The purists (well frequentist purists) will always say that we never accept the null, just fail to reject it.</p>\\n\\n<p>As a style thing, generally test functions will return an object with the test statistic, p-value, etc. and not print anything.  Then a print method will be used to print the results nicely.  But for learning or simple use what you have done is fine (I would go the other route if you are planning on building on this, or doing more of your own tests).</p>\\n\\n<p>In your last cat statement you use relative_error_warning, but I don't see it defined anywhere, did you mean rel_error_warn?  Not that that line is ever likely to be run.</p>\\n\\n<p>Everything looks correct, both your calculations and the running of t.test, I would expect the only differences you ever see to be rounding error (or due to handling of missing values).</p>\\n\",\n",
       " '<p>These recent papers by Stigler, where he argues (convincingly I believe) for the types of periods you seem to have in mind. </p>\\n\\n<blockquote>\\n  <p>Stigler, Stephen M. 2010. Darwin, Galton and the statistical\\n  enlightenment. <em>Journal of the Royal Statistical Society: Series A</em>\\n  <a href=\"http://dx.doi.org/10.1111/j.1467-985X.2010.00643.x\">173(3):469-482</a>.</p>\\n  \\n  <p>Stigler, Stephen M. 2012. Studies in the history of probability and\\n  statistics, L: Karl Pearson and the Rule of Three. <em>Biometrika</em> <a href=\"http://dx.doi.org/10.1093/biomet/asr046\">99(1):\\n  1-14</a>.</p>\\n</blockquote>\\n',\n",
       " '<p>What assumptions must be fulfilled in quantile regression?</p>\\n',\n",
       " \"<p>I plan to use libSVM for a one-class svm problem, but I'm not sure about the meaning of <code>nu</code> in <code>svm_parameter</code>. </p>\\n\\n<p>Does it mean the probability that a test point lies <em>outside</em> of a set S (estimated from the training data) equals <code>nu</code>?</p>\\n\",\n",
       " \"<p>I developed a composite score by converting four items to z-scores, then I summed up the z-scores to form a composite measure. </p>\\n\\n<p>However, when I've calculated means to compare finding for two different groups on a couple datasets using this composite score, the mean scores are the same distance from zero. </p>\\n\\n<p>For example,</p>\\n\\n<ul>\\n<li>For one dataset, M1= 0.76 and M2= -0.76 </li>\\n<li>For anotherdataset M1= 1.34 and M2= -1.34. </li>\\n</ul>\\n\\n<p>Is this typical finding for this type of composite score? </p>\\n\\n<p>It seems odd to me.</p>\\n\",\n",
       " '<p>You don\\'t need density estimation.  I think the answer to your problem is to use nonparametric tolerance intervals.  This is very well explained in the book Statistical Intervals by Hahn and Meeker.  See the following amazon link: <a href=\"http://rads.stackoverflow.com/amzn/click/0471887692\" rel=\"nofollow\">http://www.amazon.com/Statistical-Intervals-Practitioners-Probability-Statistics/dp/0471887692/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1339464464&amp;sr=1-1</a> .  To give a brief description tolerance intervals are intervals that cover at least a specified proportion of the probability distribution with a specified level of confidence.  Nonparametric tolerance intervals do this using properties of order statistics.</p>\\n',\n",
       " '<p>The simple correlation approach isn\\'t the right way to analyze results from method comparison studies. There are (at least) two highly recommended books on this topic that I referenced at the end (1,2). Briefly stated, when comparing measurement methods we usually expect that (a) our conclusions should not depend on the particular sample used for the comparison, and (b) measurement error associated to the particular measurement instrument should be accounted for. This precludes any method based on correlations, and we shall turn our attention to variance components or mixed-effects models that allow to reflect the systematic effect of item (here, item stands for individual or sample on which data are collected), which results from (a).</p>\\n\\n<p>In your case, you have single measurements collected using two different methods (I assume that none of them might be considered as a gold standard) and the very basic thing to do is to plot the differences ($X_1-X_2$) versus the means ($(X_1+X_2)/2$); this is called a Bland-Altman plot. It will allow you to check if (1) the variations between the two set of measurements are constant and (2) the variance of the difference is constant across the range of observed values. Basically, this is just a 45° rotation of a simple scatterplot of $X_1$ vs. $X_2$, and its interpretation is close to a plot of fitted <em>vs.</em> residuals values used in linear regression. Then,</p>\\n\\n<ul>\\n<li>if the difference is constant (<em>constant bias</em>), you can compute the limit of agreement (see (3))</li>\\n<li>if the difference is not constant across the range of measurement, you can fit a linear regression model between the two methods (choose the one you want as predictor)</li>\\n<li>if the variance of the differences is not constant, try to find a suitable transformation that makes the relationship linear with constant variance</li>\\n</ul>\\n\\n<p>Other details may be found in (2), chapter 4.</p>\\n\\n<p><strong>References</strong></p>\\n\\n<ol>\\n<li>Dunn, G (2004). <em>Design and Analysis of Reliability Studies</em>. Arnold. See the review in the <a href=\"http://ije.oxfordjournals.org/content/34/2/499.1.full\"><em>International Journal of Epidemiology</em></a>.</li>\\n<li>Carstensen, B (2010). <em>Comparing clinical measurement methods</em>. Wiley. See the <a href=\"http://staff.pubhealth.ku.dk/~bxc/MethComp/\">companion website</a>, including R code.</li>\\n<li>The original article from Bland and Altman, <a href=\"http://www-users.york.ac.uk/~mb55/meas/ba.htm\">Statistical methods for assessing agreement between two methods of clinical measurement</a>.</li>\\n<li>Carstensen, B (2004). <a href=\"http://biostatistics.oxfordjournals.org/content/5/3/399.full.pdf\">Comparing and predicting between several methods of measurement</a>. <em>Biostatistics</em>, <em>5(3)</em>, 399–413.</li>\\n</ol>\\n',\n",
       " '<p>The answers to this question on SO returned a set of approximately 125 one- to two-letter names:\\n<a href=\"http://stackoverflow.com/questions/6979630/what-1-2-letter-object-names-conflict-with-existing-r-objects\">http://stackoverflow.com/questions/6979630/what-1-2-letter-object-names-conflict-with-existing-r-objects</a></p>\\n\\n<pre><code>  [1] \"Ad\" \"am\" \"ar\" \"as\" \"bc\" \"bd\" \"bp\" \"br\" \"BR\" \"bs\" \"by\" \"c\"  \"C\" \\n [14] \"cc\" \"cd\" \"ch\" \"ci\" \"CJ\" \"ck\" \"Cl\" \"cm\" \"cn\" \"cq\" \"cs\" \"Cs\" \"cv\"\\n [27] \"d\"  \"D\"  \"dc\" \"dd\" \"de\" \"df\" \"dg\" \"dn\" \"do\" \"ds\" \"dt\" \"e\"  \"E\" \\n [40] \"el\" \"ES\" \"F\"  \"FF\" \"fn\" \"gc\" \"gl\" \"go\" \"H\"  \"Hi\" \"hm\" \"I\"  \"ic\"\\n [53] \"id\" \"ID\" \"if\" \"IJ\" \"Im\" \"In\" \"ip\" \"is\" \"J\"  \"lh\" \"ll\" \"lm\" \"lo\"\\n [66] \"Lo\" \"ls\" \"lu\" \"m\"  \"MH\" \"mn\" \"ms\" \"N\"  \"nc\" \"nd\" \"nn\" \"ns\" \"on\"\\n [79] \"Op\" \"P\"  \"pa\" \"pf\" \"pi\" \"Pi\" \"pm\" \"pp\" \"ps\" \"pt\" \"q\"  \"qf\" \"qq\"\\n [92] \"qr\" \"qt\" \"r\"  \"Re\" \"rf\" \"rk\" \"rl\" \"rm\" \"rt\" \"s\"  \"sc\" \"sd\" \"SJ\"\\n[105] \"sn\" \"sp\" \"ss\" \"t\"  \"T\"  \"te\" \"tr\" \"ts\" \"tt\" \"tz\" \"ug\" \"UG\" \"UN\"\\n[118] \"V\"  \"VA\" \"Vd\" \"vi\" \"Vo\" \"w\"  \"W\"  \"y\"\\n</code></pre>\\n\\n<p>And R import code:</p>\\n\\n<pre><code>nms &lt;- c(\"Ad\",\"am\",\"ar\",\"as\",\"bc\",\"bd\",\"bp\",\"br\",\"BR\",\"bs\",\"by\",\"c\",\"C\",\"cc\",\"cd\",\"ch\",\"ci\",\"CJ\",\"ck\",\"Cl\",\"cm\",\"cn\",\"cq\",\"cs\",\"Cs\",\"cv\",\"d\",\"D\",\"dc\",\"dd\",\"de\",\"df\",\"dg\",\"dn\",\"do\",\"ds\",\"dt\",\"e\",\"E\",\"el\",\"ES\",\"F\",\"FF\",\"fn\",\"gc\",\"gl\",\"go\",\"H\",\"Hi\",\"hm\",\"I\",\"ic\",\"id\",\"ID\",\"if\",\"IJ\",\"Im\",\"In\",\"ip\",\"is\",\"J\",\"lh\",\"ll\",\"lm\",\"lo\",\"Lo\",\"ls\",\"lu\",\"m\",\"MH\",\"mn\",\"ms\",\"N\",\"nc\",\"nd\",\"nn\",\"ns\",\"on\",\"Op\",\"P\",\"pa\",\"pf\",\"pi\",\"Pi\",\"pm\",\"pp\",\"ps\",\"pt\",\"q\",\"qf\",\"qq\",\"qr\",\"qt\",\"r\",\"Re\",\"rf\",\"rk\",\"rl\",\"rm\",\"rt\",\"s\",\"sc\",\"sd\",\"SJ\",\"sn\",\"sp\",\"ss\",\"t\",\"T\",\"te\",\"tr\",\"ts\",\"tt\",\"tz\",\"ug\",\"UG\",\"UN\",\"V\",\"VA\",\"Vd\",\"vi\",\"Vo\",\"w\",\"W\",\"y\")\\n</code></pre>\\n\\n<p>Since the point of the question was to come up with a memorable list of object names to avoid, and most humans are not so good at making sense out of a solid block of text, I would like to visualize this.  </p>\\n\\n<p>Unfortunately I\\'m not exactly certain of the best way to do this.  I had thought of something like a stem-and-leaf plot, only since there are no repeated values each \"leaf\" was placed in the appropriate column rather than being left justified.  Or a wordcloud-style adaptation where letters are sized according to its prevalence.</p>\\n\\n<p><strong>How might this be most clearly and efficiently be visualized?</strong></p>\\n\\n<p>Visualizations which do either of the following fit in the spirit of this question:</p>\\n\\n<ul>\\n<li><p>Primary goal: Enhance the memorizability of the set of names by revealing patterns in the data</p></li>\\n<li><p>Alternate goal: Highlight interesting features of the set of names (e.g. which help visualize the distribution, most common letters, etc.)</p></li>\\n</ul>\\n\\n<p>Answers in R are preferred, but all interesting ideas are welcome.</p>\\n\\n<p>Ignoring the single-letter names is allowed, since those are easier to just give as a separate list.</p>\\n',\n",
       " \"<p>If in post hoc testing Group 3's mean was significantly different from all the others' then you've already shown that XB is different from AB. Am I missing something?  Your statement about B's effect (and its being lost when combined with A's) would be correct.</p>\\n\",\n",
       " '<p>I heard Kernel Logistic Regression is a classical combination of kernel methods and Logistic regression, but I cannot find any major reference (book, or paper) on this topic. Can you give me any suggestions? Thanks. </p>\\n',\n",
       " '<p>Jeromy Anglim and IrishStat both give great answers, but they sound maybe a little more complex than what you\\'re looking for.</p>\\n\\n<ol>\\n<li><p>A simpler method could could be to perform a linear regression on your data, to get <code>PageViews = a * Date + b</code> for some constants <code>a</code> and <code>b</code>; the constant <code>a</code> is then a measure of the linear \"slope\" of your data, which you could use to measure how much the link is trending. However, this might not work so well if your data doesn\\'t follow a linear trend (the example in your other link looks pretty linear, but you could imagine that your link has instead been growing exponentially lately).</p></li>\\n<li><p>So another approach could be to convert your pageviews into ranks (e.g., in article 1, 100 is the lowest value, so convert that into a 1; 80 is the 2nd-lowest value, so convert that into a 2; 60 is the highest value, so convert that into a 3), and then take the correlation of these ranks with <code>(1,2,...,n)</code> (where <code>n</code> is the total number of dates you have).</p></li>\\n</ol>\\n\\n<p>For example, if your article behaves like</p>\\n\\n<pre><code>Date, PageViews, Rank\\nJune 1, 100, 1\\nJune 2, 120, 3\\nJune 3, 115, 2\\nJune 4, 125, 4\\nJune 5, 150, 5\\n</code></pre>\\n\\n<p>Then you would take the correlation between <code>(1,3,2,4,5)</code> and <code>(1,2,3,4,5)</code> to get a trending score of 0.9. (Note that under this method, though, pageviews of <code>(100, 120, 115, 125, 150)</code> have the same trending score as <code>(100, 300, 299, 7000, 35000)</code>, which may or may not be what you want, since the latter is growing faster. In other words, this method tells you how strong the <em>direction</em> of the trend is, but not the magnitude. If you do want to get a sense of the magnitude, then you could just repeat these methods on the day-by-day changes of pageviews, i.e., determine whether the day-by-day changes are trending upwards or downwards.)</p>\\n',\n",
       " '<p>I was just curious. I was watching this movie phd comics where one of the professor says grade the papers so that it has a gaussian distribution with mean 81 and standard deviation 12.</p>\\n\\n<p>I am a bit confused, to get the data to follow such distribution, I will have to change the data or the grades that I have already given. Is there a standard procedure to do this? I mean lets say I have already graded. So how can I change it to the given distribution</p>\\n',\n",
       " '<p>Reporting p-values when you did data-mining (hypothesis discovery) instead of statistics (hypothesis testing).</p>\\n',\n",
       " '<p>I would say that HBM is certainly \"more Bayesian\" than EB, as marginalizing is a more Bayesian approach than optimizing.  Essentially it seems to me that EB ignores the uncertainty in the hyper-parameters, whereas HBM attempts to include it in the analysis.  I suspect HMB is a good idea where there is little data and hence significant uncertainty in the hyper-parameters, which must be accounted for.  On the other hand for large datasets EB becomes more attractive as it is generally less computationally expensive and the the volume of data often means the results are much less sensitive to the hyper-parameter settings.</p>\\n\\n<p>I have worked on Gaussian process classifiers and quite often optimizing the hyper-parameters to maximize the marginal likelihood results in over-fitting the ML and hence significant degradation in generalization performance.  I suspect in those cases, a full HBM treatment would be more reliable, but also much more expensive.</p>\\n',\n",
       " '<p>A man and a woman asked, what is the probability that he/she went out into the street, meet a dinosaur.</p>\\n\\n<p>Man begins to count and, finally, gives his version: \"Taking into account all the possible factors - zero point three trillion.</p>\\n\\n<p>She also meets once: \"Fifty-fifty. \"Why?\" - Say it. \"It\\'s very simple: either meeting or not meeting.\"</p>\\n',\n",
       " '<p>Two random variables are defined as <a href=\"http://en.wikipedia.org/wiki/Subindependence\" rel=\"nofollow\">subindependent</a> if their covariance is zero--in other words, if they are <a href=\"http://en.wikipedia.org/wiki/Uncorrelated\" rel=\"nofollow\">uncorrelated</a>. The latter link notes that \"not all uncorrelated variables are independent. For example, if $X$ is a continuous random variable uniformly distributed on $[−1, 1]$ and $Y = X^2$, then $X$ and $Y$ are uncorrelated even though $X$ determines $Y$ and a particular value of $Y$ can be produced by only one or two values of $X$.\" So subindependence, as you can guess from the name, is a weak form of independence.</p>\\n\\n<p>Soon after reading this, I was looking at <a href=\"http://en.wikipedia.org/wiki/Pearson%27s_chi-square_test#Test_of_independence\" rel=\"nofollow\">Pearson\\'s chi-square test for independence</a>. The wikipedia page says that \"for the test of independence, a chi-square probability of less than or equal to 0.05 (or the chi-square statistic being at or larger than the 0.05 critical point) is commonly interpreted by applied workers as justification for rejecting the null hypothesis that the row variable is independent of the column variable. The alternative hypothesis corresponds to the variables having an association or relationship where the structure of this relationship is not specified.\" A previous CV answer (<a href=\"http://stats.stackexchange.com/questions/1562/what-dependence-is-implied-by-a-chi-square-test-for-independence\">here</a>) also indicates that this is what the chi-square test is looking for. </p>\\n\\n<p>Now, how can you test a null hypothesis of independence by looking for a correlation but not define the lack of a correlation as indicative of independence? Granted, my skepticism is based on my reading of the Wikipedia pages for these concepts, which could easily be flawed. But it seems to me like Pearson\\'s chi-square method must be testing for the lack of *sub*independence. Is there something wrong with my conclusions? Or, is this already common knowledge? </p>\\n',\n",
       " '<p>If i want to be Data Scientist in future,\\ncan somebody advise me <strong>sites/books/courses</strong> and other good things to learn?</p>\\n\\n<p>Thanks!</p>\\n\\n<p>P.S: i know statistical analysis and R/Matlab/Excel/SAS on different levels, but want to rise my skills in it.</p>\\n',\n",
       " '<p>In general, what the the standard hypothesis tests in all types of regression? We get some coefficients and then test the hypotheses that these coefficients are significant (i.e not equal to $0$)? For example, in logistic regression the p-value indicates that the regression coefficients are significant or not? Likewise with linear regression?</p>\\n\\n<blockquote>\\n  <p>Question: Suppose coefficients $\\\\\\\\beta_0$ and $\\\\\\\\beta_1$ are significant in linear regression. Can we deduce anything about the significance of $\\\\\\\\beta_0+\\\\\\\\beta_1$? What about $\\\\\\\\beta_0= \\\\\\\\beta_1$?</p>\\n</blockquote>\\n',\n",
       " '<p>I\\'m learning through doing here guys, so I hope this question is considered OK (I\\'ll edit the question down as I go - I\\'ll remove the intro etc).</p>\\n\\n<p>I am trying to plot the empirical cumulative distribution Frequency of a data-set with 781 observations.  The data-set looks like this: </p>\\n\\n<p>(1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n8\\n8\\n8\\n8\\n8\\n8\\n8\\n8\\n8\\n8\\n8\\n8\\n8\\n8\\n9\\n9\\n9\\n9\\n9\\n9\\n9\\n9\\n9\\n9\\n9\\n9\\n9\\n9\\n9\\n9\\n9\\n9\\n10\\n10\\n10\\n10\\n10\\n10\\n10\\n10\\n10\\n10\\n10\\n10\\n10\\n11\\n11\\n11\\n11\\n11\\n12\\n12\\n12\\n12\\n12\\n12\\n12\\n12\\n12\\n12\\n13\\n13\\n13\\n13\\n13\\n13\\n13\\n14\\n14\\n14\\n14\\n14\\n15\\n15\\n15\\n15\\n16\\n16\\n17\\n17\\n17\\n19\\n19\\n20\\n21\\n21\\n21\\n21\\n22\\n22\\n23\\n23\\n).  </p>\\n\\n<p>I use the following function in R (which I pulled from <a href=\"http://www.r-bloggers.com/example-7-8-plot-two-empirical-cumulative-density-functions-using-available-tools/\" rel=\"nofollow\">r-bloggers</a>): </p>\\n\\n<pre><code>plot(ecdf(V1), verticals=TRUE, pch=46)\\n</code></pre>\\n\\n<p>which produces the following graph:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/dy4Gp.png\" alt=\"ECDF plot\"> </p>\\n\\n<p>EDIT ===== </p>\\n\\n<p>The graph plots the the actual observation on the x-axis and the percentage of observations on the y-axis.      </p>\\n\\n<p>Thank you for your help,</p>\\n\\n<p>slotishtype</p>\\n',\n",
       " '<p>What are some hot topics that mathematical statistics researchers are studying now?</p>\\n',\n",
       " '<p>Only if your model is a random walk or a simple mean model ,otherwise you will have to forecast out 16 periods for each of your 250 time series. You might want to take into account some factors like 1) day-of-the-week-effects 2 ) auto-projective structure i.e. the impact of previous values on the forecast 3) the impact of events like holidays such as lead,contempraneous and lag effects 4) possible changes in the model parameters over time 5) possible changes in the variance of the errors over time 6) level shifts in your time series 7) local time trends in your time series 8) the impact of unusual values ( one time anomalies ) . If you deal corrrectly with these eight considerations you forecast might be useful.</p>\\n',\n",
       " '<p>How do you interpret an odds ratio of an interaction between two continuous variables $x_1, x_2$? Would you fix $x_2$ and let $x_1 = x$ and $x_1 = x+1$?</p>\\n',\n",
       " '<p>See Josh Angrist on exactly this question: <a href=\"http://www.mostlyharmlesseconometrics.com/2009/10/adding-lagged-dependent-vars-to-differenced-models/\" rel=\"nofollow\">http://www.mostlyharmlesseconometrics.com/2009/10/adding-lagged-dependent-vars-to-differenced-models/</a>.  He comes down largely against including the lagged DV in your model.  There is nothing in his response that is not in the responses above, but a further succinct answer to your question may help.</p>\\n',\n",
       " '<p>@whuber gave an exhaustive explanation, I just want to point out that there is a standard statistical distribution corresponding to this scenario: the <em>hypergeometric</em> distribution. So you can obtain any such probabilities directly in, say, R:</p>\\n\\n<p>Probability of exactly 2 out of 12 selected:</p>\\n\\n<pre><code>   &gt; dhyper(2, 12, 363-12, 232)\\n   [1] 0.0008498838\\n</code></pre>\\n\\n<p>Probability of 2 or fewer out of 12 selected:</p>\\n\\n<pre><code>   &gt; phyper(2, 12, 363-12, 232)\\n   [1] 0.000934314\\n</code></pre>\\n',\n",
       " '<p>If you use $n$ bins, you will use $n-1$ degrees of freedom. For the same cost in degrees of freedom, you can fit $n-1$ restricted cubic splines. Thus, there should definitely not be more overfitting if you use splines.</p>\\n\\n<p>Conversely, discretizing continuous variables is almost always a bad idea, see, e.g., <a href=\"http://demonstrations.wolfram.com/MedianSplit/\">here</a> or <a href=\"http://psych.colorado.edu/~mcclella/MedianSplit/\">here</a> or <a href=\"http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/CatContinuous?CGISESSID=%206e7423daad44d40ec4551da9f85ec12e\">here</a>. In your case, defining the bins by \"eye-balling\" will tempt you to change them \"just a little bit\" to get a better fit... I am not saying you will give in to this temptation, but you will need to account for this leeway in model creation in some way.</p>\\n\\n<p>Bottom line: don\\'t discretize, use restricted splines.</p>\\n',\n",
       " '<p>While drawing graphs that compare the silhouette measure of different clustering algorithms, what unit should I specify for the silhouette width?</p>\\n',\n",
       " '<p>The odds are much higher than calculated with the simple hypergeometric distribution, as the group is not chosen randomly (<em>\"12 fish are painted red before the draw\"</em>). </p>\\n\\n<p>From the description of the question, we are testing for a fraud in the draw. A specific group of 12 people complained that only 2 of them were selected, while the expected number was 232/363~2/3=8.</p>\\n\\n<p>What we really need to calculated is what are the odds that \"<strong>No</strong> group of size 12 will have only 2 member selected\". The odds that at least one group will have 2 or fewer (therefore will complain against the fairness of the draw) are much higher.</p>\\n\\n<p>When I run this simulation, and check in how many of the trials none of the 30 (=360/12) groups had 2 or fewer selections, I get about <strong>2.3%</strong> of the times. <strong>1:42</strong> is low but not impossible.</p>\\n\\n<p>You should still check the procedure of the draw as it might be biased against a specific group of people. They might have come together and received a range of the draw with less probability (the first or last numbers, for example), or whatever dependent variable on the procedure of the draw. But if you don\\'t find any flaw in the procedure, you can return to the 1:42 odds that it is simply bad luck for the group. </p>\\n',\n",
       " '<p>A very nice discussion of structural zeros in contingency tables is provided by \\nWest, L. and Hankin, R. (2008), “Exact Tests for Two-Way Contingency Tables with Structural Zeros,” Journal of Statistical Software, 28(11), 1–19.\\nURL <a href=\"http://www.jstatsoft.org/v28/i11\">http://www.jstatsoft.org/v28/i11</a></p>\\n\\n<p>As the title implies, they implement Fisher’s exact test for two-way contingency tables\\nin the case where some of the table entries are constrained to be zero.</p>\\n',\n",
       " '<p>I would like to use data mining to try to find a good workout schemes. The input dataset will contain the parameters of a set of workouts with dates and different performance and medical measures. The problem is that the influence of each individual workout will be different depending on the time that has passed after this workout. For instance on the next day after the workout the performance will degrade, but eventually will improve to the higher level than before the workout. So each performance measure will be the cumulative result of different workouts.\\nI know that I can run a series of workouts of the same type during a period of time and then use difference in the performance as a predictor. But I was wondering if there are any algorithm that allow to take into account the time component and run analysis that will take into account individual workouts.</p>\\n',\n",
       " '<p>I don\\'t know of a statistical test that does what you want, although there may be one. Probably, though, you will need to define \"bad interviewer\" more precisely.</p>\\n\\n<p>There is some literature on interviewer effects, and that might be a place to start looking.</p>\\n',\n",
       " \"<p>I think you should ask yourself, if it isn't symmetric, then why not?  Is it really likely that something that is not symmetric is not skewed in either the negative or positive direction?</p>\\n\\n<p>What is the feature about this histogram that makes you think it isn't symmetric?  Then, how would you describe that feature?</p>\\n\",\n",
       " \"<p>In my experience with dealing with multicollinearity, often removing one collinear variable from the model results in the other collinear variable(s) becoming significant (assuming that all the collinear variables are significantly correlated with the dependent variable bivariately. </p>\\n\\n<p>However, I've recently encountered a situation where I have a model shown below in which $X_2$ and $X_3$ are highly correlated ($r &gt; 0.95$) and the tolerance scores for the two variables are below $0.1$</p>\\n\\n<p>$$\\nY = \\\\\\\\beta_0\\\\\\\\ + \\\\\\\\beta_1X_1\\\\\\\\ + \\\\\\\\beta_2X_2\\\\\\\\ + \\\\\\\\beta_3X_3\\\\\\\\ +\\\\\\\\beta_4X_1X_2\\\\\\\\ +\\\\\\\\beta_5X_1X_3\\\\\\\\\\n$$</p>\\n\\n<p>*<em>all variables are continuous.</em></p>\\n\\n<p>The results of the regression model show that all 5 slopes are significant (3 slopes for the main effects and 2 slopes for the interactions). One of the possible solutions is to remove one of the highly collinear predictors. If I remove one of them - say $X_2$, I get a new model as shown below.</p>\\n\\n<p>$$\\nY = \\\\\\\\beta_0^\\\\\\\\prime + \\\\\\\\beta_1^\\\\\\\\prime X_1\\\\\\\\ + \\\\\\\\beta_3^\\\\\\\\prime X_3\\\\\\\\ +\\\\\\\\beta_5^\\\\\\\\prime X_1X_3\\\\\\\\\\n$$</p>\\n\\n<p>After $X_2$ is removed, while $\\\\\\\\beta_0^\\\\\\\\prime$, $\\\\\\\\beta_1^\\\\\\\\prime$, and $\\\\\\\\beta_3^\\\\\\\\prime$ are significant, crucially, $\\\\\\\\beta_5^\\\\\\\\prime$, the slope for the interaction term, is no longer significant. The same thing happens if I try to remove $X_3$. So I wonder what may be the reason that causes this pattern to arise and how it can be potentially dealt with. Thank you in advance!</p>\\n\",\n",
       " '<p>The first table is testing the assumption that all possible answers are equally likely, the 33 comes from $\\\\\\\\frac{195}{6} = 32.5$, which then is being rounded to 33 for display.  The testing looks correct after that (though as @whuber points out, this is probably a meaninless test to do).</p>\\n\\n<p>In the second table you compute the \"expected\" values ($f_t$) in a different way, possibly from a normal distribution, but the numbers sum to 100 instead of 195.  Your higher $\\\\\\\\chi^2$ value is due to the fact that the columns add up to different numbers (195 vs. 100) and does not in any way fit in with the idea of a $\\\\\\\\chi^2$ test.  The results are fairly meaningless.</p>\\n\\n<p>The value you report from the <code>chitest</code> looks more like it would be the p-value than the $\\\\\\\\chi^2$ value, are you sure that you are not getting the 2 confused?  That value is essentially 0 which would not be the test statistic for the differences in pattern seen there.</p>\\n\\n<p>Figure out the question first, then look to find the test that answers that question. </p>\\n',\n",
       " '<p>I might be missing a simpler answer (i.e., mis-specification in your model), but /if/ I was sure that the mixed model should converge, I would move on to using <a href=\"http://cran.r-project.org/web/packages/R2OpenBUGS/index.html\" rel=\"nofollow\">openbugs</a>, which will let you drag out a much longer mcmc sample. </p>\\n',\n",
       " '<p>It is true that most of the time \\'everybody does it this way because everybody does it this way\\' (+1)</p>\\n\\n<p>Some time ago, I realized that the t test (as implemented in R) is relatively robust to inequality of variances. You can run the following test to convince yourself.</p>\\n\\n<pre><code>pvals &lt;- rep(NA, 10000)\\nfor (i in 1:10000) {\\n    pvals[i] &lt;- t.test(rnorm(10), rnorm(10, sd=50))$p.value\\n}\\nplot(sort(pvals), type=\\'l\\')\\n</code></pre>\\n\\n<p>I also realized recently that the power difference between Wilcoxon and t tests is not that big as suggested <a href=\"http://stats.stackexchange.com/questions/27873/what-is-the-non-asymptotic-relative-power-of-non-parametric-tests/\">here</a>, at least for Gaussian data.</p>\\n\\n<p>So in my opinion, sticking to one test is OK and will make your life simpler to compute FDR. I would use Wilcoxon because it is more robust to outliers.</p>\\n\\n<p>Two more things.</p>\\n\\n<p>1/ In my humble experience, it is better to look for robust signal in microarray experiments because you usually have to do follow up experiments that have to be reproducible. Strong power allows you to detect weak effects, but weak effects are a pain. Unless I know <em>exactly</em> why I need power, I use non parametric tests.</p>\\n\\n<p>2/ That the Wilcoxon test is robust to inequality of variances does not mean that it does <em>not assume</em> equality of variances. In general, it is false to assume that the Wilcoxon test is always applicable. The Wilcoxon test is sometimes said to test for equality of median, here is a proof that this is not what it does.</p>\\n\\n<pre><code>set.seed(123)\\nx &lt;- rgamma(1000, 1)\\nx &lt;- x - median(x)    # x has median 0\\ny &lt;- -rgamma(1000, 1)\\ny &lt;- y - median(y)    # y has median 0\\nwilcox.test(x,y)      # Surprise!!\\n</code></pre>\\n\\n<p>The Wilcoxon test actually tests that one distribution is shifted relative to the other (so it assumes that they have the same variance).</p>\\n',\n",
       " '<p>I want to use AIC to compare three candidate models (labeled by m), each having K_m parameters.  However, I have M datasets over which I can make the comparison.  My ultimate goal is to report the \"relative goodness\" of each of the three models for a single fit.  How to I make use of the multiple datasets?</p>\\n\\n<p>One idea is to find the Akaike weights (See Anderson and Burnham, 2002) for each dataset and average the weights over all the M datasets (perhaps, weighting by the number of points in each dataset?).</p>\\n\\n<p>Another approach would be to say that, for model m, I have a single M*K_m parameter model that I fit over the M datasets.  In this case the AIC value, ACIC_net, is the sum of the AIC values for a fit to each of the M datasets.  In this case, I would use AIC_net to compare the three models.</p>\\n\\n<p>How should I proceed?</p>\\n',\n",
       " '<p>I have a question about model selection and model performance in logistic regression. I have three models that are based on three different hypotheses. The first two models (lets name them z and x) only have one explanatory variable in each model, and the third (lets name it w) is more complicated. I’m using AIC for variable selection for the w model and then AIC for comparing which of the three models that explain the dependent variable best. I’ve found that the w model has the lowest AIC and now want to do some performance statistics on that model to get some idea about the predictive power of the model. Since all I know is that this model is better than the other two but not how good it is.</p>\\n\\n<p>Since I’ve used all data to learn the model (to be able to compare all three models) how do I go about with model performance? From what I’ve gathered I can’t just do a k-fold cross validation on the final model I got from model selection using AIC but need to start from the beginning with all explanatory variables included, is this correct? I’d think that it is the final model I’ve chosen with AIC that I want to know how well it performs, but do realize that I’ve trained on all data so the model might be biased. So if I should start from the beginning with all explanatory variables in all folds I will get different final models for some folds, can I just choose the model from the fold which gave the best predictive power and apply that to the full data set to compare AIC with the two other models (z and x)? Or how does it work?</p>\\n\\n<p>Second part of my question is a basic question about over-parameterization. I have 156 data points, 52 is 1’s the rest are 0’s. I have 14 explanatory variables to choose from for the w model, I realize that I can’t include all due to over-parameterization, I’ve read that you should only use 10% of the group of the dependent variable with fewest observations which only would be 5 for me. I’m trying to answer a question in ecology, is it ok to select the starting variables which I think explains the dependent best simply based on ecology? Or how do I choose the starting explanatory variables? Doesn’t feel right to completely exclude some variables.</p>\\n\\n<p>So I really have three questions:</p>\\n\\n<ul>\\n<li>Could it be ok to test performance on a model trained on the full data set with  cross-validation?</li>\\n<li>If not, how do I choose the final model when doing cross-validation?</li>\\n<li>How do I choose the starting variables so I want over-parameterize?</li>\\n</ul>\\n\\n<p>Sorry for my messy questions and my ignorance. I know that similar questions have been asked but still feel a little confused. Appreciate any thoughts and suggestions.</p>\\n',\n",
       " \"<p>Since (per your comment) you have a rating from 1 to 10, I think the first thing to try is ordinal logistic regression. Another possibility that I have been reading about is beta regression, but this will be less familiar to nearly everyone (although, from what I'm reading, it looks much more flexible). </p>\\n\",\n",
       " '<p>Suppose I have a function <code>f</code>, and I want to sample it at 100 points in the interval <code>[0, 100]</code>. For some reason (that seemed smart to me at the time), I decided to not sample at equidistant intervals, but rather use the following function to determine the sample points:</p>\\n\\n<pre><code>log2(x)*(100/log2(100))\\n</code></pre>\\n\\n<p>This gives me a sequence of sample points that becomes denser as it approaches 100. The problem is, that now I need to calculate the mean over the values I have sampled, but due to the bad sampling that would be heavily biased. I cannot resample the data, this would take way too long (several days), and I am on a very tight schedule. So, the solution that comes to mind is to calculate a weighted average to correct the error. My question is, how do I determine the weights?</p>\\n',\n",
       " '<p>I am experimenting with Classification algorithms in ML and am looking for some corpus to train my model to distinguish among the different text categories like sports, weather, technology, football, cricket etc.</p>\\n\\n<p>Where I can find some dataset with these categories?</p>\\n\\n<p>An option would be to crawl Wikipedia for these 30+ categories. Is there  a better way to do this?</p>\\n\\n<p><strong>Edit</strong>: I want to train the model using the bag of words approach for these categories, then classify new/unknown websites to these predefined categories depending on the content of the webpage.</p>\\n',\n",
       " '<p>In general, you cannot interpret the coefficients from the output of a probit regression (not in any standard way, at least). You need to interpret the <em>marginal effects</em> of the regressors, that is, how much the (conditional) probability of the outcome variable changes when you change the value of a regressor, holding all other regressors constant at some values. This is different from the linear regression case where you are directly interpreting the estimated coefficients. This is so because in the linear regression case, the regression coefficients <em>are the marginal effects</em>. </p>\\n\\n<p>In the probit regression, there is an additional step of computation required to get the marginal effects once you have computed the probit regression fit.</p>\\n\\n<h2>Linear and probit regression models</h2>\\n\\n<ul>\\n<li><p><strong>Probit regression</strong>: Recall that in the probit model, you are modelling the (conditional) probability of a \"successful\" outcome, that is, $Y_i=1$,\\n$$\\n\\\\\\\\mathbb{P}\\\\\\\\left[Y_i=1\\\\\\\\mid X_{1i}, \\\\\\\\ldots, X_{Ki};\\\\\\\\beta_0, \\\\\\\\ldots, \\\\\\\\beta_K\\\\\\\\right] = \\\\\\\\Phi(\\\\\\\\beta_0 + \\\\\\\\sum_{k=1}^K \\\\\\\\beta_kX_{ki})\\n$$\\nwhere $\\\\\\\\Phi(\\\\\\\\cdot)$ is the cumulative distribution function of the standard normal distribution. This basically says that, conditional on the regressors, the probability that the outcome variable, $Y_i$ is 1, is a certain function of a linear combination of the regressors. </p></li>\\n<li><p><strong>Linear regression</strong>: Compare this to the linear regression model, where</p></li>\\n</ul>\\n\\n<p>$$\\n\\\\\\\\mathbb{E}\\\\\\\\left(Y_i\\\\\\\\mid X_{1i}, \\\\\\\\ldots, X_{Ki};\\\\\\\\beta_0, \\\\\\\\ldots, \\\\\\\\beta_K\\\\\\\\right) = \\\\\\\\beta_0 + \\\\\\\\sum_{k=1}^K \\\\\\\\beta_kX_{ki}$$\\nthe (conditional) mean of the outcome is a linear combination of the regressors.</p>\\n\\n<h2>Marginal effects</h2>\\n\\n<p>Other than in the linear regression model, coefficients rarely have any direct interpretation. We are typically interested in the <em>ceteris paribus</em> effects of changes in the regressors affecting the features of the outcome variable. This is the notion that marginal effects meaure.</p>\\n\\n<ul>\\n<li><strong>Linear regression</strong>: I would now like to know how much the <em>mean</em> of the outcome variable moves when I move one of the regressors</li>\\n</ul>\\n\\n<p>$$\\n\\\\\\\\frac{\\\\\\\\partial \\\\\\\\mathbb{E}\\\\\\\\left(Y_i\\\\\\\\mid X_{1i}, \\\\\\\\ldots, X_{Ki};\\\\\\\\beta_0, \\\\\\\\ldots, \\\\\\\\beta_K\\\\\\\\right)}{\\\\\\\\partial X_{ki}} =  \\\\\\\\beta_k\\n$$</p>\\n\\n<p>But this is just the regression coeffcient, which means that the marginal effect of a change in the $k$-th regressor is just the regression coefficient.</p>\\n\\n<ul>\\n<li><strong>Probit regression</strong>: However, it is easy to see that this is not the case for the probit regression</li>\\n</ul>\\n\\n<p>$$\\n\\\\\\\\frac{\\\\\\\\partial \\\\\\\\mathbb{P}\\\\\\\\left[Y_i=1\\\\\\\\mid X_{1i}, \\\\\\\\ldots, X_{Ki};\\\\\\\\beta_0, \\\\\\\\ldots, \\\\\\\\beta_K\\\\\\\\right]}{\\\\\\\\partial X_{ki}} = \\\\\\\\beta_k\\\\\\\\phi(\\\\\\\\beta_0 + \\\\\\\\sum_{k=1}^K \\\\\\\\beta_kX_{ki})\\n$$\\nwhich is <em>not</em> the same as the regression coefficient. These are the <em>marginal effects</em> for the probit model, and the quantity we are after. In particular, this depends on the values of all the other regressors, and the regression coefficients. Here $\\\\\\\\phi(\\\\\\\\cdot)$ is the standard normal probability density function.</p>\\n\\n<p>How are you to compute this quantity, and what are the choices of the other regressors that should enter this formula? Thankfully, Stata provides this computation after a probit regression, and provides some defaults of the choices of the other regressors (there is no universal agreement on these defaults). </p>\\n\\n<h3>Discrete regressors</h3>\\n\\n<p>Note that much of the above applies to the case of continuous regressors, since we have used calculus. In the case of discrete regressors, you need to use discrete changes. SO, for example, the discrete change in a regressor $X_{ki}$ that takes the values $\\\\\\\\{0,1\\\\\\\\}$ is</p>\\n\\n<p>$$\\n\\\\\\\\small\\n\\\\\\\\begin{align}\\n\\\\\\\\Delta_{X_{ki}}\\\\\\\\mathbb{P}\\\\\\\\left[Y_i=1\\\\\\\\mid X_{1i}, \\\\\\\\ldots, X_{Ki};\\\\\\\\beta_0, \\\\\\\\ldots, \\\\\\\\beta_K\\\\\\\\right]&amp;=\\\\\\\\beta_k\\\\\\\\phi(\\\\\\\\beta_0 + \\\\\\\\sum_{l=1}^{k-1} \\\\\\\\beta_lX_{li}+\\\\\\\\beta_k + \\\\\\\\sum_{l=k+1}^K\\\\\\\\beta_l X_{li}) \\\\\\\\\\\\\\\\\\n&amp;\\\\\\\\quad- \\\\\\\\beta_k\\\\\\\\phi(\\\\\\\\beta_0 + \\\\\\\\sum_{l=1}^{k-1} \\\\\\\\beta_lX_{li}+ \\\\\\\\sum_{l=k+1}^K\\\\\\\\beta_l X_{li})\\n\\\\\\\\end{align}\\n$$</p>\\n\\n<h2>Computing marginal effects in Stata</h2>\\n\\n<p><strong>Probit regression</strong>: Here is an example of computation of marginal effects after a probit regression in Stata.<br/></p>\\n\\n<pre><code>webuse union   \\nprobit union age grade not_smsa south##c.year\\nmargins, dydx(*)\\n</code></pre>\\n\\n<p>Here is the output you will get from the <code>margins</code> command</p>\\n\\n<pre><code>. margins, dydx(*)\\n\\nAverage marginal effects                          Number of obs   =      26200\\nModel VCE    : OIM\\n\\nExpression   : Pr(union), predict()\\ndy/dx w.r.t. : age grade not_smsa 1.south year\\n\\n------------------------------------------------------------------------------\\n             |            Delta-method\\n             |      dy/dx   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\\n-------------+----------------------------------------------------------------\\n         age |    .003442    .000844     4.08   0.000     .0017878    .0050963\\n       grade |   .0077673   .0010639     7.30   0.000     .0056822    .0098525\\n    not_smsa |  -.0375788   .0058753    -6.40   0.000    -.0490941   -.0260634\\n     1.south |  -.1054928   .0050851   -20.75   0.000    -.1154594   -.0955261\\n        year |  -.0017906   .0009195    -1.95   0.051    -.0035928    .0000115\\n------------------------------------------------------------------------------\\nNote: dy/dx for factor levels is the discrete change from the base level.\\n</code></pre>\\n\\n<p>This can be interpreted, for example, that the a one unit change in the <code>age</code> variable, increases the probability of union status by 0.003442. Similarly, being from the south, <em>decreases</em> the probability of union status by 0.1054928</p>\\n\\n<p><strong>Linear regression</strong>: As a final check, we can confirm that the marginal effects in the linear regression model are the same as the regression coefficients (with one small twist). Running the following regression and computing the marginal effects after</p>\\n\\n<pre><code>sysuse auto, clear\\nregress mpg weight c.weight#c.weight foreign\\nmargins, dydx(*)\\n</code></pre>\\n\\n<p>just gives you back the regression coefficients. Note the interesting fact that Stata computes the <em>net</em> marginal effect of a regressor including the effect through the quadratic terms if included in the model. </p>\\n\\n<pre><code>. margins, dydx(*)\\n\\nAverage marginal effects                          Number of obs   =         74\\nModel VCE    : OLS\\n\\nExpression   : Linear prediction, predict()\\ndy/dx w.r.t. : weight foreign\\n\\n------------------------------------------------------------------------------\\n             |            Delta-method\\n             |      dy/dx   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\\n-------------+----------------------------------------------------------------\\n      weight |  -.0069641   .0006314   -11.03   0.000    -.0082016   -.0057266\\n     foreign |    -2.2035   1.059246    -2.08   0.038    -4.279585   -.1274157\\n------------------------------------------------------------------------------\\n</code></pre>\\n',\n",
       " '<p>There\\'s a thread on the R-help list about this; see for example: <a href=\"http://tolstoy.newcastle.edu.au/R/e8/help/09/12/9200.html\" rel=\"nofollow\">http://tolstoy.newcastle.edu.au/R/e8/help/09/12/9200.html</a></p>\\n\\n<p>The first suggestion there is to repeat the test a large of number of times with different jittering and then combine the p-values to get an overall p-value, either by taking an average or a maximum.  They also suggest that a straightforward permutation test could be used instead (of the two, that\\'s what I\\'d prefer).  See the question <a href=\"http://stats.stackexchange.com/q/6127/3601\">Which permutation test implementation in R to use instead of t-tests (paired and non-paired)?</a> for some examples of permutation tests.</p>\\n\\n<p>Elsewhere in that thread, Greg Snow writes:\\nAdding random noise to data in order to avoid a warning is like removing the batteries from a smoke detector to silence it rather than investigating the what is causing the alarm to go off. \\n(See <a href=\"http://tolstoy.newcastle.edu.au/R/e8/help/09/12/9195.html\" rel=\"nofollow\">http://tolstoy.newcastle.edu.au/R/e8/help/09/12/9195.html</a> )</p>\\n',\n",
       " '<p>When running the Automatic Linear Modeling function of SPSS version 20, it will return an accuracy value.</p>\\n\\n<p>The higher the accuracy, the more predictive the model (I assume). However, what does this accuracy really indiciate?</p>\\n\\n<p>Is it the R2? Adjusted R2? Something else?</p>\\n',\n",
       " '<p>The Wikipedia entry on the CLT states at one point: \"For fixed large $n$ one can also say that the distribution of $S_n$ is close to the normal distribution with mean $\\\\\\\\mu$ and variance $\\\\\\\\frac1n\\\\\\\\sigma^2$.\"\\n$S_n = (\\\\\\\\sum_{i=1}^nX_n) / n$ and $X_i$ are iids with mean $\\\\\\\\mu$ and variance $\\\\\\\\sigma^2$.</p>\\n\\n<p>I don\\'t quite see how this follows from the other, more formal definitions. Is this statement true and what is the source/proof?</p>\\n',\n",
       " '<p>Statistics Explained by Hinton (ISBN: 0415332850) is a pretty good reference for something like this.</p>\\n\\n<p>There\\'s also a little bit meatier of a book \"Statistics for the Behavioral Sciences\" by Gravetter (ISBN: 0495602205) which I thought was easy to follow and refer to but you may find less useful because it\\'s phrased more for psychology types of students. </p>\\n',\n",
       " \"<p>I was wondering if someone could please help me with the following. I am trying to find the pdf of a set of dependent $\\\\\\\\chi^2$ random variables. Suppose $x_1,x_2,...,x_n$ are independent normals $x_i \\\\\\\\sim N(\\\\\\\\mu_i, \\\\\\\\sigma_i^2)$, not necessarily identical. Assume $n$ and all $\\\\\\\\mu_i$'s and $\\\\\\\\sigma_i's$ are known parameters, so this is really a probability, not an inferential statistics question. Select all possible subsets of size $n-1, n-2,..., n-k$ with $(n-k \\\\\\\\geq 1)$ from this set. Each subset will then have a mean and variance equal to $\\\\\\\\sum_{j\\\\\\\\mbox{th subset}} \\\\\\\\mu_j$  and $\\\\\\\\sum_{j\\\\\\\\mbox{th subset}} \\\\\\\\sigma_j^2$, respectively. For each subset then compute the standardized sums: </p>\\n\\n<p>$(\\\\\\\\sum_{j\\\\\\\\mbox{th subset}} x_j - \\\\\\\\sum_{j\\\\\\\\mbox{th subset}} \\\\\\\\mu_j)^2/\\\\\\\\sum_{j\\\\\\\\mbox{th subset}} \\\\\\\\sigma_j^2$</p>\\n\\n<p>Each sum is then a $\\\\\\\\chi^2_1$ (one degree of freedom). Question is to find the pdf of the $l$th order statistic of such set of $\\\\\\\\chi_1^2$'s. Clearly, the sums cannot be independent $\\\\\\\\chi^2$'s, since the subsets share some of the original normals. </p>\\n\\n<p>I have seen some related work on 'subset selection' problems in regression, specifically, a paper by Arvesen and McCabe of 1974. They mention their problem involves exactly this distribution, but then I believe they do not provide it, only providing asymptotic results and Monte Carlo simulations.</p>\\n\\n<p>Any help in this regard will be greatly appreciated! (I hope this edited version of the question is clearer--thanks very much!)</p>\\n\",\n",
       " '<p>What are the most significant annual Data Mining conferences?</p>\\n\\n<p>Rules:</p>\\n\\n<ol>\\n<li>One conference per answer</li>\\n<li>Include a link to the conference </li>\\n</ol>\\n',\n",
       " '<p>The part of the printout at the end is the model you are left with. You can also get it if you capture the value of the <code>step</code> function:  </p>\\n\\n<pre><code>final.mod &lt;- step(lm1)\\nfinal.mod\\n</code></pre>\\n',\n",
       " '<p>It does not apply to discrete distributions. See <a href=\"http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm\">http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm</a> for example.</p>\\n\\n<p>Is there any reason you can\\'t use a chi-square goodness of fit test?\\nsee <a href=\"http://www.itl.nist.gov/div898/handbook/eda/section3/eda35f.htm\">http://www.itl.nist.gov/div898/handbook/eda/section3/eda35f.htm</a> for more info.</p>\\n',\n",
       " '<p>Here is a sequence of R commands that may be helpful. Feel free to comment or edit if you spot any mistakes.</p>\\n\\n<pre><code>set.seed(1)\\nx.poi&lt;-rpois(n=200,lambda=2.5) # a vector of random variables from the Poisson distr.\\n\\nhist(x.poi,main=\"Poisson distribution\")\\n\\nlambda.est &lt;- mean(x.poi) ## estimate of parameter lambda\\n(tab.os&lt;-table(x.poi)) ## table with empirical frequencies\\n\\n\\nfreq.os&lt;-vector()\\nfor(i in 1: length(tab.os)) freq.os[i]&lt;-tab.os[[i]]  ## vector of emprical frequencies\\n\\nfreq.ex&lt;-(dpois(0:max(x.poi),lambda=lambda.est)*200) ## vector of fitted (expected) frequencies\\n\\nacc &lt;- mean(abs(freq.os-trunc(freq.ex))) ## absolute goodness of fit index acc\\nacc/mean(freq.os)*100 ## relative (percent) goodness of fit index\\n\\nh &lt;- hist(x.poi ,breaks=length(tab.os))\\nxhist &lt;- c(min(h$breaks),h$breaks)\\nyhist &lt;- c(0,h$density,0)\\nxfit &lt;- min(x.poi):max(x.poi)\\nyfit &lt;- dpois(xfit,lambda=lambda.est)\\nplot(xhist,yhist,type=\"s\",ylim=c(0,max(yhist,yfit)), main=\"Poison density and histogram\")\\nlines(xfit,yfit, col=\"red\")\\n\\n#Perform the chi-square goodness of fit test \\n#In case of count data we can use goodfit() included in vcd package\\nlibrary(vcd) ## loading vcd package\\ngf &lt;- goodfit(x.poi,type= \"poisson\",method= \"MinChisq\")\\nsummary(gf)\\nplot(gf,main=\"Count data vs Poisson distribution\")\\n</code></pre>\\n',\n",
       " '<p>I did a Google search and bumped into the following title:</p>\\n\\n<p>\"A study of cross-validation and bootstrap for accuracy estimation and model selection\"</p>\\n\\n<p>I would post the link but it\\'s one of those citeseerx.ist.psu.edu links that comes across with an IP number for the URL.   If you do the search, you should be able to find it.    In that article, they give a calculation for a confidence interval.</p>\\n\\n<p>Here\\'s a similar article:</p>\\n\\n<p><a href=\"http://www.public.asu.edu/~ltang9/papers/ency-cross-validation.pdf\" rel=\"nofollow\">http://www.public.asu.edu/~ltang9/papers/ency-cross-validation.pdf</a></p>\\n\\n<p>If those don\\'t help, here are some links on \"Multiple Comparison Tests\":</p>\\n\\n<p><a href=\"http://www.itl.nist.gov/div898/handbook/prc/section4/prc47.htm\" rel=\"nofollow\">http://www.itl.nist.gov/div898/handbook/prc/section4/prc47.htm</a></p>\\n\\n<p><a href=\"http://www.uvm.edu/~dhowell/StatPages/More_Stuff/RepMeasMultComp/RepMeasMultComp.html\" rel=\"nofollow\">http://www.uvm.edu/~dhowell/StatPages/More_Stuff/RepMeasMultComp/RepMeasMultComp.html</a></p>\\n\\n<p><a href=\"http://www.utdallas.edu/~herve/Abdi-Bonferroni2007-pretty.pdf\" rel=\"nofollow\">http://www.utdallas.edu/~herve/Abdi-Bonferroni2007-pretty.pdf</a></p>\\n\\n<p><a href=\"http://www.csub.edu/~psmith3/Teaching/310-8.pdf\" rel=\"nofollow\">http://www.csub.edu/~psmith3/Teaching/310-8.pdf</a></p>\\n\\n<p><a href=\"http://www.graphpad.com/faq/viewfaq.cfm?faq=1091\" rel=\"nofollow\">http://www.graphpad.com/faq/viewfaq.cfm?faq=1091</a></p>\\n\\n<p><a href=\"http://en.wikipedia.org/wiki/Multiple_comparisons\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Multiple_comparisons</a></p>\\n\\n<p>I\\'m no expert, but I\\'m in a similar situation.</p>\\n\\n<p>In a recent problem (that\\'s still going on), until I looked at the data, I had no idea what was a reasonable null, much less what was the \"right\" null.   After some playing around with the data, I noticed some possible relationships.   Am I data snooping (dredging)?   Who knows?  I do know that I can\\'t use an uncorrected t-test, p-value, etc on any of my results/discoveries.</p>\\n\\n<p>What I started looking for was a way to \"put a fence around the problem\".  In other words, some way to define how many relationship combinations were available when I found \"my\" relationship.   Do you have a way to do that?    In other words, can you categorize/count the POSSIBLE solutions that you\\'ve tested/allowed-for into XX groups?  If so, that might be a way to approach a Multiple Comparison Test.</p>\\n',\n",
       " '<p>You could create a scatterplot with time on the x-axis and price on the y-axis.\\nYou could make the size of the points proportional to the amount sold (or some function of amount sold such as log of amount sold). You could add a line that passed through the minimum values or some estimate of the minimum, where the data was unavailable. </p>\\n\\n<p>If you are trying to model the minimum price at any given point in time, it seems like you would need to make some assumptions about the duration of any particular price. If you knew the minimum price for each time period you could graph just that data. If there are time points with missing data, you could use some form of missing data imputation.</p>\\n\\n<p>Local quantile regression might also be of interest.</p>\\n\\n<p>You might also need to consider issues of invalid data: e.g., very low prices that only last a short period of time or have a minimal quantity of stock that for what ever reason should not be interpolated to future time points.</p>\\n',\n",
       " '<blockquote>\\n  <p>60% of the time, it works every time.</p>\\n</blockquote>\\n\\n<p>-Brian Fantana</p>\\n',\n",
       " '<p>There are three excellent books on nonlinear regression that are at the level of your interest.   I don\\'t know if they look at goodness of fit tests for the model though.  Anyway one is <a href=\"http://rads.stackoverflow.com/amzn/click/0471802603\" rel=\"nofollow\">Gallant</a> </p>\\n\\n<p>and a second one is <a href=\"http://rads.stackoverflow.com/amzn/click/0470139005\" rel=\"nofollow\">Bates and Watts</a></p>\\n\\n<p>the third is <a href=\"http://rads.stackoverflow.com/amzn/click/0471471356\" rel=\"nofollow\">Seber and Wild</a></p>\\n',\n",
       " '<p>This is related to another <a href=\"http://stats.stackexchange.com/questions/4175/resampling-binomial-z-and-t-test-help-with-real-data\">question</a> I asked recently. To recap:</p>\\n\\n<p>[<em>I had 30 people call a number and then roll a 5 sided die. If the call matches the subsequent face then the trial is a hit, else it is a miss. Each subject completes 25 trials (rolls) and thus, each participant has a score out of 25. Since the die is a virtual one, it cannot be biased. Before the experiment was conducted I was going to compare the subjects score with a one-sample t-test (compared to mu of 5). However I was pointed towards the more powerful z-test, which is appropriate because we know the population parameters for the null hypothesis: that everyone should score at chance. Since NPQ means the binomial approximates to the normal or Gaussian distribution, we can use a parametric test. So I could just forget about it all and go back to the t-test I planned to use, but it seems to me that although the z-test is not often used in real research it is appropriate here. That was the conclusion from my previous question. Now I am trying to understand how to use resampling methods (either permutation or bootstrap) to compliment my parametric analysis.</em>]</p>\\n\\n<p>Okay. I am trying to program a one-sample permutation <a href=\"http://statistic-on-air.blogspot.com/2009/07/one-sample-z-test.html\" rel=\"nofollow\">z-test</a>, using the <a href=\"http://pbil.univ-lyon1.fr/library/DAAG/DESCRIPTION\" rel=\"nofollow\">DAAG</a> package <a href=\"http://pbil.univ-lyon1.fr/library/DAAG/html/onet.permutation.html\" rel=\"nofollow\">onet.permutation</a> as inspiration. This is as far as I\\'ve got:</p>\\n\\n<pre><code>perm.z.test = function(x, mu, var, n, prob, nsim){\\n  nx &lt;- length(x)\\n  mx &lt;- mean(x)\\n  z &lt;- array(, nsim)\\n  for (i in 1:nsim) {\\n    mn &lt;- rbinom(nx*1, size=n, prob=prob)\\n    zeta = (mean(mn) - mu) / (sqrt(var/nx))\\n    z[i] &lt;- zeta\\n  }\\n  pval &lt;- (sum(z &gt;= abs(mx)) + sum(z &lt;= -abs(mx)))/nsim\\n  print(signif(pval, 3))\\n}\\n</code></pre>\\n\\n<p>Where: <code>x</code> is the variable to test, <code>n</code> is the the number of trials (=25) and <code>prob</code> is the probability of getting it correct (=.2). The population value (<code>mu</code>) of the mean number correct is n*p. The population standard deviation, <code>var</code>, is square-root(n*p*[1-p]).</p>\\n\\n<p>Now I guess this compares x to an array composed of randomly generated binomial sample. If I centre x at 0 (variable-mu) I get a p-value. <strong>Can somebody confirm that it is doing what I think it is doing?</strong></p>\\n\\n<p>My testing gives this:</p>\\n\\n<pre><code>&gt; binom.samp1 &lt;- as.data.frame(matrix(rbinom(30*1, size=25, prob=0.2), \\n                                     ncol=1))\\n&gt; z.test(binom.samp1$V1, mu=5, sigma.x=2)\\ndata:  binom.samp1$V1 \\nz = 0.7303, p-value = 0.4652\\n\\n&gt; perm.z.test(binom.samp1$V1-5, 5, 2, 25, .2, 2000)\\n[1] 0.892\\n\\n&gt; binom.samp1 &lt;- as.data.frame(matrix(rbinom(1000*1, size=25, prob=0.2),\\n                                     ncol=1))\\n&gt; perm.z.test(binom.samp1$V1-5, 5, 2, 25, .2, 2000)\\n[1] 0.937\\n</code></pre>\\n\\n<p><strong>Does this look right?</strong></p>\\n\\n<p>UPDATE:</p>\\n\\n<p>Since this obviously doesn\\'t do what I want, I do have another angle. This <a href=\"http://www.stat.umn.edu/geyer/old/5601/examp/perm.html#one\" rel=\"nofollow\">website</a> offers this advice:</p>\\n\\n<blockquote>\\n  <p>There is no reason whatsoever why a\\n  permutation test has to use any\\n  particular test statistic. Any test\\n  statistic will do! ... For one-sample\\n  or paired two-sample tests, in\\n  particular, for Wilcoxon signed rank\\n  tests, the permutations are really\\n  subsets. The permutation distribution\\n  choses an arbitrary subset to mark +\\n  and the complementary subset is marked\\n  -. Either subset can be empty.</p>\\n</blockquote>\\n\\n<p>What about an arbitrary subset with a one-sample z-test?</p>\\n',\n",
       " '<p>Pairwise testing in this situation is not (yet) justified by the data description. You should be using multi-variable regression methods. An R call might be:</p>\\n\\n<pre><code>lm( weight_end ~ shop_habit + age_grp + weight_begin)\\n</code></pre>\\n\\n<p>Constructing 3 categories is not the best method of controlling for age (or analyzing its contribution if that is the primary question) since categorization can distort continuous relationships, and spline terms remove the need to pick arbitrary split-points. Once there os sufficient evidence of an association of weight change after a proper analysis, then there will be ad-hoc test options that can deployed.</p>\\n\\n<p>(I did agree with most of what @whuber expressed in a comment, and I generally find his commentary authoritative, but do not understand his stance regarding regression approaches.)</p>\\n',\n",
       " \"<p>In large sample theory, I'm told that as $n$ grows larger and larger ($n$ being the number of samples in a dataset) that $\\\\\\\\sqrt n(\\\\\\\\hat \\\\\\\\beta_1-\\\\\\\\beta_1)$ gets closer and closer to normal distribution. What exactly is $\\\\\\\\sqrt n(\\\\\\\\hat \\\\\\\\beta_1-\\\\\\\\beta_1)$?</p>\\n\",\n",
       " '<p>It is possible to display complete cases <em>and</em> missing values altogether: just assign a pair of artificial values to observations having one missing value, such that they appear, e.g., in the lower left of your display without overlapping with pairwise complete observations (it is also possible to add small perturbations to their coordinates, like random jittering, to avoid overlapping, and use a different color or symbol shape). An alternative solution is to plot missing values in the margin, as shown below (Source: <a href=\"http://www.ggobi.org/book/2007-infovis/03-missing.pdf\" rel=\"nofollow\">InfoVis 03 conference</a>):</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/RMvT6.png\" alt=\"enter image description here\"></p>\\n\\n<p>In any case, you should work with pairwise complete observations, for visualization purpose, not complete cases on the entire data set.</p>\\n\\n<p><a href=\"http://rosuda.org/Klimt/\" rel=\"nofollow\">Klimt</a> also handles missing values in a nice way as they are displayed along the horizontal (when y-value is missing) and vertical (when x-value is missing) axis, as shown in the following picture.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/nx6ry.png\" alt=\"enter image description here\"></p>\\n\\n<p>Here is a little R function that should handle this:</p>\\n\\n<pre><code>miss.value.plot &lt;- function(x, y, ...) {\\n  plot(x, y, ...)\\n  rug(jitter(x[is.na(y)], amount=.1), side=1, lwd=1.2)\\n  rug(jitter(y[is.na(x)], amount=.1), side=2, lwd=1.2)\\n}\\n</code></pre>\\n\\n<p>It can be used as a panel for <code>pairs</code> as well: Just replace <code>plot</code> with <code>points</code>. Of course, you can replace <code>rug</code> with <code>points</code> if your prefer to draw points or other symbols. In this case, you will have to provide constant values for the x or y complementary coordinates.</p>\\n\\n<p>It is also possible to rely on software that allows dynamic brushing and linked plots, like <a href=\"http://www.ggobi.org/\" rel=\"nofollow\">GGobi</a>. Basically, the idea is to have your original data matrix, and a copy of it, only composed of 0\\'s and 1\\'s and coding for missingness. Linking the two tables allows to study relationships between any two variables while looking at pattern of missingness in the other variables. Some of these techniques are discussed in the chapter on <a href=\"http://www.ggobi.org/book/chap-miss.pdf\" rel=\"nofollow\">Missing Data</a> from <a href=\"http://www.ggobi.org/book/\" rel=\"nofollow\">Interactive and Dynamic Graphics for Data Analysis: With Examples Using R</a> and GGobi, by Cook and Swayne. There is an <a href=\"http://cran.r-project.org/web/packages/rggobi/index.html\" rel=\"nofollow\">R interface</a> that allows to work directly from R. This really is for situations where missing patterns are of interest. See also <a href=\"http://www.math.montana.edu/~jimrc/classes/stat506/notes/MissingDataGUI.pdf\" rel=\"nofollow\">MissingDataGUI: A Graphical User Interface for Exploring Missing Values in Data</a>.</p>\\n\\n<p>Another software for interactive scatterplot displays is <a href=\"http://stats.math.uni-augsburg.de/mondrian/\" rel=\"nofollow\">Mondrian</a>. (There was also <a href=\"http://stats.math.uni-augsburg.de/Manet/\" rel=\"nofollow\">Manet</a> but I cannot get it to work on my Intel Mac anymore.)</p>\\n',\n",
       " \"<p>You cannot do this. At least, not ONE effect size for all cases.  That's what an interaction means.</p>\\n\\n<p>That is, an interaction means that the effect of one IV is different at different levels of the other variable.  </p>\\n\\n<p>So, you could calculate the effect of one variable at any particular level of the other. </p>\\n\",\n",
       " '<p><em><strong>Shifting/scaling variables will not affect their correlation with the response</em></strong> </p>\\n\\n<p>To see why this is true, suppose that the correlation between $Y$ and $X$ is $\\\\\\\\rho$. Then the correlation between $Y$ and $(X-a)/b$ is </p>\\n\\n<p>$$ \\\\\\\\frac{ {\\\\\\\\rm cov}(Y,(X-a)/b) }{ {\\\\\\\\rm SD}((X-a)/b) \\\\\\\\cdot {\\\\\\\\rm SD}(Y) } = \\\\\\\\frac{ {\\\\\\\\rm cov}(Y,X/b) }{ {\\\\\\\\rm SD}(X/b) \\\\\\\\cdot {\\\\\\\\rm SD}(Y) } = \\\\\\\\frac{ \\\\\\\\frac{1}{b} \\\\\\\\cdot {\\\\\\\\rm cov}(Y,X) }{ \\\\\\\\frac{1}{b}{\\\\\\\\rm SD}(X) \\\\\\\\cdot {\\\\\\\\rm SD}(Y) } = \\\\\\\\rho$$</p>\\n\\n<p>which follows from <a href=\"http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient#Definition\" rel=\"nofollow\">the definition of correlation</a> and three facts: </p>\\n\\n<ul>\\n<li><p>${\\\\\\\\rm cov}(Y, X+a) = {\\\\\\\\rm cov}(Y,X) + \\\\\\\\underbrace{{\\\\\\\\rm cov}(Y,a)}_{=0} = {\\\\\\\\rm cov}(Y,X)$</p></li>\\n<li><p>${\\\\\\\\rm cov}(Y,aX) = a {\\\\\\\\rm cov}(Y,X)$</p></li>\\n<li><p>${\\\\\\\\rm SD}(aX) = a \\\\\\\\cdot {\\\\\\\\rm SD}(X)$ </p></li>\\n</ul>\\n\\n<p>Therefore, <strong>in terms of model fit (e.g. $R^2$ or the fitted values), shifting or scaling your variables (e.g. putting them on the same scale) will not change the model</strong>, since linear regression coefficients are related to the correlations between variables. It will only <em>change the scale of your regression coefficients</em>, which should be kept in mind when you\\'re interpreting the output if you choose to transform your predictors. </p>\\n\\n<p><em>Edit: The above has assumed that you\\'re talking about ordinary regression <strong>with</strong> the intercept. A couple more points related to this (thanks @cardinal):</em></p>\\n\\n<ul>\\n<li><p><em>The intercept can change when you transform your variables and, as @cardinal points out in the comments, the coefficients will change when you shift your variables if you omit the intercept from the model, although I assume you\\'re not doing that unless you have a good reason (see e.g. <a href=\"http://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-lm/7950#7950\">this answer</a>).</em></p></li>\\n<li><p><em>If you\\'re regularizing your coefficients in some way (e.g. Lasso, ridge regression), then centering/scaling will impact the fit. For example, if you\\'re penalizing $\\\\\\\\sum \\\\\\\\beta_{i}^{2}$ (the ridge regression penalty) then you cannot recover an equivalent fit after standardizing unless all of the variables were on the same scale in the first place, i.e. there is no constant multiple that will recover the same penalty.</em></p></li>\\n</ul>\\n\\n<p><em><strong>Regarding when/why a researcher may want to transform predictors</em></strong></p>\\n\\n<p>A common circumstance (discussed in the subsequent answer by @Paul) is that researchers will <a href=\"http://en.wikipedia.org/wiki/Standard_score\" rel=\"nofollow\"><em>standardize</em></a> their predictors so that all of the coefficients will be on the same scale. In that case, the size of the point estimates can give a rough idea of which predictors have the largest effect once the numerical magnitude of the predictor has been standardized.</p>\\n\\n<p>Another reason a researcher may like to scale very large variables is so that the regression coefficients are not on an extremely tiny scale. For example, if you wanted to look at the influence of population size of a country on crime rate (couldn\\'t think of a better example), you might want to measure population size in <em>millions</em> rather than in its original units, since the coefficient may be something like $.00000001$. </p>\\n',\n",
       " '<p>Not having a copy of Applied Survival Analysis, I\\'m guessing you\\'re looking for something like this: <a href=\"http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=Design:hazard.ratio.plot\" rel=\"nofollow\">http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=Design:hazard.ratio.plot</a></p>\\n',\n",
       " '<p>Looks like I was thinking of <a href=\"http://en.wikipedia.org/wiki/Stein%27s_example\" rel=\"nofollow\">Stein\\'s example</a>, although it does not appear to lead a practical improvement in the estimate of a parameter.  If anyone has a nice intuitive explanation of Stein\\'s example, I\\'d be glad to hear it.</p>\\n',\n",
       " \"<p>I may, after all this time, finally have understood the question.  The data, if I'm correct, are a set of tuples $(i, j, y(i,j))$ where $i$ is one player, $j \\\\\\\\ne i$ is another player, and $y(i,j)$ is the number of attacks of $i$ on $j$.  In this notation the objective is to relate $y(i,j)$ to $y(j,i)$.  There are some natural ways to do this, including:</p>\\n\\n<ol>\\n<li><p>Analyze the data set $\\\\\\\\{(y(i,j), y(j,i))\\\\\\\\}$ by means of a scatterplot or PCA (to find the principal eigenvalue).  Note this is <em>not</em> a regression situation because both components of each ordered pair are observed: neither can be considered under the control of an experimenter nor observed without error.  It is this scatterplot, I believe, that appears triangular.  This already suggests that any attempt to describe it succinctly, such as by means of a principal direction, is doomed.</p></li>\\n<li><p>Model $y(i,j)$ in terms of characteristics of $i$ and $j$.  This is a classic regression situation.  The solution provides an indirect, but possibly powerful, way to relate $y(i,j)$ to $y(j,i)$.</p>\\n\\n<p>In this case, also consider re-expressing the data in terms of <em>relative</em> numbers of attacks.  That is, instead of using $y(i,j)$ use $x(i,j) = y(i,j)/\\\\\\\\sum_{j}{y(i,j)}$.</p></li>\\n</ol>\\n\",\n",
       " '<p>We have $X_1\\\\\\\\sim N(\\\\\\\\mu_1,\\\\\\\\sigma^2)$ and $X_2\\\\\\\\sim N(\\\\\\\\mu_2,\\\\\\\\sigma^2)$, hence </p>\\n\\n<p>$$EY_1=E(-X_1/\\\\\\\\sqrt{2}+X_2/\\\\\\\\sqrt{2})=-1/\\\\\\\\sqrt{2}EX_1+1/\\\\\\\\sqrt{2}EX_2=0$$</p>\\n\\n<p>\\\\\\\\begin{align*}\\nEY_1^2&amp;=E(-X_1/\\\\\\\\sqrt{2}+X_2/\\\\\\\\sqrt{2})^2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n&amp;=E(X_1/\\\\\\\\sqrt{2})^2-2E(X_1X_2/2)+E(X_2/\\\\\\\\sqrt{2})^2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n&amp;=1/2\\\\\\\\sigma^2+1/2\\\\\\\\sigma^2=\\\\\\\\sigma^2\\n\\\\\\\\end{align*}</p>\\n\\n<p>Hence $Y_1\\\\\\\\sim N(0,\\\\\\\\sigma^2)$ since it is the linear combination of normal variables. </p>\\n\\n<p>Similarly we get $Y_2\\\\\\\\sim N(0,\\\\\\\\sigma^2)$  and $Y_3\\\\\\\\sim N(0,\\\\\\\\sigma^2)$</p>\\n\\n<p>Now</p>\\n\\n<p>$$EY_1Y_2=1/\\\\\\\\sqrt{6}E(X_1)^2-1/\\\\\\\\sqrt{6}EX_2^2=0$$</p>\\n\\n<p>and similarly $EY_2Y_3=EY_1Y_3=0$, hence $Y_1$, $Y_2$ and $Y_3$ are independent, since for normal variables independece coincided with zero correlation.</p>\\n\\n<p>Having established that we have </p>\\n\\n<p>$$(Y_1^2+Y_2^2+Y_3^2)/\\\\\\\\sigma^2=\\\\\\\\left(\\\\\\\\frac{Y_1}{\\\\\\\\sigma}\\\\\\\\right)^2+\\\\\\\\left(\\\\\\\\frac{Y_2}{\\\\\\\\sigma}\\\\\\\\right)^2+\\\\\\\\left(\\\\\\\\frac{Y_3}{\\\\\\\\sigma}\\\\\\\\right)^2=Z_1^2+Z_2^2+Z_3^2$$,</p>\\n\\n<p>where $Z_i=Y_i/\\\\\\\\sigma$. Since $Y_i\\\\\\\\sim N(0,\\\\\\\\sigma^2)$, we have $Z_i\\\\\\\\sim N(0,1)$. </p>\\n\\n<p>We have showed that our quantity of interest is a sum of squares of 3 independent standard normal variables, which by definition is $\\\\\\\\chi^2$ with 3 degrees of freedom. </p>\\n\\n<p>As I\\'ve said in the comments you do not need to calculate the densities. If you on the other hand want to do that, your formula is wrong. Here is why. Denote by $G(x)$ distribution of $Y_1^2$ and $F(x)$ the distribution of $Y_1$. Then we have</p>\\n\\n<p>$$G(x)=P(Y_1^2&lt;x)=P(-\\\\\\\\sqrt{x}&lt;Y_1&lt;\\\\\\\\sqrt{x})=F(\\\\\\\\sqrt{x})-F(-\\\\\\\\sqrt{x})$$</p>\\n\\n<p>Now the density of $Y_1^2$ is $G\\'(x)$, so</p>\\n\\n<p>$$G\\'(x)=\\\\\\\\frac{1}{2\\\\\\\\sqrt{x}}(F\\'(\\\\\\\\sqrt{x})+F\\'(-\\\\\\\\sqrt{x})$$ </p>\\n\\n<p>We have that</p>\\n\\n<p>$$F\\'(x)=\\\\\\\\frac{1}{\\\\\\\\sigma\\\\\\\\sqrt{2\\\\\\\\pi}}e^{-\\\\\\\\frac{x^2}{\\\\\\\\sigma^2}},$$</p>\\n\\n<p>so</p>\\n\\n<p>$$G\\'(x)=\\\\\\\\frac{1}{\\\\\\\\sigma\\\\\\\\sqrt{2\\\\\\\\pi x}}e^{-\\\\\\\\frac{x}{2}}$$</p>\\n\\n<p>If $\\\\\\\\sigma^2=1$ we have a pdf of $\\\\\\\\chi^2$ with one degree of freedom. (Note that for $Z_1$ instead of $Y_1$ the calculation is similar and $\\\\\\\\sigma^2=1$ ) As @whuber pointed out, this is <a href=\"http://en.wikipedia.org/wiki/Gamma_distribution\" rel=\"nofollow\">gamma</a> distribution, and sums of independent gamma distributions is again gamma, the exact formula is provided in the wikipedia page.</p>\\n',\n",
       " '<p>I am not sure what kind of variable is being audited, so I give 2 alternatives:</p>\\n\\n<ol>\\n<li><p>To be able to compute the required sample size to give an acceptable estimate to a <strong>continuous variable</strong> (= given confidence interval) you have to know a few parameters: mean, standard deviation (and to be precise: population size). If you do not know these, you have to be able to give an accurate estimate to those (based on e.g. researches in the past). \\n$$n=\\\\\\\\left(\\\\\\\\frac{Z_{c}\\\\\\\\sigma}{E}\\\\\\\\right)^2,$$\\nwhere $n$ is sample size, $Z_{c}$ is choosen from standard normal distributon table based on $\\\\\\\\alpha$ and $\\\\\\\\sigma$ is the standard deviation.</p></li>\\n<li><p>I could image that the variable being examined is a <strong>discrete</strong> one, and the confidence interval shows that how many percent of the population is about to choose one category based on the sample (proportion). That way the required sample size could be computed easily with:$$n=p(1-p)\\\\\\\\left(\\\\\\\\frac{Z_{c}}{E}\\\\\\\\right)^2$$ where $n$ is sample size, $p$ is proportion in population, $Z_{c}$ is choosen from standard normal distributon table based on $\\\\\\\\alpha$, and $E$ is the margin of error.</p></li>\\n</ol>\\n\\n<p>Note: you can find a lot of online calculators also (<a href=\"http://www.rad.jhmi.edu/jeng/javarad/samplesize/\" rel=\"nofollow\">e.g.</a>). Worth reading <a href=\"http://www.ltcconline.net/greenl/courses/201/estimation/ciprop.htm\" rel=\"nofollow\">this article</a> also.</p>\\n',\n",
       " '<p>I asked the same question on math.stackexchange and had some responses, but I would also like to hear some input specifically from statisticians and data analysts and I feel that you guys may have something to offer.</p>\\n\\n<p>The question is about finding reasonable ways of dividing a prize among $n$ people in the following situation:</p>\\n\\n<p>To make the example specific, we have $6$ people in total who are going to share a prize of $100$ dollars, and let us denote the amount received by each person $i$ as $q_i$. In addition, each person $i$ is given a score $s_i$, and we can think of $s_i$ as a way of measuring how well person $i$ deserves some portion of the prize. The intuition here is that we would like a higher-scoring person to receive a larger portion of the prize than a lower-scoring person, that is, $q_i\\\\\\\\geqslant q_j$ if and only  if $s_i\\\\\\\\geqslant s_j$. Further, the scores are bounded, so $s_{min} \\\\\\\\le s_i \\\\\\\\le s_{max}$. The problematic thing here is that $s_i$ can be either negative or positive. For example, $s_1=1.3, s_2=2.1, s_3=-0.8, s_4=-3.7, s_5=0.7, s_6=5.2$.</p>\\n\\n<p>So, what would be the proper ways of dividing the prize given these scores?</p>\\n\\n<p>One interesting answer by @opt suggests to use the so-called Softmax function in the context of neural networks, and it is basically \\n${\\\\\\\\displaystyle p_i=\\\\\\\\frac{\\\\\\\\exp(s_i)}{\\\\\\\\sum^n_j\\\\\\\\exp(s_j)}}$, and $\\\\\\\\sum^n_ip_i=1$. In other words, $p_i$ would be the portion of the prize that $i$ should receive given her score. I would like to hear your thoughts/opinions on this method.</p>\\n\\n<p>Many thanks.</p>\\n',\n",
       " '<p>I have a dataset where there is a high degree of multicollinearity, with all variables correlating positively with each other and the dependent variable. However, on some of the models I run I get a couple of significant negative coefficients. Basically there are two coefficients that depending on what variables I include in the model, I can manipulate their signs.</p>\\n\\n<p>My understanding is that if the variance-covariance matrix only contain positive values, then all coefficients should also be positive. Is this correct?</p>\\n',\n",
       " '<p><a href=\"http://scikit-learn.org/dev/auto_examples/applications/plot_stock_market.html#example-applications-plot-stock-market-py\" rel=\"nofollow\">This example from the scikit-learn project</a> might give you some ideas on how to combine sparse covariance graph estimation with traditional clustering so as to identify some of the underlying structure of a market from daily price data.</p>\\n\\n<p>Disclaimer: I contribute to the scikit-learn project even though I am not the one who wrote this example.</p>\\n',\n",
       " '<p><strong>Cover\\'s Theorem:</strong> Roughly stated, it says given any random set of finite points (with arbitrary labels), then with high probability these points can be made linearly separable [1] by mapping them to a higher dimension [2].</p>\\n\\n<p><strong>Implication:</strong> Great, what this theorem tells me is that if I take my dataset and map these points to a higher dimension, then I can easily find a linear classifier. However, most classifiers need to compute some kind of similarity like dot product and this means that the time complexity of a classification algorithm is proportional to the dimension of the data point. So, higher dimension means larger time complexity (not to mention space complexity to store those large dimensional points).</p>\\n\\n<p><strong>Kernel Trick:</strong> Let $n$ be the original dimension of data points and $f$ be the map which maps these points to a space of dimension $N (&gt;&gt; n)$. Now, if there is a function $K$ which takes inputs $x$ and $y$ from the original space and computes $K(x, y) = \\\\\\\\langle f(x), f(y) \\\\\\\\rangle$, then I am able to compute the dot product in higher dimensional space but in complexity $O(n)$ instead of $O(N)$.</p>\\n\\n<p><strong>Implication:</strong> So, if the classification algorithm is only dependent on the dot product and has no dependency on the actual map $f$, I can use the kernel trick to run the algorithm in high dimensional space with almost no additional cost.</p>\\n\\n<p><strong>Does Linear separability imply that points from the same class will get closer than the points from different classes?</strong>\\nNo, there is no such guarantee as such. Linear separability doesn\\'t really imply that the point from same class has gotten closer or that the points from two different classes have gotten any further.</p>\\n\\n<p><strong>So why would kNN work?</strong>\\nIt need not! However, if it does, then it is purely because of the kernel. </p>\\n\\n<p><strong>What does that mean?</strong>\\nConsider the boolean feature vector $x = (x_1, x_2)$. When you use degree two polynomial kernel, the feature vector $x$ is mapped to the vector $(x_1^2, \\\\\\\\sqrt{2} x_1x_2, x_2^2)$. From a vector of boolean features, just by using degree two polynomial, we have obtained a feature vector of \"conjunctions\". Thus, the kernels themselves produce some brilliant feature maps. If your data has good original features and if your data could benefit from the feature maps created by these kernels. By benefit, I mean that the features produced by these feature maps can bring the points from the same class closer to each other and push points from different classes away, then kNN stands to benefit from using kernels. Otherwise, the results won\\'t be any different than what you get from running kNN on the original data.</p>\\n\\n<p><strong>Then why use kernel kNN?</strong>\\nWe showed that the computation complexity of using kernels is just slightly more than the usual kNN and if data benefits from using kernels then why not use them anyway?</p>\\n\\n<p><strong>Is there any paper that has studied which class of data that can benefit from kernels in kNN?</strong>\\nAs far as I know, No.</p>\\n\\n<p>[1] <a href=\"http://en.wikipedia.org/wiki/Linear_separability\">http://en.wikipedia.org/wiki/Linear_separability</a> <br />\\n[2] <a href=\"http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4038449&amp;tag=1\">http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4038449&amp;tag=1</a></p>\\n',\n",
       " '<p>I play a game online (Heroes of Newerth) which has a large ladder of players, each player having a couple of different ratings. I\\'ve manually gathered the rating data for all percentiles of players but am blanking at how to turn this into a histogram and compute the Mean and Standard Deviation. I\\'d also like to have a graph to show the curve.</p>\\n\\n<p>I could easily do so using my TI-83+ and I consider myself very proficient at Excel, but I don\\'t see an obvious way to do this. Below is a link to the data, or if you\\'d prefer to give me instructions on how to create the distribution I\\'d be happy to do so myself.</p>\\n\\n<p><a href=\"https://spreadsheets.google.com/spreadsheet/ccc?key=0Atwn_lcLizk9dDZ4WW1YV3BZQnFSaGNoQVdRa0JVZ3c&amp;hl=en_US&amp;authkey=CKyI38QJ#gid=0\" rel=\"nofollow\">https://spreadsheets.google.com/spreadsheet/ccc?key=0Atwn_lcLizk9dDZ4WW1YV3BZQnFSaGNoQVdRa0JVZ3c&amp;hl=en_US&amp;authkey=CKyI38QJ#gid=0</a></p>\\n',\n",
       " '<p>I\\'ll add some paper references when I\\'m back at a computer, but here are some simple suggestions:</p>\\n\\n<p>Definitely start by working with returns. This is critical to deal with the irregular spacing where you can naturally get big price gaps (especially around weekends). Then you can apply a simple filter to remove returns well outside the norm (eg. vs a high number of standard deviations). The returns will adjust to the new absolute level so large real changes will result in the loss of only one tick. I suggest using a two-pass filter with returns taken from 1 step and <em>n</em> steps to deal with clusters of outliers.</p>\\n\\n<p><em>Edit 1:</em> Regarding the usage of prices rather than returns: asset prices tend to not be stationary, so IMO that can pose some additional challenges. To account for the irregularity and power law effects, I would advise some kind of adjustment if you want to include them in your filter. You can scale the price changes by the time interval or by volatility.  You can refer to the \"realized volatility\" literture for some discussion on this.  Also discussed in Dacorogna et. al.   </p>\\n\\n<p>To account for the changes in volatility, you might try basing your volatility calculation from the same time of the day over the past week (using the seasonality).  </p>\\n',\n",
       " '<p>The easiest way to do what you want is probably to use the <code>psych</code> package for R. This includes functions such as <code>sim.rasch</code> and <code>sim.irt</code> which will simulate appropriate data of whatever size for you. </p>\\n',\n",
       " '<p>In boosting we use weak learners mostly since they are trained faster compared to strong learners. Think about it. If I use Multi Layer Neural Network as learner, then I need to train lots of them. On the otherhand a decision tree may be a lot faster , then I can train lots of them.</p>\\n\\n<p>Lets say I use 100 learners. I train NN in 100 seconds and decision tree in 10 seconds. My first boosting with NN will take 100*100 seconds  while second boosting with decision tree will take 100*10 seconds. </p>\\n\\n<p>That said I have seen articles, which uses strong learners in boosting. But in that problems that strong learners was fast in my opinion.</p>\\n\\n<p>I tried to train MLP on KDD99 Intrusion Detection Dataset, (4+ Million) using Weka. It took more than 72 hours on my machine. But boosting  (AdaBoostM1 with Decision Tree - Decision Stump) took only 3 hours. In this problem it is clear that I can not use boosting with strong learner, that is a learner which takes too much time.</p>\\n',\n",
       " \"<p>I have a huge dataset ($n$ around five million, $p$ around three thousand) for a classification problem, where my interest is predictive class probabilities for test data, not the target. I shall be using bootstrap samples to estimate the smoothed probabilities from various models.</p>\\n\\n<p>The problem is when I bootstrap the cases not all the levels in the categorical predictors will be in every resample. Given that I need parameters for all levels when working with the test set, I'm unsure of what to do.</p>\\n\\n<p>I read Agresti's <em>Categorical Data Analysis</em>, but there doesn't seem to be mention of this.</p>\\n\\n<p>I have thought of 2 possibilities:</p>\\n\\n<ol>\\n<li><p>Insert a base composition of varied cases in every resample so that all levels are included for all predictors.</p></li>\\n<li><p>Define all levels for each categorical variable with reference to the data schema and run the models.</p></li>\\n</ol>\\n\\n<p>I'm having to think about this because my resamples are pulled as CSV data from a SQL database and I would usually use <code>read.csv()</code> which automagically handles levels, preference relations and levels for categorical data using the cases in the CSV file. (Pulling all of the data in one csv is not an option due to resource constraints.)</p>\\n\",\n",
       " \"<p>1) 2.661 is the effect size not the z score.  You don't know the value of the test statistic when doing a power/sample size calculation.  This is done prior to collecting the data to tell you what you can reasonably expect for the given sample size.\\n2) 2.661 multiplied by the standard deviation is the true difference in means that will be large enough to that you can expect the null hypothesis will be rejected 80% of the time that you apply such a test with that big a true mean difference.\\n3) You can invert the hypothesis test to get a confidence interval.  The construction will depend on the observed value of the mean difference and its known or estimated standard deviation but cannot be inferred from this power calculation at a specified alternative.</p>\\n\\n<p>I am not attempting to answer the last part of your question regarding the concrete example.</p>\\n\",\n",
       " '<p>As a follow-up to my comment, if <code>independence.test</code> refers to <code>coin::independence_test</code>, then you can reproduce a Cochrane and Armitage trend test, as it is used in GWAS analysis, as follows:</p>\\n\\n<pre><code>&gt; library(SNPassoc)\\n&gt; library(coin)\\n&gt; data(SNPs)\\n&gt; datSNP &lt;- setupSNP(SNPs,6:40,sep=\"\")\\n&gt; ( tab &lt;- xtabs(~ casco + snp10001, data=datSNP) )\\n     snp10001\\ncasco T/T C/T C/C\\n    0  24  21   2\\n    1  68  32  10\\n&gt; independence_test(casco~snp10001, data=datSNP, teststat=\"quad\",\\n                    scores=list(snp10001=c(0,1,2)))\\n\\nAsymptotic General Independence Test\\n\\ndata:  casco by snp10001 (T/T &lt; C/T &lt; C/C) \\nchi-squared = 0.2846, df = 1, p-value = 0.5937\\n</code></pre>\\n\\n<p>This is a conditional version of the CATT. About scoring of the ordinal variable (here, the frequency of the minor allele denoted by the letter <code>C</code>), you can play with the <code>scores=</code> arguments of <code>independence_test()</code> in order to reflect the model you want to test (the above result is for a log-additive model).</p>\\n\\n<p>There are five different genetic models that are generally considered\\nin GWAS, and they reflect how genotypes might be collapsed: codominant\\n(T/T (92) C/T (53) C/C (12), yielding the usual $\\\\\\\\chi^2(2)$\\nassociation test), dominant (T/T (92) vs. C/T-C/C (65)), recessive\\n(T/T-C/T (145) vs. C/C (12)), overdominant (T/T-C/C (104) vs. C/T\\n(53)) and log-additive (0 (92) &lt; 1 (53) &lt; 2 (12)). Note that genotype\\nrecoding is readily available in inheritance functions from the\\n<a href=\"http://cran.r-project.org/web/packages/SNPassoc/index.html\" rel=\"nofollow\">SNPassoc</a>\\npackage. The \"scores\" should reflect these collapsing schemes.</p>\\n\\n<p>Following Agresti (<a href=\"http://www.stat.ufl.edu/~aa/cda/cda.html\" rel=\"nofollow\">CDA</a>, 2002, p. 182), CATT is computed as $n\\\\\\\\cdot r^2$,\\nwhere $r$ stands for the linear correlation between the numerical\\nscores and the binary outcome (case/control), that is</p>\\n\\n<pre><code>z.catt &lt;- sum(tab)*cor(datSNP$casco, as.numeric(datSNP$snp10001))^2\\n1 - pchisq(z.catt, df = 1)  # p=0.5925\\n</code></pre>\\n\\n<p>There also exist various built-in CATT functions in R/Bioconductor\\necosystem for GWAS, e.g.</p>\\n\\n<ul>\\n<li><p><code>CATT()</code> from\\n<a href=\"http://cran.r-project.org/web/packages/Rassoc/index.html\" rel=\"nofollow\">Rassoc</a>, e.g.</p>\\n\\n<pre><code>with(datSNP, CATT(table(casco, snp10001), 0.5)) # p=0.5925 \\n</code></pre></li>\\n</ul>\\n\\n<p>(additive/multiplicative)</p>\\n\\n<ul>\\n<li>in <a href=\"http://www.bioconductor.org/packages/2.3/bioc/html/snpMatrix.html\" rel=\"nofollow\">snpMatrix</a>, there are headed as 1-df $\\\\\\\\chi^2$-test when you call\\n<code>single.snp.tests()</code> (see the\\n<a href=\"http://www.bioconductor.org/packages/2.6/bioc//vignettes/snpMatrix/inst/doc/snpMatrix-vignette.pdf\" rel=\"nofollow\">vignette</a>);\\nplease note that the default mode of inheritance is the codominant/additive effect.</li>\\n</ul>\\n\\n<p>Finally, here are two references that discuss the choice of scoring scheme\\ndepending on the genetic model under consideration, and some issues\\nwith power/robustness</p>\\n\\n<ol>\\n<li>Zheng, G, Freidlin, B, Li, Z and Gastwirth, JL (2003). Choice of\\nscores in trend tests for case-control studies of candidate-gene\\nassociations. <em>Biometrical Journal</em>, <strong>45</strong>: 335-348. </li>\\n<li>Freidlin, B, Zheng, G, Li, Z, and Gastwirth, JL (2002). <a href=\"http://wiki.bmi.utah.edu/student/images/f/f5/Trend_Tests_for_Case-Control_Studies_of_Genetic_Markers_%28Power_Sample_Size_and_Robustness%29.pdf\" rel=\"nofollow\">Trend Tests\\nfor Case-Control Studies of Genetic Markers: Power, Sample Size and\\nRobustness</a>. <em>Human Heredity</em>, <strong>53</strong>: 146-152.</li>\\n</ol>\\n\\n<p>See also the <a href=\"http://www.bioconductor.org/packages/2.3/bioc/html/GeneticsDesign.html\" rel=\"nofollow\">GeneticsDesign</a> (bioc) package for power calculation with\\nlinear trend tests.</p>\\n',\n",
       " '<p>The second choice is just silly. There are 400 people. That\\'s the N. The question is what to do with those 400. As @AndyW pointed out, the second argument seems to be saying that these should be treated via a mixed model, but I agree with him. This makes no sense here.</p>\\n\\n<p>The problem with the first argument is with the statement \"ignore the discrete nature....\"; by stating that you have previously determined that the level of stress in the groups is directly proportional to the group number, you are saying that group number is really a proxy for a continuous variable that just happens to be measured at only 4 levels. (How you would establish this is beyond me, but that\\'s what you stated).</p>\\n',\n",
       " '<p>One approach is to fit a density estimator to your first posterior distribution, then use the estimated density as the prior in your update.  One option for a density estimator to use is logspline densities, see the logspline package in R for one way to fit these.</p>\\n\\n<p>It depends on what tool you are using to do your gibbs sampling as to how you would specify the logspline (or other estimator) as you prior (once you have the coefficients from the logspline function the log density is just a sum of cubic polynomials so it should be fairly simple to translate to most Gibbs samplers).  I have seen a trick in WinBugs and OpenBugs where you use a chisquare as an intermediate distribution but specify your own function that gives your prior.  Tools like Stan let you program your own prior.  If you do this in regular R then you can just use the logspline functions there.</p>\\n',\n",
       " '<p>I have a prototype machine producing parts.</p>\\n\\n<p>In a first test the machine produces $N_1$ parts and a binary classifier tells me that $d_1$ parts are defective ($d_1 &lt; N_1$, usually $d_1/N_1&lt;0.01$ and $N_1\\\\\\\\approx10^4$) and $N_1-d_1$ parts are good.</p>\\n\\n<p>Then a technician makes some changes in the machine in order to decrease the number of defective parts.</p>\\n\\n<p>In a second and following test the modified machine produces $N_2$ parts and the same binary classifier (untouched) tells me that $d_2$ parts are defective, anyway $d_2/N_2$ is quite similar to $d_1/N_1$.</p>\\n\\n<p>The technician would like to know if his changes are effective.</p>\\n\\n<p>Assuming that the classifiers is perfect (its sensitivity is 100% and its specificity is 100%), I can perform a test for proportions (with R, I just type <code>prop.test(c(d1,d2),c(N1,N2))</code>).</p>\\n\\n<p>But the classifier is not perfect, so how can I take in account the sensitivity and the specificity, both unknown, of the classifier in order to properly answer to the technician?</p>\\n',\n",
       " '<p>$X$ is the total number of events observed. See <a href=\"http://en.wikipedia.org/wiki/Poisson_distribution#Confidence_interval\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Poisson_distribution#Confidence_interval</a> .</p>\\n',\n",
       " '<p>Suppose $X_1,\\\\\\\\ldots,X_n$ are a random sample of a continuous and strictly increasing distribution $F(x)$ with mean $\\\\\\\\mu$. If</p>\\n\\n<p>$$\\\\\\\\nY_i = \\\\\\\\begin {cases} 2 &amp; \\\\\\\\text{if}\\\\\\\\ X_i&gt;\\\\\\\\mu \\\\\\\\\\\\\\\\\\\\\\\\n1 &amp; \\\\\\\\text{if} \\\\\\\\ X_i\\\\\\\\leq\\\\\\\\mu  \\\\\\\\end {cases}\\\\\\\\n$$</p>\\n\\n<p>Determine the MLE of parameter $\\\\\\\\mu$</p>\\n',\n",
       " \"<p>I don't think that the fact that you have found significant differences in all fifteen of your comparisons makes a difference. To maintain the familywise error rate, I would be tempted to simply apply a Bonferroni correction. Perhaps it's good to be conservative in this instance given your small sample size (indeed, even smaller than the number of comparisons you are making) and the fact that you began the analysis with no predefined assumptions. Although if this latter fact is true, replication in an independent population would really be in order if you want to make stronger conclusions.</p>\\n\",\n",
       " '<p>I doubt you fill find a suitable range of distinct colours with so much categories. Anyway, here are some ideas:</p>\\n\\n<ol>\\n<li>For stacked barchart, you need <code>barplot()</code> with <code>beside=FALSE</code> (which is the default) -- this is in base R (@Chase\\'s solution with <a href=\"http://cran.r-project.org/web/packages/ggplot2/index.html\">ggplot2</a> is good too)</li>\\n<li>For generating a color ramp, you can use the <a href=\"http://cran.r-project.org/web/packages/RColorBrewer/index.html\">RColorBrewer</a> package; the example shown by @fRed can be reproduced with <code>brewer.pal</code> and any one of the diverging or sequential palettes. However, the number of colour is limited, so you will need to recycle them (e.g., every 6 items)</li>\\n</ol>\\n\\n<p>Here is an illustration:</p>\\n\\n<pre><code>library(RColorBrewer)\\nx &lt;- sample(LETTERS[1:20], 100, replace=TRUE)\\ntab &lt;- as.matrix(table(x))\\nmy.col &lt;- brewer.pal(6, \"BrBG\") # or brewer.pal(6, \"Blues\")\\nbarplot(tab, col=my.col)\\n</code></pre>\\n\\n<p>There is also the <a href=\"http://cran.r-project.org/web/packages/colorspace/index.html\">colorspace</a> package, which has a nice accompagnying vignette about the design of good color schemes. Check also Ross Ihaka\\'s course on <a href=\"http://www.stat.auckland.ac.nz/~ihaka/courses/787/\">Topic in Computational Data Analysis and Graphics</a>.</p>\\n\\n<p>Now, a better way to display such data is probably to use a so-called Cleveland dot plot, i.e.</p>\\n\\n<pre><code>dotchart(tab)\\n</code></pre>\\n',\n",
       " '<p>I am running a frequency count on the instances of powerless of language in the language of prosecuting attorneys and the same frequency count of the defendants language. </p>\\n\\n<p>I am asking if the frequency of the powerless language by women makes a favorable verdict and less powerless language by men creates a favorable verdict. I have 400 cases/transcripts. </p>\\n\\n<p>Below is my hypothesis:\\nIs the use of powerless language used by defense attorneys in arguments independent of the verdict? </p>\\n\\n<pre><code>a. Is the use of powerless language used by female defense \\n    attorneys in closing    arguments independent of the verdict? \\nHa) The use of powerless language by a female defense attorney is \\n    not independent of the verdict.\\nHo) The use of powerless language by a female defense attorney is               \\n    independent of the verdict   \\nb. Is the use of powerless language used by male defense attorneys \\n    in closing  arguments independent of the verdict? \\nHa) The use of powerless language by a male defense attorney is \\n    not independent of the verdict.\\nHo) The use of powerless language by a male defense attorney is                 \\n    independent of the verdict. \\n</code></pre>\\n\\n<p>Would I run two separate chi squares: one on all of the prosecuting attorneys and one on all the defending attorneys with the verdicts?</p>\\n',\n",
       " '<p>To determine organic carbon concentrations in a water solution from UV spectroscopy I made a calibration curve with known carbon concentrations and measured the UV absorbance. I used R for the data analysis and the data are found below.</p>\\n\\n<p>My question now is, how to determine the accuracy of my calibration curve. I would like to tell people that I can measure the concentrations with an accuracy of 15% or 4 mg l-1, for example. How can I go on with this? Can I just use the $R^2$ (0.91) and say that I have an uncertainty of 9% (1-0.91)? What is the correct way to do this?</p>\\n\\n<pre><code>concentration &lt;- c(27.2,32.4,16.5,11.6,11.9,9.87,46.0,73.6,75.4,73.1,59.5,49.0,\\n            79.0,81.6,66.7,26.7)\\n\\nabsorbance &lt;- c(0.764, 0.923, 0.678, 0.373, 0.287, 0.253, 1.660, 2.331, 2.255, \\n            2.019, 1.130, 1.858, 2.404, 2.812, 2.362, 0.636)\\n\\nplot(absorbance, concentration, xlab = \"absorbance (254 nm)\", ylab = \"concentration (mg l-1)\")\\n\\nfit &lt;- lm(concentration ~ absorbance)\\nabline(fit)\\nlm_coef &lt;- round(coef(fit), 3) # extract coefficients\\nmyr2 &lt;- format(summary(fit)$adj.r.squared, digits = 2)\\nlegend(\"topleft\", legend = as.expression(c(bquote(paste(\"lin. equation: \",\\n                                                    y == .(lm_coef[2])*x + .(lm_coef[1]))), \\n                                       bquote(paste(\"\", R^2 == .(myr2))))), cex = 0.6)\\n</code></pre>\\n',\n",
       " '<p>It looks like you are trying to compare two odds ratios. Try looking at</p>\\n\\n<p><a href=\"http://stats.stackexchange.com/questions/1405/statistical-test-for-difference-between-two-odds-ratios\">Statistical test for difference between two odds ratios?</a></p>\\n',\n",
       " '<p>Say, for example, that I want to determine the market share or relative popularity of coffee houses within a certain population through a survey.  What is the best way to write a question that accurately measures this?</p>\\n\\n<p>Some issues that I am thinking of:</p>\\n\\n<p>I don\\'t want to ask a single choice question (\"Which coffee house do you go to?\"), because the coffee houses are not mutually exclusive.  I may enjoy more than one equally often.</p>\\n\\n<p>If I ask a multiple choice question, then I can\\'t really get a real \"market share,\" since the sum of the proportions of people who go to each coffee house sum to over 100%.  I can only say that \"x% of people in this population go to this coffee house.\"</p>\\n\\n<p>Is it possible to ask a series of questions (\"Which coffee house do you prefer the most?\" \"Which coffee house do you prefer the second most?\", or \"Rank the following coffee houses\")?</p>\\n\\n<p>Can I ask a multiple choice question then rebase the proportion to the total number of responses?  For example, if I have 100 respondents, but they selected 200 coffee houses (because each respondent said they went to two coffee houses, maybe), can I calculate a frequency table based on 200, the number of selections, instead of the number of respondents?</p>\\n',\n",
       " \"<p>I'll work out the first one and leave the other two to you. First, find $\\\\\\\\Pr(X \\\\\\\\leq x)$. We have\\n$$\\\\\\\\begin{align*}\\\\\\\\Pr(X \\\\\\\\leq x) &amp;= 1 - \\\\\\\\Pr(X &gt; x) \\\\\\\\\\\\\\\\\\\\\\\\n&amp;= 1 - \\\\\\\\Pr(U &gt; x, V &gt; x) \\\\\\\\\\\\\\\\ &amp;= 1 - \\\\\\\\Pr(U &gt; x)\\\\\\\\Pr(V &gt; x) \\\\\\\\\\\\\\\\ &amp;= 1 - (1-x)^2 \\\\\\\\end{align*}$$\\nThe first step follows that, for $x$ to be a minimum $U$ and $V$ must be greater than that value. Independence gives us the second line. Use of the CDF of the uniform gives us the third.</p>\\n\\n<p>To get the density, take the derivative to get $2(1-x)$.</p>\\n\",\n",
       " \"<p>A sentence or two?  Yikes!  </p>\\n\\n<p>It's all about random vs fixed effects, I suppose, and so I would focus on shrinking individual estimates toward the population mean (aka BLUP).</p>\\n\",\n",
       " '<p>With respect to loess the <a href=\"http://biopython.org/wiki/Biopython\" rel=\"nofollow\">BioPython</a> project has a <a href=\"http://www.koders.com/python/fid5A91A606E15507B6823DEC7A059488A6624C4832.aspx?s=python\" rel=\"nofollow\">lowess() function.</a></p>\\n\\n<p>As for ARIMA model fitting, <a href=\"http://www.vni.com/campaigns/pyimslstudioeval/\" rel=\"nofollow\">PyIMSL Studio</a> contains a number of very useful time series analysis functions, including ones for <a href=\"http://www.vni.com/products/imsl/documentation/PyIMSL/stat/wwhelp/wwhimpl/js/html/wwhelp.htm#href=Csch8.9.9.html\" rel=\"nofollow\">automatically fitting the best ARIMA model</a> and <a href=\"http://www.vni.com/products/imsl/documentation/PyIMSL/stat/wwhelp/wwhimpl/js/html/wwhelp.htm#href=Csch8.9.7.html\" rel=\"nofollow\">identifying outliers</a> from it. These same functions are available in the other IMSL libraries if you choose to instead code in C, C#, Java, or Fortran.</p>\\n',\n",
       " '<p>The entire 4-level variable <em>x</em> is what is associated with <em>y</em>.  No individual level of <em>x</em> (no value that <em>x</em> can take on) can be said to be associated with <em>y</em> because to be associated requires variation.  As an example, ethnicity can be said to be associated with income, but \"being Hispanic\" cannot.  On the other hand, if the variable is <em>whether one is Hispanic</em> (with 2 levels, yes and no), then such a variable can be said to be associated with another.</p>\\n',\n",
       " \"<p>I'm trying to normalize a set of columns of data in an excel spreadsheet.</p>\\n\\n<p>I need to get the values so that the highest value in a column is = 1 and lowest is = to 0, so I've come up with the formula:</p>\\n\\n<p><code>=(A1-MIN(A1:A30))/(MAX(A1:A30)-MIN(A1:A30))</code></p>\\n\\n<p>This seems to work fine, but when I drag down the formula to populate the cells below it, now only does <code>A1</code> increase, but <code>A1:A30</code> does too.</p>\\n\\n<p>Is there a way to lock the range while updating just the number I'm interested in?</p>\\n\\n<p>I've tried putting the Max and min in a different cell and referencing that but it just references the cell under the one that the Max and min are in and I get divide by zero errors because there is nothing there.</p>\\n\",\n",
       " \"<p>What are the key differences between the following two models?</p>\\n\\n<pre><code>lmefit = lmer(MathAch ~ SES + (1 |School) , MathScores) \\n\\nlmfit = lm(MathAch ~ SES + factor(School) -1 , MathScores) \\n</code></pre>\\n\\n<p>To me, they seem to be the same, except that lmefit takes less parameters (because it used Normal distribution to model the levels at the group level...)</p>\\n\\n<p>Am I right?</p>\\n\\n<hr>\\n\\n<p>And what's the difference between these two models?</p>\\n\\n<pre><code>M0 &lt;- lmer (y ~ 1 + (1 | county))\\nM1 &lt;- lmer (y ~ -1 + (1 | county))\\n</code></pre>\\n\",\n",
       " '<p>A class teacher has the following absentee record of students:</p>\\n\\n<pre><code>No. of days         0-6    6-10  10-14   14-20   20-28   28-30  38- 40    \\nNo. of students      11     10      7      4        4      3      1\\n</code></pre>\\n\\n<p>What will be the mean no. of days a student was absent? \\nI am unable to understand since the class sizes are different.  </p>\\n\\n<p><strong>Update</strong></p>\\n\\n<p>In both the first 2 answers, the authors have assumed symmetry in the class, and thus they are taking the means and are finding the final \"weighted mean\" (if the term is wrong kindly correct me). However, suppose this assumption is not valid, then what is the best approach to solve this problem?        </p>\\n',\n",
       " '<p>I need to perform a logistic regression to to see if a group of variables which are found to be significantly associated with an outcome (by univariate tests) have significant impact on the outcome when they are put together.</p>\\n\\n<p>In this case, from my reading, I gathered that in SPSS I need to use the enter method. When I applied this method, my new model decreased the prediction percent (not sure if this is the correct term) in the classification table in comparison to the constant only model (90.5 in constant only to 89.8 in new model). I have excluded multiple collinearity earlier. </p>\\n\\n<pre><code>Omnibus Tests of Model Coefficients             \\n        Chi-squaredf    Sig.\\nStep 1  Step    27.441  7   .000\\n         Block  27.441  7   .000\\n         Model  27.441  7   .000\\n\\nModel Summary           \\nStep    -2 Log likelihood   Cox &amp; Snell R Square    Nagelkerke R Square\\n  1               164.185a                .086                   .184\\n\\n(a Estimation terminated at iteration number 6 because parameter estimates changed \\nby less than .001)          \\n\\nHosmer and Lemeshow Test            \\nStep     Chi-square    df     Sig.\\n1            6.317      7     .503\\n</code></pre>\\n\\n<p>Given my objective of seeing if the independent variables when put together can have a significant impact on the outcome or not, can I use this model for that purpose to identify the variables that have and do not have independent effects on the outcome when compared together?</p>\\n\\n<p>In answer to the comments:  </p>\\n\\n<ol>\\n<li>As for the \"measure of predictive accuracy\", could I please know it in layman\\'s terms please? My statistical vocabulary is inefficient to answer precisely though I understand what is should mean.</li>\\n<li>I ran collinearity diagnostics to see there is an issue of multicollinearity between independent variables.</li>\\n</ol>\\n\\n<p>I cannot paste the ROC curve itself, but the tables is given below, given the values I guess I may still use the model as it is rather than changing any other parameter. Opnion on this is welcome. Thank you everyone for suggestions. </p>\\n\\n<pre><code>Area Under the Curve                \\nTest Result Variable(s): Predicted probability\\nArea    Std. Error a   Asymptotic Sig. b    Asymptotic 95% Confidence Interval\\n                                            Lower Bound             Upper Bound\\n.756    .042           .000              .67                           .839\\n\\nThe test result variable(s): Predicted probability has at least one tie between the \\npositive actual state group and the negative actual state group. Statistics may be biased.      \\na Under the nonparametric assumption                \\nb Null hypothesis: true area = 0.5          \\n</code></pre>\\n\\n<p>Now I have an additional problem, or two rather\\n1. I have another outcome variable, again my onjective is to identify independant predictors for the given outcome amongst predictors with a p&lt;0.01 in univariate analysis. I tried different combinations of covariates manually using the enter method, and non of a combinations seem to give a statistically acceptable models in terms of correctly classified proportions in classification tables or in ROC curves. Individially taken, these variables would give acceptable values (>0.8) in ROC despite not improving the correctly classified rates. Can I take this as being evidence of these covariants not haveing independant effects on the outcome? (number of independant variables available 7)</p>\\n\\n<p>As the dependant variable in this is 30 day mortality, I think I can try Cox regression. However, the only time dependant variable I have is length of hospitalization, I am not too sure if I can use this at the \"time\" cage in SPSS. </p>\\n\\n<p>Thank you in advance. (I am not too sure if I need to post this as a new question) </p>\\n',\n",
       " '<p>I\\'m going to offer a different answer, given the OP\\'s comments.  It would be inappropriate to spend much of your time learning regression in order to do this.  Though I\\'d love for you to get something out of it, I feel it\\'s better to put effort into the final project.  :)</p>\\n\\n<p>A much simpler method is to use what\\'s known as a trimmed mean.  If you have Excel, then the function is described at this page: <a href=\"http://office.microsoft.com/en-us/excel-help/trimmean-HP005209322.aspx\" rel=\"nofollow\">http://office.microsoft.com/en-us/excel-help/trimmean-HP005209322.aspx</a>.</p>\\n\\n<p>In my regression answer, I assumed that each test would have a bunch of different instructions and you\\'d have access only to the final timing.  If you are able to time each instruction separately, then the trimmed mean is the way to go.  It will drop outliers, and you can specify what fraction to eliminate.  For many cases, I frequently drop the outer 5%, i.e. trim at the 2.5%ile and 97.5%ile.  It\\'s good to look at those and see what\\'s going on for the extreme values (e.g. optimizations or garbage collection), but I don\\'t think you\\'ll need to worry about it much.</p>\\n\\n<p>Good luck!</p>\\n\\n<hr>\\n\\n<p>(Update) I just realized that I didn\\'t explain the trimmed mean.  This is basically the mean of observations after the extreme values have been dropped (trimmed).  There are other ways to get a robust estimate of the mean value, but this is probably the simplest.  (The median is another robust estimator, but it\\'s not always the same as the mean.)  In any case, this gives you an estimate of the mean, disregarding some fraction of the extreme values, and that fraction is set by you.</p>\\n',\n",
       " \"<p>You can't interpret the $p$-values. The long-tailed errors you're describing often act to underestimate the standard errors, making your $p$-values too small (not to mention that $\\\\\\\\hat{\\\\\\\\beta}$ isn't normally distributed in finite samples). I suggest a non-parametric bootstrap so you can characterize the sampling distribution of your coefficient estimates without making an unwarranted assumptions about the error distribution. </p>\\n\",\n",
       " \"<p>I would be more comfortable answering this question if you provided more information. I think what you are saying is that the F statistic is not significant but given Y= a+B1+B2+B3+e you find a statistically significant t statistic on B1 and B3. </p>\\n\\n<p>I'm then interpreting your question as whether you can make a valid statistical inference about B1 and B3 given a F statistic that is not significant. </p>\\n\\n<p>The short answer is no. </p>\\n\\n<p>In order to calculate a coefficient such as B1 you need to interpret the dependent variable Y at a statistically significant level which shows up in a significant F statistic. </p>\\n\\n<p>See slide 7 here (http://monogan.myweb.uga.edu/teaching/ols/lm7.pdf) for the equation that calculates a coefficient. Notice the relationship with Y. </p>\\n\\n<p>Does this help to answer your question? </p>\\n\",\n",
       " '<p>In this interpretation, the triangle is a right triangle of side lengths $X$ and $Y$ distributed binormally with expectations $\\\\\\\\mu_x$ and $\\\\\\\\mu_y$, standard deviations $\\\\\\\\sigma_x$ and $\\\\\\\\sigma_y$, and correlation $\\\\\\\\rho$.  We seek the distribution of $\\\\\\\\arctan(Y/X)$.  To this end, standardize $X$ and $Y$ so that</p>\\n\\n<p>$$X = \\\\\\\\sigma_x \\\\\\\\xi + \\\\\\\\mu_x$$ and $$Y = \\\\\\\\sigma_y \\\\\\\\eta + \\\\\\\\mu_y$$</p>\\n\\n<p>with $\\\\\\\\xi$ and $\\\\\\\\eta$ standard normal variates with correlation $\\\\\\\\rho$.  Let $\\\\\\\\theta$ be an angle and for convenience write $q = \\\\\\\\tan(\\\\\\\\theta)$.  Then</p>\\n\\n<p>$$\\\\\\\\mathbb{P}[\\\\\\\\arctan(Y/X) \\\\\\\\le \\\\\\\\theta] = \\\\\\\\mathbb{P}[Y \\\\\\\\le q X]$$</p>\\n\\n<p>$$=\\\\\\\\mathbb{P}[\\\\\\\\sigma_y \\\\\\\\eta + \\\\\\\\mu_y \\\\\\\\le q \\\\\\\\left( \\\\\\\\sigma_x \\\\\\\\xi + \\\\\\\\mu_x \\\\\\\\right)$$</p>\\n\\n<p>$$=\\\\\\\\mathbb{P}[\\\\\\\\sigma_y \\\\\\\\eta - q \\\\\\\\sigma_x \\\\\\\\xi \\\\\\\\le q \\\\\\\\mu_x - \\\\\\\\mu_y]$$</p>\\n\\n<p>The left hand side, being a linear combination of Normals, is normal, with mean $\\\\\\\\mu_y \\\\\\\\sigma_y - q \\\\\\\\mu_x \\\\\\\\sigma_x$ and variance $\\\\\\\\sigma_y^2 + q^2 \\\\\\\\sigma_x^2 - 2 q \\\\\\\\rho \\\\\\\\sigma_x \\\\\\\\sigma_y$.  </p>\\n\\n<p>Differentiating the Normal cdf of these parameters with respect to $\\\\\\\\theta$ yields the pdf of the angle.  The expression is fairly grisly, but a key part of it is the exponential</p>\\n\\n<p>$$\\\\\\\\exp \\\\\\\\left(-\\\\\\\\frac{\\\\\\\\left(\\\\\\\\mu _y \\\\\\\\left(\\\\\\\\sigma _y+1\\\\\\\\right)-\\\\\\\\mu _x \\\\\\\\left(\\\\\\\\sigma _x+1\\\\\\\\right) \\\\\\\\tan (\\\\\\\\theta )\\\\\\\\right){}^2}{2 \\\\\\\\left(-2 \\\\\\\\rho  \\\\\\\\sigma _x \\\\\\\\sigma _y \\\\\\\\tan (\\\\\\\\theta )+\\\\\\\\sigma _x^2+\\\\\\\\sigma _y^2+\\\\\\\\tan ^2(\\\\\\\\theta )\\\\\\\\right)}\\\\\\\\right),$$</p>\\n\\n<p>showing right away that the angle is <em>not</em> normally distributed.  However, as your simulations show and intuition suggests, it should be approximately normal provided the variations of the side lengths are small compared to the lengths themselves.  In this case a <a href=\"http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aoms/1177728652\">Saddlepoint approximation</a> ought to yield good results for specific values of $\\\\\\\\mu_x$, $\\\\\\\\mu_y$, $\\\\\\\\sigma_x$, $\\\\\\\\sigma_y$, and $\\\\\\\\rho$, even though a closed-form general solution is not available.  The approximate standard deviation will drop right out upon finding the second derivative (with respect to $\\\\\\\\theta$) of the logarithm of the pdf (as shown in equations (2.6) and (3.1) of the reference).  I recommend a computer algebra system (like MatLab or Mathematica) for carrying this out!</p>\\n',\n",
       " '<p>The help of the R command <code>hist</code> <a href=\"http://stat.ethz.ch/R-manual/R-patched/library/grDevices/html/nclass.html\" rel=\"nofollow\">http://stat.ethz.ch/R-manual/R-patched/library/grDevices/html/nclass.html</a> has some references to algorithms for computing the number of the bins:</p>\\n\\n<p>Sturges, H. A. (1926) The choice of a class interval. Journal of the American Statistical Association 21, 65–66.</p>\\n\\n<p>Scott, D. W. (1979) On optimal and data-based histograms. Biometrika 66, 605–610.</p>\\n\\n<p>Freedman, D. and Diaconis, P. (1981) On the histogram as a density estimator: L_2 theory. Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete 57, 453–476.</p>\\n',\n",
       " '<p>What would you guys suggest as a good introduction book to small area estimation?</p>\\n',\n",
       " '<p>I would support your suggestion that it depends on what you want to do.</p>\\n\\n<p>For example, assume that you want to \"evaluate\" the expected value of $X+\\\\\\\\frac{1}{2}Y$, where $X$ and $Y$ are independent random variables. You would create very messy notation using the $\\\\\\\\mu$-Notations, whereas \\n$$E\\\\\\\\left(X+\\\\\\\\frac{1}{2}Y\\\\\\\\right)=E(X)+\\\\\\\\frac{1}{2}E(Y)$$\\nis as clear as it can get. </p>\\n\\n<p>Note that the expected value above is potentially unknown, or at least not explicitly given. </p>\\n\\n<p>However, when you refer to the property of the underlying distribution as in $X_i\\\\\\\\sim N(\\\\\\\\mu_{X_i}, \\\\\\\\sigma)$ with $\\\\\\\\mu_{X_i}\\\\\\\\in\\\\\\\\mathbb{R}$, your $\\\\\\\\mu_{X_i}$ describes a value that is constant and assumed to be known beforehand. In this case, it would make no sense to write $X_i\\\\\\\\sim N(E(X_i), \\\\\\\\sigma)$. </p>\\n\\n<p>I would say that the \"known/unknown value distinction\" is probably a rough guideline. However, I can easily imagine situations where you would want to use them differently. (e.g. say your discussion repeats $E(X)$ 20 times within a few rows. Then it might be helpful to define $\\\\\\\\mu_X:=E(X)$ and replace it all over that part to keep your notation clean).</p>\\n\\n<p>As a last comment I have to say that I come with a strong mathematical background and not practical statistics. And as you know, practice is like theory, but different...</p>\\n',\n",
       " '<p>This problem is encountered in quality control/<a href=\"http://en.wikipedia.org/wiki/Statistical_process_control\" rel=\"nofollow\">statistical process control</a> settings.  There\\'s a large literature, as you have hinted, because different parameters as estimated in various ways from different forms of sampling different distributions can be expected to vary in different ways.  The purpose is to detect that variation on-line as soon as possible after it occurs without triggering too many false detections along the way.  Consider using a control chart (<a href=\"http://p://en.wikipedia.org/wiki/Control_chart\">1</a>, <a href=\"http://www.itl.nist.gov/div898/handbook/pmc/section3/pmc31.htm\" rel=\"nofollow\">2</a>).  In your concrete situation a good choice is a combined Shewhart-CUSUM control chart.</p>\\n',\n",
       " \"<p>Apologies in advance for the (possibly?) poor terminology as I'm a bit of a novice in the field. I was torn whether to ask this on stackoverflow or here, so hope its the right place.</p>\\n\\n<p>Anyway, my problem is I'm trying to efficiently calculate the best weights for a portfolio of assets that will maximize the sharpe ratio (or similar measure of risk adjusted returns) of the portfolio as a whole, where portfolio returns are unknown, non-normal and dependent on each other, but simulated by a seperate function.</p>\\n\\n<p>The processing chain is as follows:</p>\\n\\n<ol>\\n<li>Simulate returns from portfolio (I already have this part; assume\\nthat the simulation uses a VAR Copula Garch model incorporating all\\nassets in the portfolio, though how it arrives at the simulation is not important for this question, just that the asset returns are not independent but can be simulated.) </li>\\n<li>Calculate weights for each asset that will maximize some risk/return\\nmeasure (e.g. sharpe ratio) of the whole portfolio </li>\\n<li>Repeat above until stable points are found (e.g. each 'simulation' is very noisy by itself/far from the average over all simulations).</li>\\n</ol>\\n\\n<p>I thought of doing this the brute force way via monte carlo simulation, but thought it best to seek expert advise first. So my question is essentially:</p>\\n\\n<ol>\\n<li>How would you recommend I go about doing the above in R.</li>\\n<li>Are there any packages, algorithms or resources you would recommend I use/look into for this task? Goal is to have a proof of concept up asap, and if the ideas show practical utility to delve deeper into the theory and better methods. </li>\\n</ol>\\n\\n<p>The easier it is to get setup the better; ideal would be recommendation on a flexible/efficient optimization framework allowing constraint specification (e.g. can have negative weights; must not have above 20%, must not be below 2% (else %0 weighted) etc... Yes, it would be nice if it also included a free lunch ;-) )</p>\\n\\n<p>Thank you for taking the time to read this, and any thoughts or comments you have in advance!</p>\\n\",\n",
       " '<p>docs.google.com</p>\\n',\n",
       " '<p>Is \"margin of error\" the same as \"standard error\"?</p>\\n\\n<p>A (simple) example to illustrate the difference would be great!</p>\\n',\n",
       " 'A vast area which includes generating results from computer models.',\n",
       " '<p>My view is that the models used in economics and the other social sciences are useful only insofar as they have predictive power in the real world - a model which doesn\\'t predict the real world is just some clever math. A favorite saying of mine to colleagues is that \"data is king\".</p>\\n\\n<p>It seems to me that your question raises two critiques of a predictive approach. First, you point out that the models produced by machine learning techniques may not be <em>interpretable</em>. Second, you suggest that the methods used by those in the social sciences are more useful for uncovering causal relationships than machine learning.</p>\\n\\n<p>To address the first point, I\\'d offer the following counter argument. The present fad in machine learning favours methods (like SVMs and NN) which are not at all easy for a layperson to understand. This does not mean that <em>all</em> machine learning techniques have this property. For example, the venerable C4.5 decision tree is still widely used 20 years after reaching the final stage of its development, and produces as output a number of classification rules. I would argue that such rules lend themselves better to interpretation than do concepts like the log odds ratio, but that\\'s a subjective claim. In any case, such models <em>are</em> interpretable.</p>\\n\\n<p>In addressing the second point, I will concede that if you train a machine learning model in one environment, and test it in another, it will likely fail, however,  there is no reason to suppose a priori that this is not also true of a more conventional model: if you build your model under one set of assumptions, and then evaluate it under another, you\\'ll get bad results. To co-opt a phrase from computer programming: \"garbage in, garbage out\" applies equally well to both machine learning and designed models. </p>\\n',\n",
       " '<p>The geometric distribution is given by:</p>\\n\\n<p>$$p(X|\\\\\\\\theta)=(1-\\\\\\\\theta)^{X-1}\\\\\\\\theta \\\\\\\\;\\\\\\\\;\\\\\\\\; X=1,2,3,\\\\\\\\dots$$</p>\\n\\n<p>The log likelihood is thus given by:</p>\\n\\n<p>$$\\\\\\\\log[p(X|\\\\\\\\theta)]=L=(X-1)\\\\\\\\log(1-\\\\\\\\theta)+\\\\\\\\log(\\\\\\\\theta)$$</p>\\n\\n<p>Differentiate once:</p>\\n\\n<p>$$\\\\\\\\frac{\\\\\\\\partial L}{\\\\\\\\partial \\\\\\\\theta}=\\\\\\\\frac{1}{\\\\\\\\theta}-\\\\\\\\frac{X-1}{1-\\\\\\\\theta}$$</p>\\n\\n<p>And again:</p>\\n\\n<p>$$\\\\\\\\frac{\\\\\\\\partial^{2} L}{\\\\\\\\partial \\\\\\\\theta^{2}}=-\\\\\\\\frac{1}{\\\\\\\\theta^{2}}-\\\\\\\\frac{X-1}{(1-\\\\\\\\theta)^{2}}$$</p>\\n\\n<p>Take the negative expectation of this conditional on $\\\\\\\\theta$ (called Fisher information), note that $E(X|\\\\\\\\theta)=\\\\\\\\frac{1}{\\\\\\\\theta}$</p>\\n\\n<p>And so we have:</p>\\n\\n<p>$$I(\\\\\\\\theta)=\\\\\\\\frac{1}{\\\\\\\\theta^{2}}+\\\\\\\\frac{\\\\\\\\theta^{-1}-1}{(1-\\\\\\\\theta)^{2}}=\\\\\\\\theta^{-2}\\\\\\\\left(1+\\\\\\\\frac{\\\\\\\\theta}{1-\\\\\\\\theta}\\\\\\\\right)=\\\\\\\\theta^{-2}(1-\\\\\\\\theta)^{-1}$$</p>\\n\\n<p>The Jeffreys prior is given by the square root of this:</p>\\n\\n<p>$$p(\\\\\\\\theta|I) \\\\\\\\propto \\\\\\\\sqrt{I(\\\\\\\\theta)}=\\\\\\\\theta^{-1}(1-\\\\\\\\theta)^{-\\\\\\\\frac{1}{2}}$$</p>\\n',\n",
       " '<p>At the <a href=\"http://www.alglib.net/dataanalysis/decisionforest.php\">Alglib page</a> you cited, it says, </p>\\n\\n<blockquote>\\n  <p>\"The RDF [Random decision forest] algorithm is a modification of the\\n  original Random Forest algorithm designed by Leo Breiman and Adele\\n  Cutler.\"</p>\\n</blockquote>\\n\\n<p><a href=\"http://rapid-i.com/rapidforum/index.php?topic=2996.0\">A question at rapid-i.com</a> refers to </p>\\n\\n<blockquote>\\n  <p>Ho TK (1998) The Random Subspace Method for Constructing Decision\\n  Forests. IEEE Trans Pattern Anal Mach Intel 20(8) 832-844 [<a href=\"http://www.computer.org/portal/web/csdl/doi/10.1109/34.709601\">Abstract</a>]</p>\\n</blockquote>\\n\\n<p>which might be yet another thing.</p>\\n\\n<p><a href=\"http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\">At Breiman\\'s web page at Berkeley</a>, it says, </p>\\n\\n<blockquote>\\n  <p>\"Random Forests(tm) is a trademark of Leo Breiman and Adele Cutler and\\n  is licensed exclusively to Salford Systems for the commercial release\\n  of the software. Our trademarks also include RF(tm),\\n  RandomForests(tm), RandomForest(tm) and Random Forest(tm).\"</p>\\n</blockquote>\\n\\n<p>So I conclude that there are subtle differences, but mostly it\\'s a trademark issue regarding the name \"random forest\".</p>\\n',\n",
       " '<p>The \"a\" in \"gam\" stands for \"additive\" which means no interactions, so if you fit interactions you are really not fitting a gam model any more.</p>\\n\\n<p>That said, there are ways to get some interaction like terms within the additive terms in a gam, you are already using one of those by using the <code>by</code> argument to <code>s</code>.  You could try extending this to having the argument <code>by</code> be a matrix with a function (sin, cos) of doy or tod.  You could also just fit smoothing splines in a regular linear model that allows interactions (this does not give the backfitting that gam does, but could still be useful).</p>\\n\\n<p>You might also look at projection pursuit regression as another fitting tool.  Loess or more parametric models (with sin and/or cos) might also be useful.</p>\\n\\n<p>Part of decision on what tool(s) to use is what question you are trying to answer.  Are you just trying to find a model to predict future dates and times? are you trying to test to see if particular predictors are significant in the model?  are you trying to understand the shape of the relationship between a predictor and the outcome?  Something else?</p>\\n',\n",
       " '<p>I have read different papers about Gaussian Processes and Carl book as well, I noticed that they sometimes refer to parameter optimization by maximizing marginal liklihood, and in other  contexts they reffere to it as optimizing by minimizing -ve log marginal liklihood!</p>\\n\\n<p>Is there a difference between the two? or they are both the same animal?</p>\\n',\n",
       " '<p>The whole goal was to generate standard normal conditional on the $C$ (using all of the data). That proved to be a rather difficult problem and therefore we have chosen to do this sequentially. You have correctly understood the meaning of $C_t$ and $Q_t$. </p>\\n\\n<p>By the way, $C_t$ is formally defined in the middle of the page 9. You are correct the definition of $Q_t$ has slipped through the cracks of several revisions but you did guess its meaning correctly. Thank you for pointing this out.</p>\\n\\n<p>The main idea is that it is easy to move from $C_t$ to $C_{t+1}$&mdash;that is all the business with $m_t$ and $M_t$ that follows. The reason why we choose sequential MC is that we can easily generate from the conditional distribution $Z_{t+1} I_{C_{t+1}}$ given $Z_t$ [using $m_t$ and $M_t$]. However there is a need to re-weight due to the fact that marginal distribution of $Z_{t+1} I_{C_{t+1}}$ marginalized to the time $t$ is different than the distribution of $Z_t I_{C_t}$. </p>\\n\\n<p>The details of how much to re-weight is on the page 10 with more details in the Appendix C.</p>\\n',\n",
       " \"<p>I have a panel chart composed of 4 panels, each with 12 bars that essentially show data from a pivot table.  The default layout has the first 11 rows show individual points, with the 12th row showing a total of the entire column.  Similarly, the final column shows a total of the 3 preceding columns.  The result is the total for all values is bottom right, like in pivot table.</p>\\n\\n<p>My question is whether there's a benefit or preferred method to rearranging the panels and data points so that the aggregate data is top left in the chart, making it the first value seen (in traditional left-right, top-bottom reading), or keeping the more traditional mathematic approach with totals at the bottom and to the right?  Thanks for your help.</p>\\n\",\n",
       " '<p>Here my 2 cents.  When 20/1,000 samples are contaminated, you could use a Poisson distribution to model the event of a person eating a contaminated sample (mind the difference between the weight of a sample in the lab and on your plate).</p>\\n\\n<p>When a Monte Carlo sample is simulated to be positive in the previous step, you can then simulate the ingested number of bacteria using the empirical distribution of your 20 values, or fit for example an exponential distribution.  In the latter case, have a look at the fitdistrplus package.  In the former case, you could do it as follows:</p>\\n\\n<pre><code>Fn &lt;- ecdf(d)\\nrandom_samples &lt;- quantile(x=Fn, prob=runif(n=1e05, min=0, max=1))\\n</code></pre>\\n\\n<p>Take care that you transfer your units correctly (log values, sample weights, etc.) and realize that this simulation is only a very crude approximation of what happens in reality.</p>\\n',\n",
       " '<p>Yes, It is possible, you may send details so can do specific answer. I will suggest you to used faisalconjoint package of R, It is useful for all type of conjoint data, and testes with many published data and studies.</p>\\n',\n",
       " '<blockquote>\\n  <p><strong>Possible Duplicate:</strong><br>\\n  <a href=\"http://stats.stackexchange.com/questions/7/locating-freely-available-data-samples\">Locating freely available data samples</a><br>\\n  <a href=\"http://stats.stackexchange.com/questions/11142/sites-for-predictive-modeling-competitions\">Sites for predictive modeling competitions</a>  </p>\\n</blockquote>\\n\\n\\n\\n<p>Are there sites or sources of \"datasets\" (either artificially created or taken from actual experiments/sources) someone can use to test their own abilities in detecting clusters, patterns, and to formulate/validate hypothesis?</p>\\n\\n<p>Possibly with answers, that could be consulted, later to determine what you got and what the expected results/techniques should have been?</p>\\n',\n",
       " '<p>I\\'ve published a method for identifying outliers in nonlinear regression, and it can be also used when fitting a linear model.</p>\\n\\n<p>HJ Motulsky and RE Brown. <a href=\"http://www.biomedcentral.com/1471-2105/7/123/abstract/\">Detecting outliers when fitting data with nonlinear regression – a new method based on robust nonlinear regression and the false discovery rate</a>. BMC Bioinformatics 2006, 7:123</p>\\n',\n",
       " '<p>I simulated my data successfully in R by applying the R codes below.\\nHowever, I need to simulate errors separately for each different Y outcome by using covariance method in R.</p>\\n\\n<p>I don\\'t know how to use covariance to simulate errors in R.</p>\\n\\n<p>Could you help me about this problem?</p>\\n\\n<p>Thanks</p>\\n\\n<pre><code>library(MASS)\\n\\n## var1(Verb.comp), var2(word.Id), var3(word.att) ~ N(0, 1)\\n## r_12 = 0.5, r_23 = 0.6, r_13 = 0.5\\n\\ncovmat &lt;- matrix(c(1, 0.5, 0.5,\\n0.5, 1, 0.7,\\n0.5, 0.7, 1), ncol = 3, byrow = TRUE)\\n\\n## this is what my variance-covariance matrix looks like\\ncovmat\\n\\n## simulate data\\ndf &lt;- mvrnorm(1000, mu = c(0, 0, 0), Sigma = covmat)\\nhead(df)\\n## estimate pm-corelations\\ncor(df)\\nevalA &lt;- matrix(c(1,1,1,0,1,1,0,0,1),3,3)\\nhead(evalA)\\n\\nhead(mydat &lt;- as.matrix(df)%*%evalA)\\ncolnames(mydat) &lt;- c(\"VC\", \"WI\", \"WA\")\\nhead(mydat)\\n\\nerr&lt;-rnorm(1000,0,1)\\nhead(err)\\n\\nY.vc &lt;- 0.8 + 1*mydat[,\\'VC\\'] + 0.54*mydat[,\\'WI\\']+0.51*mydat[,\\'WA\\'] + err\\nY.wi &lt;- 0.8 + 0.54*mydat[,\\'VC\\'] + 1*mydat[,\\'WI\\']+0.75*mydat[,\\'WA\\'] + err\\nY.wa &lt;- 0.8 + 0.51mydat[,\\'VC\\']+ 0.75*mydat[,\\'WI\\']+1*mydat[,\\'WA\\'] + err\\n</code></pre>\\n',\n",
       " '<ol>\\n<li>You don\\'t \"correct\" your covariates; the heterogeneity is in the <em>residuals</em>.  </li>\\n<li>If you\\'re simply running <a href=\"http://en.wikipedia.org/wiki/Levene%27s_test\" rel=\"nofollow\">Levene\\'s test</a> as a factorial ANOVA &amp; finding heterogeneity for <em>some</em> of your explanatory variables, you probably have interactions with those variables that you\\'re currently missing &amp; need to add to your model.  </li>\\n<li>Transformations <em>of the response variable</em> are a good place to start once you\\'ve accounted for possible omitted variables (such as interactions).  </li>\\n</ol>\\n',\n",
       " '<p>I am having difficulty finding information on the assumptions for the intraclass correlation. Can someone please tell me what they are?</p>\\n',\n",
       " '<p>@mnel is correct in that because of the unbalanced design, the order of the terms matter in the output of <code>model.tables</code>.  </p>\\n\\n<p>ADDED: In the help file for <code>aov</code>, we read that it \"is designed for balanced designs, and the results can be hard to interpret without balance.\"  So if you want simple descriptive statistics, better to ask for them directly.</p>\\n\\n<p>Now, it would be better if you had posted a full data set yourself, even if you had to make one an alternate one that showed the same problem.  But you got lucky and a curious reader wanted to know what was going on, so I did that for you.  Here\\'s a sample data set:</p>\\n\\n<pre><code>library(reshape2)\\nset.seed(5)\\nd &lt;- expand.grid(a=factor(LETTERS[1:2]), b=factor(letters[1:2]))\\nd &lt;- d[rep(1:4, c(15,9,11,10)),]\\nd$y &lt;- round(rnorm(nrow(d), mean=10, sd=2),1)\\n</code></pre>\\n\\n<p>And we see that the order of the terms in the model matters (output truncated):</p>\\n\\n<pre><code>&gt; model.tables(aov(y ~ a*b, data=d), \"means\")\\n a      A      B\\n    10.43  9.921\\n b      a      b\\n    9.843  10.64\\n\\n&gt; model.tables(aov(y ~ b*a, data=d), \"means\")\\n b       a      b\\n     9.867  10.61\\n a       A      B\\n     10.46  9.877\\n</code></pre>\\n\\n<p>The first term in the model agrees with the actual mean and the other is different.</p>\\n\\n<pre><code>&gt; tapply(d$y, d$a, mean)\\n        A         B \\n10.426923  9.921053 \\n&gt; tapply(d$y, d$b, mean)\\n        a         b \\n 9.866667 10.609524 \\n</code></pre>\\n\\n<p>Note that I say different, not wrong.  It\\'s telling you something correct about the model.  I\\'m not sure what, actually, but I\\'m curious enough that I may look into the code for <code>model.tables</code> to see what. (Or maybe not, it\\'s getting late.)</p>\\n',\n",
       " '<p>How about a simulation based approach?  Here\\'s some R code to generate 100000 students each trying the 40 tosses.</p>\\n\\n<pre><code>theSum = c()\\nfor (i in 1:100000) {\\n  coin1 = rbinom(40,1,.5)\\n  coin2 = rbinom(40,1,.5)\\n  coin3 = rbinom(40,1,.5)\\n  coin4 = rbinom(40,1,.5)\\n  coin5 = rbinom(40,1,.5)\\n  theSum[i] = sum(coin1+coin2+coin3+coin4+coin5 == 1)\\n}\\n\\nsummary(theSum)\\nhist(theSum, xlim = c(0,40), freq = F, main = \"\", xlab = \"\")\\n</code></pre>\\n\\n<p>The range of times the HTTTT combination occurred (in any order): 0-18 (out of 40), with a mean of around 6.  </p>\\n\\n<p>Below: a histogram of the 100000 attempts and how many times the magical combination occurred.  You\\'d have to be very lucky indeed to get it 39 times out of 40 with fair coins.  But stranger things have happened by chance (e.g., our evolution).</p>\\n\\n<p><img src=\"http://img80.imageshack.us/img80/9268/coinflips.png\" alt=\"alt text\"></p>\\n',\n",
       " '<p>You have several options.  Two of them might be:</p>\\n\\n<ol>\\n<li>If you transform your $Y$ through the $\\\\\\\\log(\\\\\\\\frac{y}{1-y})$ logistic\\ntransformation you could try fitting a linear regression via\\nordinary least squares to that transformed response variable.</li>\\n<li>Alternatively, you could fit the original variable into a\\ngeneralized linear model with the logistic transform as your link\\nvariable and with a relationship between $Y$\\'s variance and mean the\\nsame as though it were a binomial variable, fitting by iterative\\nreweighted least squares.  This is basically the same as \"using\\nlogistic regression\".</li>\\n</ol>\\n\\n<p>Which one to use would depend on the error structure, and the only way to decide is to fit them both and see which one has a residual structure that best fits the model\\'s assumptions.  My suspicion is that there would not be much to choose between them.  Certainly, either of these options would be a big improvement on straight linear regression with the untransformed $Y$, for the reasons you say.</p>\\n',\n",
       " '<p>According, to the help for these two function (use ?AIC and ?extractAIC) this is expected. </p>\\n\\n<p>Note that the AIC is just defined up to an additive constant, because this is also the case for the log-likelihood. This means you should check whether </p>\\n\\n<pre><code>extractAIC(full.modell) - extractAIC(null.modell)\\n</code></pre>\\n\\n<p>and</p>\\n\\n<pre><code>AIC(full.modell) - AIC(null.modell)\\n</code></pre>\\n\\n<p>give the same result. As long as they do, both functions are equivalent for all practical purposes.</p>\\n',\n",
       " '<p>Since you are combining months in the analysis the correlation between months affects how you compute the test statistic and/or confidence interval because you have to scale the mean difference by its standard error.  The variance of sums in this case is not just the sum of the variances but it also includes terms like $2 \\\\\\\\cdot {\\\\\\\\rm cov}(X_i,X_j)$ for $i ≠ j$.  If there is a high degree of positive correlation this variance will be a lot larger than hand the variables been uncorrelated.  If they were highly negatively correlated it could be smaller. So you encounter two problems (1) get a good estimate of the standard error, (2) identify a good approximating distribution for the test statistic under the null hypothesis.</p>\\n',\n",
       " \"<p>I have to compute and compare the least squares method on a model to the least trimmed method.  </p>\\n\\n<p>The LS model results were:</p>\\n\\n<pre><code>Coefficients:\\n               Estimate Std. Error t value Pr(&gt;|t|)    \\n(Intercept)    -39.9197    11.8960  -3.356  0.00375 ** \\nAir.Flow         0.7156     0.1349   5.307  5.8e-05 ***\\nWater.Temp       1.2953     0.3680   3.520  0.00263 ** \\nAcid.Conc.      -0.1521     0.1563  -0.973  0.34405    \\n</code></pre>\\n\\n<p>The LTS results were:  </p>\\n\\n<pre><code>  (Intercept)     Air.Flow      Water.Temp      Acid.Conc. \\n-3.580556e+01  7.500000e-01    3.333333e-01    3.489094e-17 \\n</code></pre>\\n\\n<p>How do I interpret the results for LTS? I know <code>Air.Flow</code> and <code>Water.Temp</code> are significant and <code>ACid.conc</code> is not. But don't know about LTS coefficients since there are no standard errors.</p>\\n\",\n",
       " '<p>There is no true measurement of clustering quality.</p>\\n\\n<p>The problem is that by the very nature of the clustering task, you use clustering to find something <em>new</em> on your data. You do <em>not</em> use it to minimize a mathematical measure such as variance (which btw. heavily depends on the data set and normalization, and is sensitive to e.g. the number of clusters).</p>\\n\\n<p>I see the strong risk in your approach that you <strong>might not measure whether it is a true clustering or not, but whether it is a variance-optimized clustering</strong>. So you might be able to say whether this was generated by k-means or not, but you might consider a non-k-means-clustering to be invalid.</p>\\n\\n<p>You <em>really</em> need to be careful to <em>not overfit</em> your method to particular clustering concepts and algorithms (and in particular, not to k-means).</p>\\n',\n",
       " '<p>I think it depends on the situation.  If you don\\'t expect any particular problems you can probably check these in any order.  If you expect outliers and might have a reason to remove them after detecting them then check for outliers first.  The other issues with the model could change after observations are removed.  After that the order between multicollinaerity and heteroscedasticity doesn\\'t matter. I agree with Chris that outliers should not be removed arbitrarily.  You need to have a reason to think the observations are wrong.</p>\\n\\n<p>Of course if you observe multicollinearity or heteroscedasticity you may need to change your approach.  The multicollinearity problem is observed in the covariance matrix but there are specific diagnostic tests for detecting multicollinearity and other problems like leverage points look at the <em>Regression Diagnostics</em> book by <a href=\"http://rads.stackoverflow.com/amzn/click/0471691178\" rel=\"nofollow\">Belsley, Kuh and Welsch</a> or one of <a href=\"http://rads.stackoverflow.com/amzn/click/047131711X\" rel=\"nofollow\">Dennis Cook\\'s regression books</a>.</p>\\n',\n",
       " '<p>Just set a threshold like 30% and if the number of \"zeroes\" exceeds this threshold then declare it to be an intermittent demand series. For guidelines to deal with \"unusual demands\" rather than believing them and Level Shifts ( n.b. A level Shift is not a time trend  ) . Also since intermittent demand can yield rates that are auto-regressive ( i.e autocorrelated ) models like the Poisson Model or the Croston approach are of limited value. Please see the discussion Please see my comments in <a href=\"http://stats.stackexchange.com/questions/23860/how-to-forecast-based-on-aggregated-data-over-irregular-intervals/23904#23904\">How to forecast based on aggregated data over irregular intervals?</a> regarding this.</p>\\n',\n",
       " \"<p>This is probably really simple but I can't figure it out... was trying for few days...</p>\\n\\n<p>If <em>all the coefficients of a positive-definite covariance matrix are positive,</em> how can I prove that the coefficients of the first principal component are all of the same sign and the coefficients of all other principal components cannot all be of the same sign?</p>\\n\\n<p>I have tried to prove from the definition but can't get the idea... can any one help?</p>\\n\",\n",
       " '<p>People tend to use DHOG (Dense Histograms of Orientations of Gradient), which is closely related to SIFT and other image description method for this purpose. There is another commonly used image description method called SURF.</p>\\n\\n<p>You can take a look at <a href=\"http://www.vlfeat.org\" rel=\"nofollow\">VLFeat</a>.</p>\\n',\n",
       " \"<p>This is a pretty general problem in time series analysis. I'd probably start by looking at some descriptive statistics like the cross-correlation to see if the samples are roughly independent over time. You could also test whether the correlation between successive samples is significant.</p>\\n\\n<p>Or you could go the model-fitting route in which case one simple thing to do is to fit an auto-regressive model with some order k and then do model comparison versus the static model. If you assume that $\\\\\\\\theta$ just follows a Gaussian random walk, then model you're describing is exactly a Kalman filter. So that might be another thing to look at.</p>\\n\",\n",
       " '<p>The classic \"orange horror\" remains an excellent introduction: <em>Exploratory Data Analysis</em> by John Tukey. </p>\\n\\n<p><a href=\"http://rads.stackoverflow.com/amzn/click/0201076160\" rel=\"nofollow\">http://www.amazon.com/Exploratory-Data-Analysis-John-Tukey/dp/0201076160</a></p>\\n',\n",
       " '<p>I have received these errors when not using appropriate initial values for my parameters.  Rather than writing functions to generate initial values (which I presume you have done), I would provide starting values that you know are consistent with one another. </p>\\n',\n",
       " '<p>For i.i.d. random variables $X_1, \\\\\\\\dotsc, X_n$, the unbiased estimator for the variance $s^2$ (the one with denominator $n-1$) has variance:</p>\\n\\n<p>$$\\\\\\\\mathrm{Var}(s^2) = \\\\\\\\sigma^4 \\\\\\\\left(\\\\\\\\frac{2}{n-1} + \\\\\\\\frac{\\\\\\\\kappa}{n}\\\\\\\\right)$$ </p>\\n\\n<p>where $\\\\\\\\kappa$ is the excess kurtosis of the distribution (reference: <a href=\"http://en.wikipedia.org/wiki/Sample_variance#Distribution_of_the_sample_variance\" rel=\"nofollow\">Wikipedia</a>). So now you need to estimate the kurtosis of your distribution as well. You can use a quantity sometimes described as $\\\\\\\\gamma_2$ (also from <a href=\"http://en.wikipedia.org/wiki/Sample_variance#Distribution_of_the_sample_variance\" rel=\"nofollow\">Wikipedia</a>):</p>\\n\\n<p>$$\\\\\\\\gamma_2 = \\\\\\\\frac{\\\\\\\\mu_4}{\\\\\\\\sigma_4} - 3$$</p>\\n\\n<p>I would assume that if you use $s$ as an estimate for $\\\\\\\\sigma$ and $\\\\\\\\gamma_2$ as an estimate for $\\\\\\\\kappa$, that you get a reasonable estimate for $\\\\\\\\mathrm{Var}(s^2)$, although I don\\'t see a guarantee that it is unbiased. See if it matches with the variance among the subsets of your 500 data points reasonably, and if it does don\\'t worry about it anymore :)</p>\\n',\n",
       " '<p>I have a raw discrete data(curves). I need to find methods for detecting features of each curve. Some example features:</p>\\n\\n<p>1) Stable growing<br>\\n2) Fast growing<br>\\n3) Stable falling<br>\\n4) Fast falling<br>\\n5) And so on</p>\\n\\n<p>Here is good example of such curves type(russian):\\n<img src=\"http://i.stack.imgur.com/OWefy.png\" alt=\"enter image description here\"></p>\\n\\n<p>How can i do it?</p>\\n',\n",
       " '<ol>\\n<li><p>I agree with the point that statistics consultants are often brought in later on a project when it\\'s too late to remedy design flaws. It\\'s also true that many statistics books give scant attention to study design issues.</p></li>\\n<li><p>You say you want designs \"preferably for a wide range of methods (e.g. t-test, GLM, GAM, ordination techniques...\". I see designs as relatively independent of statistical method: e.g., experiments (between subjects and within subjects factors) versus observational studies; longitudinal versus cross-sectional; etc. There are also a lot of issues related to measurement, domain specific theoretical knowledge, and domain specific study design principles that need to be understood in order to design a good study.</p></li>\\n<li><p>In terms of books, I\\'d be inclined to look at domain specific books. In psychology (where I\\'m from) this means books on psychometrics for measurement, a book on research methods, and a book on statistics, as well as a range of even more domain specific research method books. You might want to check out <a href=\"http://www.socialresearchmethods.net/kb/index.php\" rel=\"nofollow\">Research Methods Knowledge Base</a> for a free online resource for the social sciences.</p></li>\\n<li><p>Published journal articles are also a good guide to what is best practice in a particular domain. </p></li>\\n</ol>\\n',\n",
       " \"<p>I have some problems in understanding the results of two linear regressions on the same data.</p>\\n\\n<p>I have 2 variables and I want to know if there is a linear correlation between the two. Let's say velocity of a car and perceived velocity of the conductor.\\nLet's say I have 4 velocities and 2 evaluations of the perceived velocities performed in 2 trials by 15 participants, on a 7 points Likert scale where 1 is very slow and 7 = very fast.</p>\\n\\n<p>Now if I take the AVERAGE values of the evaluations given by the 15 participants for the 4 velocities I get a clear linear relationship with $R^2$ = 0.7.</p>\\n\\n<p>However if I perform the analysis on the raw data (i.e. 15 participants * 2 trials = 30 values for each velocity) I get a $R^2$ = 0.2. Plotting the graph of the velocity data vs perceived velocity I get a mess of points very widely distributed.</p>\\n\\n<p>Why do I get this behavior?\\nWhich of the two analyses should I use? The first or the second?</p>\\n\\n<p>How can I interpret my values?</p>\\n\\n<p>Please let me know</p>\\n\\n<p>Thanks in advance</p>\\n\",\n",
       " '<p>This is another way to get the formulas </p>\\n\\n<pre><code>comb_list_forms &lt;- unlist(lapply(1:length(comb_list), function(i)\\n  lapply(1:dim(comb_list[[i]])[2], function(x) \\n    as.formula(paste(\"Y ~ \", paste(names(df[1+comb_list[[i]][,x]]), collapse= \"+\"))))))\\n</code></pre>\\n',\n",
       " '<p>Stata provides the -gnbreg- command, which allows you to model the dispersion parameter. You can view Stata help for the command at <a href=\"http://www.stata.com/help.cgi?nbreg\">http://www.stata.com/help.cgi?nbreg</a></p>\\n\\n<p>Stata calls this the generalised negative binomial model. Joseph Hilbe discusses it in his book \"Negative Binomial Regression\", section 10.4, as \"NB-H: Heterogeneous negative binomial regression\".</p>\\n',\n",
       " '<p>I have a Bayesian model here in R/WinBUGS. This data \"pumps\" has two columns, time and fail. Each observation is a pump and fail[1] tells how many failures there were in time[1]. I want to know how I can write this same model in SAS using Proc MCMC. Thank you!</p>\\n\\n<pre><code>pump &lt;- function() {\\n    for(i in 1:10){\\n        fail[i] ~ dpois(lam[i])\\n        theta[i] ~ dgamma(a,b)\\n        lam[i] &lt;- theta[i]*time[i]\\n    }\\n    a ~ dgamma(1.5,.25) \\n    b ~ dgamma(1.5,.25)\\n}\\n\\nfilename &lt;- file.path(tempdir(),\\'pump.bug\\') \\nwrite.model(pump,filename)\\ntime &lt;- pumps[,1]\\nfail &lt;- pumps[,2]\\n\\ndata &lt;- c(\\'time\\',\\'fail\\')\\nparameters &lt;- c(\\'fail\\',\\'lam\\',\\'theta\\',\\'a\\',\\'b\\')\\npump.sim &lt;- bugs(data, inits=NULL, parameters, model.file=\\'pump.bug\\',\\n    n.iter=20000, n.burnin=1000, n.chains=1, n.thin=1, debug=T) \\n</code></pre>\\n',\n",
       " '<p>Lets say I have the following data frame</p>\\n\\n<pre><code>library(survival)\\nlibrary(multcomp)\\ndata(cml)\\ncml$group&lt;-sample(1:5, 507, replace=T)\\n    plot(survfit(Surv(time=cml$time, cml$status)~factor(cml$group)))\\n(survdiff(Surv(time=cml$time, cml$status)~factor(cml$group)))\\n</code></pre>\\n\\n<p>I want to perform multiple comparison test  comparing (logrank) for example group0 vs. all other groups or even every group with each other? Is it necessary to correct for multiple comparisons? If yes, is there a nice way of plotting these multiple comparisons (as for example in <code>plot.TukeyHSD()</code> in <code>aov()</code>)?</p>\\n\\n<p>I have posted the same question in <a href=\"http://stackoverflow.com/questions/11176762/kaplan-meier-multiple-group-comparisons\">http://stackoverflow.com/questions/11176762/kaplan-meier-multiple-group-comparisons</a> but the answer presents multiple comparison test with parametric survival not with non parametric (as in my case)</p>\\n',\n",
       " '<p>I\\'m running an EFA using orthogonal/varimax rotation, and assigning variables to a factors based on maximum load (so only each variable only gets one factor). I then want to validate the model using SEM... since the rotation I used to determine the variable&lt;->factor loads was orthogonal, is it \"wrong\" to let the factors in my model have a covariance with one another? (eg, using RAM: Factor1&lt;->Factor2,theta,NA)</p>\\n\\n<p>I ask, as I get a much better model fit if I allow for this to occur.</p>\\n\\n<p>More explicitly, what does it actual mean for underlying factors to have a correlation between them?</p>\\n\\n<p>Thanks!</p>\\n',\n",
       " \"<p>k-means cannot use arbitrary distance functions. It is designed for Euclidean distance.</p>\\n\\n<p>Euclidean distance however does not work well for high-dimensional data such as your time series (unless you have a really low sampling rate, say 24 months)</p>\\n\\n<p>For time series, you will probably want to use a <em>time series distance</em>. There are quire a lot designed specifically for different kinds of time series. You really should look at these.</p>\\n\\n<p>They won't work with k-means, but there are various distance and density-based cluster algorithms (where usually density is defined by distance!) that you should try. However, I have no idea what SPSS supports. I don't know if it has any time series distances, either.</p>\\n\",\n",
       " '<p>I still can\\'t figure out the advantage of median polish over a regular median for the summarisation of probe to make probesets in microarray analysis.</p>\\n\\n<p>As I understand from here the idea is to use the median of the different arrays to summarise the data along with the median of the different probes: <a href=\"http://stats.stackexchange.com/questions/8251/the-use-of-median-polish-for-feature-selection\">The use of median polish for feature selection</a></p>\\n\\n<p>I\\'ve been making some simulations to try to see what might happen.</p>\\n\\n<p>If we imagine the following is a simulation microarray dataset, where the columns are the probes we wish to summarise, and the rows are the different microarray chips:</p>\\n\\n<pre><code>library(preprocessCore)\\n&gt;  y &lt;- matrix(10+rnorm(100),20,5)\\n&gt; y\\n           [,1]      [,2]      [,3]      [,4]      [,5]\\n [1,]  9.334358  9.993648  8.551274 10.109988 10.243317\\n [2,] 11.448786  8.908376 11.536720 10.679236 10.831209\\n [3,] 11.979813  8.726539  9.740086 10.103683  9.349783\\n [4,] 10.552108 12.772855 10.484486  9.362849  9.426693\\n [5,] 10.056581  9.890734  9.907472 10.283063  9.909602\\n [6,] 12.187766 10.290644  8.770036 11.241425 12.856710\\n [7,] 11.071675  9.932000 11.761954 10.806470 11.013961\\n [8,]  9.560737  9.234133 11.307681  8.672639  9.570637\\n [9,]  8.952978  8.549438  9.962865  8.527808  8.421271\\n[10,]  8.853584 10.117102 10.040929  9.551693  9.880730\\n[11,]  8.794862 11.276158  7.579099  9.167762  8.863161\\n[12,] 10.923659  9.873338  9.081718  9.501927 11.956930\\n[13,] 10.150289  8.472951  9.367948  9.376648  9.963847\\n[14,] 10.271810 10.738851 11.253192  8.169423 10.286973\\n[15,] 10.424398 10.356835 10.004031 10.790331  9.922300\\n[16,] 10.494939  7.793422 10.182820  9.597307  8.726760\\n[17,] 11.447273 10.366120 11.620400  9.698011 10.706059\\n[18,]  9.597951 11.161659  9.923795  8.462029 10.945888\\n[19,]  8.796656 10.962160 10.204844  7.944251  9.332819\\n[20,]  9.462135 10.621933  9.697430 11.112817  9.905340\\n</code></pre>\\n\\n<p>So we can summarise the columns ussing the median (which treats each probeset (that is, column) separately, and also using median polish (which takes into account the columns and the rows, more detail in <a href=\"http://www.stats.ox.ac.uk/pub/MASS4/VR4stat.pdf\">http://www.stats.ox.ac.uk/pub/MASS4/VR4stat.pdf</a></p>\\n\\n<pre><code>&gt; colSummarizeMedian(y)\\n$Estimates\\n[1] 10.211050 10.055375  9.983448  9.574500  9.915951\\n\\n$StdErrors\\n[1] NA NA NA NA NA\\n\\n&gt; colSummarizeMedianpolish(y)\\n$Estimates\\n[1] 9.975078 9.898037 9.907471 9.788084 9.907471\\n\\n$StdErrors\\n[1] NA NA NA NA NA\\n</code></pre>\\n\\n<p>Then I add an \"outlier\" by changing one of the arrays and recalculate. As we would expect the median stays the same, but the median polish doesn\\'t.</p>\\n\\n<p>However, I don\\'t see why the median polish is a better answer here than the median answer? I don\\'t see why we are borrowing information from the other columns, and what we gain from this. It seems especially true since most of the time some of the columns will be chips from differential treatments.</p>\\n\\n<p>Perhaps I have missed something rather fundamental here, but any help would be much appreciated, I am at somewhat of a mental block.</p>\\n\\n<p>Also, if people think this is better suited to a more bioinformatics q+a site, please let me know!</p>\\n\\n<pre><code>&gt; y[20,] &lt;- y[20,] + rnorm(5, sd=5)\\n&gt; colSummarizeMedian(y)\\n$Estimates\\n[1] 10.348104  9.990152  9.983448  9.526810  9.915951\\n\\n$StdErrors\\n[1] NA NA NA NA NA\\n\\n&gt; colSummarizeMedianpolish(y)\\n$Estimates\\n[1] 10.030423  9.879213  9.933043  9.713632  9.893364\\n\\n$StdErrors\\n[1] NA NA NA NA NA\\n</code></pre>\\n',\n",
       " '<p>When I was teaching graduate level statistics, I was telling my students: \"I don\\'t care what package you use, and you can use anything for your homework, as I expect you to provide substantive explanations, and will take points off if I see <code>tr23y5m</code> variable names in your submissions. I can support your learning very well in Stata, and reasonably well, in R. With SAS, you are on your own, as you claim you have taken a course in it. With SPSS or Minitab, God bless you\". I imagine that the reasonable employers would think the same. What matters is your productivity in terms of the project outcomes. If you can achieve the goal in R with 40 hours of work, fine; if you can achieve it in C++ in 40 hours of work, fine; if you know how to do this in R in 40 hours, but your supervisor wants you to do this in SAS, and you have to spend 60 hours just to learn some basics and where the semicolons go, that can only be wise in the context of the large picture of the rest of the code being in SAS... and then the manager was not very wise in having hired an R programmer.</p>\\n\\n<p>From this perspective of the total cost, \"free\" R is a hugely overblown myth. Any serious project requires custom code, if just for the data input and formatting the output, and that\\'s a non-zero cost of professional time. If this data input and formatting requires 10 hours of SAS code and 20 hours of R code, R is a more expensive software <em>at the margin</em>, as an economist would say, i.e., in terms of the additional cost to produce a given piece of functionality. If a big project requires 200 hours of R programmer\\'s time and 100 hours of Stata programmer\\'s time to provide identical functionality, Stata is cheaper <em>overall</em>, even accounting for the ~$1K license that you need to buy. It would be interesting to see such direct comparisons; I was involved in re-writing a huge mess of 2Mb of SPSS code that was said to have been accumulated over about 10 person-years into ~150K of Stata code that ran about as fast, may be a tad faster; that was about 1 person-year project. I don\\'t know if this 10:1 efficiency ratio is typical for SPSS:Stata comparisons, but I won\\'t be surprised if it were. For me, working with R is always a large expense because of the search costs: I have to determine which of the five packages with similar names does what I need to do, and gauge whether it does it reliably enough for me to use it in my work. It often means that it is cheaper for me to write my own Stata code in less time that I would be spending figuring out how to make R work in a given task. It should be understood that this is my personal idiosyncrasy; most people on this site are better useRs than I am.</p>\\n\\n<p>Funny that your prof would prefer Stata or GAUSS over R because \"R was not written by economists\". Neither were Stata or GAUSS; they are written by computer scientists using computer scientists\\' tools. If your prof gets ideas about programming from CodeAcademy.com, that\\'s better than nothing, but professional grade software development is as different from typing in CodeAcademy.com text box as driving a freight truck is different from biking. (Stata was started by a labor econometrician converted computer scientist though, but he has not been doing this labor econometrics thing for about 25 years by now.)</p>\\n\\n<p><strong>Update</strong>: As AndyW commented below, you can write terrible code in any language. The question of cost then becomes, which language is easier to debug. To me this looks like a combination of how accurate and informative the output is, and how easy and transparent the syntax itself is, and I don\\'t have a good answer for that, of course. For instance, Python enforces code indenting, which is a good idea. Stata and R code can be folded over the brackets, and that\\'s not going to work with SAS. Use of subroutines is a two-edged sword: the use of <code>*apply()</code> with ad-hoc <code>function</code>s in R is obviously very efficient, but harder to debug. By a similar token, Stata <code>local</code>s can mask nearly anything, and defaulting to an empty string, while useful, may also lead to difficult-to-catch errors.</p>\\n',\n",
       " '<p>Here is an example that can be pasted to an IPython prompt and generate an image like below (it uses random data):</p>\\n\\n<pre><code>import numpy as np\\nimport matplotlib\\nimport matplotlib.pyplot as plt\\n\\n#Make a random array and then make it positive-definite\\nnum_vars = 6\\nnum_obs = 9\\nA = np.random.randn(num_obs, num_vars)\\nA = np.asmatrix(A.T) * np.asmatrix(A)\\nU, S, V = np.linalg.svd(A) \\neigvals = S**2 / np.cumsum(S)[-1]\\n\\nfig = plt.figure(figsize=(8,5))\\nsing_vals = np.arange(num_vars) + 1\\nplt.plot(sing_vals, eigvals, \\'ro-\\', linewidth=2)\\nplt.title(\\'Scree Plot\\')\\nplt.xlabel(\\'Principal Component\\')\\nplt.ylabel(\\'Eigenvalue\\')\\n#I don\\'t like the default legend so I typically make mine like below, e.g.\\n#with smaller fonts and a bit transparent so I do not cover up data, and make\\n#it moveable by the viewer in case upper-right is a bad place for it \\nleg = plt.legend([\\'Eigenvalues from SVD\\'], loc=\\'best\\', borderpad=0.3, \\n                 shadow=False, prop=matplotlib.font_manager.FontProperties(size=\\'small\\'),\\n                 markerscale=0.4)\\nleg.get_frame().set_alpha(0.4)\\nleg.draggable(state=True)\\nplt.show()\\n</code></pre>\\n\\n<p><img src=\"http://img38.imageshack.us/img38/7573/svdscreeplotexample.png\" alt=\"scree plot\"></p>\\n',\n",
       " \"<p>Suppose the target population is a group of 11 company CEOs and 7 CEOs responded to a question about their level of annual bonus on a 7 point Likert scale. </p>\\n\\n<p>The responses are as follows: 7, 7, 7, 7, 7, 7, and 5. The average response score then is 6.71.</p>\\n\\n<p>If the last two levels of the Likert scale are '6 = satisfied' and '7 = very satisfied', this finding can be expressed as a 'very satisfied' rating.</p>\\n\\n<p>Do we apply a margin of error to Likert scale responses?</p>\\n\\n<p>(The margin of error for a sample of 7 is 38%. This is 1 divided by the square root of the sample - a rough guide!) </p>\\n\",\n",
       " '<p>Something I see a surprising amount in conference papers and even journals is making multiple comparisons (e.g. of bivariate correlations) and then reporting all the p&lt;.05s as \"significant\" (ignoring the rightness or wrongness of that for the moment).</p>\\n\\n<p>I know what you mean about psychology graduates, as well- I\\'ve finished a PhD in psychology and I\\'m still only just learning really. It\\'s quite bad, I think psychology needs to take quantitative data analysis more seriously if we\\'re going to use it (which, clearly, we should)</p>\\n',\n",
       " \"<p>I have data from an experiment that is testing how the order of two studying methods (visual or auditory) affects word recall. For analysis a multi-factor anova with a repeated measure is appropriate, but I am not sure if I am structuring my data correctly. </p>\\n\\n<p>This is the command I'm using:\\n<code>aov(recalled_items~task*order)+Error(subject/task)+(order))</code></p>\\n\\n<p>Here is an example of the data structure:</p>\\n\\n<pre><code>Subject    Task    Order    Recalled Items\\nA        Visual    First       13\\nA       Auditory   Second      22\\nB        Visual    First       14\\nB       Auditory   Second      28\\nC        Visual    Second      10\\nC       Auditory   First       15\\nD        Visual    Second      14\\nD       Auditory   First       29\\n</code></pre>\\n\\n<ul>\\n<li>Does R know to compare Visual 1 and Visual 2 recall values and Auditory 1 and Auditory 2 recall values? </li>\\n</ul>\\n\\n<p>I am worried that because of the way I structured my data R is just comparing Visual 1 and Auditory 1 and as a result I am getting no effect.</p>\\n\",\n",
       " '<p>For two continuous variables then you can do what you want (whether this is an interaction or not I\\'ll leave others to discuss as per comments to @Greg\\'s Answer) using:</p>\\n\\n<pre><code>mod1 &lt;- gam(Temp ~ Loc + s(Doy, bs = \"cc\", k = 5) + \\n                         s(Doy, bs = \"cc\", by = Loc, k = 5) + \\n                         s(Tod, bs = \"cc\", k = 5) + \\n                         s(Tod, bs = \"cc\", by = Loc, k = 5) +\\n                         te(ToD, Day, by = Loc, bs = rep(\"cc\",2)),\\n            data = DatNew, method = \"ML\")\\n</code></pre>\\n\\n<p>The simpler model should then be nested within the more complex model above. That simpler model is:</p>\\n\\n<pre><code>mod0 &lt;- gam(Temp ~ Loc + s(Doy, bs = \"cc\", k = 5) + \\n                         s(Doy, bs = \"cc\", by = Loc, k = 5) + \\n                         s(Tod, bs = \"cc\", k = 5) + \\n                         s(Tod, bs = \"cc\", by = Loc, k = 5),\\n            data = DatNew, method = \"ML\")\\n</code></pre>\\n\\n<p>Note two things here:</p>\\n\\n<ol>\\n<li>The basis-type for each smoother is stated. In this case we would expect that there are no discontinuities in Temp between 23:59 and 00:00 for <code>ToD</code> nor between <code>DoY == 1</code> and <code>DoY == 365.25</code>. Hence cyclic cubic splines are appropriate, indicated here via <code>bs = \"cc\"</code>.</li>\\n<li>The basis dimension is stated explicitly (<code>k = 5</code>). This matches the default basis dimension for each smooth in a <code>te()</code> term.</li>\\n</ol>\\n\\n<p>Together these features ensure that the simpler model really is nested within the more complex model.</p>\\n\\n<p>For more see <code>?gam.models</code> in <strong>mgcv</strong>.</p>\\n',\n",
       " '<p>To resume I think your question is whether you can reject the null hypothesis that the B group has a significantly higher average game score than the control group A.</p>\\n\\n<p>For this you would have to first calculate a <a href=\"http://en.wikipedia.org/wiki/Test_statistic\" rel=\"nofollow\">test statistic</a> for your experiment, then test for significance. The test statistic can be defined as $$\\\\\\\\frac{\\\\\\\\hat{Y} - \\\\\\\\bar{Y}}{SE}$$ where $\\\\\\\\hat{Y}$ is the observed group B average score, $\\\\\\\\bar{Y}$ is the group A expected average score and $SE$ is the standard error of the group B scores, or $\\\\\\\\sqrt{N} * SD$.</p>\\n\\n<p>Now to test for significance when the number of observations is small we want to use the <a href=\"http://en.wikipedia.org/wiki/Student%27s_t-distribution\" rel=\"nofollow\">Student\\'s curve</a>, rather than the normal curve. It complicates the P-value calculation a bit, because you will have to choose the curve that match your group degrees of freedom (df = observations - 1), but each one of these specific curves give a better probability distribution for reduced sample sizes.\\nThis final number will give you the probability that we can\\'t reject the null hypothesis. Standard practices are that if the p-value is &lt; 5% we can reject the null hypothesis but in practice I would be less strict, especially for small sample sizes. I would say a p-value &lt; 10% is worth investigating.</p>\\n',\n",
       " '<p>I need to calculate matrix inverse and have been using <code>solve</code> function. While it works well on small matrices, <code>solve</code> tends to be very slow on large matrices. I was wondering if there is any other function or combination of functions (through SVD, QR, LU, or other decomposition functions) that can give me faster results.</p>\\n',\n",
       " \"<p>Now I have two data set which is generated by two listing flow of a e-commerce site. And these data set has different conversion rate, my task is to analysis the two data set and found out which factors lead the different conversion rate. BTW, most of the data is categorical data. Could you help point me some tutorial or method that I can refer to do the analysis ? Currently what I did is just according my intuition, I'd like to find out to some systematic method for factory analysis of categorical data.</p>\\n\",\n",
       " '<p><strong>Regarding Question 1</strong></p>\\n\\n<p>You are currently doing a lot of extra stuff that is not needed. What you are trying to calculate is the standardized mean change, where you are standardizing by the raw score standard deviation. This is usually calculate with:</p>\\n\\n<p>$d = (\\\\\\\\bar{Y}_1 - \\\\\\\\bar{Y}_2) / SD_1$,</p>\\n\\n<p>where $SD_1$ is the standard deviation of the pretreatment scores (one could instead use $SD_2$). Since you mentioned that you have the pre and post-treatment means and SDs, you should be able to calculate this easily.</p>\\n\\n<p>What is more tricky is calculating the sampling variance for d. The usual equation to compute (or rather: estimate) the sampling variance is:</p>\\n\\n<p>$v = 2(1-r)/n + d^2/(2n)$,</p>\\n\\n<p>where $r$ is the pre-post correlation, and $n$ is the sample size of the group. The tricky thing is that $r$ is typically not reported. In that case, you could try to \"guestimate\" $r$ and then do a sensitivity analysis (i.e., you check whether the conclusions of your meta-analysis depend on which value(s) you plug in for $r$).</p>\\n\\n<p><strong>Regarding Question 2</strong></p>\\n\\n<p>If the scales differ between the pre- and the post-test, then the standardized mean change (or even just the raw mean change) is not very meaningful. In that case, I don\\'t think that you can really estimate the change -- since that would require that the same thing has been measured before and after the treatment. So, in that case, you could do separate meta-analyses of the pre-treatment means and of the post-treatment means. I am assuming here that the units of the pre-treatment means are the same across studies (i.e., the same measurement instrument has been used across all studies for the pre-treatment assessment) and the same thing for the post-treatment means (i.e., the same measurement instrument has been used across all studies for the post-treatment assessment).</p>\\n',\n",
       " '<p>I have a good feel for the types of data that go with most colormaps. But I don\\'t know what kind of data one would want to use the following types of colormaps:</p>\\n\\n<p>Flag:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/T3uMo.jpg\" alt=\"Flag in &lt;code&gt;matplotlib&lt;/code&gt;\"></p>\\n\\n<p>Prism:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/yZ0zA.jpg\" alt=\"Prism in &lt;code&gt;matplotlib&lt;/code&gt;\"></p>\\n\\n<p>Source: <a href=\"http://dept.astro.lsa.umich.edu/~msshin/science/code/matplotlib_cm/\" rel=\"nofollow\">http://dept.astro.lsa.umich.edu/~msshin/science/code/matplotlib_cm/</a></p>\\n\\n<p>I can\\'t see any use for them except as a fill pattern. </p>\\n\\n<p>What kinds of data benefit from these types of colormaps? And does this type of map have a specific name?</p>\\n',\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " 'Testing marginal homogeniety with multivariate outcomes and matched pairs',\n",
       " 'Interpretation lin-log regression where the covariate is log(x1 + 1) transformed',\n",
       " '3 groups of 8 with a different treatment each, over 45 minutes - confused as to stats test needed',\n",
       " 'A strange coefficient in a negative binomial model',\n",
       " 'How to test if all the levels have the same success/fail probability?',\n",
       " 'Applications of principal component analysis versus principal component regression?',\n",
       " 'What sense does it make to compare p-values to each other?',\n",
       " 'How can I remove the z-order bias of a coloured scatter plot?',\n",
       " '$\\\\\\\\eta^2$ and paired t-tests',\n",
       " 'Smoothing function for displaying stacked lines without the smoothing introducing crossings',\n",
       " 'How can I optimise computational efficiency when fitting a complex model to a large data set repeatedly?',\n",
       " 'Experiment design: Can unbalanced dataset be better than balanced?',\n",
       " 'How to change data between wide and long formats in R?',\n",
       " 'Likert scale questionnaire and logistic regression',\n",
       " 'Given Two 95% Confidence Intervals',\n",
       " 'How to assess prevalence of grossly inaccurate data in recorded body weights in medical records?',\n",
       " 'How to report two-way anova results?',\n",
       " 'Optimization of models (ANN, radial basis, etc.) in R to target predictor levels to produce a desired response',\n",
       " 'Adjusted $R^2$ versus $R^2$ in multiple regression',\n",
       " 'Non-significant group effect, but LSD post hoc is significant. Why?',\n",
       " 'What test should I use to determine if a policy change had a statistically significant impact on website registrations?',\n",
       " 'Why do lme and aov return different results for repeated measures ANOVA in R?',\n",
       " 'How to standardize a data-set',\n",
       " 'Two-Parameter Exponential Family Conjugate Prior',\n",
       " 'What is this method for seasonal adjustment calculation?',\n",
       " 'When submitting analyses to FDA how does using R affect issues of software validation?',\n",
       " \"Significance test on the difference of Spearman's correlation coefficient\",\n",
       " 'Multinomial logistic regression?',\n",
       " 'Conflicting results with GoF tests',\n",
       " 'Definition of quantile',\n",
       " 'decision tree using user defined split function in rpart: No splits returned when tree is run',\n",
       " 'Python as a statistics workbench',\n",
       " 'Forecast with arima model',\n",
       " 'How to generate ROC Plot for semi-supervised algorithm?',\n",
       " 'Inter-feature ratio explicitly or implicitly?',\n",
       " 'What are the assumptions of the Games-Howell multiple comparisons procedure?',\n",
       " 'Doing correlation on one variable vs many',\n",
       " 'How to test homogeneity of variance if the data may not be normally distributed?',\n",
       " 'How to obtain covariance matrix for constrained regression fit?',\n",
       " 'How to compare accuracy measures in method comparison studies?',\n",
       " 'Likelihood ratio test',\n",
       " 'MCMC method -- good sources',\n",
       " 'Boxplot for several distributions?',\n",
       " 'How does \"ward\" clustering (in R\\'s hclust function) work?',\n",
       " 'Tutorial for using R to do multivariate regression?',\n",
       " 'How to use residuals to train a model?',\n",
       " 'How do I model a sine/cosine on a cycle derived from a Baxter filter?',\n",
       " 'Assuming two Gaussian distributions of equal mean and variance, then how different can we expect the top X members of each group to be?',\n",
       " 'What is the difference between \"margin of error\" and \"standard error\"?',\n",
       " 'How to convert this equation into an equation where I can use regression to estimate coefficients?',\n",
       " 'Confidence intervals in a mixed model',\n",
       " 'Distribution of ratio $X/Y$ where $X$ is normal, $Y$ is half normal',\n",
       " 'randomForest and variable importance bug?',\n",
       " 'Incremental learning methods in R',\n",
       " 'Error exponent in hypothesis testing',\n",
       " 'Correct enunciation of $ \\\\\\\\, \\\\\\\\sum \\\\\\\\big( {Y_{i}}-{ \\\\\\\\hat{Y}_{i}} \\\\\\\\big)^2$',\n",
       " 'Explain data visualization',\n",
       " 'Use of kernel density estimate in Naive Bayes Classifier?',\n",
       " 'How to choose the 1st threshold/classifier/ weak learner in Adaboost?',\n",
       " 'Goodness of fit for a regression with multiple predictors',\n",
       " 'Perform simple regression without raw data',\n",
       " 'Resampling within a survey to account for missing data',\n",
       " 'Other substitution matrices for missing value state in sequence analysis with TraMineR?',\n",
       " 'Preferred methods for graphing time-series data to present \"averages\"?',\n",
       " 'Creating a \"certainty score\" from the votes in random forests?',\n",
       " 'Measuring statistical significance of machine learning algorithms comparison',\n",
       " 'Steady state probabilities for a continuous-time Markov chain',\n",
       " 'Sample size vs. number of samples',\n",
       " 'What are the rules for picking a distribution for modelling a data set?',\n",
       " 'How to create a bivariate distribution from copula and marginals?',\n",
       " 'Is the Binomial Effect Size Display (BESD) usually an unsound, misleading technique?',\n",
       " 'Bayesian updating - which distribution to use',\n",
       " '\"Paired data\" / \"repeated measures\" in 2x2 experiment',\n",
       " 'Predicting long-memory processes',\n",
       " 'Independent t-tests and Technical Indicators: Voodoo, Axes, and Objectivity',\n",
       " 'Determining sample size',\n",
       " 'What is a meaning of \"p-value F\" from Friedman test?',\n",
       " 'Does random effects allow me to do repeated measures?',\n",
       " 'Why is expectation the same as the arithmetic mean?',\n",
       " 'Fitting special variance structure in mixed model',\n",
       " 'Multiple hypothesis testing correction with Benjamini-Hochberg, p-values or q-values?',\n",
       " 'Can a probability distribution have infinite standard deviation?',\n",
       " 'I want to do an nested ANOVA but my variances are very unequal',\n",
       " 'Who invented profile maximum likelihood estimation?',\n",
       " 'Estimating a survival probability in R',\n",
       " 'ARIMA identification process',\n",
       " '(Mahalanobis) distance to a multivariate distribution of which I have few sample points',\n",
       " 'Difference between histogram and pdf?',\n",
       " 'How to deal with information of variable length?',\n",
       " 'Outliers spotting in time series analysis, should I pre-process data or not?',\n",
       " 'Generalizing Add-one/Laplacian Smoothing',\n",
       " 'How can one extract meaningful factors from a sparse matrix?',\n",
       " 'Probability of an order statistic',\n",
       " 'Using control charts for data with non-constant mean?',\n",
       " 'Pearson and Spearman correlations - how to determine if they are \"similar\"?',\n",
       " 'Ideal transformation for consumption variable in a probit model',\n",
       " 'Determining statistical significance in election polls from the MoE / confidence interval',\n",
       " 'What is the name of the distribution whose support is (0,1) and whose pdf kernel is exp(ax) - 1?',\n",
       " 'Relation between reduced chi square and Akaike criterion',\n",
       " 'Time series for count data, with counts < 20',\n",
       " 'Detrending method for nonstationary data',\n",
       " 'How do I extract difficulty and discrimination from a non-parametric IRT model',\n",
       " 'How to compute asymptotic confidence intervals for differences in quantiles?',\n",
       " 'Pesaran Cross sectional independence test in panel data',\n",
       " 'Generalized Least Squares: Estimation of Variance-Covariance matrix',\n",
       " 'Categorical coefficients',\n",
       " 'Effect of delivery mode of questionnaire (online/hardcopy) on results',\n",
       " 'How to predict how much data to collect?',\n",
       " 'Implementation and Interpretation of Fixed versus Random Effects',\n",
       " 'Experimental Design for Comparative Responses',\n",
       " 'Is a predictor with greater variance \"better\"? ',\n",
       " 'Is it possible to use machine learning as a method for learning stats, rather than vice-versa?',\n",
       " 'Find most likely fit for subgroup size and average density from product and supergroup size',\n",
       " 'Can lmer() use splines as random effects?',\n",
       " 'Biplots: Adding supplementry points to a biplot which only use a subset of the variables',\n",
       " 'Normalizing or detrending groups of samples',\n",
       " 'Optimal rate of convergence for nonparametric estimators in Sobolev space',\n",
       " 'Is the sample variance a useful measure for non-normal data?',\n",
       " 'Fast linear regression robust to outliers',\n",
       " 'Decision Tree as variable selection for Logistic Regression',\n",
       " 'Multiple comparisons on nested subsets of data',\n",
       " \"Excel's confidence interval function throws #NUM! when standard deviation is 0\",\n",
       " 'Detecting parts of a song',\n",
       " 'Finding expectation of log',\n",
       " 'Confidence interval about a multivariate normal probability',\n",
       " 'How to obtain optimally scaled data from homals in R?',\n",
       " 'Statistical analysis of mutant proteins sequences',\n",
       " 'Multiple regression problem S Shape Residuals',\n",
       " 'Independent categorical variables and multiple regression with numeric dependent variables',\n",
       " 'How to store checks of gradient algorithm in a matrix using R?',\n",
       " 'P-value 2-way ANOVA vs 1-way ANOVA',\n",
       " 'How to learn Data Analysis?',\n",
       " 'What data structure is necessary for survival analysis?',\n",
       " 'Finding mean when class sizes are unequal',\n",
       " 'Best method for transforming low discrepancy sequence into normal distribution?',\n",
       " 'Is there always a maximizer for any MLE problem?',\n",
       " '\"Normalizing\" variables for SVD / PCA',\n",
       " 'Comparing standard deviations between variables with very different ranges',\n",
       " 'Does high log-likelihood imply high R^2',\n",
       " 'Understanding of persp plot',\n",
       " \"Forecasting beyond one season using Holt-Winters' exponential smoothing\",\n",
       " 'Factor analysis of dyadic data',\n",
       " 'Where are the t-values in my pairwise comparisons?',\n",
       " 'Multiclass transductive SVM software package?',\n",
       " 'How to improve SVM performance on data with missing features and outliers?',\n",
       " 'How to run a logistic regression with multiple related dependent variables?',\n",
       " 'Multinomial regression with categorical choice and predictos and factor analysis',\n",
       " 'Rank of an expression in Linear Model (Reference Searle)',\n",
       " 'How to interpret regression coefficients in a log-log model',\n",
       " 'What is the estimation bias of the top estimate in a list sorted by value?',\n",
       " 'Sample size and documentation for discriminant analysis',\n",
       " 'Coordinate descent for the lasso or elastic net',\n",
       " 'Offset needed in regression when response is continuous?',\n",
       " 'Including the interaction but not the main effects in a model',\n",
       " \"Is Pearson's correlation the best method to compare strength of relationship between two Likert items across groups?\",\n",
       " 'Normalization in pairwise hypothesis testing',\n",
       " 'The organization as a unit of analysis and sampling issue',\n",
       " 'Rating how closely one graph models another',\n",
       " 'Alpha adjustment for multiple testing',\n",
       " 'Drawing Chernoff graph in R with only four face features?',\n",
       " 'Chi-squared fitting with uncertainties estimated from the data',\n",
       " 'Conditional heteroscedasticity vs iid hypothesis - McLeod-Li test',\n",
       " 'How can I pool posterior means and credible intervals after multiple imputation?',\n",
       " 'Likelihood test for dividing a distribution into two separate distributions',\n",
       " 'Is the mean squared error used to assess relative superiority of one estimator over another?',\n",
       " 'Confusion related to a paper related to multivariate spatio-temporal data with missing attributes',\n",
       " 'Why is it bad to teach students that p-values are the probability that findings are due to chance?',\n",
       " 'Fitting a continuous result to categorical predictors semiparametrically',\n",
       " 'What is the best way to compare fluctuations of two signals?',\n",
       " 'When we are dealing with instrumental variables is this correct?',\n",
       " 'Bayesian analysis of an opinion poll',\n",
       " 'Rules of thumb for \"modern\" statistics',\n",
       " 'Lies, Damn Lies and Statistics',\n",
       " 'Given my networks of friends - can I detect my most \"Central\" friends?',\n",
       " 'What to do with almost-continuous variable in regression?',\n",
       " 'How to calculate a probability of a sequence of observations in Hidden Markov Model?',\n",
       " 'Increase the sample size of a Latin Hypercube study?',\n",
       " 'Improving transformation of dependent variable and robust regression',\n",
       " 'Time dummies in ordered probit regression',\n",
       " 'Is a sequence of random variables indexed by a homogeneous Poisson process process strictly stationary?',\n",
       " 'Box-Cox like transformation for independent variables?',\n",
       " 'Regression Lines With Same Intercept',\n",
       " 'Determining sample size for correlated data',\n",
       " 'How to check for bivariate Gaussianity without the use of regression?',\n",
       " 'When subject-based analysis is better than answer-based (and vice versa)?',\n",
       " 'Why am I getting a zero parameter estimate and SE for one of my variables in a logistic regression?',\n",
       " 'Calculating probability question from a student',\n",
       " 'Determine gradient from past samples',\n",
       " 'Looking for help identifing outliers in a pilot study to guide future hypothesis testing',\n",
       " 'Methods for estimating the survival function',\n",
       " 'How well does R scale to text classification tasks?',\n",
       " 'How to compare AUCs using the boot package in R?',\n",
       " 'Can one cluster web log data without performing user or session identification?',\n",
       " 'Find range of possible values for probability of intersection given individual probabilities',\n",
       " 'What is the outline for the procedure of model selection, with different models based on likelihood functions?',\n",
       " 'Sampling distribution of random effects estimator',\n",
       " 'Adding noise to a matrix/vector',\n",
       " 'Approximate order statistics for normal random variables',\n",
       " 'How to add two dependent random variables?',\n",
       " 'Bayesian inference for multinomial distribution with asymmetric prior knowledge?',\n",
       " 'How to make age pyramid like plot in R?',\n",
       " 'Posterior probability using forward backward algorithm in R',\n",
       " 'Autocorrelation from multiple time series samples',\n",
       " 'Dimensional reduction with synonomy and polysemy',\n",
       " 'effect of parameters on ARMA process',\n",
       " 'Dividing controls between two groups of cases',\n",
       " 'Transforming arbitrary distributions to distributions on $[0,1]$',\n",
       " 'Equation to calculate a smooth line given an irregular time series?',\n",
       " 'Any out source outlier (anomaly) detection package for Weka?',\n",
       " 'arima regression Improving the model',\n",
       " \"PCA's principle and available algorithms\",\n",
       " 'References for methods for calculating the confidence interval for Theil-Sen Estimator',\n",
       " 'Techniques for incremental online learning of classifier on stream data',\n",
       " 'Type of data missingness in R',\n",
       " 'How likely are various outcomes in a lottery with multiple prizes',\n",
       " 'xts object to ts object / seasonal dummies for xts',\n",
       " 'Why does the OLS estimator simplify as follows for the single regressor case?',\n",
       " 'How to compare coefficients of a negative binomial regression for determining relative importance?',\n",
       " \"Can Spearman's correlation be run on z scores?\",\n",
       " 'What is the probability P(X > Y) given X ~ Be(a1, b1), and Y ~ Be(a2, b2), and X and Y are independent?',\n",
       " 'Multiple imputation with the Amelia package',\n",
       " \"Distribution of atan2 of normal r.v.'s\",\n",
       " \"EM for truncated/censored data, how do those CDF's cancel?\",\n",
       " 'R/Stata package for zero-truncated negative binomial GEE?',\n",
       " 'How to shape a distribution?',\n",
       " 'Assessing DNA sequencing quality',\n",
       " 'Predicting percent of male and female party attendees',\n",
       " 'Hurst exponent calculation methodology',\n",
       " 'Parametric, semiparametric and nonparametric bootstrapping for mixed models',\n",
       " 'Evaluation method when using a large training set and a small test set',\n",
       " 'In genome wide association studies, what are principal components?',\n",
       " 'Random Balls in Random Buckets: What are the characteristics of the distribution?',\n",
       " 'How to test for increases in categorical variable values measured over time?',\n",
       " 'What should be the optimal parameters for Random Forest classifier?',\n",
       " 'How to calculate all the combinations in R?',\n",
       " 'Latent Dirichlet allocation Implementation',\n",
       " 'Logistic regression with an log transformed variable, how to determine economic significance',\n",
       " 'Peak detection in a ucontroller using FFT',\n",
       " 'Ordinal predictors in linear multiple regression in SPSS or R',\n",
       " 'Using Singular Value Decomposition to Compute Variance Covariance Matrix from linear regression model',\n",
       " 'Which regression method for life satisfaction score?',\n",
       " 'What is the role of MDS in modern statistics?',\n",
       " 'Generative model that penalizes clumping of data',\n",
       " 'How to perform a Wilcoxon signed rank test for survival data in R?',\n",
       " 'What is the \"$R^2$\" value given in the summary of a coxph model in R',\n",
       " 'How to determine percentiles/quantiles in SPSS?',\n",
       " 'Correlation of bivariate grouped data?',\n",
       " 'Improving an estimate of mean with observations of sign',\n",
       " 'Does a sparse training set adversely affect an SVM?',\n",
       " 'Generate correlated IMA(1,1) using R',\n",
       " 'Monte Carlo variation in bootstrapping?',\n",
       " 'Multinomial logistic predictor matrix',\n",
       " 'Classifiers with post-training constraints on the prediction space',\n",
       " 'Set of numbers statistically higher than other',\n",
       " 'Is there a style guide for statistical graphs intended for presentations?',\n",
       " 'Small dimensional classification (< 20 features), one (or two) dominant predictors',\n",
       " 'Anscombe transform and normal approximation',\n",
       " \"What's the correct way to test the significance of classification results\",\n",
       " 'What does $E_yT_y$ mean?',\n",
       " 'Are Measurements made on the same patient independent?',\n",
       " 'Rao-Cramér inequality',\n",
       " 'Suspiciously high pseudo r^2 values in gaussian & ordered regressions (R)',\n",
       " 'In R, what is the best graphics driver for using the graphs in Microsoft Word?',\n",
       " 'Is sampling from a folded normal distribution equivalent to sampling from a normal distribution truncated at 0?',\n",
       " 'Use of propensity score in a case-control study',\n",
       " 'Logistic regression across groups',\n",
       " 'Figuring out probabilities with Hidden Markov Models',\n",
       " \"Friedman's test for binary data - possible or not?\",\n",
       " 'Simulation of maximum likelihood ratio test to test two poisson random variables',\n",
       " 'Least squares with non-linear constraints',\n",
       " 'How to sample uniformly from an intersection of simplices?',\n",
       " 'Calculating % unsampled in sampling with replacement',\n",
       " 'Should I keep or remove random effects?',\n",
       " 'What is the difference between empirical variance and variance?',\n",
       " 'Data fitting with repeated measurements',\n",
       " 'Confusion related to derivation of the log likelihood',\n",
       " 'Rationale of using AUC?',\n",
       " 'Formulate hypotheses when $\\\\\\\\mu_A < \\\\\\\\mu_B$ is different from $\\\\\\\\mu_A > \\\\\\\\mu_B$',\n",
       " 'How to compare whether models built using two different outcomes are significantly different',\n",
       " 'Flipping test and control groups',\n",
       " 'Significance test for non-normal population?',\n",
       " 'Strange result of post-hoc test',\n",
       " 'What is the name for a \"typical\" range of values?',\n",
       " 'Lucene-based text feature construction',\n",
       " 'Natural interpretation for LDA hyperparameters',\n",
       " 'For what type of problems nearest neighbor performs better',\n",
       " 'How to, or what is the best way, to apply propensity scores after matching?',\n",
       " 'Measure to use in experiment',\n",
       " 'Methods for merging / reducing categories in ordinal or nominal data?',\n",
       " 'Determine accuracy of model which estimates probability of event',\n",
       " 'How do you calculate the expectation of $\\\\\\\\left(\\\\\\\\sum_{i=1}^n {X_i} \\\\\\\\right)^2$?',\n",
       " 'Calculate regression parameters by hand',\n",
       " 'How do I test to see if the demographic proportions between two unequally sized groups are statistically different?',\n",
       " 'Centering constraints for regression - specifically GAM',\n",
       " 'Reference for age-period-cohort models',\n",
       " 'How to get to a t variable from linear regression',\n",
       " 'VGAM checkwz errors',\n",
       " 'How to decide what to do with missing data when doing data analysis?',\n",
       " 'What is the curse of dimensionality?',\n",
       " 'How to extract residuals from function cv.lm in R?',\n",
       " 'Estimating 2D mean from uneven samples',\n",
       " 'Can you say that statistics and probability is like induction and deduction?',\n",
       " 'Which statistical method of anlysis should I use?',\n",
       " 'Is it possible to fit a data curve to another data curve?',\n",
       " 'Understanding AIC and Schwarz criterion',\n",
       " 'Resampling without replacement in R, with a loop',\n",
       " 'Deriving risk estimates using forecasting confidence limits and out of sample hold-out cases',\n",
       " 'Repeated coin flip experiments: what counts as a sample?',\n",
       " 'Determining polynomial model coefficients forcing slope = 1 and intercept =0',\n",
       " 'How can I use optimal scaling to scale an ordinal categorical variable?',\n",
       " 'How to determine multivariate confidence in laymans terms',\n",
       " 'Testing overlapping binary and categorical data',\n",
       " 'Predicting a semi-deterministic process',\n",
       " 'When would it be appropriate to report variance instead of standard deviation?',\n",
       " 'Multinomial choice with binary observations',\n",
       " 'Create positive-definite 3x3 covariance matrix given specified correlation values',\n",
       " \"Algorithm to calculate difference in users' tastes\",\n",
       " 'Best way to classify hierarchical data',\n",
       " 'Can covariates in a Cox Proportional Hazard model be combined in any meaningful way?',\n",
       " 'Computing probability of proportions of (possibly) overlapping sets',\n",
       " 'Estimating the functional form of the slowly time-varying variance of a Gaussian process',\n",
       " 'What are alternatives to uniform distribution when trying to fit observed data distribution?',\n",
       " 'How do I handle very different weights in a least squares fit?',\n",
       " 'What are the mean and variance of the ratio of two lognormal variables?',\n",
       " 'Homogeneity of variance in regression',\n",
       " 'How to measure the \"well-roundedness\" of SE contributors?',\n",
       " 'Does it make sense to \"cluster\" when you use a regression discontinuity?',\n",
       " 'How to determine how many unique visitors should be in the control group',\n",
       " 'How to calculate class-specific accuracy on PCA?',\n",
       " 'Comparing a single variable regression model to a model with interactions?',\n",
       " 'Eigenfunctions of an adjacency matrix of a time series?',\n",
       " 'How to implement a regression with non-independent observations in the DV?',\n",
       " 'Exponentially weighted moving linear regression',\n",
       " 'Correcting standard errors when the independent variables are autocorrelated',\n",
       " 'How to deal with this design?',\n",
       " 'Finding the best features in interaction models',\n",
       " 'Relationship between poisson and exponential distribution',\n",
       " 'Permutation paired test with signed statistic',\n",
       " 'Different ways to write interaction terms in lm?',\n",
       " 'Had statisticians predicted 2008 financial crisis?',\n",
       " 'Preferred method for identifying curvilinear effect in multi-variable regression framework',\n",
       " 'Correlation analysis of parameters',\n",
       " 'Choose best model between logit, probit and nls',\n",
       " 'Poisson with an autoregressive term',\n",
       " 'Determining how well given real-life data fits to a given probability distribution',\n",
       " 'How do I weight words in title, body text, and links differently in document clustering?',\n",
       " 'is there any package / simulator for discrete time markov chain simulation',\n",
       " 'How to get SSE for predictions using SAS?',\n",
       " 'R from command line',\n",
       " 'Multiple Chi-Squared Tests',\n",
       " 'Predicting index from multiple predictors using panel data over 10 years: logit or probit? Fixed or random?',\n",
       " 'Analyze and generate \"clumpy\" distributions?',\n",
       " 'Legitimate IV for panel?',\n",
       " \"How can I discern whether a classifier's outcome is significantly different?\",\n",
       " 'Residuals correlated positively with response variable strongly in linear regression',\n",
       " 'Characterizing the inter-arrival time of software threads',\n",
       " 'Modelling the effect of a 2 by 4 mixed design on a three-level nominal dependent variable',\n",
       " 'Are ratio, interval, ordinal and nominal variables nested?',\n",
       " 'Updating the probability distribution when removing links from a Bayesian network',\n",
       " 'R software implementation of combining mixed treatment comparisons and meta-regression',\n",
       " 'Interpretation of odds ratio when outcome is a percentage',\n",
       " 'High dimensional volume entropy estimator',\n",
       " 'Using standardized Y in Elastic Net',\n",
       " 'Assessing quality of similarity measure',\n",
       " 'Does this quantity related to independence have a name?',\n",
       " 'Poisson Rate Regression: Offset?',\n",
       " 'Expected value of spurious correlation',\n",
       " 'Relative predictive power of predictors used in time series models like kalman filter',\n",
       " 'Which high performance methods exist to determine the statistical significance of web analytics metrics?',\n",
       " 'Methods for probability estimates',\n",
       " 'Linear regression and arithmetic mean',\n",
       " 'What kind of sample is this?',\n",
       " 'Estimating parameters for a spatial process',\n",
       " 'Plotting Options in R: Setting Axis Limits',\n",
       " \"Is it plausible to get a Cronbach's alpha of .85 with only two Likert-type items?\",\n",
       " 'Change detection algorithm - likelihood ratio',\n",
       " 'Propagation of error using 2nd-order Taylor series',\n",
       " 'Symmetric measure of variable contribution to a regression',\n",
       " \"Data transposition from 'clustered rows' into columns\",\n",
       " 'Acceptable r-square value for multiple linear regression model',\n",
       " \"How best to represent a web app's usage?\",\n",
       " 'Communicating Regression Model Results',\n",
       " 'Normalization prior to cross-validation',\n",
       " 'Statistic hypothesis testing - Standard deviation less than 0.4',\n",
       " 'Estimating joint distributions using copula package in R',\n",
       " 'Should Kevin Durant be benched?',\n",
       " 'Use of cosine similarity on DGGE data',\n",
       " 'How do I detect shifts in sales mix?',\n",
       " 'Is goodness of fit needed for regression models when interpreted causally?',\n",
       " 'Comparing two discrete distributions (with small cell counts)',\n",
       " 'How to denote element-wise difference of two matrices',\n",
       " 'Correlation between one variable and few others',\n",
       " 'Testing the parameter of a sub-sample',\n",
       " 'Specifying conditional probabilities in hybrid Bayesian networks',\n",
       " \"What are the ANOVA's benefits over a normal linear model?\",\n",
       " 'How to replace the top 1% of the data with a new value in R?',\n",
       " 'Sample size for unequal groups in logistic regression',\n",
       " 'Bivariate CDF estimation',\n",
       " 'Plot a generalised mixed effects model with binomial errors',\n",
       " 'Proportions of different types of nuts in bags of mixed nuts',\n",
       " 'A random censored regression problem?',\n",
       " 'Why is the first postulate of the Poisson process that $\\\\\\\\lambda dt$ is the probability of exactly one event in $[t,t+dt]$?',\n",
       " 'Variable ordering using PCA',\n",
       " 'How to get started with applying item response theory and what software to use?',\n",
       " 'Machine learning predicted value',\n",
       " 'What is the most suitable statistical test for a series of data?',\n",
       " 'Correct variance for minimum detectable difference',\n",
       " 'Should I include interactions if I know they are not significant?',\n",
       " 'Parametrizing the Behrens–Fisher distributions',\n",
       " 'Short and long-run trend',\n",
       " 'Logistic Regression - Multicollinearity Concerns/Pitfalls',\n",
       " 'Accessing errors of 3D surface generation algorithm',\n",
       " \"What does an ACF graph tell me that a PACF graph doesn't?\",\n",
       " 'How to perform 1-to-1 matching when there are more treated than control subjects?',\n",
       " 'Where does the definition of the hyperplane in a simple SVM come from?',\n",
       " 'When is logistic regression solved in closed form?',\n",
       " 'What test should I choose if I want to see if two groups are different from each other in many categories?',\n",
       " 'Understanding entropy/salience of probability distribution of two data sets',\n",
       " 'How to fit a regression like $y=\\\\\\\\lfloor ax+b \\\\\\\\rfloor$ in R?',\n",
       " 'Difference between ANOVA power simulation and power calculation',\n",
       " 'Formula for weighted simple linear regression',\n",
       " 'What is a representative sample?',\n",
       " 'Correcting sample bias',\n",
       " 'Distance independent approximation of Nearest Neighbor/k-NN.',\n",
       " 'Kernel in PenalizedSVM R package',\n",
       " 'What is the meaning of p values and t values in statistical tests?',\n",
       " 'Generalized linear model as non-parametric ANCOVA vs. modern robust methods and non-parametric equivalant of ANCOVA',\n",
       " 'Correlated factors in a global model',\n",
       " 'Trying to run statistical tests in R but struggling as I am new to the language',\n",
       " 'Odds Ratio Between Overlapping Groups',\n",
       " 'Column Means Significance Tests in R',\n",
       " 'Prediction with Bayesian networks in R',\n",
       " 'Theoretical corrections of the training error for time series data',\n",
       " 'Real examples of multinomial distribution',\n",
       " 'Minimum number of observations for logistic regression?',\n",
       " 'Statistical test for weighted sum of random variables?',\n",
       " 'How to predict optimal reachability of outbound calls?',\n",
       " 'Estimating the dimension of a data set',\n",
       " 'Reproducing table 18.1 from \"Elements of Statistical Learning\"',\n",
       " 'Inter-rater statistic for skewed rankings',\n",
       " 'How to do prediction from a linear regression?',\n",
       " 'Is it valid to include several interaction terms but exclude main effects because only interaction effects are significant?',\n",
       " 'Plotting the fitted values and their confidence intervals',\n",
       " 'Logistic regression discrimination threshold with cross validation',\n",
       " 'Difference between t-test and ANOVA in linear regression',\n",
       " 'Which type of regression to use, considering one variable with upper bound?',\n",
       " 'P-value adjustment in correlational analysis',\n",
       " 'OLS estimator. Should it be left in terms of Y?',\n",
       " 'Joining results from different laboratories',\n",
       " 'One shot events',\n",
       " 'How do I calculate probability',\n",
       " \"Are truncated numbers from a random number generator still 'random'?\",\n",
       " 'How to choose between ANOVA and regression?',\n",
       " 'How to handle online time series forecast?',\n",
       " 'Comparing means and variance of one group in two conditions',\n",
       " 'No-simultaneous-events assumption of the Poisson process',\n",
       " 'What does it indicate when the Spearman correlation is a definite amount less than Pearson?',\n",
       " 'How to define a distribution such that draws from it correlate with a draw from another pre-specified distribution?',\n",
       " 'Is significance more important than absolute value?',\n",
       " 'Will the silhouette formula change depending on the distance metric?',\n",
       " 'Follow-up updates in case-cohort designs',\n",
       " 'How to get correct Covariance components in SAS Proc mixed? Struggling with infinite likelihoods and negative variance components',\n",
       " 'Two-stage linear regression',\n",
       " \"How to show whether there's a significant higher rate?\",\n",
       " 'Tutorials for drawing an ROC curve',\n",
       " 'Good reference on analysis of dependent samples',\n",
       " 'Markowitz portfolio mean variance optimization in R',\n",
       " 'Determining the reliability of weather forecast',\n",
       " 'Trick in survey design for \"problematic\" questions',\n",
       " 'Univariate feature ranking in classification',\n",
       " 'Why is semipartial correlation cited so seldom?',\n",
       " 'Link between moment-generating function and characteristic function',\n",
       " 'Papers on the risks of using regular subsets',\n",
       " 'Help with Johansen procedure to check the cointegration',\n",
       " 'Very easy question : Sample Variance',\n",
       " 'What is randomness?',\n",
       " 'Methods for evaluating partial autocorrelation for identification of ARIMA models',\n",
       " 'Probability for finding a double-as-likely event',\n",
       " 'standard errors for marginal models with no clustering',\n",
       " \"Are Fisher's linear discriminant and logistic regression classifier related?\",\n",
       " 'Statistical similarity of time series',\n",
       " 'How to test for parallelism for two linear models?',\n",
       " 'Beta binomial Bayesian updating over many iterations',\n",
       " 'How to apply clustering analysis to help identify criminal entities out from credit card usage data?',\n",
       " 'Learning a value of a parameter u given \"true\" or \"false\" prediction for each data-point x',\n",
       " 'Individuals standard deviations and/or standard errors for groups after implementing ANOVA?',\n",
       " 'interpreting chi square value for validating random numbers',\n",
       " 'Implementing Gaussian mixture model for a HMM library',\n",
       " 'Dirichlet distribution plot in R',\n",
       " 'How to find the best input value for this simple problem?',\n",
       " 'Multiple measurements within groups test',\n",
       " 'Is it possible to extract fixed effects in the fixed effects model in Stata?',\n",
       " 'conditional on the total, what is the distribution of negative binomials',\n",
       " 'How to find the number of runs?',\n",
       " \"What descriptive statistics should be reported in tables and graphs when using Friedman's nonparametric test?\",\n",
       " 'Spatial statistics models -- CAR vs SAR',\n",
       " 'pLSA - Probabilistic Latent Semantic Analysis, how to choose topic number?',\n",
       " 'Does MLE require i.i.d. data? Or just independent parameters?',\n",
       " 'phantom NA/NaN/Inf in foreign function call (or something altogether different?)',\n",
       " 'Which search range for determining SVM optimal parameters?',\n",
       " 'Predicting user selections based on similar user',\n",
       " 'Simulate data for stratified Cox model',\n",
       " 'Nonparametric test for comparing trends in two time series',\n",
       " 'Variation of Skewness and Excess Kurtosis',\n",
       " 'How to do factor analysis for categorical data?',\n",
       " 'Categorising continous data in logistic regression',\n",
       " 'How to assess whether a correlation between play and number of friends holds in different contexts?',\n",
       " 'How to test for serial correlation of a time series itself (not residuals)?',\n",
       " 'What is discriminative partition matching?',\n",
       " 'Trying to compute Gini index on StackOverflow reputation distribution?',\n",
       " 'What kind of statistical analysis should I do to aggregate these values?',\n",
       " 'Maximum entropy inference for k-means clustering',\n",
       " 'Removing duplicated rows from R data frame',\n",
       " 'How low multiple R-squared value is enough to reject a model?',\n",
       " 'Good introduction book on small area estimation?',\n",
       " 'Pre- post- questionnaire (t-test)',\n",
       " 'Simultaneous Non-parametric regression and Non-parametric density estimation',\n",
       " 'References about the theory of linear regression or regression in general',\n",
       " 'Do I need p-value adjustment for multiple ANOVAs on the same dataset?',\n",
       " 'Statistical properties of a 3D field from spatial averages at different scales',\n",
       " 'Building artificial state space model from noise-less data',\n",
       " 'Confidence Interval for $\\\\\\\\eta^2$',\n",
       " 'What are the standard statistical tests to see if data follows exponential or normal distributions?',\n",
       " 'Variance of resistors in parallel',\n",
       " 'Using total score from multi-scale instrument in structural equation modeling',\n",
       " 'Prove this theorem related with specification tests',\n",
       " 'SVM regression with longitudinal data',\n",
       " 'How to tell when factors \"disagree\" in linear regression to produce noisy predictions?',\n",
       " 'What books provide an overview of engineering statistics?',\n",
       " \"What happens if a survival curve doesn't reach 0.5?\",\n",
       " 'How to use R gbm with distribution = \"adaboost\"?',\n",
       " 'Upper bounds for the copula density?',\n",
       " 'Calculation of natural cubic splines in R',\n",
       " 'Difference among bias, systematic bias, and systematic error?',\n",
       " 'Why use a certain measure of forecast error (e.g. MAD) as opposed to another (e.g. MSE)?',\n",
       " 'Defining the \"uniformity\" of a dataset',\n",
       " 'Can chi square be used to compare proportions?',\n",
       " 'Welch statistic',\n",
       " 'GLM after model selection or regularization',\n",
       " 'What free tool can I use to do simple Monte Carlo simulations on OS X?',\n",
       " 'If mean is so sensitive, why use it in the first place?',\n",
       " 'Type of probability used by insurance company (and influence of gender)',\n",
       " 'How are classifications merged in an ensemble classifier?',\n",
       " 'Rolling analysis with out-of sample',\n",
       " 'Heteroskedasticity and standard deviation',\n",
       " 'Comparison of the tails of two sample distributions',\n",
       " 'sample size for well-established questionnaire',\n",
       " 'Using a particle filter for robot localization',\n",
       " 'How to get the prediction values for two response variables from random forest?',\n",
       " 'Specifying parameter constraints in nls()',\n",
       " \"AIC with Mantel's tests\",\n",
       " 'Use ARIMA equation outside R',\n",
       " 'Singularity issues when fitting an ARIMAX model',\n",
       " 'How to analyse three independent variables and two dependent variables?',\n",
       " \"If correlation doesn't imply causality then what's the value of knowing the correlation between two variables?\",\n",
       " 'Mean structure and mean/variance relationship in regression',\n",
       " \"Proving that the squares of normal rv's is Chi-square distributed\",\n",
       " 'How to calculate precision and recall when some of the test data remains unclassified',\n",
       " 'hierarchical Bayesian models vs. empirical Bayes',\n",
       " 'Is it allowed to include time as a predictor in mixed models?',\n",
       " 'Geometrical interpretation of correlation of a variable and the residual',\n",
       " 'LOESS that allows discontinuities',\n",
       " 'In Excel, how do I plot two rows against each other?',\n",
       " 'What machine learning algorithm can be used to predict the stock market?',\n",
       " 'Group comparison: joint model vs. separate model, contradictory result',\n",
       " 'How to fit a smooth curve to data in order to mimic an elbow?',\n",
       " 'Logistic regression for time series',\n",
       " 'Compendium of cross-validation techniques',\n",
       " 'Sufficient statistical multivariate Gaussian',\n",
       " 'How do to calculate Likelihood Ratio Test/Power in hypothesis testing?',\n",
       " 'Traffic visualization in Excel: five sources, two metrics, month-by-month',\n",
       " 'Non-nested model selection',\n",
       " 'Introductory book for multivariate statistics',\n",
       " 'Are regressions with student-t errors useless?',\n",
       " 'Assessing spurious effect of a third variable on the relationship between a response and a predictor',\n",
       " 'Can I test all possible contrasts in a regression with a categorical explanatory variable?',\n",
       " 'Time-varying linear regression',\n",
       " 'how to add second order terms into the model in R?',\n",
       " 'To what extent is the distinction between correlation and causation relevant to Google?',\n",
       " 'Simple problem formulation for EM algorithm',\n",
       " 'Time series modeling with high-frequency data',\n",
       " 'Nearest neighbor information for recommendation engines',\n",
       " 'ANOVA with uncertain values - attempt to redefine node',\n",
       " 'testing that mean(x) = k with a small sample and unknown distribution of x',\n",
       " 'Generalization of multivariate normal distribution and classification',\n",
       " 'SVM, variable interaction and training data fit',\n",
       " 'Asymptotic relative efficiency of median vs mean for Student t distribution',\n",
       " 'Multivariate Interpolation Approaches',\n",
       " 'calculation of correlation time?',\n",
       " 'Repeated measure ANOVA',\n",
       " 'What does \"to center two IVs\" mean in the context of an interaction?',\n",
       " 'How to take many samples of 10 from a large list, without replacement overall',\n",
       " 'Proper variable selection method for glm',\n",
       " 'Is there a preferred panel chart layout?',\n",
       " 'An elementary question on binomial test: why should I take a sum?',\n",
       " 'How to calculate sample size needed for comparing the \"change from baseline\" scores between two groups?',\n",
       " 'exact Monte Carlo simulation of a large sample histogram',\n",
       " 'How can I make a continuous distribution out of simulation results in R?',\n",
       " 'Partialling or regressing out a categorical variable?',\n",
       " 'Meaning of p-values in regression',\n",
       " 'Learning the parameter of linear model',\n",
       " 'How can I model a proportion with BUGS/JAGS/STAN?',\n",
       " 'Determining the probability that two functions produce the same outputs, when not all outputs of one function are known',\n",
       " 'Effect of treatment on correlation between two sets of variables',\n",
       " 'Is Naive Bayes fine for simple \"Suggested products\" solution?',\n",
       " 'Generating from Dirichlet distribution with the differences in a sequence of ordered uniform',\n",
       " 'How to get the required data?',\n",
       " 'Question about calculating mutual information',\n",
       " 'Variance or standard deviation from a value other than the mean',\n",
       " 'Recommendations for visualization type when data has an extremely wide variance',\n",
       " 'Post processing random forests using regularised regression: what about bias?',\n",
       " 'Anova repeated measures is significant, but all the multiple comparisons with Bonferroni correction are not?',\n",
       " 'What is the best survey method to use for conducting an annual risk and fraud survey?',\n",
       " 'Can R geeglm handle proportion data?',\n",
       " 'How defensible is it to choose $\\\\\\\\lambda$ in a LASSO model so that it yields the number of nonzero predictors one desires?',\n",
       " 'Parallel solving Ax=b?',\n",
       " 'How auto.arima works?',\n",
       " 'Understanding similarity sensitive hashing algorithm in AdaBoost',\n",
       " 'prior distribution to the binomial distribution probability distributions urn model',\n",
       " 'Useful heuristic for inferring multicollinearity from high standard errors',\n",
       " 'Is there a name for the \"kernel principle\"?',\n",
       " 'Similarity calculations for arrays',\n",
       " 'Looking for 2D artificial data to demonstrate properties of clustering algorithms',\n",
       " 'State-of-the-art in smoothing splines',\n",
       " 'What to do with heterogeneity of variance when spread decreases with larger fitted values',\n",
       " 'Analysis of influence against weighted edges and attributes',\n",
       " 't-stats in OLS regression',\n",
       " 'Are PCA solutions unique?',\n",
       " \"What if results from a Breusch-Pagan test for heteroscedasticity contradicts those of a White's test?\",\n",
       " 'Resources for developing prognostic index-scores',\n",
       " 'Calculating the 95th percentile: Comparing normal distribution, R Quantile, and Excel approaches',\n",
       " 'In linear regression, is the $R^2$ value enough to assess whether the relationship between the independent and dependent variable is linear?',\n",
       " 'Causation implication',\n",
       " 'Bayesian inference on a sum of iid real-valued random variables',\n",
       " 'Negative Expectation of a constant is positive or negative',\n",
       " 'Kullback–Leibler vs Kolmogorov-Smirnov distance',\n",
       " 'How can I compute a posterior density estimate from a prior and likelihood?',\n",
       " \"Possibility of analyzing the hypothesis using Friedman's test \",\n",
       " 'Statistically significant vs. independent/dependent',\n",
       " 'How to estimate the out crosses rate in crops?',\n",
       " 'Fitting model for two normal distributions in PyMC',\n",
       " 'Distribution of variable for discriminant function analysis',\n",
       " 'Frequency ratio test with correlation structures',\n",
       " 'Summarize time series regression coefficients after giving 1 coefficient a fixed number (from a distribution)',\n",
       " 'How to compute the standard error of an L-estimator?',\n",
       " '52 variables after backward variable selection on logistic regression on 160 variable at beginning, whether it is illusion or good modeling',\n",
       " 'Prediction interval for simple linear regression',\n",
       " 'Quantile regression: Which standard errors?',\n",
       " 'Bootstrapping robust regression',\n",
       " 'How to examine interactions between factor and covariate in a mixed effects model?',\n",
       " 'Using the Cox axioms to derive unknown probabilities from known probabilities',\n",
       " 'Prediction model problem',\n",
       " 'Problem with R code for spectral clustering',\n",
       " 'Measuring error in the lag when cross-correlating data',\n",
       " 'Whats on the causal path?',\n",
       " 'Probability that multiple confidence intervals contain the true population mean',\n",
       " 'What is the difference between 2x2 factorial design experiment and a 2-way ANOVA?',\n",
       " 'Finding the fitted and predicted values for a statistical model',\n",
       " 'Residualized Coefficient Using Solely Variances and Covariances',\n",
       " 'Calculating probability of a random sample without replacement',\n",
       " \"Recreating R's hist function's bin counting\",\n",
       " 'Sampler method to choose in Monte Carlo Markov chain estimation',\n",
       " 'Proof without Jensen',\n",
       " 'Probability density function between -1 and 1?',\n",
       " 'Would a Random Forest with multiple outputs be possible/practical?',\n",
       " 'What is the conventional definition of recurrence-free survival?',\n",
       " 'Confusion related to a derivation in a paper',\n",
       " 'What type of Design is this?',\n",
       " 'Discrepancy between stepwise and nominal logistic regression results in JMP',\n",
       " \"When sampling without replacement from a given distribution, what's the total expected weight of the last k sampled items?\",\n",
       " 'Is it ok to fit a Bayesian model first, then begin weakening priors?',\n",
       " 'Weighted kernel density plot in R',\n",
       " 'Alpha Expansion algorithm over graph with vertices having different sets of labels',\n",
       " 'Question about a derivative of the 2nd-step moments in a two-step estimator as a joint GMM-estimators approach',\n",
       " 'On the use of oblique rotation after PCA',\n",
       " 'CDF raised to a power?',\n",
       " 'Predict future student outcomes (binary and continuous) with historic cross-sectional data?',\n",
       " 'Two-level hierarchical model using time-series cross sectional data?',\n",
       " 'What is a good academic citation for cross-validation?',\n",
       " 'Softmax regression bias and prior probabilities for unequal classes',\n",
       " 'Why regression of fitted vs. observed or observed vs. fitted values yield different results? ',\n",
       " 'How to interpret decreasing AIC but higher standard errors in model selection?',\n",
       " 'What is the mathematical difference between Cpk and Ppk?',\n",
       " 'Time series forecasting lookback windows -- sliding or growing?',\n",
       " 'Variant of discriminant analysis for known multiple independent classifications?',\n",
       " 'Are sample means for quantiles of sorted data unbiased estimators of the true means?',\n",
       " 'Do all interactions terms need their individual terms in regression model?',\n",
       " 'Statistical validation of RandomForest models',\n",
       " 'Standard error of parameter estimates in regularized regression',\n",
       " 'How to get a confidence interval on parameters that were fitted using multiple functions and datasets at once?',\n",
       " 'Which diagnostics can validate the use of a particular family of GLM?',\n",
       " 'Multilevel model with an ordinal outcome',\n",
       " 'ROC curve fit characteristic',\n",
       " 'Distribution of the difference of two independent uniform variables, truncated at 0',\n",
       " 'Weighted geometric mean vs weighted mean',\n",
       " 'Comparing test-retest reliabilities',\n",
       " 'Discussing binomial regression and modeling strategies',\n",
       " 'How much RAM is needed for simulation studies using R?',\n",
       " 'Poisson regression assumptions and how to test them in R',\n",
       " 'When is it appropriate to pool data?',\n",
       " 'Interaction of terms',\n",
       " 'Extending forecast model to ADL and comparing with VAR',\n",
       " 'Exponent for non-linear regression (in R)?',\n",
       " 'Multiplicative mixed models for analyzing variety-by-environment data',\n",
       " 'Sample size via confidence intervals',\n",
       " 'Paired, repeated-measures ANOVA or a mixed model?',\n",
       " 'How to estimate a multiple regression function that has interactions between the independent variables and is potentially non-linear?',\n",
       " 'When dealing with time series data must the time periods be equal intervals or annual?',\n",
       " 'Formal way of defining a weighted average',\n",
       " 'How does Fisher LDA work?',\n",
       " 'Derivation of Rayleigh-distributed random variable',\n",
       " 'Skills & coursework needed to be a data analyst',\n",
       " 'Sample size when one hypothesis is overwhelming',\n",
       " 'How to replicate a plot of means for a 2 by 3 by 4 design in R?',\n",
       " 'Multivariate regression for spatial dataset',\n",
       " 'Compound Poisson, preliminary work in R',\n",
       " 'Explain ridge in the log-likelihood for Logistic Regression classifier',\n",
       " 'Constructs and multiple regression',\n",
       " 'How to tell how extreme an outlier is?',\n",
       " 'Best distribution to model durations',\n",
       " 'Analyse within-subjects data where participants get exposed to some but not all treatments',\n",
       " \"How to interpret 95%CI's in conjunction with significance tests for differences between group means?\",\n",
       " 'For classification with Random Forests in R, how should one adjust for imbalanced class sizes?',\n",
       " 'How to measure/argue the goodness of fit of a trendline to a power law?',\n",
       " 'Oversampling in logistic regression',\n",
       " 'Why does a finite, irreducible and aperiodic Markov chain with a doubly-stochastic matrix P have a uniform limiting distribution?',\n",
       " 'Inserting small arrows (triangles) in bottom axis pointing up with R',\n",
       " 'How to perform a non-equi-spaced histogram in R?',\n",
       " 'finding x intercept from a set of values',\n",
       " 'Analysing wind data with R',\n",
       " 'Why would R return NA as a lm() coefficient?',\n",
       " 'How to test whether the probability of a given colour of skittle is equal to that of another colour?',\n",
       " 'Calculating hourly volatility and peak-to-average ratio in R',\n",
       " 'SAS for regression with categorical and quantitative explanatory variables',\n",
       " 'Comparing two genetic algorithms',\n",
       " 'Overview of (common) classifiers and their characteristics',\n",
       " 'What is the colloquial name for the \"look elsewhere\" effect?',\n",
       " 'Confusion related to calculation of pi',\n",
       " 'SVM hyperplane equation',\n",
       " 'Test for equal variance',\n",
       " 'Two questions on significance testing',\n",
       " 'Nonparametric credible intervals',\n",
       " 'Which is the strongest prediction in linear regression?',\n",
       " 'Asymmetry between high order and low order interaction terms',\n",
       " 'X-mean algorithm BIC calculation question',\n",
       " 'Simple question about the asymptotics of estimators',\n",
       " 'Estimating adjusted rate ratios for categorical independent variable measured only as population percents',\n",
       " 'Can p values be used to show impact of treatment',\n",
       " 'Why does the model change when using relevel?',\n",
       " 'Are the $1-SSe/SSt$ and $cor^2$ calculations of $R^2$ always equivalent?',\n",
       " 'How problematic is it to control for non-independent covariates in an observational (i.e., non-randomized) study?',\n",
       " 'Confidence interval on a standardized risk difference',\n",
       " 'Difference between two slopes',\n",
       " 'A robust R package to do MCMC and Gibbs sampling',\n",
       " 'Average case analysis of learning algorithms',\n",
       " 'How to name the ticks in a python matplotlib boxplot',\n",
       " 'Predicting raffle winners',\n",
       " 'How to simulate from a Gaussian copula?',\n",
       " \"How are sample sizes determined for interpretations that 'straddle the fence' between anova and correlation?\",\n",
       " 'Calculate R-squared with JAGS and R',\n",
       " 'Analysis of intervals between events',\n",
       " 'Non-parametric multilevel analysis in SPSS',\n",
       " 'How do I find a 95% percent ML confidence interval of the probability of category 5 strength',\n",
       " 'How to train and validate a neural network model in R?',\n",
       " 'Troublesome residual plot from linear mixed model',\n",
       " 'Maximum likelihood estimation in a Poisson model for football (soccer) scores',\n",
       " 'How to determine reliability of a change in an arbitary metric',\n",
       " 'Kernel matrix normalisation',\n",
       " 'Detecting a peak in an averaged profile',\n",
       " 'Average of rate of change in a set of independent change events',\n",
       " 'Seeking for a fast non parametic clustering algorithm',\n",
       " 'Goodness of fit between columns of a 10000x2 table (with low counts)',\n",
       " 'Computing inter-rater reliability in R with variable number of ratings?',\n",
       " 'How to analyse multiple between subjects factors in a mixed ANOVA in SPSS?',\n",
       " 'Poisson binomial distribution PMF conjecture',\n",
       " 'Why squared residuals instead of absolute residuals in OLS estimation?',\n",
       " 'Bonferroni correction in generalised linear model',\n",
       " 'Weighted loss function for non-random sample',\n",
       " 'How to assess whether experimental measurements obtained from different technicians are biased?',\n",
       " 'How to calculate the \"unfairness\" for repeated random selection?',\n",
       " 'Tobit model with R',\n",
       " 'An approach to visualise/investigate multi-dimensional data',\n",
       " 'Can I use all pairwise comparisons at a factor level to perform correlation analysis?',\n",
       " 'How can I show that for any $a > 0$, $\\\\\\\\lim_{n\\\\\\\\to \\\\\\\\infty}P \\\\\\\\left(\\\\\\\\sum_{i=1}^n X_i^2\\\\\\\\leq a\\\\\\\\right)=0$',\n",
       " 'Conjugate of Weibull with shape known',\n",
       " 'How do you create a multivariate distribution with both continuous and discrete data?',\n",
       " 'Pseudo R squared formula for GLMs',\n",
       " 'What programming language for statistical inference?',\n",
       " 'How to get unbiased estimate for error variance in ordinary linear model',\n",
       " 'Significance test for two mean differences? Like a t-test but for comparing differences',\n",
       " 'Flying Bomber aircraft through SAM sites - combining normal distributions',\n",
       " 'Differential expression gene analysis',\n",
       " 'R working the tsDyn and Time series',\n",
       " 'How to find and describe regularities in a distribution of interarrival times of a recurring event?',\n",
       " 'Comparing model fit with heteroscedastic data',\n",
       " 'Categorical inputs in a linear regression',\n",
       " 'What is your favorite \"data analysis\" cartoon?',\n",
       " 'Multicategory choice model with given categories',\n",
       " 'Difference between Matrix Factorization and PCA',\n",
       " 'Is ANOVA good for non-normally distributed series?',\n",
       " 'Correlation plots with missing values',\n",
       " 'How to perform exponential regression with more than one predictors using R',\n",
       " 'What differences and relations are between common random numbers, antithetic variates, and control variates',\n",
       " 'When is it acceptable to use only two items (variables) per factor in factor analysis?',\n",
       " 'How to get odds ratio in a table in SPSS?',\n",
       " 'What is Deviance? (specifically in CART/rpart)',\n",
       " \"Is it OK to do additive smoothing before applying Pearson's chi-square test for independence?\",\n",
       " 'Log-Ratio \\\\\\\\ Compositional analysis',\n",
       " 'How to test whether a sample of data fits the family of Gamma distribution?',\n",
       " 'Uncertainty of conditional probability evaluated from sample',\n",
       " 'What software allows non-parametric repeated-measures multi-way Anova?',\n",
       " \"How to compute Confidence Interval associated to a Binomial proportion's increase?\",\n",
       " 'How to calculate probability for n dependent events',\n",
       " 'How can I calculate the probability of model $g_i$ given a set of $n$ models and AIC values?',\n",
       " 'How to combine three different scores',\n",
       " 'R: looking for \"time\" clusters in a data set',\n",
       " 'How to interpret result for MultiModelByRegression in RapidMiner?',\n",
       " 'Significance and extending simple time series correlation',\n",
       " 'Why law of large numbers does not apply in the case of Apple share price?',\n",
       " 'Whether a AR(P) process is stationary or not?',\n",
       " 'GLM model is singular',\n",
       " 'A good summary measure on analyzing repeated measures of longitudinal data?',\n",
       " 'Refugee from SPSS having issues with fit.contrast in R',\n",
       " 'Dimensionality Reduction using PCA, with SVD of correlation matrix',\n",
       " \"AIC and deviance don't agree\",\n",
       " 'How to compute median conditional on several factors in hierarchical data?',\n",
       " 'Iteration of the $\\\\\\\\alpha$ parameter in a beta distribution yields a beta distribution?',\n",
       " 'What are finite window effects?',\n",
       " 'Ordering of time series for machine learning',\n",
       " 'Chi Square Test for QC Samples',\n",
       " 'Analysis of binary variables',\n",
       " \"What's the difference between $p(x \\\\\\\\mid \\\\\\\\theta)$ and $p(x; \\\\\\\\theta)$?\",\n",
       " 'How to combine a SVM classifier and a Naive Bayes classifier',\n",
       " 'The use of median polish for feature selection',\n",
       " 'How to statistically describe chain-like patterns in bivariate data?',\n",
       " 'Is it impossible for clinical and laboratory data to be normally distributed?',\n",
       " \"What is the difference between the 'Pivot table' and 'Contingency table'?\",\n",
       " 'Joint probabilities for continuous and binomial variables',\n",
       " \"About goodness-of-fit p-value and Pearson's $\\\\\\\\chi^2$ statistic\",\n",
       " 'Interpretation of results in non-inferiority study',\n",
       " \"Within group correlation of pearson's residuals\",\n",
       " 'Why it is often assumed Gaussian distribution?',\n",
       " 'How is it possible that these variances are equal?',\n",
       " 'Modelling a binary response variable with a binary explanatory variable',\n",
       " 'Using autocorrelation to find commonly occurring signal fragments',\n",
       " 'Correlation between variables',\n",
       " 'WiFi localization using machine learning',\n",
       " 'Evidence on red-purple-blue graphs',\n",
       " 'Collinearity between categorical variables',\n",
       " 'Probability of Unique Minimum (Discrete)',\n",
       " 'STL on time series with missing values for anomaly detection',\n",
       " 'Dual problem for L2 support vector machine',\n",
       " 'Fitting conditional functions in nls',\n",
       " 'What does weighted cumulative frequency distribution mean?',\n",
       " 'Calculating life expectancy in Stata',\n",
       " 'Boxplot with respect to two factors using ggplot2 in R',\n",
       " 'Odd problem with a histogram in R with a relative frequency axis',\n",
       " 'Proportional odds model for repeated measures ordinal outcomes',\n",
       " 'Deseasonalizing count data',\n",
       " 'Variance of nominal variable?',\n",
       " 'How to assess regression performance when using PCA components as response variables?',\n",
       " 'Transform continuous variable to discrete variable',\n",
       " 'Fitting data/determining parameter values for custom (biPareto) distribution',\n",
       " 'How important is it to define the nested design in a within subject ANOVA?',\n",
       " 'How to plot a learning curve based on a sequence of date stamped successes and failures?',\n",
       " 'Clustering with 3 attributes',\n",
       " 'Does this break down adding independent probabilities?',\n",
       " 'ARIMA and linear regression',\n",
       " 'Best analysis for count data as response variable',\n",
       " 'Combining between and within-subjects designs?',\n",
       " 'How to visualize a GraphML multitree?',\n",
       " 'How to improve regression?',\n",
       " 'Learning statistical concepts through data analysis exercises',\n",
       " 'Kernel PCA for LSI using R',\n",
       " 'A limit theorem for non-independent variables',\n",
       " 'How to characterize a problem of standardizing product descriptions',\n",
       " 'Expectation Subscript',\n",
       " 'How to make forecasts for a time series?',\n",
       " 'Mixed model in R with paired design and covariates',\n",
       " 'Calculating the parameters of a Beta distribution using the mean and variance',\n",
       " 'Computation of hypergeometric function in R',\n",
       " 'What are some better ways to encode these symbols?',\n",
       " 'How do I generate a QQ-Plot for data fitted using fitdistr?',\n",
       " 'Cross-validation of a Cox model',\n",
       " 'Proving a sequence decreases (supported by plotting a large number of pts)',\n",
       " '\"Peakedness\" of a skewed probability density function',\n",
       " 'Can I use paired t-test in this case?',\n",
       " 'Clinical trial design',\n",
       " \"Using R's phyper to get the probability of list overlap\",\n",
       " 'What method to detect structural breaks on time series?',\n",
       " 'Is there any proper way to fix a sample to adjust for known demographic overrepresentation?',\n",
       " 'What is the most appropriate test for this situation?',\n",
       " 'Сonfidence interval of histogram probability density function estimator',\n",
       " 'What is tied data in the context of a rank correlation coefficient?',\n",
       " 'How do I vertically stack two graphs with the same x scale, but a different y scale in R?',\n",
       " 'Treatment of outliers produced by Kurtosis',\n",
       " 'Modeling membership function given some survey data or empirical distribution',\n",
       " 'Understanding statistical control charts',\n",
       " '\"Importance\" metric for discrete variable value',\n",
       " 'Confidence intervals on differences in choices in a GEE framework: methods and alternatives?',\n",
       " 'Is it advantageous to use dummy variables when learning a regression model?',\n",
       " 'What is the number of free parameters for a directed acyclic graph?',\n",
       " 'How many copies of shredded text are needed for getting 99.9% reconstruction of mutated text?',\n",
       " 'Factor models with small noises',\n",
       " 'Understanding intercept in simple linear regression and why one variable is a predictor and the other is an outcome variable',\n",
       " 'Adjusting for zero mean (standardizing) in a multiple regression model',\n",
       " 'Cubic and linear relationships in multiple regression model',\n",
       " 'Error in normal approximation to a uniform sum distribution',\n",
       " 'For \"Was this page helpful\" data, should I take response rate into account?',\n",
       " 'Bias items and probability estimates in LibSVM',\n",
       " 'Use a trendline formula to get values for any given X with Excel',\n",
       " \"Conditional kernel density plot with R's np package\",\n",
       " 'Canonical correlation analysis on a MICE data set',\n",
       " 'How to impute an ordinal variable with MICE but prevent it from taking one value?',\n",
       " 'Why is correlation between y and $\\\\\\\\hat{y}$ in a model with and without intercept equal?',\n",
       " 'Estimate of parameter $\\\\\\\\beta$',\n",
       " 'Which non-parametric regression could I apply to fit a curve to this data set?',\n",
       " 'Should I re-scale the PCA score before classification?',\n",
       " 'Hierarchical recommender',\n",
       " 'How do I decide which family of variance/link functions to use in a generalized linear model?',\n",
       " 'Nonparameteric multivariate density approximation -- where do I start?',\n",
       " 'Difficulty in understanding Hidden Markov Model for syntax parsing using Viterbi algorithm',\n",
       " 'More efficient plot functions in R when millions of points are present?',\n",
       " 'Combining metrics for score card',\n",
       " 'What kind of distribution is it?',\n",
       " 'Is Lorenz curve the same as QQ-plot?',\n",
       " 'Decision boundaries from coefficients of linear discriminants?',\n",
       " 'Calculating linear regression from limited information',\n",
       " 'Interpreting the results of an analysis',\n",
       " 'Will logistic regression fit this situation?',\n",
       " 'How do I complete the square with normal likelihood and normal prior?',\n",
       " 'What normality assumptions are required for an unpaired t-test? And when are they met?',\n",
       " 'Estimating k in d=kv',\n",
       " 'How to test for differences between two group means when the data is not normally distributed?',\n",
       " 'What is the computational complexity of the EM algorithm?',\n",
       " 'Binomial GLM post-hoc tests for unequal sample sizes',\n",
       " 'How to log transform Z-scores?',\n",
       " 'Example implementation of random forest',\n",
       " 'Why does one get a 0.000 Mann-Whitney U value (not p-value) etc.?',\n",
       " 'On connection weights in an Artificial Neural Network',\n",
       " 'Logistic regression with longitudinal data',\n",
       " 'Geometric mean of two numbers',\n",
       " \"Least Squares Estimators' Forms\",\n",
       " 'How to statistically compare the performance of machine learning classifiers?',\n",
       " 'Is the following an error-in-variance problem, and is there a recommended R (or SAS) package for it?',\n",
       " 'P-value of a survival ROC c-index',\n",
       " 'party vs. rpart vs. ??? for partitioning trees in R',\n",
       " 'Use of row rank and column rank in regression',\n",
       " 'Correlation between two time series',\n",
       " 'Plotting regression predictors',\n",
       " 'What do I do if my predicted values are out of the dependent variable range?',\n",
       " 'Validity of pseudo-panel data constructed from repeated cross sectional data as a panel data',\n",
       " 'The expressive power of PoE over MoE',\n",
       " 'How to do multiple regression with limited experience and (hopefully) excel?',\n",
       " 'What is it that a statistician does?',\n",
       " 'How do you plot interaction effects in SAS?',\n",
       " 'Testing significance of a data point against an data set',\n",
       " \"Role of kalman filter prior, the 'right' prior?\",\n",
       " 'Can a small sample size cause type 1 error?',\n",
       " 'How to correlate two time series with gaps and different time bases?',\n",
       " 'Sampling an interpolated model with MCMC',\n",
       " 'Significant variable with no effect in logistic regression',\n",
       " 'Changing reference level on mixed model changes pMCMC value',\n",
       " 'Distributions on the simplex with correlated components',\n",
       " 'How to correctly weight predicted values from a fitted linear model?',\n",
       " 'What are the advantages / disadvantages of using splines, smoothed splines, and gaussian process emulators?',\n",
       " 'Using fast99 sensitivity method in R with normal distributions',\n",
       " 'Numerical accuracy of multivariate normal distribution',\n",
       " \"Working through Lenth's maximum likelihood estimation in JS\",\n",
       " 'How to calculate cumulative distribution in R?',\n",
       " 'Clojure versus R: advantages and disadvantages for data analysis',\n",
       " 'What test do I use for language variance data?',\n",
       " 'Alternatives to Pearson correlation',\n",
       " 'Summary of \"Large p, Small n\" results',\n",
       " 'Correlation versus cause-effect regression',\n",
       " 'How does SAS calculate exact non-parametric statistics?',\n",
       " 'Dichotomous DV and 3 points of measurement per condition within each subject',\n",
       " 'Difference in means in multiple-choice poll',\n",
       " 'How can I perform a chi-square test for independence on signal samples?',\n",
       " 'Any ideas about how to analyze survival data with pseudo-replication (dependent data)?',\n",
       " 'I am use caret package to do model comparion and met error message',\n",
       " 'Sample size formula for an F-test?',\n",
       " 'Constructing a naive recession forecast',\n",
       " 'Creating a maximum entropy Markov model from an existing multi-input maximum entropy classifier',\n",
       " 'Effective cutoff frequency for LOESS smoothing',\n",
       " 'Calculating predicted probabilities for ordinal logistic regression',\n",
       " 'Graphing versus F-test: contradictory outcome',\n",
       " 'Determining trend significance in a time series',\n",
       " 'Why (or when) to use the log-mean?',\n",
       " 'What are good datasets to illustrate particular aspects of statistical analysis?',\n",
       " 'Trend test for two ordinal variables both containing three factors',\n",
       " 'Similarities and differences between regression and estimation',\n",
       " 'Calculate one median for data from five experimental repetitions',\n",
       " 'Route to understanding and implementing statistical timing models',\n",
       " 'Comparison of Cox models',\n",
       " 'Interpreting results from Sobol sensitivity analysis in R',\n",
       " 'How to plot an ellipse from eigenvalues and eigenvectors in R?',\n",
       " 'ordered probit in JAGS',\n",
       " 'What is the marginal distribution of a single draw from the Normal-Inverse-Wishart prior?',\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " '<monte-carlo><rejection-sampling>',\n",
       " '<machine-learning><classification><data-mining>',\n",
       " '<machine-learning><classification><matlab><svm><multi-class>',\n",
       " '<time-series><forecasting><quantiles><aggregation><combination>',\n",
       " '<logistic><standardization><quadratic-form>',\n",
       " '<anova><interaction><p-value>',\n",
       " '<hypothesis-testing><mean><median>',\n",
       " '<scales><z-statistic>',\n",
       " '<statistical-significance><modeling><predictive-models><interpretation>',\n",
       " '<time-series><sampling>',\n",
       " '<clustering><large-data>',\n",
       " '<dataset><descriptive-statistics>',\n",
       " '<data-mining><software><weka>',\n",
       " '<spss><pca><factor-analysis>',\n",
       " '<normal-distribution><mixture><histogram><unsupervised-learning>',\n",
       " '<correlation><data-mining><pearson>',\n",
       " '<r><mixed-model><repeated-measures><post-hoc><lme>',\n",
       " '<estimation><variance><references>',\n",
       " '<hypothesis-testing><density-function>',\n",
       " '<r><monte-carlo><computational-statistics>',\n",
       " '<distributions><data-visualization><clustering><dataset>',\n",
       " '<regression><goodness-of-fit><polynomial>',\n",
       " '<reliability><latent-variable><path-model>',\n",
       " '<distributions><statistical-significance><outliers><biostatistics><methodology>',\n",
       " '<r><poisson>',\n",
       " '<clustering><inference><mixture><dirichlet-distribution><dirichlet-process>',\n",
       " '<time-series><variance><markov-process><change-point>',\n",
       " '<distributions><moments>',\n",
       " '<mean><median><skewness><mad>',\n",
       " '<bayesian><estimation><mean>',\n",
       " '<experiment-design><unbalanced-classes>',\n",
       " '<r><dataset><experiment-design>',\n",
       " '<machine-learning><classification><dataset><precision-recall>',\n",
       " '<r><copula>',\n",
       " '<distributions><expected-value><skewness>',\n",
       " '<regression><logistic><spss><effect-size><contrasts>',\n",
       " '<exponential-smoothing>',\n",
       " '<machine-learning><logistic><normal-distribution><multilevel-analysis>',\n",
       " '<model-selection><aic><bic>',\n",
       " '<r><mathematical-statistics><histogram><skewness>',\n",
       " '<r><graph-theory>',\n",
       " '<self-study><sampling><survey>',\n",
       " '<multivariate-analysis><outliers>',\n",
       " '<distance-functions><information-theory>',\n",
       " '<boosting><ensemble>',\n",
       " '<odds-ratio><propensity-scores>',\n",
       " '<anova><repeated-measures><missing-data><unbalanced-classes>',\n",
       " '<information-theory><entropy>',\n",
       " '<regression><logistic><statistical-significance><maximum-likelihood><z-test>',\n",
       " '<r><data-visualization><graph-theory>',\n",
       " '<logistic><generalized-linear-model><simulation><power-analysis>',\n",
       " '<normal-distribution><marginal>',\n",
       " '<regression><variance><bias>',\n",
       " '<machine-learning><regularization><sparse>',\n",
       " '<gamma-distribution><beta>',\n",
       " '<model-selection><econometrics><panel-data>',\n",
       " '<self-study><epidemiology>',\n",
       " '<hidden-markov-model><open-source><nlp><database>',\n",
       " '<r><time-series><dataframe>',\n",
       " '<mathematical-statistics><finite-population>',\n",
       " '<statistical-bias>',\n",
       " '<regression><bayesian><repeated-measures><multilevel-analysis><logistic>',\n",
       " '<distributions><time-series><signal-processing><quantiles><compression>',\n",
       " '<r><mixed-effect>',\n",
       " '<correlation><multiple-comparisons><statistical-significance><genetics>',\n",
       " '<feature-selection><dimensionality-reduction>',\n",
       " '<r><bioinformatics>',\n",
       " '<excel><matrix>',\n",
       " '<categorical-data><stata><survival><panel-data>',\n",
       " '<r><time-series><data-mining><outliers>',\n",
       " '<r><logistic><maximum-likelihood>',\n",
       " '<books><stata><big-list>',\n",
       " '<data-visualization>',\n",
       " '<r><confidence-interval><survival><cox-model><case-cohort>',\n",
       " '<hypothesis-testing><correlation><statistical-significance><sample-size><spearman-rho>',\n",
       " '<sample-size><estimation><least-squares><maximum-likelihood>',\n",
       " '<correlation><anova><multivariate-analysis><rule-of-thumb><manova>',\n",
       " '<machine-learning><classification><multilabel>',\n",
       " '<r><poisson><autoregressive>',\n",
       " '<predictive-models><sample-size><generalized-linear-model><psychometrics>',\n",
       " '<r><feature-selection>',\n",
       " '<probability><expected-value><terminology>',\n",
       " '<panel-data><data-imputation><unbalanced-classes>',\n",
       " '<r><missing-data><data-imputation><multiple-imputation><mice>',\n",
       " '<psychometrics><scales><reliability><likert>',\n",
       " '<correlation><estimation><censoring><variance-covariance><unbiased-estimator>',\n",
       " '<classification><information-retrieval><text-mining>',\n",
       " '<r><linear-model><genetics><continuous-data><permutation>',\n",
       " '<ggplot2><scatterplot>',\n",
       " '<matching><paired-comparisons>',\n",
       " '<normal-distribution><data-transformation><t-test><lognormal>',\n",
       " '<anova><experiment-design><unbiased-estimator>',\n",
       " '<regression><correlation><spss>',\n",
       " '<mean><count-data><proportion><population>',\n",
       " '<correlation><random-variable><lognormal>',\n",
       " '<regression><anova><t-test><big-list>',\n",
       " '<censoring><data-visualization><calibration>',\n",
       " '<machine-learning><maximum-likelihood><algorithms><software>',\n",
       " '<anova><spss><median>',\n",
       " '<homogeneity>',\n",
       " '<skewness><group-differences>',\n",
       " '<machine-learning><data-mining><careers><phd>',\n",
       " '<markov-process>',\n",
       " '<self-study><random-variable>',\n",
       " '<books><stochastic-processes><references><numerics>',\n",
       " '<distributions><python>',\n",
       " '<chi-squared><multinomial><pearson>',\n",
       " '<hypothesis-testing><monte-carlo><random-generation><randomness>',\n",
       " '<data-mining><software>',\n",
       " '<statistical-significance><excel><association-measure>',\n",
       " '<regression><likert>',\n",
       " '<chi-squared><independence><sample>',\n",
       " '<regression><classification><logistic><rapidminer>',\n",
       " '<r><regression><estimation>',\n",
       " '<pca><experiment-design><normalization><microarray>',\n",
       " '<anova><genetics><interpretation><bioinformatics><biostatistics>',\n",
       " '<variance><mean><central-limit-theorem>',\n",
       " '<anova><ancova><mediation>',\n",
       " '<distributions><confidence-interval><estimation>',\n",
       " '<excel><summary-statistics><business-intelligence><pivot-table>',\n",
       " '<r><bayesian><dynamic-regression>',\n",
       " '<statistical-significance><robust><mad>',\n",
       " '<time-series><python><signal-detection>',\n",
       " '<probability><entropy><information-theory>',\n",
       " '<distributions><hypothesis-testing><statistical-significance>',\n",
       " '<regression><multivariate-analysis><predictive-models><recommender-system>',\n",
       " '<probability><stochastic-processes><mixture><queueing><poisson-process>',\n",
       " '<logistic><feature-construction>',\n",
       " '<probability><self-study><conditional-probability>',\n",
       " '<manova><contrasts><hotelling>',\n",
       " '<time-series><estimation><variance><standard-error>',\n",
       " '<goodness-of-fit><power-law>',\n",
       " '<sampling><mcmc><gibbs>',\n",
       " '<distributions><stata>',\n",
       " '<data-visualization><latex><gnuplot>',\n",
       " '<survey><standard-error>',\n",
       " '<predictive-models><model-selection><survival><feature-selection><lasso>',\n",
       " '<distributions><statistical-significance><population>',\n",
       " '<modeling><causal-inference>',\n",
       " '<logistic><ordinal><trend>',\n",
       " '<anova><sums-of-squares>',\n",
       " '<statistical-significance><categorical-data><proportion>',\n",
       " '<machine-learning><classification><error>',\n",
       " '<r><generalized-least-squares>',\n",
       " '<regression><correlation><pca><kernel><dimensionality-reduction>',\n",
       " '<reliability><psychometrics><inter-rater><intraclass-correlation>',\n",
       " '<r><data-visualization><random-forest><cart>',\n",
       " '<self-study><dataset><data-mining><cross-validation>',\n",
       " '<model-selection><nonlinear-regression><goodness-of-fit><likelihood-ratio>',\n",
       " '<r><distributions><kernel><density>',\n",
       " '<bootstrap><computational-statistics>',\n",
       " '<distributions><beta-distribution>',\n",
       " '<regression><mediation>',\n",
       " '<boxplot><bivariate>',\n",
       " '<r><regression><bias><dynamic-regression>',\n",
       " '<regression><time-series><econometrics><multicollinearity><bias>',\n",
       " '<r><mixed-model><variance><validation>',\n",
       " '<mathematical-statistics><fitting>',\n",
       " '<sampling><experiment-design><sample-size>',\n",
       " '<correlation><pca><mathematical-statistics><eigenvalues>',\n",
       " '<correlation><self-study><variance><unbiased-estimator>',\n",
       " '<modeling><dataset><data-mining><data-transformation>',\n",
       " '<repeated-measures><linear-model><residuals>',\n",
       " '<self-study><sampling><bias><non-response>',\n",
       " '<regression><ordinal>',\n",
       " '<r><random-generation>',\n",
       " '<regression><repeated-measures><references>',\n",
       " '<normal-distribution><expected-value>',\n",
       " '<r><regression><data-visualization><interaction><continuous-data>',\n",
       " '<normalization><pdf><logarithm><lognormal>',\n",
       " '<mean><standard-deviation><standard-error><basic-concepts>',\n",
       " '<bayesian><maximum-likelihood><model-comparison>',\n",
       " '<correlation><multivariate-analysis><spss>',\n",
       " '<classification><neural-networks><text-mining><measurement-error><performance>',\n",
       " '<self-study><pca><mathematical-statistics><regression-coefficients>',\n",
       " '<bayesian><outliers><gumbel>',\n",
       " '<classification><multivariate-analysis><information-theory><mutual-information><ensemble>',\n",
       " '<r><least-squares>',\n",
       " '<machine-learning><neural-networks><networks><self-organizing-maps>',\n",
       " '<pca><categorical-data>',\n",
       " '<em-algorithm>',\n",
       " '<algorithms><python><summary-statistics>',\n",
       " '<r><multivariate-analysis><cart>',\n",
       " '<hypothesis-testing><model-selection><chi-squared><maximum-likelihood><likelihood-ratio>',\n",
       " '<r><regression><bayesian><model-selection>',\n",
       " '<t-test>',\n",
       " '<machine-learning><fuzzy-set>',\n",
       " '<matlab><neural-networks>',\n",
       " '<t-test><small-sample><group-differences>',\n",
       " '<r><dataset><references><big-list>',\n",
       " '<probability><discrete-data>',\n",
       " '<r><anova><confidence-interval><lm>',\n",
       " '<regression><finite-population>',\n",
       " '<terminology><normalization>',\n",
       " '<nonparametric><kolmogorov-smirnov>',\n",
       " '<r><multiple-regression><lmer>',\n",
       " '<r><standard-deviation><biostatistics><error-propagation>',\n",
       " '<spss><forecasting><arima><random-forest>',\n",
       " '<time-series><predictive-models><seasonality>',\n",
       " '<correlation><standard-deviation><covariance>',\n",
       " '<machine-learning><maximum-likelihood><mcmc><graphical-model><gradient-descent>',\n",
       " '<time-series><econometrics><finance><macroeconomics>',\n",
       " '<correlation><rank-correlation><spearman-rho>',\n",
       " '<bootstrap><uncertainty>',\n",
       " '<regression><lasso><elastic-net><regularization>',\n",
       " '<r><sas><cox-model>',\n",
       " '<distributions><variance><binomial><proportion>',\n",
       " '<r><multilevel-analysis><lmer><regression-coefficients><standardization>',\n",
       " '<r><clustering><unsupervised-learning><segmentation><marketing>',\n",
       " '<r><regression><hypothesis-testing><contingency-tables>',\n",
       " '<hypothesis-testing><distance>',\n",
       " '<r><data-visualization><ggplot2>',\n",
       " '<time-series><variance-covariance>',\n",
       " '<classification><matlab><random-forest>',\n",
       " '<r><density-function><dirichlet-distribution>',\n",
       " '<multivariate-analysis><standard-deviation>',\n",
       " '<survival><cox-model><hazard>',\n",
       " '<r><anova><linear-model><ancova>',\n",
       " '<logistic><model-comparison><cox-model><logrank>',\n",
       " '<r><lmer><random-effects-model><mixed-effect>',\n",
       " '<regression><time-series><lags>',\n",
       " '<bayesian><hypothesis-testing><ab-test>',\n",
       " '<regression><correlation><multiple-regression><linear-model>',\n",
       " '<logistic><multinomial><multivariate-regression>',\n",
       " '<books><resources>',\n",
       " '<chi-squared><goodness-of-fit><power-analysis><roc>',\n",
       " '<time-series><multiple-comparisons><outliers>',\n",
       " '<r><regression><data-visualization><binomial><glmer>',\n",
       " '<confidence-interval><bootstrap><permutation>',\n",
       " '<r><time-series><cross-validation>',\n",
       " '<bootstrap><p-value>',\n",
       " '<machine-learning><predictive-models><gaussian-process>',\n",
       " '<multivariate-analysis>',\n",
       " '<time-series><anova><spss>',\n",
       " '<regression><mixed-model>',\n",
       " '<distributions><hypothesis-testing><variance-covariance><kullback-leibler><information-theory>',\n",
       " '<mixed-model><learning>',\n",
       " '<clustering><distance-functions><distance>',\n",
       " '<finance><notation>',\n",
       " '<repeated-measures><nonparametric>',\n",
       " '<r><repeated-measures><longitudinal><nested>',\n",
       " '<sas><binomial><reliability><frequency>',\n",
       " '<hypothesis-testing><anova><variance><t-test>',\n",
       " '<self-study><umvue>',\n",
       " '<algorithms><mathematical-statistics><moments>',\n",
       " '<hypothesis-testing><binomial><censoring>',\n",
       " '<statistical-significance><modeling><likelihood-function>',\n",
       " '<distributions><mixed-model><sas><monte-carlo><weighted-sampling>',\n",
       " '<regression><multiple-regression><outliers>',\n",
       " '<regression><data-mining><references><large-data>',\n",
       " '<multiple-regression><residuals><residual-analysis>',\n",
       " '<experiment-design><matching>',\n",
       " '<r><cointegration><ranks>',\n",
       " '<multivariate-analysis><conditional-expectation>',\n",
       " '<spss><experiment-design>',\n",
       " '<books><software><learning>',\n",
       " '<r><correlation><count-data><contingency-tables><association-measure>',\n",
       " '<statistical-significance><p-value><kolmogorov-smirnov><model-comparison>',\n",
       " '<r><mathematical-statistics><integral>',\n",
       " '<anova><data-transformation><logarithm>',\n",
       " '<bayesian><discrete-data>',\n",
       " '<multilevel-analysis><covariance>',\n",
       " '<multivariate-analysis><distance-functions><discriminant-analysis><linear-algebra>',\n",
       " '<bootstrap><permutation>',\n",
       " '<multiple-regression><least-squares>',\n",
       " '<r><survival><hazard><cox-model>',\n",
       " '<modeling>',\n",
       " '<multivariate-analysis><poisson>',\n",
       " '<matlab><mathematica>',\n",
       " '<regression><statistical-significance><bootstrap>',\n",
       " '<nonparametric><dirichlet-distribution><topic-models><dirichlet-process>',\n",
       " '<distributions><sampling><kullback-leibler>',\n",
       " '<r><logistic><spss>',\n",
       " '<validation>',\n",
       " '<uncertainty><notation>',\n",
       " '<estimation><maximum-likelihood><econometrics><asymptotics>',\n",
       " '<confidence-interval><cross-validation><prediction><prediction-interval>',\n",
       " '<confounding><definition>',\n",
       " '<probability><distributions><order-statistics><joint-distribution>',\n",
       " '<spss><logistic>',\n",
       " '<regression><clustering><sampling>',\n",
       " '<time-series><hypothesis-testing><data-mining><markov-process><censoring>',\n",
       " '<distributions><t-test><javascript>',\n",
       " '<regression><prediction-interval>',\n",
       " '<r><probability><basic-concepts><population>',\n",
       " '<hypothesis-testing><correlation><dataset><autocorrelation><dependence>',\n",
       " '<r><machine-learning><hypothesis-testing><cross-validation><references>',\n",
       " '<r><normal-distribution>',\n",
       " '<classification><multivariate-analysis><distance-functions>',\n",
       " '<regression><multiple-regression><weighted-regression>',\n",
       " '<predictive-models><model-selection><sas>',\n",
       " '<r><regression><diagnostic><quantile-regression>',\n",
       " '<fitting><em-algorithm><iid>',\n",
       " '<r><pca><missing-data>',\n",
       " '<bayesian><simulation><computational-statistics>',\n",
       " '<sample-size><manova>',\n",
       " '<hypothesis-testing><inter-rater><kappa>',\n",
       " '<nonparametric><likert><psychometrics>',\n",
       " '<dimensionality-reduction><pca>',\n",
       " '<machine-learning><classification><data-mining><svm><boosting>',\n",
       " '<r><optimization><library><package>',\n",
       " '<r><distributions><machine-learning><classification>',\n",
       " '<poisson><java>',\n",
       " '<probability><self-study><mathematical-statistics>',\n",
       " '<r><regression><software><poisson-process>',\n",
       " '<spss><multilevel-analysis>',\n",
       " '<correlation><spss><pearson>',\n",
       " '<optimization><regularization><learning>',\n",
       " '<statistical-significance><confidence-interval><wilcoxon><paired-data>',\n",
       " '<time-series><factor-analysis>',\n",
       " '<sampling><cluster-sample>',\n",
       " '<p-value><meta-analysis><genetics>',\n",
       " '<partitioning>',\n",
       " '<r><mixed-model><nlme><toeplitz>',\n",
       " '<machine-learning><classification><roc><learning>',\n",
       " '<hypothesis-testing><t-test><permutation>',\n",
       " '<bayesian><kde>',\n",
       " '<power><ancova>',\n",
       " '<optimization><svm><python>',\n",
       " '<distributions><histogram>',\n",
       " '<p-value><references>',\n",
       " '<r><mixed-model><random-effects-model><lme>',\n",
       " '<distributions><probability><reliability><project-management>',\n",
       " '<forecasting><excel><exponential-smoothing>',\n",
       " '<regression><self-study><model-selection><nonlinear-regression>',\n",
       " '<time-series><modeling><forecasting><arima>',\n",
       " '<multiple-regression><experiment-design>',\n",
       " '<r><mixed-model><interaction><longitudinal><splines>',\n",
       " '<correlation><terminology>',\n",
       " '<r><regression><machine-learning><logistic>',\n",
       " '<estimation><least-squares><robust>',\n",
       " '<anova><repeated-measures><interaction><contrasts>',\n",
       " '<clustering><mixed-model><panel-data><longitudinal><fixed-effects-model>',\n",
       " '<logistic><continuous-data>',\n",
       " '<r><boxplot>',\n",
       " '<sampling><theory>',\n",
       " '<nonparametric><python><outliers><matplotlib>',\n",
       " '<lasso><model><cox-model>',\n",
       " '<mixed-model><nonparametric-bayes>',\n",
       " '<correlation><average><spearman>',\n",
       " '<multivariate-analysis><variance-covariance><matrix-inverse><hotelling>',\n",
       " '<r><modeling><logistic><gradient-descent>',\n",
       " '<r><chi-squared><missing-data><epidemiology><fishersexact>',\n",
       " '<generalized-linear-model><interpretation><aic><bic>',\n",
       " '<hypothesis-testing><bootstrap><p-value>',\n",
       " '<genetics><permutation><multiple-comparisons>',\n",
       " '<regression><multiple-regression><large-data><fitting>',\n",
       " '<interpretation><chow-test>',\n",
       " '<r><bayesian>',\n",
       " '<pca><generalized-linear-model><feature-selection>',\n",
       " '<self-study><random-variable><marginal><joint-distribution>',\n",
       " '<variance><proportion><hypothesis-testing>',\n",
       " '<r><interpretation><ridge-regression>',\n",
       " '<machine-learning><matlab><markov-process><hidden-markov-model>',\n",
       " '<r><sample-size><epidemiology>',\n",
       " '<r><data-visualization><confidence-interval><interaction>',\n",
       " '<r><estimation><missing-data><kernel><density>',\n",
       " '<r><confidence-interval><books><effect-size>',\n",
       " '<regression><correlation><causal-inference>',\n",
       " '<anova><statistical-significance><multivariate-analysis><factor-analysis>',\n",
       " '<scales><reliability><likert><psychology>',\n",
       " '<e-m><intuition>',\n",
       " '<variance><optimization>',\n",
       " '<r><confidence-interval><chi-squared>',\n",
       " '<machine-learning><variance><cart>',\n",
       " '<modeling><interaction>',\n",
       " '<bootstrap><simulation><panel-data>',\n",
       " '<psychometrics><irt><information>',\n",
       " '<data-mining><references><finance><application>',\n",
       " '<psychometrics><inter-rater><rating>',\n",
       " '<median><robust>',\n",
       " '<r><books>',\n",
       " '<repeated-measures><longitudinal>',\n",
       " '<hypothesis-testing><geostatistics>',\n",
       " '<probability><theory>',\n",
       " '<r><standard-deviation><predictive-models><simulation>',\n",
       " '<regression><self-study><lasso><regularization><gradient-descent>',\n",
       " '<probability><self-study><normal-distribution>',\n",
       " '<best-practices><open-source>',\n",
       " '<python><cart><boosting>',\n",
       " '<covariance>',\n",
       " '<regression><maximum-likelihood><heteroscedasticity><errors-in-variables>',\n",
       " '<regression><interaction>',\n",
       " '<time-series><correlation><multivariate-analysis><repeated-measures><meta-analysis>',\n",
       " '<statistical-significance><panel-data>',\n",
       " '<distributions><power-law>',\n",
       " '<estimation><spatial><interpolation><k-nearest-neighbour>',\n",
       " '<probability><modeling><data-mining><predictive-models>',\n",
       " '<svm><optimization>',\n",
       " '<blog>',\n",
       " '<regression><logistic><residuals><binary-data>',\n",
       " '<scales><ranking><ranks>',\n",
       " '<modeling><poisson><large-data>',\n",
       " '<regression><bayesian><multiple-regression><multilevel-analysis>',\n",
       " '<distributions><queueing><interarrival-time>',\n",
       " '<estimation><robust>',\n",
       " '<books><bootstrap><resampling>',\n",
       " '<regression><correlation><modeling><instrumental-variables><identifiability>',\n",
       " '<r><machine-learning><caret><ensemble>',\n",
       " '<self-study><likelihood-ratio>',\n",
       " '<sampling><simulation>',\n",
       " '<machine-learning><kalman-filter><graphical-model>',\n",
       " '<survival><weibull>',\n",
       " '<hypothesis-testing><contingency-tables>',\n",
       " '<r><statistical-significance><ordinal><hypothesis-testing>',\n",
       " '<bayesian><rejection-sampling><posterior><abc>',\n",
       " '<r><regression><assumptions><homogeneity>',\n",
       " '<machine-learning><classification><text-mining>',\n",
       " '<distributions><lognormal><bounds>',\n",
       " '<r><survival><epidemiology><censoring><nested>',\n",
       " '<r><correlation>',\n",
       " '<pie-chart>',\n",
       " '<distributions><correlation><covariance><sample>',\n",
       " '<machine-learning><classification><data-mining><nlp><topic-models>',\n",
       " '<statistical-significance><polling>',\n",
       " '<svm><notation>',\n",
       " '<probability><expected-value><philosophical>',\n",
       " '<teaching><validity>',\n",
       " '<classification><svm><multilevel-analysis>',\n",
       " '<machine-learning><classification><cross-validation><weka>',\n",
       " '<split-plot><nested>',\n",
       " '<scales><missing-data><data-imputation>',\n",
       " '<gbm>',\n",
       " '<categorical-data><logistic><multiple-comparisons><chi-squared>',\n",
       " '<correlation><mean><likert><binary-data>',\n",
       " '<variance><standard-deviation><online><summations>',\n",
       " '<sample><terminology><definition>',\n",
       " '<spss><reliability>',\n",
       " '<regression><probability><variance><stochastic-processes>',\n",
       " '<data-transformation><categorical-data><generalized-linear-model><assumptions>',\n",
       " '<probability><simulation><conditional-probability><scipy><numpy>',\n",
       " '<regression><model-selection><multiple-regression>',\n",
       " '<r><arima><matrix><matrix-decomposition>',\n",
       " '<machine-learning><reinforcement-learning><multiarmed-bandit>',\n",
       " '<r><maximum-likelihood><optimization>',\n",
       " '<r><interaction><meta-analysis>',\n",
       " '<random-forest><algorithms><performance>',\n",
       " '<distributions><data-visualization><summary-statistics>',\n",
       " '<model-selection><logistic><aic>',\n",
       " '<repeated-measures><pca><scales>',\n",
       " '<sweave><presentation>',\n",
       " '<repeated-measures><spss><multivariate-analysis><gee>',\n",
       " '<self-study><odds-ratio>',\n",
       " '<spss><interaction><interpretation><gee>',\n",
       " '<mathematics><books><mathematical-statistics>',\n",
       " '<r><kernel><java><kde>',\n",
       " '<estimation><simulation><monte-carlo><queueing><operations-research>',\n",
       " '<machine-learning><predictive-models><svm><feature-selection>',\n",
       " '<time-series><predictive-models><model-selection><multiple-regression>',\n",
       " '<probability><sampling>',\n",
       " '<spss><meta-analysis><effect-size>',\n",
       " '<probability><normal-distribution><joint-distribution><copula>',\n",
       " '<r><correlation><matrix><maximum>',\n",
       " '<regression><anova><ancova><unbalanced-classes><sums-of-squares>',\n",
       " '<chi-squared><nonparametric><contingency-tables><association-measure>',\n",
       " '<statistical-significance><web>',\n",
       " '<clinical-trials>',\n",
       " '<regression><time-series><linear-model>',\n",
       " '<r><self-study><multidimensional-scaling>',\n",
       " '<bayesian><basic-concepts>',\n",
       " '<distributions><probability><bayesian>',\n",
       " '<time-series><kalman-filter><linear-algebra>',\n",
       " '<r><regression><errors-in-variables>',\n",
       " '<probability><hypothesis-testing><ab-test>',\n",
       " '<r><statistical-significance>',\n",
       " '<r><distributions><variance><measurement>',\n",
       " '<interpretation><aic><bic>',\n",
       " '<probability><statistical-significance><sampling>',\n",
       " '<random-variable><feature-construction><engineering-statistics>',\n",
       " '<regression><algorithms><missing-data><variance-covariance>',\n",
       " '<r><multiple-regression><nonlinear-regression>',\n",
       " '<sampling><polling>',\n",
       " '<data-cleaning>',\n",
       " '<r><factor-analysis><categorical-data><ordinal>',\n",
       " '<bayesian><winbugs><error-message>',\n",
       " '<r><regression><machine-learning><cross-validation><validation>',\n",
       " '<correlation><spearman>',\n",
       " '<r><data-visualization><k-nearest-neighbour>',\n",
       " '<probability><bayesian><decision-theory>',\n",
       " '<classification><k-means><discriminant-analysis><unsupervised-learning>',\n",
       " '<robust>',\n",
       " '<interpretation><effect-size><rule-of-thumb>',\n",
       " '<hypothesis-testing><forecasting>',\n",
       " '<data-mining><feature-selection>',\n",
       " '<variance><mean><normalization>',\n",
       " '<ranking><weighted-mean>',\n",
       " '<algorithms><random-variable><random-generation>',\n",
       " '<r><ordered-variables>',\n",
       " '<curves>',\n",
       " '<missing-data><cox-model>',\n",
       " '<distributions><sampling><variance><covariance>',\n",
       " '<r><anova><multiple-comparisons><lme>',\n",
       " '<computational-statistics><model>',\n",
       " '<r><regression><multiple-regression><fitting>',\n",
       " '<r><covariance><variance-covariance>',\n",
       " '<probability><normal-distribution><variance>',\n",
       " '<interpretation><lognormal><qq-plot>',\n",
       " '<machine-learning><roc><auc>',\n",
       " '<survival><change-point><cox-model>',\n",
       " '<t-test><mann-whitney-u-test>',\n",
       " '<distributions><self-study>',\n",
       " '<logistic>',\n",
       " '<r><cart><boosting>',\n",
       " '<sampling><variance><survey><finite-population>',\n",
       " '<bayesian><methodology>',\n",
       " '<distributions><t-distribution><hotelling>',\n",
       " '<software><cdf><pareto-distribution>',\n",
       " '<clustering><variance><optimal-stopping><hac>',\n",
       " '<r><panel-data><multilevel-analysis>',\n",
       " '<error><logistic>',\n",
       " '<r><bayesian><bugs><frequentist><winbugs>',\n",
       " '<regression><machine-learning><logistic><gradient-descent>',\n",
       " '<multilevel-analysis><information><fisher-information>',\n",
       " '<modeling><censoring>',\n",
       " '<forecasting><exponential-smoothing>',\n",
       " '<probability><correlation><mathematical-statistics><expected-value>',\n",
       " '<regression><categorical-data><interpolation>',\n",
       " '<text-mining><natural-language><nlp>',\n",
       " '<clustering><bootstrap>',\n",
       " '<regression><logistic><modeling><generalized-linear-model><maximum-likelihood>',\n",
       " '<r><regression><generalized-linear-model><r-squared>',\n",
       " '<model-selection><multivariate-regression>',\n",
       " '<r><estimation><poisson><maximum-likelihood><em-algorithm>',\n",
       " '<regression><normal-distribution><probit>',\n",
       " '<self-study><normal-distribution><binomial>',\n",
       " '<distributions><standard-deviation>',\n",
       " '<spss><sem><mediation>',\n",
       " '<jmp><tukey-hsd>',\n",
       " '<interaction><effect-size>',\n",
       " '<forecasting>',\n",
       " '<dataset><spss><binomial>',\n",
       " '<standard-deviation><logarithm><quantiles><self-study>',\n",
       " '<statistical-significance><cart>',\n",
       " '<self-study><biostatistics><probability><bioinformatics>',\n",
       " '<distributions><randomness>',\n",
       " '<correlation><t-test><power-analysis>',\n",
       " '<machine-learning><neural-networks><spatio-temporal>',\n",
       " '<data-visualization><networks><neuroimaging>',\n",
       " '<correlation><psychometrics><scales>',\n",
       " '<regression><bias><autoregressive><nested>',\n",
       " '<statistical-significance><mean>',\n",
       " '<data-visualization><confidence-interval><uncertainty>',\n",
       " '<stata><reliability><inter-rater><intraclass-correlation>',\n",
       " '<time-series><survival><count-data>',\n",
       " '<self-study><confounding>',\n",
       " '<autocorrelation><multinomial><nominal>',\n",
       " '<r><distance>',\n",
       " '<sampling><dataset>',\n",
       " '<correlation><statistical-significance><chi-squared>',\n",
       " '<regression><econometrics><logarithm><treatment-effect>',\n",
       " '<r><time-series><dlm>',\n",
       " '<regression><multiple-regression><regression-coefficients>',\n",
       " '<r><correlation><sampling><multivariate-analysis>',\n",
       " '<regression><logarithm>',\n",
       " '<r><confidence-interval><ggplot2><scatterplot>',\n",
       " '<correlation><conditional-probability><causal-inference>',\n",
       " '<random-variable><method-of-moments><weibull>',\n",
       " '<chi-squared><goodness-of-fit><nonlinear-regression>',\n",
       " '<regression><statistical-significance><interpretation>',\n",
       " '<r><scoring>',\n",
       " '<sas><variance-covariance>',\n",
       " '<distributions><probability><poisson>',\n",
       " '<regression><confidence-interval><nonparametric><references>',\n",
       " '<r><regression><random-generation>',\n",
       " '<logistic><anova><chi-squared><generalized-linear-model>',\n",
       " '<time-series><algorithms>',\n",
       " '<variance><sampling><monte-carlo>',\n",
       " '<machine-learning><books><predictive-models>',\n",
       " '<survey><experiment-design>',\n",
       " '<r><regression><confidence-interval><mathematical-statistics>',\n",
       " '<panel-data><simulation>',\n",
       " '<books><references>',\n",
       " '<distributions><mathematical-statistics><beta-binomial><proof>',\n",
       " '<correlation><spearman><pearson>',\n",
       " '<r><probability><conditional-probability>',\n",
       " '<stata><text-mining>',\n",
       " '<machine-learning><bayesian><references><gibbs>',\n",
       " '<probability><self-study><normality><naive-bayes>',\n",
       " '<r><confidence-interval><generalized-linear-model><prediction>',\n",
       " '<spatial><kolmogorov-smirnov><kullback-leibler>',\n",
       " '<time-series><outliers><robust><mad>',\n",
       " '<pca><smoothing><svd>',\n",
       " '<r><matrix><distance>',\n",
       " '<correlation><variance><garch>',\n",
       " '<regression><goodness-of-fit>',\n",
       " '<machine-learning><terminology><ensemble>',\n",
       " '<tobit-regression><interval-censoring>',\n",
       " '<sas><sample-size><power>',\n",
       " '<correlation><bayesian><jags><bugs>',\n",
       " '<confidence-interval><normal-distribution><estimation><central-limit-theorem>',\n",
       " '<variance><meta-analysis>',\n",
       " '<time-series><estimation><maximum-likelihood><standard-error><arma>',\n",
       " '<time-series><anova><t-test><heteroscedasticity>',\n",
       " '<confidence-interval><sampling><sample-size><survey>',\n",
       " '<scales><psychometrics><rating>',\n",
       " '<distributions><data-visualization><self-study><graphical-model>',\n",
       " '<time-series><python><curve-fitting>',\n",
       " '<auc>',\n",
       " '<r><bayesian><confidence-interval><conditional-probability>',\n",
       " '<sampling><small-sample>',\n",
       " '<estimation><nonparametric><kernel>',\n",
       " '<goodness-of-fit><residuals>',\n",
       " '<data-visualization><effect-size>',\n",
       " '<data-visualization><nonparametric><excel><ranking>',\n",
       " '<binomial><standard-error>',\n",
       " '<time-series><repeated-measures>',\n",
       " '<bayesian><bugs>',\n",
       " '<clustering><bayesian>',\n",
       " '<binomial><standard-deviation>',\n",
       " '<r><time-series><hypothesis-testing><spss><entropy>',\n",
       " '<machine-learning><hidden-markov-model><kalman-filter><pattern-recognition><graphical-model>',\n",
       " '<data-transformation><normality><skewness><kurtosis>',\n",
       " '<logistic><survey><clogit>',\n",
       " '<aggregation>',\n",
       " '<spss><survival>',\n",
       " '<confidence-interval><outliers><median>',\n",
       " '<regression><feature-selection><algorithms><random-forest><theory>',\n",
       " '<least-squares><point-process><similarities>',\n",
       " '<bayesian><history><philosophical>',\n",
       " '<bayesian><classification><naive-bayes><pattern-recognition>',\n",
       " '<distributions><kolmogorov-smirnov><java>',\n",
       " '<regression><spss><panel-data>',\n",
       " '<data-visualization><splines>',\n",
       " '<multiple-comparisons><post-hoc>',\n",
       " '<finance><prediction><copula>',\n",
       " '<regression><statistical-significance><covariance><randomization>',\n",
       " '<self-study><standard-deviation><p-value>',\n",
       " '<anova><standard-deviation><weighted-mean><partitioning>',\n",
       " '<likert><scales><weighted-mean>',\n",
       " '<confidence-interval><chi-squared><histogram>',\n",
       " '<confidence-interval><random-variable><bootstrap><random-generation>',\n",
       " '<machine-learning><classification><weka>',\n",
       " '<distributions><summary-statistics><proof><empirical>',\n",
       " '<bayesian><classification><matlab>',\n",
       " '<probability><bayesian><maximum-likelihood><references>',\n",
       " '<regression><missing-data><gee><ordered-variables>',\n",
       " '<books><big-list><linear-algebra>',\n",
       " '<python><hidden-markov-model>',\n",
       " '<distributions><hypothesis-testing><mathematical-statistics><stochastic-processes>',\n",
       " '<machine-learning><curve-fitting><functional-data-analysis>',\n",
       " '<r><longitudinal>',\n",
       " '<ridge-regression>',\n",
       " '<smoothing><histogram>',\n",
       " '<time-series><outliers><hypothesis-testing><prediction><quality-control>',\n",
       " '<conditional-probability><bayes-network>',\n",
       " '<mathematical-statistics><average>',\n",
       " '<clustering><image-processing><online>',\n",
       " '<normal-distribution><inference>',\n",
       " '<independence><weighted-sampling>',\n",
       " '<r><lmer>',\n",
       " '<time-series><mse>',\n",
       " '<regression><logistic><cross-validation>',\n",
       " '<categorical-data><survey><interaction>',\n",
       " '<r><clustering><cross-validation><model-based-clustering>',\n",
       " '<machine-learning><logistic><estimation><maximum-likelihood>',\n",
       " '<r><regression><residuals><linear-model>',\n",
       " '<scales><causal-inference><sem><observational-study>',\n",
       " '<spss><interpretation>',\n",
       " '<time-series><causal-inference><filter>',\n",
       " '<hypothesis-testing><chi-squared><goodness-of-fit><uniform><dice>',\n",
       " '<time-series><bayesian><bugs><likelihood-function><likelihood>',\n",
       " '<econometrics><spatial>',\n",
       " '<r><time-series><forecasting><exponential-smoothing>',\n",
       " '<interpretation><variance-covariance>',\n",
       " '<r><time-series><chi-squared><arima><parametric>',\n",
       " '<hypothesis-testing><statistical-significance><wilcoxon><mann-whitney-u-test>',\n",
       " '<distributions><probability><matlab><independence><moments>',\n",
       " '<r><pareto-distribution>',\n",
       " '<time-series><data-visualization><python><matplotlib>',\n",
       " '<bayesian><modeling><sparse>',\n",
       " '<time-series><regression><correlation>',\n",
       " '<standard-deviation><basic-concepts>',\n",
       " '<optimization><curve-fitting><white-noise>',\n",
       " '<distributions><modeling><goodness-of-fit><pdf>',\n",
       " '<repeated-measures><clinical-trials><multivariate-regression><change-scores>',\n",
       " '<anova><books>',\n",
       " '<self-study><normal-distribution>',\n",
       " '<r><experiment-design><split-plot>',\n",
       " '<regression><time-series><arima>',\n",
       " '<time-series><aggregation>',\n",
       " '<r><modeling><multiple-regression><sem>',\n",
       " '<regression-coefficients>',\n",
       " '<machine-learning><model-comparison><bic><hidden-markov-model>',\n",
       " '<normal-distribution><outliers><mixture>',\n",
       " '<r><confidence-interval><binomial>',\n",
       " '<calibration>',\n",
       " '<spss><excel>',\n",
       " '<probability><multivariate-analysis><normal-distribution>',\n",
       " '<r><factor-analysis><likert>',\n",
       " '<data-transformation><econometrics>',\n",
       " '<fitting><lognormal>',\n",
       " '<clustering><k-means><multidimensional-scaling><metric>',\n",
       " '<bonferroni>',\n",
       " '<statistical-significance><genetics>',\n",
       " '<classification><random-forest><cart>',\n",
       " '<smoothing><average><weighted-mean>',\n",
       " '<meta-analysis><standard-error><effect-size><funnel-plot><publication-bias>',\n",
       " '<data-mining>',\n",
       " '<normalization><signal-processing>',\n",
       " '<mixed-model><repeated-measures><experiment-design>',\n",
       " '<time-series><survival><online>',\n",
       " '<distributions><sampling><dataset>',\n",
       " '<statistical-significance><sample-size><survey><t-test><power-analysis>',\n",
       " '<correlation><modeling><dataset>',\n",
       " '<data-visualization><confidence-interval><software><meta-analysis>',\n",
       " '<distributions><degrees-of-freedom><students-t>',\n",
       " '<meta-analysis><clinical-trials>',\n",
       " '<chi-squared><categorical-data><generalized-linear-model><contingency-tables><proportion>',\n",
       " '<predictive-models><error><measurement><measurement-error>',\n",
       " '<r><data-visualization><correlation><eda>',\n",
       " '<hypothesis-testing><statistical-significance><summary-statistics>',\n",
       " '<entropy><information-theory>',\n",
       " '<regression><mixed-model><multiple-comparisons><ancova><neuroimaging>',\n",
       " '<r><time-series><arima><convergence>',\n",
       " '<modeling><sampling><conditional-probability><gibbs>',\n",
       " '<r><wilcoxon>',\n",
       " '<r><mixed-model><interpretation><longitudinal>',\n",
       " '<regression><logistic><validation><train>',\n",
       " '<poisson><negative-binomial><gamma-distribution><exponential-family>',\n",
       " '<probability><graphical-model>',\n",
       " '<mean><terminology><average><notation>',\n",
       " '<regression><bayesian><missing-data>',\n",
       " '<time-series><machine-learning><neural-networks>',\n",
       " '<data-visualization><continuous-data><correspondence-analysis>',\n",
       " '<chi-squared><nonparametric><marginal><ordered-variables>',\n",
       " '<sampling><monte-carlo><latin-square>',\n",
       " '<regression><multiple-regression><multicollinearity>',\n",
       " '<self-study><normal-distribution><chi-squared>',\n",
       " '<categorical-data><continuous-data><data-transformation>',\n",
       " '<r><generalized-linear-model><glmnet>',\n",
       " '<hypothesis-testing><books>',\n",
       " '<arima><randomness>',\n",
       " '<probability><stochastic-processes><pdf><asymptotics><graph-theory>',\n",
       " '<machine-learning><neural-networks>',\n",
       " '<bayesian><multilevel-analysis><conjugate-prior>',\n",
       " '<dataset><data-transformation><normalization>',\n",
       " '<machine-learning><clustering><classification><text-mining>',\n",
       " '<r><generalized-linear-model><linear-model><gamma-distribution><link-function>',\n",
       " '<anova><t-test><multiple-comparisons><within-subjects>',\n",
       " '<data-visualization><data-transformation>',\n",
       " '<normal-distribution><nonparametric><t-test><central-limit-theorem><wilcoxon>',\n",
       " '<r><distributions><probability><pdf><dirichlet-distribution>',\n",
       " '<random-variable><random-generation>',\n",
       " '<r><data-visualization><geometry>',\n",
       " '<software><control-chart>',\n",
       " '<hypothesis-testing><statistical-significance><chi-squared><summary-statistics><effect-size>',\n",
       " '<predictive-models><negative-binomial><incidence-rate-ratio>',\n",
       " '<r><nonlinear-regression>',\n",
       " '<categorical-data><scales><survey>',\n",
       " '<regression><pca><factor-analysis>',\n",
       " '<distributions><continuous-data><case-control-study>',\n",
       " '<confidence-interval><monte-carlo><standard-error><simulation>',\n",
       " '<r><python><random-generation>',\n",
       " '<time-series><self-study><forecasting><descriptive-statistics><particle-filter>',\n",
       " '<machine-learning><pca><matlab>',\n",
       " '<hypothesis-testing><distributions><poisson><goodness-of-fit>',\n",
       " '<r><data-visualization><jags><bugs>',\n",
       " '<sampling><games><probability>',\n",
       " '<matlab><python><mixture>',\n",
       " '<hypothesis-testing><statistical-significance><markov-process>',\n",
       " '<r><generalized-linear-model>',\n",
       " '<statistical-significance><independence>',\n",
       " '<finance><garch><arma>',\n",
       " '<variance><random-variable><sample><hypothesis-testing>',\n",
       " '<experiment-design><epidemiology>',\n",
       " '<dataset><outliers><checking>',\n",
       " '<data-visualization><anova><mixed-model><standard-error>',\n",
       " '<multivariate-analysis><finance><moments><conditional-expectation>',\n",
       " '<machine-learning><predictive-models><terminology>',\n",
       " '<text-mining><topic-models>',\n",
       " '<bayesian><estimation><normal-distribution>',\n",
       " '<probability><correlation><bayesian>',\n",
       " '<multiple-regression><least-squares><multicollinearity>',\n",
       " '<estimation><error><application>',\n",
       " '<machine-learning><classification><data-mining><text-mining>',\n",
       " '<r><generalized-linear-model><nested>',\n",
       " '<logistic><data-transformation>',\n",
       " '<time-series><correlation><hypothesis-testing><independence>',\n",
       " '<kalman-filter>',\n",
       " '<references><quotation>',\n",
       " '<machine-learning><mathematical-statistics>',\n",
       " '<mixed-model><sas><covariance><genetics><variance-covariance>',\n",
       " '<generalized-linear-model><multinomial><proportion><logit><log-linear>',\n",
       " '<r><confidence-interval>',\n",
       " '<distributions><estimation><scale-invariance>',\n",
       " '<regression><assumptions><heteroscedasticity>',\n",
       " '<probability><normal-distribution><pdf><bivariate>',\n",
       " '<classification><feature-selection><naive-bayes><entropy><information>',\n",
       " '<smoothing><kalman-filter>',\n",
       " '<self-study><sample><population>',\n",
       " '<dataset><sql>',\n",
       " '<r><logistic><python>',\n",
       " '<machine-learning><computational-statistics><recommender-system>',\n",
       " '<confidence-interval><multivariate-analysis><multiple-comparisons>',\n",
       " '<classification><error-propagation>',\n",
       " '<sampling><t-test><survey>',\n",
       " '<prior><ridge-regression>',\n",
       " '<bayesian><confidence-interval><inference><residual-analysis>',\n",
       " '<multivariate-analysis><ancova><covariance>',\n",
       " '<categorical-data><experiment-design>',\n",
       " '<normal-distribution><poisson><random-generation><anderson-darling>',\n",
       " '<hazard><cox-model>',\n",
       " '<regression><time-series><anova><repeated-measures>',\n",
       " '<stata><least-squares><hypothesis-testing>',\n",
       " '<spss><binomial><model>',\n",
       " '<time-series><variance><reliability><kolmogorov-smirnov><resampling>',\n",
       " '<regression><spss><model-selection><ordinal>',\n",
       " '<econometrics><optimization>',\n",
       " '<r><curve-fitting><nonlinear-regression>',\n",
       " '<correlation><effect-size>',\n",
       " '<splus>',\n",
       " '<clustering><classification><data-mining><ordinal>',\n",
       " '<regression><logistic><r-squared>',\n",
       " '<distributions><clustering><skewness><distance>',\n",
       " '<logistic><statistical-significance><model-comparison>',\n",
       " '<bayesian><graphical-model><expectation-maximization>',\n",
       " '<r><regression><ordinal><predictor>',\n",
       " '<r><nonparametric><kolmogorov-smirnov>',\n",
       " '<outliers>',\n",
       " '<hypothesis-testing><anova><spss><repeated-measures>',\n",
       " '<classification><k-nearest-neighbour>',\n",
       " '<probability><books><learning>',\n",
       " '<distributions><confidence-interval><normal-distribution>',\n",
       " '<time-series><bias>',\n",
       " '<classification><svm><neural-networks><unbalanced-classes>',\n",
       " '<r><machine-learning><classification><rbm>',\n",
       " '<hypothesis-testing><maximum>',\n",
       " '<distributions><sampling><teaching>',\n",
       " '<mixed-model><logistic><interaction><interpretation>',\n",
       " '<r><permutation><paired-data>',\n",
       " '<modeling><regression><multilevel-analysis>',\n",
       " '<mean><average>',\n",
       " '<statistical-significance><data-mining><quality-control><history><functional-data-analysis>',\n",
       " '<time-series><seasonality>',\n",
       " '<machine-learning><classification><naive-bayes>',\n",
       " '<factor-analysis><confirmatory-factor>',\n",
       " '<spss><regression-coefficients><path-model>',\n",
       " '<generalized-linear-model><multinomial><residuals>',\n",
       " '<probability><hypothesis-testing>',\n",
       " '<bayesian><particle-filter><kalman-filter>',\n",
       " '<correlation><tables><reporting>',\n",
       " '<bayesian><normal-distribution><likelihood-function>',\n",
       " '<r><data-visualization><confidence-interval><roc><ggplot2>',\n",
       " '<classification><neural-networks><feature-selection><hyperparameter>',\n",
       " '<terminology><biostatistics>',\n",
       " '<pca><mean><python><numpy>',\n",
       " '<sample-size><polling>',\n",
       " '<regression><forecasting><gls>',\n",
       " '<r><variance><group-differences><microarray>',\n",
       " '<variance><cart>',\n",
       " '<regression><machine-learning><pls>',\n",
       " '<distributions><mathematical-statistics>',\n",
       " '<r><confidence-interval><jags><multilevel-analysis>',\n",
       " '<ordered-variables>',\n",
       " '<regression><forecasting>',\n",
       " '<confidence-interval><t-distribution>',\n",
       " '<time-series><logistic><classification><forecasting>',\n",
       " '<r><outliers><residuals>',\n",
       " '<modeling><best-practices><rule-of-thumb><eda>',\n",
       " '<chi-squared><goodness-of-fit><kolmogorov-smirnov>',\n",
       " '<machine-learning><cross-validation><bootstrap><random-forest>',\n",
       " '<self-study><conditional-probability><probability>',\n",
       " '<r><anova><statistical-significance>',\n",
       " '<multiple-regression><model-selection><interaction>',\n",
       " '<data-mining><algorithms><neural-networks><markov-process>',\n",
       " '<lasso><generalized-least-squares><lars>',\n",
       " '<r><biostatistics><bioinformatics>',\n",
       " '<regression><log-linear>',\n",
       " '<r><modeling><caret><glmnet>',\n",
       " '<r><t-test>',\n",
       " '<time-series><correlation><finance>',\n",
       " '<normal-distribution><mean><median>',\n",
       " '<careers><phd><academia>',\n",
       " '<distributions><poisson><beta>',\n",
       " '<clinical-trials><randomization>',\n",
       " '<bayesian><pca>',\n",
       " '<kalman-filter><data-association>',\n",
       " '<variance><moments><mgf>',\n",
       " '<r><distributions><bayesian><jags><bugs>',\n",
       " '<estimation><kernel><reliability><kde>',\n",
       " '<probability><estimation><predictive-models>',\n",
       " '<distributions><beta>',\n",
       " '<regression><statistical-significance><model-selection><categorical-data>',\n",
       " '<big-list><application>',\n",
       " '<time-series><variance><bootstrap>',\n",
       " '<jags><probit><ordered-variables>',\n",
       " '<machine-learning><classification><data-mining><logistic><stratification>',\n",
       " '<r><interaction><contrasts>',\n",
       " '<mixed-model><simulation><fiducial>',\n",
       " '<machine-learning><distributions><neural-networks><networks>',\n",
       " '<r><regression><mcmc>',\n",
       " '<cross-validation><linear-model>',\n",
       " '<regression><rating>',\n",
       " '<software><best-practices><feature-construction>',\n",
       " '<bayesian><normal-distribution><svm><bivariate>',\n",
       " '<r><overdispersion><beta-binomial><quasi-binomial>',\n",
       " '<estimation><nonlinear-regression>',\n",
       " '<maximum-likelihood><least-squares>',\n",
       " '<clustering><standard-error><panel-data>',\n",
       " '<self-study><moments><function>',\n",
       " '<correlation><self-study>',\n",
       " '<anova><variance>',\n",
       " '<conditional-probability><gamma-distribution>',\n",
       " '<normal-distribution><binomial>',\n",
       " '<bayesian><likelihood-function>',\n",
       " '<r><clustering><microarray>',\n",
       " '<r><time-series><correlation><cross-correlation>',\n",
       " '<classification><text-mining><naive-bayes><binary-data>',\n",
       " '<r><time-series><maximum-likelihood><gls>',\n",
       " '<time-series><clustering><trend>',\n",
       " '<factor-analysis><em-algorithm>',\n",
       " '<distributions><books><gaussian-process>',\n",
       " '<confidence-interval><sample-size><multinomial>',\n",
       " '<hypothesis-testing><anova><t-test><ordinal>',\n",
       " '<hypothesis-testing><books><chi-squared>',\n",
       " '<regression><terminology><multivariable>',\n",
       " '<chi-squared><excel>',\n",
       " '<meta-analysis><funnel-plot><publication-bias>',\n",
       " '<correlation>',\n",
       " '<spss><data-transformation><generalized-linear-model>',\n",
       " '<confidence-interval><normal-distribution>',\n",
       " '<anova><repeated-measures><multivariate-analysis><heteroscedasticity>',\n",
       " '<roc><rank-correlation><spearman-rho><gini><kendall-tau>',\n",
       " '<regression><normality>',\n",
       " '<classification><logistic>',\n",
       " '<regression><distributions><probability><normal-distribution><mathematical-statistics>',\n",
       " '<regression><multiple-regression><interaction><regression-coefficients><beta-regression>',\n",
       " '<r><hypothesis-testing><sas><simulation><computational-statistics>',\n",
       " '<correlation><standard-deviation><kurtosis>',\n",
       " '<regression><residuals><assumptions>',\n",
       " '<categorical-data><multiple-comparisons>',\n",
       " '<r><anova><dataset>',\n",
       " '<r><computational-statistics>',\n",
       " '<confidence-interval><binomial><survey>',\n",
       " '<stata><multicollinearity><diagnostic><conditioning>',\n",
       " '<r><sequence-analysis><traminer>',\n",
       " '<sampling><chi-squared>',\n",
       " '<regression><correlation><multiple-regression><r-squared>',\n",
       " '<spss><scales><reliability>',\n",
       " '<anova><multiple-comparisons><post-hoc><tukey-hsd>',\n",
       " '<statistical-significance><predictive-models><graph-theory>',\n",
       " '<data-visualization><gnuplot>',\n",
       " '<terminology><post-hoc>',\n",
       " '<clustering><text-mining>',\n",
       " '<regression><panel-data><instrumental-variables>',\n",
       " '<machine-learning><model-selection><cross-validation><best-practices>',\n",
       " '<hypothesis-testing><maximum-likelihood><likelihood-function>',\n",
       " '<r><software><bugs><jags><gibbs>',\n",
       " '<confidence-interval><stata><logit>',\n",
       " '<distributions><bayesian><quantiles><density><credible-interval>',\n",
       " '<time-series><heteroscedasticity><basic-concepts><stationarity>',\n",
       " '<machine-learning><mcmc><inference><references><application>',\n",
       " '<r><correlation><resampling>',\n",
       " '<sampling><hypergeometric><finite-population><sequential-analysis><stratification>',\n",
       " '<machine-learning><probability><bayesian><multilevel-analysis><lsa>',\n",
       " '<terminology><median><census>',\n",
       " '<repeated-measures><spss>',\n",
       " '<distributions><quantiles><fitting>',\n",
       " '<multivariate-analysis><algorithms>',\n",
       " '<signal-processing><image-processing>',\n",
       " '<r><clustering><distance-functions><similarities>',\n",
       " '<r><time-series><change-point>',\n",
       " '<books><survey><validity>',\n",
       " '<regression><least-squares><assumptions><residual-analysis>',\n",
       " '<confidence-interval><goodness-of-fit><extreme-value><threshold>',\n",
       " '<hypothesis-testing><code><uniform>',\n",
       " '<modeling><compression>',\n",
       " '<regression><parameterization>',\n",
       " '<bayesian><multivariate-analysis><approximation><cart>',\n",
       " '<dataset><methodology>',\n",
       " '<svm><feature-selection><segmentation>',\n",
       " '<r><distributions><mathematical-statistics><simulation>',\n",
       " '<logistic><generalized-moments>',\n",
       " '<matlab><covariance>',\n",
       " '<r><algorithms><multiple-imputation>',\n",
       " '<normal-distribution><gamma-distribution>',\n",
       " '<correlation><statistical-significance>',\n",
       " '<modeling><cross-validation><stata><tobit-regression>',\n",
       " '<distributions><normal-distribution><estimation>',\n",
       " '<r><psychometrics><rasch><irt>',\n",
       " '<modeling><econometrics>',\n",
       " '<mixed-model><experiment-design><sem>',\n",
       " '<logistic><aic><model>',\n",
       " '<hypothesis-testing><variance><mean>',\n",
       " '<sample-size><sample><population><observational-study>',\n",
       " '<logistic><kernel><svm>',\n",
       " '<r><data-visualization><ggplot2><funnel-plot>',\n",
       " '<machine-learning><cart>',\n",
       " '<poisson><maximum-likelihood><likelihood-ratio>',\n",
       " '<libsvm>',\n",
       " '<machine-learning><predictive-models><cross-validation><svm>',\n",
       " '<regression><matlab><gaussian-process>',\n",
       " '<r><mixed-model><random-effects-model><error-message>',\n",
       " '<experiment-design><sample>',\n",
       " '<mean><least-squares>',\n",
       " '<distributions><chi-squared><exponential><order-statistics><minimum>',\n",
       " '<regression><regression-coefficients>',\n",
       " '<regression><maximum-likelihood><ordinal><probit>',\n",
       " '<machine-learning><svm><python><kernel>',\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 13.0,\n",
       " 17.0,\n",
       " 14.0,\n",
       " 19.0,\n",
       " 16.0,\n",
       " 20.0,\n",
       " 21.0,\n",
       " 23.0,\n",
       " 24.0,\n",
       " 25.0,\n",
       " 26.0,\n",
       " 27.0,\n",
       " 28.0,\n",
       " 29.0,\n",
       " 30.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 35.0,\n",
       " 41.0,\n",
       " 36.0,\n",
       " 44.0,\n",
       " 40.0,\n",
       " 38.0,\n",
       " 46.0,\n",
       " 42.0,\n",
       " 45.0,\n",
       " 43.0,\n",
       " 49.0,\n",
       " 47.0,\n",
       " 48.0,\n",
       " 52.0,\n",
       " 55.0,\n",
       " 50.0,\n",
       " 57.0,\n",
       " 56.0,\n",
       " 53.0,\n",
       " 59.0,\n",
       " 54.0,\n",
       " 62.0,\n",
       " 61.0,\n",
       " 64.0,\n",
       " 65.0,\n",
       " 58.0,\n",
       " 67.0,\n",
       " 68.0,\n",
       " 70.0,\n",
       " 71.0,\n",
       " 72.0,\n",
       " 74.0,\n",
       " 76.0,\n",
       " 90.0,\n",
       " 93.0,\n",
       " 94.0,\n",
       " 22.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 32.0,\n",
       " 33.0,\n",
       " 34.0,\n",
       " 37.0,\n",
       " 39.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 51.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 31.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 15.0,\n",
       " 18.0,\n",
       " 5.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 32.0,\n",
       " 49.0,\n",
       " 55.0,\n",
       " 59.0,\n",
       " 61.0,\n",
       " 64.0,\n",
       " 65.0,\n",
       " 72.0,\n",
       " 78.0,\n",
       " 80.0,\n",
       " 85.0,\n",
       " 89.0,\n",
       " 91.0,\n",
       " 94.0,\n",
       " 104.0,\n",
       " 110.0,\n",
       " 111.0,\n",
       " 131.0,\n",
       " 135.0,\n",
       " 147.0,\n",
       " 149.0,\n",
       " 151.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 174.0,\n",
       " 179.0,\n",
       " 191.0,\n",
       " 198.0,\n",
       " 200.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 207.0,\n",
       " nan,\n",
       " 209.0,\n",
       " 217.0,\n",
       " 221.0,\n",
       " 229.0,\n",
       " 232.0,\n",
       " 262.0,\n",
       " nan,\n",
       " nan,\n",
       " 265.0,\n",
       " nan,\n",
       " nan,\n",
       " 268.0,\n",
       " nan,\n",
       " nan,\n",
       " 271.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 279.0,\n",
       " nan,\n",
       " nan,\n",
       " 282.0,\n",
       " 293.0,\n",
       " 26111.0,\n",
       " 306.0,\n",
       " 307.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 329.0,\n",
       " 338.0,\n",
       " nan,\n",
       " 353.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 376.0,\n",
       " nan,\n",
       " 378.0,\n",
       " 379.0,\n",
       " nan,\n",
       " 381.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 385.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 416.0,\n",
       " 428.0,\n",
       " 439.0,\n",
       " 445.0,\n",
       " 457.0,\n",
       " 479.0,\n",
       " 482.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 506.0,\n",
       " 518.0,\n",
       " nan,\n",
       " nan,\n",
       " 522.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 532.0,\n",
       " 538.0,\n",
       " 553.0,\n",
       " 559.0,\n",
       " 560.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 566.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 585.0,\n",
       " nan,\n",
       " nan,\n",
       " 588.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 599.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 606.0,\n",
       " 611.0,\n",
       " 625.0,\n",
       " nan,\n",
       " nan,\n",
       " 628.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 634.0,\n",
       " 635.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 644.0,\n",
       " nan,\n",
       " 646.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 677.0,\n",
       " 680.0,\n",
       " 682.0,\n",
       " 711.0,\n",
       " 720.0,\n",
       " 721.0,\n",
       " 741.0,\n",
       " 758.0,\n",
       " 759.0,\n",
       " 762.0,\n",
       " 767.0,\n",
       " 772.0,\n",
       " 777.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 796.0,\n",
       " 822.0,\n",
       " 824.0,\n",
       " 827.0,\n",
       " 829.0,\n",
       " 839.0,\n",
       " 844.0,\n",
       " 849.0,\n",
       " 851.0,\n",
       " 858.0,\n",
       " 862.0,\n",
       " 867.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 875.0,\n",
       " 876.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 904.0,\n",
       " 905.0,\n",
       " 916.0,\n",
       " 917.0,\n",
       " 918.0,\n",
       " 926.0,\n",
       " 932.0,\n",
       " 937.0,\n",
       " 940.0,\n",
       " 941.0,\n",
       " nan,\n",
       " nan,\n",
       " 956.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 962.0,\n",
       " 963.0,\n",
       " 971.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 978.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1000.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1011.0,\n",
       " 1014.0,\n",
       " 1020.0,\n",
       " 1026.0,\n",
       " 1029.0,\n",
       " 1041.0,\n",
       " 1048.0,\n",
       " 1056.0,\n",
       " 1058.0,\n",
       " 1061.0,\n",
       " 1065.0,\n",
       " 1068.0,\n",
       " 1086.0,\n",
       " 1097.0,\n",
       " 1098.0,\n",
       " 1106.0,\n",
       " 1108.0,\n",
       " 1113.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1170.0,\n",
       " 1171.0,\n",
       " 1187.0,\n",
       " 1189.0,\n",
       " 26290.0,\n",
       " 1201.0,\n",
       " 1210.0,\n",
       " nan,\n",
       " 1212.0,\n",
       " 1213.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1220.0,\n",
       " 1251.0,\n",
       " 1263.0,\n",
       " 1273.0,\n",
       " 1276.0,\n",
       " 1277.0,\n",
       " 1282.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1297.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1306.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1322.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1326.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1335.0,\n",
       " nan,\n",
       " 1349.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1353.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1364.0,\n",
       " 1365.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 78427.0,\n",
       " 1372.0,\n",
       " 1384.0,\n",
       " nan,\n",
       " nan,\n",
       " 1390.0,\n",
       " nan,\n",
       " 1392.0,\n",
       " 1393.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1397.0,\n",
       " 1401.0,\n",
       " 1402.0,\n",
       " 1404.0,\n",
       " 1407.0,\n",
       " 1414.0,\n",
       " 1416.0,\n",
       " 1425.0,\n",
       " 1428.0,\n",
       " 1431.0,\n",
       " 1435.0,\n",
       " 1446.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1456.0,\n",
       " 1457.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1473.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1479.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1489.0,\n",
       " 1503.0,\n",
       " 1504.0,\n",
       " 1518.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1532.0,\n",
       " 1537.0,\n",
       " 1539.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1543.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1558.0,\n",
       " 1560.0,\n",
       " 1567.0,\n",
       " 1573.0,\n",
       " 1579.0,\n",
       " 1582.0,\n",
       " 1589.0,\n",
       " nan,\n",
       " 1594.0,\n",
       " nan,\n",
       " 1596.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1607.0,\n",
       " 1608.0,\n",
       " 1609.0,\n",
       " 1616.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1632.0,\n",
       " 1633.0,\n",
       " nan,\n",
       " 1635.0,\n",
       " 1636.0,\n",
       " nan,\n",
       " 1639.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1648.0,\n",
       " 1650.0,\n",
       " 1665.0,\n",
       " 1693.0,\n",
       " 26388.0,\n",
       " 1706.0,\n",
       " 1717.0,\n",
       " 1721.0,\n",
       " 1730.0,\n",
       " 1732.0,\n",
       " 1733.0,\n",
       " 1738.0,\n",
       " 1741.0,\n",
       " 1751.0,\n",
       " 1754.0,\n",
       " nan,\n",
       " 1762.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1775.0,\n",
       " 1785.0,\n",
       " 1791.0,\n",
       " 1792.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1806.0,\n",
       " nan,\n",
       " nan,\n",
       " 1816.0,\n",
       " 1823.0,\n",
       " 1827.0,\n",
       " 1830.0,\n",
       " 1843.0,\n",
       " 1845.0,\n",
       " nan,\n",
       " 1849.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1855.0,\n",
       " nan,\n",
       " nan,\n",
       " 1861.0,\n",
       " 1869.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1879.0,\n",
       " nan,\n",
       " 1887.0,\n",
       " 1901.0,\n",
       " 1903.0,\n",
       " 1917.0,\n",
       " 1918.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1930.0,\n",
       " nan,\n",
       " 1945.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1953.0,\n",
       " 1959.0,\n",
       " 1974.0,\n",
       " 1976.0,\n",
       " 1981.0,\n",
       " 1989.0,\n",
       " 1994.0,\n",
       " 2019.0,\n",
       " 2026.0,\n",
       " 2036.0,\n",
       " 2040.0,\n",
       " 2043.0,\n",
       " 2053.0,\n",
       " 2057.0,\n",
       " 2074.0,\n",
       " 2080.0,\n",
       " 2081.0,\n",
       " 2084.0,\n",
       " 2087.0,\n",
       " 2094.0,\n",
       " 2097.0,\n",
       " 2100.0,\n",
       " 2103.0,\n",
       " 2106.0,\n",
       " 2115.0,\n",
       " 2123.0,\n",
       " 2128.0,\n",
       " 2133.0,\n",
       " 2138.0,\n",
       " 2139.0,\n",
       " 2141.0,\n",
       " 2153.0,\n",
       " 2157.0,\n",
       " 2168.0,\n",
       " 2172.0,\n",
       " 2177.0,\n",
       " 2180.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2202.0,\n",
       " 2206.0,\n",
       " 2218.0,\n",
       " 2221.0,\n",
       " 2222.0,\n",
       " 2232.0,\n",
       " 2233.0,\n",
       " 2235.0,\n",
       " nan,\n",
       " nan,\n",
       " 2246.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2251.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2263.0,\n",
       " 2257.0,\n",
       " 2265.0,\n",
       " 2279.0,\n",
       " 2283.0,\n",
       " 2285.0,\n",
       " 2287.0,\n",
       " 2288.0,\n",
       " 2297.0,\n",
       " 2303.0,\n",
       " 2305.0,\n",
       " 2307.0,\n",
       " 2310.0,\n",
       " 2319.0,\n",
       " 2332.0,\n",
       " 2338.0,\n",
       " 2340.0,\n",
       " 2342.0,\n",
       " 2346.0,\n",
       " 2359.0,\n",
       " 2360.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2366.0,\n",
       " nan,\n",
       " nan,\n",
       " 2375.0,\n",
       " 2387.0,\n",
       " 2389.0,\n",
       " 2392.0,\n",
       " 2394.0,\n",
       " 2406.0,\n",
       " 2408.0,\n",
       " 2409.0,\n",
       " 2411.0,\n",
       " 2415.0,\n",
       " 2421.0,\n",
       " 2424.0,\n",
       " 2426.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2442.0,\n",
       " 2445.0,\n",
       " 2440.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2456.0,\n",
       " 2463.0,\n",
       " 2472.0,\n",
       " 2473.0,\n",
       " 2478.0,\n",
       " 2482.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2495.0,\n",
       " nan,\n",
       " 2497.0,\n",
       " 2498.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2506.0,\n",
       " 2509.0,\n",
       " 2511.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2519.0,\n",
       " nan,\n",
       " 2521.0,\n",
       " nan,\n",
       " nan,\n",
       " 2524.0,\n",
       " 2530.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2538.0,\n",
       " 2539.0,\n",
       " nan,\n",
       " nan,\n",
       " 2542.0,\n",
       " nan,\n",
       " 2550.0,\n",
       " 2558.0,\n",
       " 2570.0,\n",
       " 2571.0,\n",
       " nan,\n",
       " 2593.0,\n",
       " nan,\n",
       " 2595.0,\n",
       " 2596.0,\n",
       " nan,\n",
       " nan,\n",
       " 2599.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2605.0,\n",
       " 2606.0,\n",
       " 2607.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2618.0,\n",
       " 2621.0,\n",
       " 2622.0,\n",
       " 2627.0,\n",
       " 2629.0,\n",
       " 2633.0,\n",
       " 2647.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2651.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2664.0,\n",
       " 2665.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2674.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2680.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2701.0,\n",
       " 2707.0,\n",
       " 2721.0,\n",
       " 2735.0,\n",
       " 2738.0,\n",
       " 2740.0,\n",
       " 2741.0,\n",
       " 2747.0,\n",
       " 2750.0,\n",
       " 2766.0,\n",
       " 2773.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2781.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2778.0,\n",
       " 2789.0,\n",
       " 2790.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2798.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2822.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2834.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2847.0,\n",
       " nan,\n",
       " nan,\n",
       " 2850.0,\n",
       " nan,\n",
       " nan,\n",
       " 2853.0,\n",
       " nan,\n",
       " nan,\n",
       " 2872.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2887.0,\n",
       " 2888.0,\n",
       " 2890.0,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 14.0,\n",
       " 17.0,\n",
       " 15.0,\n",
       " 19.0,\n",
       " 20.0,\n",
       " 21.0,\n",
       " 22.0,\n",
       " 23.0,\n",
       " 24.0,\n",
       " 25.0,\n",
       " 26.0,\n",
       " 27.0,\n",
       " 28.0,\n",
       " 29.0,\n",
       " 30.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 40.0,\n",
       " 35.0,\n",
       " 43.0,\n",
       " 44.0,\n",
       " 38.0,\n",
       " 45.0,\n",
       " 47.0,\n",
       " 48.0,\n",
       " 49.0,\n",
       " 50.0,\n",
       " 51.0,\n",
       " 52.0,\n",
       " 53.0,\n",
       " 54.0,\n",
       " 55.0,\n",
       " 56.0,\n",
       " 57.0,\n",
       " 58.0,\n",
       " 59.0,\n",
       " 60.0,\n",
       " 61.0,\n",
       " 62.0,\n",
       " 63.0,\n",
       " 64.0,\n",
       " 65.0,\n",
       " 66.0,\n",
       " 67.0,\n",
       " 68.0,\n",
       " 69.0,\n",
       " 70.0,\n",
       " 71.0,\n",
       " 72.0,\n",
       " 73.0,\n",
       " 74.0,\n",
       " 75.0,\n",
       " 76.0,\n",
       " 77.0,\n",
       " 78.0,\n",
       " 79.0,\n",
       " 80.0,\n",
       " 81.0,\n",
       " 82.0,\n",
       " 83.0,\n",
       " 84.0,\n",
       " 85.0,\n",
       " 86.0,\n",
       " 87.0,\n",
       " 88.0,\n",
       " 89.0,\n",
       " 90.0,\n",
       " 91.0,\n",
       " 92.0,\n",
       " 93.0,\n",
       " 94.0,\n",
       " 95.0,\n",
       " 96.0,\n",
       " 97.0,\n",
       " 98.0,\n",
       " 99.0,\n",
       " 100.0,\n",
       " 101.0,\n",
       " 102.0,\n",
       " 103.0,\n",
       " 104.0,\n",
       " 105.0,\n",
       " 106.0,\n",
       " 107.0,\n",
       " 108.0,\n",
       " 109.0,\n",
       " 110.0,\n",
       " 111.0,\n",
       " 112.0,\n",
       " 113.0,\n",
       " 114.0,\n",
       " 115.0,\n",
       " 116.0,\n",
       " 117.0,\n",
       " 118.0,\n",
       " 119.0,\n",
       " 120.0,\n",
       " 121.0,\n",
       " 122.0,\n",
       " 123.0,\n",
       " 124.0,\n",
       " 125.0,\n",
       " 126.0,\n",
       " 127.0,\n",
       " 128.0,\n",
       " 129.0,\n",
       " 130.0,\n",
       " 131.0,\n",
       " 132.0,\n",
       " 133.0,\n",
       " 134.0,\n",
       " 135.0,\n",
       " 136.0,\n",
       " 137.0,\n",
       " 138.0,\n",
       " 139.0,\n",
       " 140.0,\n",
       " 141.0,\n",
       " 142.0,\n",
       " 143.0,\n",
       " 144.0,\n",
       " 145.0,\n",
       " 146.0,\n",
       " 147.0,\n",
       " 148.0,\n",
       " 149.0,\n",
       " 150.0,\n",
       " 151.0,\n",
       " 152.0,\n",
       " 153.0,\n",
       " 154.0,\n",
       " 155.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 161.0,\n",
       " 158.0,\n",
       " 167.0,\n",
       " 169.0,\n",
       " 166.0,\n",
       " 170.0,\n",
       " 171.0,\n",
       " 172.0,\n",
       " 174.0,\n",
       " 175.0,\n",
       " 176.0,\n",
       " 177.0,\n",
       " 178.0,\n",
       " 179.0,\n",
       " 180.0,\n",
       " 181.0,\n",
       " 182.0,\n",
       " 183.0,\n",
       " 184.0,\n",
       " 185.0,\n",
       " 186.0,\n",
       " 187.0,\n",
       " 188.0,\n",
       " 189.0,\n",
       " 190.0,\n",
       " 191.0,\n",
       " 37.0,\n",
       " 193.0,\n",
       " 194.0,\n",
       " 195.0,\n",
       " 196.0,\n",
       " 39.0,\n",
       " 198.0,\n",
       " 199.0,\n",
       " 200.0,\n",
       " 201.0,\n",
       " 202.0,\n",
       " 203.0,\n",
       " 204.0,\n",
       " 205.0,\n",
       " 206.0,\n",
       " 41.0,\n",
       " 208.0,\n",
       " 207.0,\n",
       " 210.0,\n",
       " 209.0,\n",
       " 212.0,\n",
       " 213.0,\n",
       " 42.0,\n",
       " 215.0,\n",
       " 216.0,\n",
       " 217.0,\n",
       " 218.0,\n",
       " 219.0,\n",
       " 220.0,\n",
       " 221.0,\n",
       " 222.0,\n",
       " 223.0,\n",
       " 224.0,\n",
       " 225.0,\n",
       " 226.0,\n",
       " 227.0,\n",
       " 228.0,\n",
       " 229.0,\n",
       " 230.0,\n",
       " 231.0,\n",
       " 232.0,\n",
       " 233.0,\n",
       " 234.0,\n",
       " 235.0,\n",
       " 236.0,\n",
       " 237.0,\n",
       " 238.0,\n",
       " 239.0,\n",
       " 240.0,\n",
       " 241.0,\n",
       " 242.0,\n",
       " 243.0,\n",
       " 244.0,\n",
       " 245.0,\n",
       " 246.0,\n",
       " 247.0,\n",
       " 248.0,\n",
       " 249.0,\n",
       " 250.0,\n",
       " 251.0,\n",
       " 252.0,\n",
       " 253.0,\n",
       " 254.0,\n",
       " 255.0,\n",
       " 256.0,\n",
       " 257.0,\n",
       " 258.0,\n",
       " 259.0,\n",
       " 260.0,\n",
       " 261.0,\n",
       " 262.0,\n",
       " 263.0,\n",
       " 264.0,\n",
       " 265.0,\n",
       " 266.0,\n",
       " 267.0,\n",
       " 268.0,\n",
       " 269.0,\n",
       " 270.0,\n",
       " 271.0,\n",
       " 272.0,\n",
       " 273.0,\n",
       " 274.0,\n",
       " 275.0,\n",
       " 276.0,\n",
       " 277.0,\n",
       " 278.0,\n",
       " 279.0,\n",
       " 280.0,\n",
       " 281.0,\n",
       " 282.0,\n",
       " 283.0,\n",
       " 284.0,\n",
       " 285.0,\n",
       " 286.0,\n",
       " 287.0,\n",
       " 288.0,\n",
       " 289.0,\n",
       " 290.0,\n",
       " 291.0,\n",
       " 292.0,\n",
       " 293.0,\n",
       " 294.0,\n",
       " 295.0,\n",
       " 296.0,\n",
       " 297.0,\n",
       " 298.0,\n",
       " 299.0,\n",
       " 300.0,\n",
       " 301.0,\n",
       " 302.0,\n",
       " 303.0,\n",
       " 304.0,\n",
       " 305.0,\n",
       " 306.0,\n",
       " 307.0,\n",
       " 308.0,\n",
       " 309.0,\n",
       " 310.0,\n",
       " 311.0,\n",
       " 312.0,\n",
       " 313.0,\n",
       " 314.0,\n",
       " 315.0,\n",
       " 316.0,\n",
       " 317.0,\n",
       " 318.0,\n",
       " 319.0,\n",
       " 320.0,\n",
       " 321.0,\n",
       " 322.0,\n",
       " 323.0,\n",
       " 324.0,\n",
       " 325.0,\n",
       " 326.0,\n",
       " 327.0,\n",
       " 328.0,\n",
       " 329.0,\n",
       " 330.0,\n",
       " 331.0,\n",
       " 332.0,\n",
       " 333.0,\n",
       " 334.0,\n",
       " 335.0,\n",
       " 336.0,\n",
       " 337.0,\n",
       " 338.0,\n",
       " 339.0,\n",
       " 340.0,\n",
       " 341.0,\n",
       " 342.0,\n",
       " 343.0,\n",
       " 344.0,\n",
       " 345.0,\n",
       " 346.0,\n",
       " 347.0,\n",
       " 348.0,\n",
       " 349.0,\n",
       " 350.0,\n",
       " 351.0,\n",
       " 352.0,\n",
       " 353.0,\n",
       " 354.0,\n",
       " 355.0,\n",
       " 356.0,\n",
       " 357.0,\n",
       " 358.0,\n",
       " 359.0,\n",
       " 360.0,\n",
       " 361.0,\n",
       " 362.0,\n",
       " 363.0,\n",
       " 364.0,\n",
       " 365.0,\n",
       " 366.0,\n",
       " 367.0,\n",
       " 368.0,\n",
       " 369.0,\n",
       " 370.0,\n",
       " 371.0,\n",
       " 372.0,\n",
       " 373.0,\n",
       " 374.0,\n",
       " 375.0,\n",
       " 376.0,\n",
       " 377.0,\n",
       " 378.0,\n",
       " 379.0,\n",
       " 380.0,\n",
       " 381.0,\n",
       " 382.0,\n",
       " 383.0,\n",
       " 384.0,\n",
       " 385.0,\n",
       " 386.0,\n",
       " 387.0,\n",
       " 388.0,\n",
       " 389.0,\n",
       " 390.0,\n",
       " 391.0,\n",
       " 392.0,\n",
       " 393.0,\n",
       " 394.0,\n",
       " 395.0,\n",
       " 396.0,\n",
       " 397.0,\n",
       " 398.0,\n",
       " 399.0,\n",
       " 400.0,\n",
       " 401.0,\n",
       " 402.0,\n",
       " 403.0,\n",
       " 404.0,\n",
       " 405.0,\n",
       " 406.0,\n",
       " 407.0,\n",
       " 408.0,\n",
       " 409.0,\n",
       " 410.0,\n",
       " 411.0,\n",
       " 412.0,\n",
       " 413.0,\n",
       " 414.0,\n",
       " 415.0,\n",
       " 416.0,\n",
       " 417.0,\n",
       " 418.0,\n",
       " 419.0,\n",
       " 420.0,\n",
       " 421.0,\n",
       " 422.0,\n",
       " 423.0,\n",
       " 424.0,\n",
       " 425.0,\n",
       " 426.0,\n",
       " 427.0,\n",
       " 428.0,\n",
       " 429.0,\n",
       " 430.0,\n",
       " 431.0,\n",
       " 432.0,\n",
       " 433.0,\n",
       " 434.0,\n",
       " 435.0,\n",
       " 436.0,\n",
       " 437.0,\n",
       " 438.0,\n",
       " 439.0,\n",
       " 440.0,\n",
       " 441.0,\n",
       " 442.0,\n",
       " 443.0,\n",
       " 444.0,\n",
       " 445.0,\n",
       " 446.0,\n",
       " 447.0,\n",
       " 448.0,\n",
       " 449.0,\n",
       " 450.0,\n",
       " 451.0,\n",
       " 452.0,\n",
       " 453.0,\n",
       " 454.0,\n",
       " 455.0,\n",
       " 456.0,\n",
       " 457.0,\n",
       " 458.0,\n",
       " 459.0,\n",
       " 460.0,\n",
       " 461.0,\n",
       " 462.0,\n",
       " 463.0,\n",
       " 464.0,\n",
       " 465.0,\n",
       " 466.0,\n",
       " 467.0,\n",
       " 468.0,\n",
       " 469.0,\n",
       " 470.0,\n",
       " 471.0,\n",
       " 472.0,\n",
       " 473.0,\n",
       " 474.0,\n",
       " 475.0,\n",
       " 476.0,\n",
       " 477.0,\n",
       " 478.0,\n",
       " 479.0,\n",
       " 480.0,\n",
       " 481.0,\n",
       " 482.0,\n",
       " 483.0,\n",
       " 484.0,\n",
       " 485.0,\n",
       " 486.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 490.0,\n",
       " 491.0,\n",
       " 492.0,\n",
       " 493.0,\n",
       " nan,\n",
       " nan,\n",
       " 496.0,\n",
       " 497.0,\n",
       " 498.0,\n",
       " 494.0,\n",
       " 500.0,\n",
       " 501.0,\n",
       " 502.0,\n",
       " 503.0,\n",
       " 504.0,\n",
       " 499.0,\n",
       " 506.0,\n",
       " 507.0,\n",
       " 508.0,\n",
       " 505.0,\n",
       " 510.0,\n",
       " 511.0,\n",
       " 512.0,\n",
       " 513.0,\n",
       " 514.0,\n",
       " 515.0,\n",
       " 516.0,\n",
       " 517.0,\n",
       " 518.0,\n",
       " 519.0,\n",
       " 520.0,\n",
       " 521.0,\n",
       " 522.0,\n",
       " 523.0,\n",
       " 524.0,\n",
       " 525.0,\n",
       " 526.0,\n",
       " 527.0,\n",
       " 528.0,\n",
       " 529.0,\n",
       " 530.0,\n",
       " 531.0,\n",
       " 532.0,\n",
       " 533.0,\n",
       " 534.0,\n",
       " 535.0,\n",
       " 536.0,\n",
       " 537.0,\n",
       " 538.0,\n",
       " 539.0,\n",
       " 540.0,\n",
       " 541.0,\n",
       " 542.0,\n",
       " 543.0,\n",
       " 544.0,\n",
       " 545.0,\n",
       " 546.0,\n",
       " 547.0,\n",
       " 548.0,\n",
       " 549.0,\n",
       " 550.0,\n",
       " 551.0,\n",
       " 552.0,\n",
       " 553.0,\n",
       " 554.0,\n",
       " 555.0,\n",
       " 556.0,\n",
       " 557.0,\n",
       " 558.0,\n",
       " 559.0,\n",
       " 560.0,\n",
       " 561.0,\n",
       " 562.0,\n",
       " 563.0,\n",
       " 564.0,\n",
       " 565.0,\n",
       " 566.0,\n",
       " 567.0,\n",
       " 568.0,\n",
       " 569.0,\n",
       " 570.0,\n",
       " 571.0,\n",
       " 572.0,\n",
       " 573.0,\n",
       " 574.0,\n",
       " 575.0,\n",
       " 576.0,\n",
       " 577.0,\n",
       " 578.0,\n",
       " 579.0,\n",
       " 580.0,\n",
       " 581.0,\n",
       " 582.0,\n",
       " 583.0,\n",
       " 584.0,\n",
       " 585.0,\n",
       " 586.0,\n",
       " 587.0,\n",
       " 588.0,\n",
       " 589.0,\n",
       " 590.0,\n",
       " 591.0,\n",
       " 592.0,\n",
       " 593.0,\n",
       " 594.0,\n",
       " 595.0,\n",
       " 596.0,\n",
       " 597.0,\n",
       " 598.0,\n",
       " 599.0,\n",
       " 600.0,\n",
       " 601.0,\n",
       " 602.0,\n",
       " 603.0,\n",
       " 604.0,\n",
       " 605.0,\n",
       " 606.0,\n",
       " 607.0,\n",
       " 608.0,\n",
       " 609.0,\n",
       " 610.0,\n",
       " 611.0,\n",
       " 612.0,\n",
       " 613.0,\n",
       " 614.0,\n",
       " 615.0,\n",
       " 616.0,\n",
       " 617.0,\n",
       " 618.0,\n",
       " 619.0,\n",
       " 620.0,\n",
       " 621.0,\n",
       " 622.0,\n",
       " 623.0,\n",
       " 624.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 629.0,\n",
       " 630.0,\n",
       " nan,\n",
       " 632.0,\n",
       " 633.0,\n",
       " 634.0,\n",
       " 635.0,\n",
       " 636.0,\n",
       " 637.0,\n",
       " 638.0,\n",
       " 639.0,\n",
       " 631.0,\n",
       " 640.0,\n",
       " 642.0,\n",
       " 641.0,\n",
       " 644.0,\n",
       " 645.0,\n",
       " 646.0,\n",
       " 647.0,\n",
       " 648.0,\n",
       " 649.0,\n",
       " 650.0,\n",
       " 651.0,\n",
       " 652.0,\n",
       " 653.0,\n",
       " 654.0,\n",
       " 655.0,\n",
       " 656.0,\n",
       " 657.0,\n",
       " 658.0,\n",
       " 659.0,\n",
       " 660.0,\n",
       " 661.0,\n",
       " 662.0,\n",
       " 663.0,\n",
       " 664.0,\n",
       " 665.0,\n",
       " 666.0,\n",
       " 667.0,\n",
       " 668.0,\n",
       " 669.0,\n",
       " 670.0,\n",
       " 671.0,\n",
       " 672.0,\n",
       " 673.0,\n",
       " 674.0,\n",
       " 675.0,\n",
       " 676.0,\n",
       " 677.0,\n",
       " 678.0,\n",
       " 679.0,\n",
       " 680.0,\n",
       " 681.0,\n",
       " 682.0,\n",
       " 684.0,\n",
       " 685.0,\n",
       " 686.0,\n",
       " 687.0,\n",
       " 688.0,\n",
       " 689.0,\n",
       " 690.0,\n",
       " 691.0,\n",
       " 692.0,\n",
       " 693.0,\n",
       " 694.0,\n",
       " 695.0,\n",
       " 696.0,\n",
       " 697.0,\n",
       " 698.0,\n",
       " 699.0,\n",
       " 700.0,\n",
       " 701.0,\n",
       " 702.0,\n",
       " 703.0,\n",
       " 704.0,\n",
       " 705.0,\n",
       " 706.0,\n",
       " 707.0,\n",
       " 708.0,\n",
       " 709.0,\n",
       " 710.0,\n",
       " 711.0,\n",
       " 712.0,\n",
       " 713.0,\n",
       " 714.0,\n",
       " 715.0,\n",
       " 716.0,\n",
       " 717.0,\n",
       " 718.0,\n",
       " 719.0,\n",
       " 720.0,\n",
       " 721.0,\n",
       " 722.0,\n",
       " 723.0,\n",
       " 724.0,\n",
       " 725.0,\n",
       " 726.0,\n",
       " 727.0,\n",
       " 728.0,\n",
       " 729.0,\n",
       " 730.0,\n",
       " 731.0,\n",
       " 732.0,\n",
       " 733.0,\n",
       " 734.0,\n",
       " 735.0,\n",
       " 736.0,\n",
       " 737.0,\n",
       " 738.0,\n",
       " 739.0,\n",
       " 740.0,\n",
       " 741.0,\n",
       " 742.0,\n",
       " 743.0,\n",
       " 744.0,\n",
       " 745.0,\n",
       " 746.0,\n",
       " 747.0,\n",
       " 748.0,\n",
       " 749.0,\n",
       " 750.0,\n",
       " 751.0,\n",
       " 752.0,\n",
       " 753.0,\n",
       " 754.0,\n",
       " 755.0,\n",
       " 756.0,\n",
       " 757.0,\n",
       " 758.0,\n",
       " 759.0,\n",
       " 760.0,\n",
       " 761.0,\n",
       " 762.0,\n",
       " 763.0,\n",
       " 764.0,\n",
       " 765.0,\n",
       " 766.0,\n",
       " 767.0,\n",
       " 768.0,\n",
       " 769.0,\n",
       " 770.0,\n",
       " 771.0,\n",
       " 772.0,\n",
       " 773.0,\n",
       " 774.0,\n",
       " 775.0,\n",
       " 776.0,\n",
       " 777.0,\n",
       " 778.0,\n",
       " 779.0,\n",
       " 780.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 156.0,\n",
       " 157.0,\n",
       " 786.0,\n",
       " 788.0,\n",
       " 795.0,\n",
       " 796.0,\n",
       " 797.0,\n",
       " 790.0,\n",
       " 799.0,\n",
       " 159.0,\n",
       " 801.0,\n",
       " 802.0,\n",
       " 800.0,\n",
       " 804.0,\n",
       " 805.0,\n",
       " 160.0,\n",
       " 807.0,\n",
       " 806.0,\n",
       " 803.0,\n",
       " 808.0,\n",
       " 811.0,\n",
       " 32.0,\n",
       " 813.0,\n",
       " 814.0,\n",
       " 815.0,\n",
       " 162.0,\n",
       " 816.0,\n",
       " 818.0,\n",
       " 817.0,\n",
       " 820.0,\n",
       " 819.0,\n",
       " 163.0,\n",
       " 823.0,\n",
       " 824.0,\n",
       " 821.0,\n",
       " 164.0,\n",
       " 827.0,\n",
       " 825.0,\n",
       " 829.0,\n",
       " 830.0,\n",
       " 831.0,\n",
       " 165.0,\n",
       " 833.0,\n",
       " 834.0,\n",
       " 832.0,\n",
       " 836.0,\n",
       " 33.0,\n",
       " 838.0,\n",
       " 835.0,\n",
       " 840.0,\n",
       " 841.0,\n",
       " 842.0,\n",
       " 843.0,\n",
       " 844.0,\n",
       " 845.0,\n",
       " 168.0,\n",
       " 847.0,\n",
       " 846.0,\n",
       " 849.0,\n",
       " 850.0,\n",
       " 851.0,\n",
       " 852.0,\n",
       " 853.0,\n",
       " 854.0,\n",
       " 855.0,\n",
       " 856.0,\n",
       " 857.0,\n",
       " 858.0,\n",
       " 859.0,\n",
       " 860.0,\n",
       " 861.0,\n",
       " 34.0,\n",
       " 862.0,\n",
       " 864.0,\n",
       " 865.0,\n",
       " 866.0,\n",
       " 867.0,\n",
       " 868.0,\n",
       " 869.0,\n",
       " 870.0,\n",
       " 173.0,\n",
       " 872.0,\n",
       " 873.0,\n",
       " 874.0,\n",
       " 871.0,\n",
       " 876.0,\n",
       " 877.0,\n",
       " 875.0,\n",
       " 879.0,\n",
       " 880.0,\n",
       " 881.0,\n",
       " 878.0,\n",
       " 883.0,\n",
       " 884.0,\n",
       " 885.0,\n",
       " 886.0,\n",
       " 887.0,\n",
       " 888.0,\n",
       " 889.0,\n",
       " 890.0,\n",
       " 891.0,\n",
       " 892.0,\n",
       " 893.0,\n",
       " 894.0,\n",
       " 895.0,\n",
       " 896.0,\n",
       " 897.0,\n",
       " 898.0,\n",
       " 899.0,\n",
       " 900.0,\n",
       " 901.0,\n",
       " 902.0,\n",
       " 903.0,\n",
       " 904.0,\n",
       " 905.0,\n",
       " 906.0,\n",
       " 907.0,\n",
       " 908.0,\n",
       " 909.0,\n",
       " 910.0,\n",
       " 36.0,\n",
       " 912.0,\n",
       " 913.0,\n",
       " 914.0,\n",
       " 915.0,\n",
       " 916.0,\n",
       " 911.0,\n",
       " 918.0,\n",
       " 919.0,\n",
       " 920.0,\n",
       " 921.0,\n",
       " 922.0,\n",
       " 923.0,\n",
       " 924.0,\n",
       " 925.0,\n",
       " 926.0,\n",
       " 927.0,\n",
       " 928.0,\n",
       " 929.0,\n",
       " 930.0,\n",
       " 931.0,\n",
       " 932.0,\n",
       " 933.0,\n",
       " 934.0,\n",
       " 935.0,\n",
       " 936.0,\n",
       " 937.0,\n",
       " 938.0,\n",
       " 939.0,\n",
       " 940.0,\n",
       " 941.0,\n",
       " 942.0,\n",
       " 943.0,\n",
       " 944.0,\n",
       " 945.0,\n",
       " 946.0,\n",
       " 947.0,\n",
       " 948.0,\n",
       " 949.0,\n",
       " 950.0,\n",
       " 951.0,\n",
       " 952.0,\n",
       " 953.0,\n",
       " 954.0,\n",
       " 955.0,\n",
       " 956.0,\n",
       " 957.0,\n",
       " 958.0,\n",
       " 959.0,\n",
       " 960.0,\n",
       " 961.0,\n",
       " 962.0,\n",
       " 963.0,\n",
       " 964.0,\n",
       " 965.0,\n",
       " 966.0,\n",
       " 967.0,\n",
       " 968.0,\n",
       " 969.0,\n",
       " 970.0,\n",
       " 192.0,\n",
       " 972.0,\n",
       " 973.0,\n",
       " 974.0,\n",
       " 971.0,\n",
       " 976.0,\n",
       " nan,\n",
       " 978.0,\n",
       " 979.0,\n",
       " 980.0,\n",
       " 981.0,\n",
       " 982.0,\n",
       " 983.0,\n",
       " 984.0,\n",
       " 985.0,\n",
       " 986.0,\n",
       " 987.0,\n",
       " 988.0,\n",
       " 989.0,\n",
       " 990.0,\n",
       " 991.0,\n",
       " 197.0,\n",
       " 992.0,\n",
       " 994.0,\n",
       " 993.0,\n",
       " 995.0,\n",
       " 997.0,\n",
       " 998.0,\n",
       " 999.0,\n",
       " 1000.0,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 15.0,\n",
       " 16.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 3.0,\n",
       " 19.0,\n",
       " 4.0,\n",
       " 23.0,\n",
       " 24.0,\n",
       " 21.0,\n",
       " 26.0,\n",
       " 5.0,\n",
       " 27.0,\n",
       " 28.0,\n",
       " 25.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 7.0,\n",
       " 8.0,\n",
       " 42.0,\n",
       " 35.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 56.0,\n",
       " 2.0,\n",
       " 11.0,\n",
       " 58.0,\n",
       " 14.0,\n",
       " 18.0,\n",
       " 136.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 3.0,\n",
       " 10.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 15.0,\n",
       " 21.0,\n",
       " 22.0,\n",
       " 24.0,\n",
       " 5.0,\n",
       " 27.0,\n",
       " 20.0,\n",
       " 26.0,\n",
       " 30.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 37.0,\n",
       " 7.0,\n",
       " 8.0,\n",
       " 36.0,\n",
       " 45.0,\n",
       " 9.0,\n",
       " 47.0,\n",
       " 44.0,\n",
       " 42.0,\n",
       " 46.0,\n",
       " 51.0,\n",
       " 43.0,\n",
       " 54.0,\n",
       " 55.0,\n",
       " 56.0,\n",
       " 57.0,\n",
       " 60.0,\n",
       " 12.0,\n",
       " 61.0,\n",
       " 63.0,\n",
       " 65.0,\n",
       " 66.0,\n",
       " 67.0,\n",
       " 13.0,\n",
       " 69.0,\n",
       " 68.0,\n",
       " 71.0,\n",
       " 74.0,\n",
       " 75.0,\n",
       " 78.0,\n",
       " 79.0,\n",
       " 80.0,\n",
       " 16.0,\n",
       " 82.0,\n",
       " 17.0,\n",
       " 88.0,\n",
       " 89.0,\n",
       " 18.0,\n",
       " 19.0,\n",
       " 108.0,\n",
       " 111.0,\n",
       " 113.0,\n",
       " 23.0,\n",
       " 25.0,\n",
       " 127.0,\n",
       " 135.0,\n",
       " 137.0,\n",
       " 28.0,\n",
       " 29.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 32.0,\n",
       " 33.0,\n",
       " 34.0,\n",
       " 35.0,\n",
       " 190.0,\n",
       " 39.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 233.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 5.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 8.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 22.0,\n",
       " 23.0,\n",
       " 24.0,\n",
       " 25.0,\n",
       " 26.0,\n",
       " 29.0,\n",
       " 30.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 34.0,\n",
       " 36.0,\n",
       " 39.0,\n",
       " 46.0,\n",
       " 52.0,\n",
       " 54.0,\n",
       " 56.0,\n",
       " 58.0,\n",
       " 59.0,\n",
       " 61.0,\n",
       " 62.0,\n",
       " 68.0,\n",
       " 69.0,\n",
       " 71.0,\n",
       " 72.0,\n",
       " 74.0,\n",
       " 81.0,\n",
       " 82.0,\n",
       " 87.0,\n",
       " 88.0,\n",
       " 89.0,\n",
       " 92.0,\n",
       " 96.0,\n",
       " 98.0,\n",
       " 101.0,\n",
       " 103.0,\n",
       " 107.0,\n",
       " 108.0,\n",
       " 110.0,\n",
       " 114.0,\n",
       " 115.0,\n",
       " 118.0,\n",
       " 123.0,\n",
       " 127.0,\n",
       " 138.0,\n",
       " 139.0,\n",
       " 142.0,\n",
       " 144.0,\n",
       " 154.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 159.0,\n",
       " 162.0,\n",
       " 168.0,\n",
       " 169.0,\n",
       " 171.0,\n",
       " 172.0,\n",
       " 174.0,\n",
       " 175.0,\n",
       " 179.0,\n",
       " 183.0,\n",
       " 190.0,\n",
       " 196.0,\n",
       " 198.0,\n",
       " 199.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 215.0,\n",
       " 217.0,\n",
       " 218.0,\n",
       " 219.0,\n",
       " 220.0,\n",
       " 221.0,\n",
       " 223.0,\n",
       " 226.0,\n",
       " 227.0,\n",
       " 229.0,\n",
       " 230.0,\n",
       " 240.0,\n",
       " 247.0,\n",
       " 251.0,\n",
       " 253.0,\n",
       " 255.0,\n",
       " nan,\n",
       " 264.0,\n",
       " nan,\n",
       " 266.0,\n",
       " 267.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 276.0,\n",
       " nan,\n",
       " nan,\n",
       " 279.0,\n",
       " nan,\n",
       " nan,\n",
       " 282.0,\n",
       " 287.0,\n",
       " 291.0,\n",
       " 300.0,\n",
       " 303.0,\n",
       " 307.0,\n",
       " 315.0,\n",
       " 319.0,\n",
       " 333.0,\n",
       " 334.0,\n",
       " 339.0,\n",
       " 352.0,\n",
       " 364.0,\n",
       " 368.0,\n",
       " nan,\n",
       " 375.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 380.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 386.0,\n",
       " 401.0,\n",
       " 414.0,\n",
       " 428.0,\n",
       " 438.0,\n",
       " 439.0,\n",
       " 442.0,\n",
       " 446.0,\n",
       " 447.0,\n",
       " 449.0,\n",
       " 461.0,\n",
       " 478.0,\n",
       " 485.0,\n",
       " 486.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 493.0,\n",
       " nan,\n",
       " 495.0,\n",
       " nan,\n",
       " 506.0,\n",
       " 509.0,\n",
       " 511.0,\n",
       " 516.0,\n",
       " 520.0,\n",
       " 521.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 527.0,\n",
       " 528.0,\n",
       " 529.0,\n",
       " 530.0,\n",
       " 539.0,\n",
       " 556.0,\n",
       " 557.0,\n",
       " 559.0,\n",
       " nan,\n",
       " 561.0,\n",
       " 562.0,\n",
       " nan,\n",
       " nan,\n",
       " 565.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 573.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 582.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 588.0,\n",
       " 597.0,\n",
       " 601.0,\n",
       " 603.0,\n",
       " 609.0,\n",
       " 615.0,\n",
       " 621.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 632.0,\n",
       " nan,\n",
       " 634.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 644.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 656.0,\n",
       " 660.0,\n",
       " 665.0,\n",
       " 666.0,\n",
       " 686.0,\n",
       " 696.0,\n",
       " 702.0,\n",
       " 704.0,\n",
       " 740.0,\n",
       " 741.0,\n",
       " 749.0,\n",
       " 760.0,\n",
       " 770.0,\n",
       " 775.0,\n",
       " 776.0,\n",
       " 779.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 782.0,\n",
       " 791.0,\n",
       " 795.0,\n",
       " 800.0,\n",
       " 805.0,\n",
       " 812.0,\n",
       " 825.0,\n",
       " 827.0,\n",
       " 830.0,\n",
       " 840.0,\n",
       " 845.0,\n",
       " 847.0,\n",
       " 858.0,\n",
       " nan,\n",
       " 870.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 877.0,\n",
       " nan,\n",
       " 881.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 887.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 892.0,\n",
       " 900.0,\n",
       " 913.0,\n",
       " 914.0,\n",
       " 919.0,\n",
       " 930.0,\n",
       " 937.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 957.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 961.0,\n",
       " nan,\n",
       " nan,\n",
       " 966.0,\n",
       " 968.0,\n",
       " 972.0,\n",
       " 976.0,\n",
       " 977.0,\n",
       " 979.0,\n",
       " 986.0,\n",
       " 988.0,\n",
       " 990.0,\n",
       " 994.0,\n",
       " 995.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1005.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1019.0,\n",
       " 1021.0,\n",
       " 1036.0,\n",
       " 1043.0,\n",
       " 1048.0,\n",
       " 1050.0,\n",
       " 1056.0,\n",
       " 1065.0,\n",
       " 1075.0,\n",
       " 1077.0,\n",
       " 1080.0,\n",
       " 1084.0,\n",
       " 1098.0,\n",
       " 1102.0,\n",
       " 1106.0,\n",
       " 1107.0,\n",
       " 1108.0,\n",
       " 1112.0,\n",
       " 1114.0,\n",
       " 1117.0,\n",
       " 1118.0,\n",
       " 1119.0,\n",
       " 1121.0,\n",
       " 1122.0,\n",
       " 1124.0,\n",
       " 1138.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1144.0,\n",
       " 1146.0,\n",
       " 1150.0,\n",
       " 1154.0,\n",
       " 1185.0,\n",
       " 1191.0,\n",
       " 1195.0,\n",
       " 1205.0,\n",
       " 1216.0,\n",
       " 1219.0,\n",
       " 1220.0,\n",
       " 1227.0,\n",
       " 1247.0,\n",
       " 1260.0,\n",
       " 1261.0,\n",
       " 1265.0,\n",
       " 1267.0,\n",
       " 1270.0,\n",
       " 1291.0,\n",
       " 1307.0,\n",
       " 1314.0,\n",
       " 1316.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1320.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1324.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1329.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1342.0,\n",
       " 1347.0,\n",
       " 1351.0,\n",
       " 1352.0,\n",
       " 1355.0,\n",
       " 1356.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1371.0,\n",
       " 1376.0,\n",
       " 1378.0,\n",
       " 1381.0,\n",
       " nan,\n",
       " 1389.0,\n",
       " 1390.0,\n",
       " nan,\n",
       " 1392.0,\n",
       " 1393.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1397.0,\n",
       " 1411.0,\n",
       " 1434.0,\n",
       " 1441.0,\n",
       " 1443.0,\n",
       " 1445.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1451.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1456.0,\n",
       " 1474.0,\n",
       " 1475.0,\n",
       " 1496.0,\n",
       " 1497.0,\n",
       " 1499.0,\n",
       " 1512.0,\n",
       " 1513.0,\n",
       " 1514.0,\n",
       " 1515.0,\n",
       " 1516.0,\n",
       " 1520.0,\n",
       " 1531.0,\n",
       " 1537.0,\n",
       " 1538.0,\n",
       " 1540.0,\n",
       " 1542.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1549.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1564.0,\n",
       " 1566.0,\n",
       " 1569.0,\n",
       " 1573.0,\n",
       " 1574.0,\n",
       " 1583.0,\n",
       " 1585.0,\n",
       " 1586.0,\n",
       " nan,\n",
       " nan,\n",
       " 1595.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1614.0,\n",
       " 1618.0,\n",
       " 1636.0,\n",
       " 1637.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1643.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1657.0,\n",
       " 1670.0,\n",
       " 1679.0,\n",
       " 1683.0,\n",
       " 1689.0,\n",
       " 1699.0,\n",
       " 1706.0,\n",
       " 1709.0,\n",
       " 1716.0,\n",
       " 1720.0,\n",
       " 1725.0,\n",
       " 1735.0,\n",
       " 1739.0,\n",
       " 1750.0,\n",
       " 1760.0,\n",
       " 1762.0,\n",
       " 1763.0,\n",
       " 1764.0,\n",
       " 1765.0,\n",
       " 1766.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1784.0,\n",
       " 1785.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1815.0,\n",
       " 1818.0,\n",
       " 1829.0,\n",
       " 1833.0,\n",
       " 1834.0,\n",
       " 1835.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1874.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1893.0,\n",
       " 1894.0,\n",
       " 1909.0,\n",
       " 1913.0,\n",
       " 1915.0,\n",
       " 1934.0,\n",
       " 1938.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1950.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1963.0,\n",
       " 1965.0,\n",
       " 1970.0,\n",
       " 1977.0,\n",
       " 1979.0,\n",
       " 1991.0,\n",
       " 1994.0,\n",
       " 2019.0,\n",
       " 2020.0,\n",
       " 2023.0,\n",
       " 2028.0,\n",
       " 2036.0,\n",
       " 2039.0,\n",
       " 2040.0,\n",
       " 2043.0,\n",
       " 2044.0,\n",
       " 2046.0,\n",
       " 2063.0,\n",
       " 2067.0,\n",
       " 2074.0,\n",
       " 2078.0,\n",
       " 2079.0,\n",
       " 2101.0,\n",
       " 2102.0,\n",
       " 2108.0,\n",
       " 2111.0,\n",
       " 2116.0,\n",
       " 2126.0,\n",
       " 2129.0,\n",
       " 2132.0,\n",
       " 2144.0,\n",
       " 2148.0,\n",
       " 2150.0,\n",
       " 2164.0,\n",
       " 2166.0,\n",
       " 2168.0,\n",
       " 2170.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2195.0,\n",
       " 2228.0,\n",
       " 2238.0,\n",
       " 2251.0,\n",
       " 2252.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2260.0,\n",
       " 2261.0,\n",
       " nan,\n",
       " 2271.0,\n",
       " 2310.0,\n",
       " 2316.0,\n",
       " 2322.0,\n",
       " 2325.0,\n",
       " 2328.0,\n",
       " 2342.0,\n",
       " 2343.0,\n",
       " 2344.0,\n",
       " 2361.0,\n",
       " 2376.0,\n",
       " 2379.0,\n",
       " 2387.0,\n",
       " 2392.0,\n",
       " 2403.0,\n",
       " 2405.0,\n",
       " 2423.0,\n",
       " 2425.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2451.0,\n",
       " 2456.0,\n",
       " 2485.0,\n",
       " 2510.0,\n",
       " 2513.0,\n",
       " nan,\n",
       " 2516.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2526.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2546.0,\n",
       " 2564.0,\n",
       " 2566.0,\n",
       " 2592.0,\n",
       " 2595.0,\n",
       " 2599.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2616.0,\n",
       " 2617.0,\n",
       " 2634.0,\n",
       " 2635.0,\n",
       " 2645.0,\n",
       " 2658.0,\n",
       " 2660.0,\n",
       " 2665.0,\n",
       " 2669.0,\n",
       " nan,\n",
       " 2676.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2704.0,\n",
       " 2714.0,\n",
       " 2719.0,\n",
       " 2725.0,\n",
       " 2728.0,\n",
       " 2750.0,\n",
       " 2753.0,\n",
       " 2770.0,\n",
       " 2772.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2789.0,\n",
       " 2795.0,\n",
       " 2798.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2806.0,\n",
       " 2807.0,\n",
       " 2808.0,\n",
       " nan,\n",
       " nan,\n",
       " 2816.0,\n",
       " 2817.0,\n",
       " 2820.0,\n",
       " 2824.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2830.0,\n",
       " nan,\n",
       " nan,\n",
       " 2833.0,\n",
       " nan,\n",
       " 2842.0,\n",
       " 2848.0,\n",
       " 2849.0,\n",
       " 2860.0,\n",
       " 2875.0,\n",
       " 2885.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2898.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2902.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2910.0,\n",
       " 2912.0,\n",
       " 2914.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2921.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2942.0,\n",
       " 2952.0,\n",
       " 2956.0,\n",
       " 2958.0,\n",
       " 2965.0,\n",
       " 2970.0,\n",
       " 2975.0,\n",
       " 2981.0,\n",
       " 2989.0,\n",
       " 2993.0,\n",
       " 2999.0,\n",
       " 3000.0,\n",
       " 3014.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3036.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3040.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3052.0,\n",
       " 3058.0,\n",
       " 3077.0,\n",
       " 3094.0,\n",
       " 3097.0,\n",
       " 3098.0,\n",
       " 3108.0,\n",
       " 3111.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3136.0,\n",
       " nan,\n",
       " 3143.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3178.0,\n",
       " 3184.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3198.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3248.0,\n",
       " 3262.0,\n",
       " 3265.0,\n",
       " 3270.0,\n",
       " 3272.0,\n",
       " 3277.0,\n",
       " 3280.0,\n",
       " 3293.0,\n",
       " 3301.0,\n",
       " 3305.0,\n",
       " 3306.0,\n",
       " 3309.0,\n",
       " 3310.0,\n",
       " 3330.0,\n",
       " 3331.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3342.0,\n",
       " nan,\n",
       " nan,\n",
       " 3345.0,\n",
       " 3369.0,\n",
       " 3376.0,\n",
       " 3382.0,\n",
       " 3388.0,\n",
       " 3396.0,\n",
       " 3401.0,\n",
       " 3405.0,\n",
       " 3406.0,\n",
       " 3411.0,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 7.0,\n",
       " 3.0,\n",
       " 17.0,\n",
       " 2.0,\n",
       " 11.0,\n",
       " 4.0,\n",
       " 22.0,\n",
       " 23.0,\n",
       " 21.0,\n",
       " 25.0,\n",
       " 26.0,\n",
       " 30.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 33.0,\n",
       " 40.0,\n",
       " 36.0,\n",
       " 44.0,\n",
       " 8.0,\n",
       " 39.0,\n",
       " 47.0,\n",
       " 50.0,\n",
       " 51.0,\n",
       " 10.0,\n",
       " 53.0,\n",
       " 54.0,\n",
       " 58.0,\n",
       " 62.0,\n",
       " 73.0,\n",
       " 75.0,\n",
       " 93.0,\n",
       " 95.0,\n",
       " 97.0,\n",
       " 100.0,\n",
       " 103.0,\n",
       " 109.0,\n",
       " 113.0,\n",
       " 114.0,\n",
       " 118.0,\n",
       " 124.0,\n",
       " 125.0,\n",
       " 128.0,\n",
       " 130.0,\n",
       " 134.0,\n",
       " 138.0,\n",
       " 145.0,\n",
       " 146.0,\n",
       " 152.0,\n",
       " 155.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 165.0,\n",
       " 166.0,\n",
       " 161.0,\n",
       " 168.0,\n",
       " 170.0,\n",
       " 173.0,\n",
       " 175.0,\n",
       " 35.0,\n",
       " 181.0,\n",
       " 183.0,\n",
       " 192.0,\n",
       " 194.0,\n",
       " 195.0,\n",
       " 196.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 203.0,\n",
       " nan,\n",
       " 205.0,\n",
       " 206.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 212.0,\n",
       " 213.0,\n",
       " 216.0,\n",
       " 220.0,\n",
       " 222.0,\n",
       " 223.0,\n",
       " 224.0,\n",
       " 225.0,\n",
       " 242.0,\n",
       " 249.0,\n",
       " 256.0,\n",
       " 257.0,\n",
       " 258.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 267.0,\n",
       " nan,\n",
       " 269.0,\n",
       " 270.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 276.0,\n",
       " 277.0,\n",
       " 278.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 283.0,\n",
       " 287.0,\n",
       " 288.0,\n",
       " 290.0,\n",
       " 298.0,\n",
       " 305.0,\n",
       " 321.0,\n",
       " 322.0,\n",
       " 328.0,\n",
       " 341.0,\n",
       " 346.0,\n",
       " 354.0,\n",
       " 359.0,\n",
       " 362.0,\n",
       " 363.0,\n",
       " 369.0,\n",
       " 372.0,\n",
       " 373.0,\n",
       " 374.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 395.0,\n",
       " 396.0,\n",
       " 409.0,\n",
       " 411.0,\n",
       " 414.0,\n",
       " 418.0,\n",
       " 421.0,\n",
       " 423.0,\n",
       " 452.0,\n",
       " 459.0,\n",
       " 480.0,\n",
       " 481.0,\n",
       " 485.0,\n",
       " 486.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 490.0,\n",
       " nan,\n",
       " 492.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 498.0,\n",
       " 499.0,\n",
       " 507.0,\n",
       " 517.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 524.0,\n",
       " nan,\n",
       " 526.0,\n",
       " 527.0,\n",
       " nan,\n",
       " nan,\n",
       " 534.0,\n",
       " 539.0,\n",
       " 548.0,\n",
       " 555.0,\n",
       " 558.0,\n",
       " nan,\n",
       " nan,\n",
       " 562.0,\n",
       " 563.0,\n",
       " 564.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 570.0,\n",
       " 573.0,\n",
       " 575.0,\n",
       " 577.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 581.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 590.0,\n",
       " 594.0,\n",
       " 596.0,\n",
       " 602.0,\n",
       " 604.0,\n",
       " 608.0,\n",
       " 612.0,\n",
       " 614.0,\n",
       " 622.0,\n",
       " 624.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 631.0,\n",
       " nan,\n",
       " 633.0,\n",
       " nan,\n",
       " 638.0,\n",
       " 641.0,\n",
       " 643.0,\n",
       " 645.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 652.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 660.0,\n",
       " 665.0,\n",
       " 672.0,\n",
       " 685.0,\n",
       " 692.0,\n",
       " 712.0,\n",
       " 715.0,\n",
       " 723.0,\n",
       " 725.0,\n",
       " 726.0,\n",
       " 743.0,\n",
       " 764.0,\n",
       " 769.0,\n",
       " 770.0,\n",
       " 775.0,\n",
       " 779.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 156.0,\n",
       " 790.0,\n",
       " 795.0,\n",
       " 798.0,\n",
       " 805.0,\n",
       " 806.0,\n",
       " 812.0,\n",
       " 820.0,\n",
       " 825.0,\n",
       " 834.0,\n",
       " 837.0,\n",
       " 841.0,\n",
       " 846.0,\n",
       " 847.0,\n",
       " 852.0,\n",
       " 856.0,\n",
       " 859.0,\n",
       " 866.0,\n",
       " 868.0,\n",
       " nan,\n",
       " 870.0,\n",
       " 871.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 880.0,\n",
       " 881.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 886.0,\n",
       " 887.0,\n",
       " nan,\n",
       " nan,\n",
       " 890.0,\n",
       " nan,\n",
       " nan,\n",
       " 897.0,\n",
       " 898.0,\n",
       " 899.0,\n",
       " 913.0,\n",
       " 920.0,\n",
       " 924.0,\n",
       " 927.0,\n",
       " 928.0,\n",
       " 929.0,\n",
       " 939.0,\n",
       " 942.0,\n",
       " 944.0,\n",
       " 946.0,\n",
       " 949.0,\n",
       " 951.0,\n",
       " 952.0,\n",
       " nan,\n",
       " 955.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 961.0,\n",
       " nan,\n",
       " nan,\n",
       " 964.0,\n",
       " 973.0,\n",
       " 977.0,\n",
       " 980.0,\n",
       " 1001.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1012.0,\n",
       " 1015.0,\n",
       " 1016.0,\n",
       " 1023.0,\n",
       " 1028.0,\n",
       " 1040.0,\n",
       " 1045.0,\n",
       " 1047.0,\n",
       " 1052.0,\n",
       " 1053.0,\n",
       " 1054.0,\n",
       " 1060.0,\n",
       " 1062.0,\n",
       " 1063.0,\n",
       " 1066.0,\n",
       " 1081.0,\n",
       " 1082.0,\n",
       " 1084.0,\n",
       " 1093.0,\n",
       " 1095.0,\n",
       " 1099.0,\n",
       " 1112.0,\n",
       " 1115.0,\n",
       " 1123.0,\n",
       " 1126.0,\n",
       " 1130.0,\n",
       " 1133.0,\n",
       " nan,\n",
       " nan,\n",
       " 1142.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1149.0,\n",
       " 1160.0,\n",
       " 1164.0,\n",
       " 1169.0,\n",
       " 1173.0,\n",
       " 1174.0,\n",
       " 1184.0,\n",
       " 1194.0,\n",
       " 1195.0,\n",
       " 1202.0,\n",
       " 1205.0,\n",
       " 1207.0,\n",
       " 1215.0,\n",
       " 1223.0,\n",
       " 1224.0,\n",
       " 1228.0,\n",
       " 1241.0,\n",
       " 1249.0,\n",
       " 1252.0,\n",
       " 1253.0,\n",
       " 1256.0,\n",
       " 1257.0,\n",
       " 1261.0,\n",
       " 1266.0,\n",
       " 1268.0,\n",
       " 1270.0,\n",
       " 1272.0,\n",
       " 1274.0,\n",
       " 1278.0,\n",
       " 1286.0,\n",
       " 1289.0,\n",
       " 1292.0,\n",
       " 1293.0,\n",
       " 1296.0,\n",
       " 1308.0,\n",
       " 1315.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1321.0,\n",
       " nan,\n",
       " nan,\n",
       " 1324.0,\n",
       " nan,\n",
       " 1337.0,\n",
       " 1350.0,\n",
       " 1352.0,\n",
       " 1355.0,\n",
       " 1357.0,\n",
       " 1358.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1369.0,\n",
       " nan,\n",
       " 1376.0,\n",
       " 1378.0,\n",
       " 1380.0,\n",
       " 1383.0,\n",
       " 1385.0,\n",
       " 1386.0,\n",
       " nan,\n",
       " 1389.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1395.0,\n",
       " nan,\n",
       " nan,\n",
       " 1399.0,\n",
       " 1405.0,\n",
       " 1412.0,\n",
       " 1413.0,\n",
       " 1424.0,\n",
       " 1427.0,\n",
       " 1430.0,\n",
       " 1432.0,\n",
       " 1437.0,\n",
       " 1444.0,\n",
       " 1447.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1455.0,\n",
       " nan,\n",
       " 1458.0,\n",
       " 1459.0,\n",
       " 1460.0,\n",
       " 1462.0,\n",
       " 1469.0,\n",
       " 1471.0,\n",
       " 1475.0,\n",
       " 1478.0,\n",
       " 1485.0,\n",
       " 1487.0,\n",
       " 1493.0,\n",
       " 1495.0,\n",
       " 1501.0,\n",
       " 1507.0,\n",
       " 1517.0,\n",
       " 1519.0,\n",
       " 1520.0,\n",
       " 1521.0,\n",
       " 1525.0,\n",
       " 1529.0,\n",
       " 1531.0,\n",
       " 1534.0,\n",
       " 1536.0,\n",
       " 1538.0,\n",
       " 1540.0,\n",
       " 1542.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1555.0,\n",
       " 1556.0,\n",
       " 1557.0,\n",
       " 1561.0,\n",
       " 1562.0,\n",
       " 1564.0,\n",
       " 1571.0,\n",
       " 1576.0,\n",
       " 1580.0,\n",
       " 1583.0,\n",
       " 1590.0,\n",
       " nan,\n",
       " nan,\n",
       " 1595.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1601.0,\n",
       " nan,\n",
       " 1604.0,\n",
       " 1610.0,\n",
       " 1611.0,\n",
       " 1621.0,\n",
       " 1622.0,\n",
       " 1624.0,\n",
       " 1637.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1645.0,\n",
       " 1646.0,\n",
       " nan,\n",
       " 1649.0,\n",
       " 1651.0,\n",
       " 1660.0,\n",
       " 1661.0,\n",
       " 1667.0,\n",
       " 1668.0,\n",
       " 1674.0,\n",
       " 1676.0,\n",
       " 1681.0,\n",
       " 1687.0,\n",
       " 1699.0,\n",
       " 1708.0,\n",
       " 1709.0,\n",
       " 1713.0,\n",
       " 1719.0,\n",
       " 1729.0,\n",
       " 1735.0,\n",
       " 1736.0,\n",
       " 1737.0,\n",
       " 1753.0,\n",
       " 1757.0,\n",
       " 1761.0,\n",
       " 1764.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1773.0,\n",
       " 1776.0,\n",
       " 1780.0,\n",
       " 1781.0,\n",
       " 1787.0,\n",
       " 1790.0,\n",
       " 1797.0,\n",
       " 1799.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1805.0,\n",
       " nan,\n",
       " 1807.0,\n",
       " nan,\n",
       " 1812.0,\n",
       " 1813.0,\n",
       " 1815.0,\n",
       " 1818.0,\n",
       " 1822.0,\n",
       " 1826.0,\n",
       " 1829.0,\n",
       " 1838.0,\n",
       " 1841.0,\n",
       " 1844.0,\n",
       " 1848.0,\n",
       " nan,\n",
       " 1850.0,\n",
       " nan,\n",
       " nan,\n",
       " 1853.0,\n",
       " nan,\n",
       " nan,\n",
       " 1856.0,\n",
       " nan,\n",
       " 1860.0,\n",
       " 1862.0,\n",
       " 1863.0,\n",
       " 1865.0,\n",
       " 1866.0,\n",
       " 1870.0,\n",
       " nan,\n",
       " nan,\n",
       " 1873.0,\n",
       " 1874.0,\n",
       " 1875.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1881.0,\n",
       " 1883.0,\n",
       " 1895.0,\n",
       " 1904.0,\n",
       " 1906.0,\n",
       " 1908.0,\n",
       " 1912.0,\n",
       " 1914.0,\n",
       " 1915.0,\n",
       " 1923.0,\n",
       " 1927.0,\n",
       " 1935.0,\n",
       " 1942.0,\n",
       " 1944.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1955.0,\n",
       " 1961.0,\n",
       " 1963.0,\n",
       " 1964.0,\n",
       " 1966.0,\n",
       " 1970.0,\n",
       " 1972.0,\n",
       " 1980.0,\n",
       " 1995.0,\n",
       " 1998.0,\n",
       " 2002.0,\n",
       " 2007.0,\n",
       " 2008.0,\n",
       " 2010.0,\n",
       " 2022.0,\n",
       " 2024.0,\n",
       " 2032.0,\n",
       " 2035.0,\n",
       " 2037.0,\n",
       " 2038.0,\n",
       " 2059.0,\n",
       " 2061.0,\n",
       " 2066.0,\n",
       " 2067.0,\n",
       " 2072.0,\n",
       " 2077.0,\n",
       " 2085.0,\n",
       " 2086.0,\n",
       " 2091.0,\n",
       " 2092.0,\n",
       " 2093.0,\n",
       " 2099.0,\n",
       " 2104.0,\n",
       " 2108.0,\n",
       " 2111.0,\n",
       " 2116.0,\n",
       " 2119.0,\n",
       " 2121.0,\n",
       " 2125.0,\n",
       " 2130.0,\n",
       " 2131.0,\n",
       " 2134.0,\n",
       " 2140.0,\n",
       " 2142.0,\n",
       " 2149.0,\n",
       " 2151.0,\n",
       " 2154.0,\n",
       " 2167.0,\n",
       " 2169.0,\n",
       " 2170.0,\n",
       " 2171.0,\n",
       " 2179.0,\n",
       " 2181.0,\n",
       " 2182.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2190.0,\n",
       " nan,\n",
       " nan,\n",
       " 2197.0,\n",
       " 2198.0,\n",
       " 2213.0,\n",
       " 2214.0,\n",
       " 2219.0,\n",
       " 2220.0,\n",
       " 2223.0,\n",
       " 2230.0,\n",
       " 2234.0,\n",
       " 2237.0,\n",
       " 2244.0,\n",
       " 2245.0,\n",
       " 2248.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2256.0,\n",
       " nan,\n",
       " 2258.0,\n",
       " 2259.0,\n",
       " nan,\n",
       " nan,\n",
       " 2262.0,\n",
       " 2264.0,\n",
       " 2269.0,\n",
       " 2272.0,\n",
       " 2275.0,\n",
       " 2282.0,\n",
       " 2290.0,\n",
       " 2291.0,\n",
       " 2294.0,\n",
       " 2296.0,\n",
       " 2298.0,\n",
       " 2299.0,\n",
       " 2306.0,\n",
       " 2315.0,\n",
       " 2323.0,\n",
       " 2326.0,\n",
       " 2328.0,\n",
       " 2335.0,\n",
       " 2337.0,\n",
       " 2343.0,\n",
       " 2344.0,\n",
       " 2348.0,\n",
       " 2350.0,\n",
       " 2352.0,\n",
       " 2356.0,\n",
       " 2358.0,\n",
       " 2370.0,\n",
       " 2374.0,\n",
       " 2377.0,\n",
       " 2379.0,\n",
       " 2384.0,\n",
       " 2385.0,\n",
       " 2390.0,\n",
       " 2391.0,\n",
       " 2397.0,\n",
       " 2401.0,\n",
       " 2405.0,\n",
       " 2410.0,\n",
       " 2419.0,\n",
       " 2420.0,\n",
       " 2423.0,\n",
       " 2427.0,\n",
       " 2430.0,\n",
       " 2432.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2439.0,\n",
       " 2446.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2455.0,\n",
       " 2457.0,\n",
       " 2466.0,\n",
       " 2467.0,\n",
       " 2469.0,\n",
       " 2476.0,\n",
       " 2481.0,\n",
       " 2483.0,\n",
       " 2492.0,\n",
       " 2493.0,\n",
       " 2499.0,\n",
       " 2504.0,\n",
       " 2510.0,\n",
       " 2513.0,\n",
       " nan,\n",
       " 2516.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2537.0,\n",
       " 2541.0,\n",
       " 2547.0,\n",
       " 2555.0,\n",
       " 2559.0,\n",
       " 2563.0,\n",
       " 2572.0,\n",
       " 2573.0,\n",
       " 2576.0,\n",
       " 2579.0,\n",
       " 2580.0,\n",
       " 2585.0,\n",
       " 2587.0,\n",
       " 2591.0,\n",
       " 2592.0,\n",
       " 2597.0,\n",
       " 2598.0,\n",
       " nan,\n",
       " 2602.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2611.0,\n",
       " 2613.0,\n",
       " 2615.0,\n",
       " 2617.0,\n",
       " 2619.0,\n",
       " 2623.0,\n",
       " 2628.0,\n",
       " 2631.0,\n",
       " 2635.0,\n",
       " 2639.0,\n",
       " 2641.0,\n",
       " 2648.0,\n",
       " 2650.0,\n",
       " 2661.0,\n",
       " 2667.0,\n",
       " 2670.0,\n",
       " 2675.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2679.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2684.0,\n",
       " 2686.0,\n",
       " 2688.0,\n",
       " 2689.0,\n",
       " 2691.0,\n",
       " 2711.0,\n",
       " 2715.0,\n",
       " 2717.0,\n",
       " 2726.0,\n",
       " 2728.0,\n",
       " 2730.0,\n",
       " 2739.0,\n",
       " 2742.0,\n",
       " 2743.0,\n",
       " 2746.0,\n",
       " 2748.0,\n",
       " 2764.0,\n",
       " 2768.0,\n",
       " 2770.0,\n",
       " 2772.0,\n",
       " nan,\n",
       " nan,\n",
       " 2777.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2787.0,\n",
       " 2788.0,\n",
       " 2794.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2806.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2819.0,\n",
       " 2823.0,\n",
       " 2824.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2828.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2844.0,\n",
       " 2846.0,\n",
       " 2849.0,\n",
       " 2852.0,\n",
       " 2854.0,\n",
       " 2860.0,\n",
       " 2863.0,\n",
       " 2886.0,\n",
       " 2891.0,\n",
       " 2892.0,\n",
       " 2893.0,\n",
       " 2894.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 2904.0,\n",
       " nan,\n",
       " 2906.0,\n",
       " 2909.0,\n",
       " 2910.0,\n",
       " 2914.0,\n",
       " 2915.0,\n",
       " nan,\n",
       " 2917.0,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " '2012-12-29 11:15:08',\n",
       " '2012-02-08 22:57:24',\n",
       " '2013-01-01 03:57:45',\n",
       " '2012-10-19 12:23:47',\n",
       " '2011-05-03 13:39:20',\n",
       " '2014-04-23 20:50:59',\n",
       " '2012-06-16 20:00:05',\n",
       " '2012-12-12 18:18:39',\n",
       " '2012-02-29 20:26:06',\n",
       " '2012-05-23 11:01:32',\n",
       " '2011-11-28 13:58:27',\n",
       " '2012-08-07 17:14:05',\n",
       " '2011-02-21 16:30:39',\n",
       " '2011-06-03 23:37:38',\n",
       " '2010-11-17 07:59:02',\n",
       " '2011-06-01 10:07:48',\n",
       " '2012-09-15 19:11:16',\n",
       " '2010-12-09 12:28:44',\n",
       " '2012-08-07 12:21:33',\n",
       " '2010-10-12 09:49:10',\n",
       " '2014-06-20 06:34:29',\n",
       " '2012-07-27 11:56:20',\n",
       " '2012-02-19 11:46:56',\n",
       " '2011-12-22 05:17:03',\n",
       " '2010-08-07 17:52:12',\n",
       " '2011-03-17 21:59:03',\n",
       " '2011-02-15 17:21:45',\n",
       " '2013-08-23 10:07:36',\n",
       " '2012-06-30 13:58:57',\n",
       " '2012-01-31 01:03:47',\n",
       " '2011-03-20 16:35:29',\n",
       " '2012-10-30 15:50:39',\n",
       " '2011-12-31 01:36:06',\n",
       " '2014-06-17 01:09:08',\n",
       " '2013-09-29 21:10:43',\n",
       " '2012-07-20 17:02:51',\n",
       " '2011-03-19 19:40:46',\n",
       " '2011-11-18 16:50:19',\n",
       " '2012-09-04 12:13:33',\n",
       " '2012-04-18 18:37:00',\n",
       " '2012-07-17 07:45:58',\n",
       " '2012-08-06 13:52:42',\n",
       " '2012-09-19 20:59:58',\n",
       " '2011-08-31 20:37:10',\n",
       " '2011-06-04 19:07:31',\n",
       " '2010-10-08 23:30:44',\n",
       " '2011-05-09 08:32:11',\n",
       " '2012-06-28 14:35:55',\n",
       " '2013-07-03 14:55:20',\n",
       " '2013-01-06 13:10:06',\n",
       " '2014-09-02 13:56:03',\n",
       " '2012-05-18 15:09:33',\n",
       " '2011-08-29 10:28:15',\n",
       " '2010-12-29 16:35:50',\n",
       " '2011-03-03 22:12:41',\n",
       " '2011-09-08 14:04:05',\n",
       " '2012-04-07 14:16:09',\n",
       " '2011-10-05 18:18:06',\n",
       " '2012-10-06 01:03:03',\n",
       " '2012-01-27 01:15:05',\n",
       " '2012-09-05 06:48:16',\n",
       " '2013-05-24 13:02:17',\n",
       " '2012-07-10 08:24:31',\n",
       " '2012-05-14 21:46:44',\n",
       " '2011-03-27 16:05:08',\n",
       " '2011-10-10 23:01:32',\n",
       " '2012-11-26 21:07:39',\n",
       " '2012-08-02 13:19:49',\n",
       " '2012-12-10 21:51:21',\n",
       " '2011-03-22 04:50:36',\n",
       " '2012-03-23 17:35:21',\n",
       " '2013-07-01 05:58:31',\n",
       " '2012-07-28 20:16:56',\n",
       " '2012-03-27 04:50:03',\n",
       " '2011-09-09 09:02:00',\n",
       " '2012-06-19 13:10:22',\n",
       " '2013-04-24 18:34:14',\n",
       " '2011-05-24 07:51:34',\n",
       " '2011-06-13 11:41:14',\n",
       " '2011-05-23 06:21:18',\n",
       " '2012-07-03 20:46:10',\n",
       " '2011-11-22 17:56:25',\n",
       " '2010-11-23 18:18:57',\n",
       " '2010-12-16 22:27:47',\n",
       " '2012-02-11 09:03:40',\n",
       " '2013-01-07 16:28:23',\n",
       " '2011-03-02 08:36:21',\n",
       " '2012-03-01 12:20:14',\n",
       " '2011-04-03 09:10:18',\n",
       " '2010-12-07 19:18:04',\n",
       " '2011-08-10 16:48:51',\n",
       " '2011-02-26 18:54:20',\n",
       " '2012-06-20 02:02:51',\n",
       " '2012-11-20 04:08:33',\n",
       " '2012-03-26 23:16:15',\n",
       " '2012-08-24 14:52:14',\n",
       " '2010-08-30 15:13:35',\n",
       " '2014-03-24 20:17:53',\n",
       " '2013-08-20 00:03:06',\n",
       " '2012-02-10 13:23:18',\n",
       " '2013-11-03 17:07:07',\n",
       " '2012-02-08 18:40:56',\n",
       " '2012-01-22 08:35:22',\n",
       " '2012-10-17 03:10:02',\n",
       " '2011-10-03 20:02:57',\n",
       " '2012-10-01 00:18:52',\n",
       " '2012-07-05 02:28:03',\n",
       " '2012-10-15 10:36:04',\n",
       " '2012-04-03 11:09:03',\n",
       " '2012-12-19 11:38:59',\n",
       " '2011-05-09 18:16:21',\n",
       " '2011-01-01 18:07:23',\n",
       " '2011-09-03 11:24:16',\n",
       " '2012-11-19 19:36:59',\n",
       " '2011-12-03 15:09:05',\n",
       " '2013-07-01 00:55:02',\n",
       " '2012-08-13 01:50:26',\n",
       " '2011-05-06 13:47:26',\n",
       " '2013-09-04 15:59:24',\n",
       " '2011-11-30 18:33:33',\n",
       " '2010-09-11 08:45:18',\n",
       " '2013-08-12 10:03:36',\n",
       " '2014-02-25 05:57:19',\n",
       " '2010-11-12 10:35:00',\n",
       " '2012-03-26 14:27:30',\n",
       " '2012-09-05 02:56:19',\n",
       " '2012-11-09 12:25:52',\n",
       " '2012-06-11 00:00:15',\n",
       " '2012-01-25 05:18:00',\n",
       " '2012-12-06 11:29:12',\n",
       " '2012-07-23 15:54:26',\n",
       " '2010-10-05 01:56:29',\n",
       " '2011-11-12 01:53:34',\n",
       " '2011-10-04 01:04:10',\n",
       " '2013-03-29 21:42:15',\n",
       " '2012-07-10 07:11:34',\n",
       " '2014-07-15 19:01:01',\n",
       " '2011-08-11 12:12:59',\n",
       " '2012-07-22 03:53:50',\n",
       " '2011-06-18 17:10:54',\n",
       " '2012-01-31 21:39:50',\n",
       " '2012-04-22 10:14:57',\n",
       " '2012-08-23 07:37:45',\n",
       " '2012-06-03 12:28:02',\n",
       " '2012-04-24 13:10:04',\n",
       " '2013-07-04 02:31:04',\n",
       " '2012-03-17 16:40:25',\n",
       " '2012-06-15 04:08:12',\n",
       " '2011-01-16 14:11:18',\n",
       " '2010-07-20 13:22:18',\n",
       " '2012-08-16 19:26:46',\n",
       " '2012-12-25 22:51:29',\n",
       " '2012-06-29 00:17:57',\n",
       " '2012-12-12 17:10:41',\n",
       " '2011-12-02 23:44:43',\n",
       " '2012-08-17 19:23:22',\n",
       " '2012-12-11 17:38:29',\n",
       " '2010-11-30 18:21:49',\n",
       " '2010-12-14 21:58:57',\n",
       " '2012-07-08 19:12:04',\n",
       " '2011-02-20 16:24:28',\n",
       " '2012-08-01 15:20:16',\n",
       " '2013-01-16 09:09:05',\n",
       " '2011-10-18 17:52:32',\n",
       " '2012-10-12 16:31:13',\n",
       " '2011-07-28 16:42:05',\n",
       " '2011-03-31 22:39:28',\n",
       " '2012-11-08 21:50:07',\n",
       " '2013-08-10 21:59:42',\n",
       " '2010-09-24 13:19:19',\n",
       " '2012-12-13 23:22:16',\n",
       " '2012-05-04 22:13:33',\n",
       " '2012-11-21 16:46:15',\n",
       " '2012-06-14 20:01:15',\n",
       " '2012-08-13 21:50:29',\n",
       " '2013-01-03 21:21:47',\n",
       " '2012-07-25 21:52:44',\n",
       " '2012-11-19 10:49:39',\n",
       " '2010-08-16 07:14:37',\n",
       " '2013-09-14 20:57:36',\n",
       " '2011-10-03 18:28:46',\n",
       " '2012-08-25 23:23:11',\n",
       " '2012-12-20 23:58:59',\n",
       " '2012-02-02 12:21:41',\n",
       " '2012-07-24 16:08:59',\n",
       " '2012-11-07 13:39:08',\n",
       " '2012-05-04 19:07:23',\n",
       " '2012-06-11 13:38:20',\n",
       " '2011-11-16 20:08:37',\n",
       " '2012-03-13 06:55:01',\n",
       " '2012-04-18 23:42:42',\n",
       " '2012-08-31 08:50:43',\n",
       " '2012-08-12 01:38:50',\n",
       " '2010-10-29 12:24:53',\n",
       " '2012-07-20 14:13:12',\n",
       " '2012-11-14 19:10:41',\n",
       " '2010-09-12 04:19:37',\n",
       " '2012-10-30 19:49:54',\n",
       " '2012-06-21 06:52:38',\n",
       " '2011-04-01 11:44:21',\n",
       " '2012-11-02 08:08:32',\n",
       " '2012-11-14 11:22:24',\n",
       " '2012-09-26 23:02:24',\n",
       " '2012-10-24 08:39:30',\n",
       " '2010-07-26 15:31:39',\n",
       " '2013-07-05 13:38:46',\n",
       " '2011-04-22 16:11:54',\n",
       " '2011-02-10 20:59:20',\n",
       " '2012-01-16 16:18:36',\n",
       " '2012-02-03 16:08:53',\n",
       " '2013-09-04 15:59:13',\n",
       " '2012-04-20 06:41:13',\n",
       " '2011-11-19 20:48:06',\n",
       " '2011-09-30 07:00:08',\n",
       " '2012-01-06 09:08:32',\n",
       " '2011-09-16 02:09:27',\n",
       " '2012-05-07 20:58:23',\n",
       " '2012-05-15 13:32:05',\n",
       " '2011-11-19 08:57:26',\n",
       " '2012-08-02 20:17:01',\n",
       " '2011-07-14 21:29:01',\n",
       " '2010-09-02 09:52:44',\n",
       " '2013-04-11 08:04:47',\n",
       " '2011-08-18 12:26:30',\n",
       " '2014-03-31 15:27:52',\n",
       " '2012-03-28 21:33:52',\n",
       " '2012-02-08 14:57:54',\n",
       " '2011-12-06 15:13:12',\n",
       " '2012-10-09 05:41:40',\n",
       " '2013-08-28 13:08:47',\n",
       " '2011-01-29 12:04:57',\n",
       " '2012-07-12 15:40:07',\n",
       " '2011-10-05 01:55:44',\n",
       " '2011-08-04 19:17:32',\n",
       " '2014-04-25 09:01:47',\n",
       " '2011-01-16 23:09:09',\n",
       " '2013-02-05 12:58:24',\n",
       " '2012-03-22 09:40:59',\n",
       " '2012-05-23 22:45:15',\n",
       " '2011-03-23 13:49:22',\n",
       " '2011-09-16 22:21:47',\n",
       " '2013-08-23 10:05:47',\n",
       " '2011-11-15 11:31:26',\n",
       " '2010-09-16 06:42:57',\n",
       " '2012-04-09 22:43:54',\n",
       " '2012-07-06 07:25:05',\n",
       " '2012-08-12 13:41:59',\n",
       " '2012-12-14 21:30:47',\n",
       " '2013-07-21 23:11:05',\n",
       " '2011-12-11 09:38:05',\n",
       " '2010-10-13 12:34:00',\n",
       " '2013-08-28 09:56:50',\n",
       " '2012-04-23 13:20:34',\n",
       " '2011-03-22 07:20:49',\n",
       " '2013-06-25 00:20:29',\n",
       " '2012-03-01 18:55:28',\n",
       " '2012-02-05 22:56:56',\n",
       " '2011-06-03 07:32:50',\n",
       " '2011-05-04 16:12:13',\n",
       " '2014-04-17 03:10:01',\n",
       " '2012-09-25 16:37:38',\n",
       " '2012-06-20 14:48:31',\n",
       " '2012-03-18 16:55:54',\n",
       " '2011-09-04 07:24:23',\n",
       " '2012-09-01 23:24:15',\n",
       " '2011-04-09 14:27:17',\n",
       " '2012-05-12 18:02:39',\n",
       " '2011-01-11 08:38:13',\n",
       " '2011-01-16 07:38:37',\n",
       " '2012-01-14 10:34:23',\n",
       " '2011-09-24 00:24:01',\n",
       " '2012-12-05 14:07:15',\n",
       " '2012-08-30 20:34:31',\n",
       " '2011-11-20 09:44:06',\n",
       " '2012-06-22 08:25:45',\n",
       " '2013-12-17 13:03:00',\n",
       " '2010-10-06 14:33:18',\n",
       " '2011-11-12 22:42:54',\n",
       " '2012-07-26 12:22:31',\n",
       " '2012-06-11 13:08:39',\n",
       " '2011-11-01 16:34:23',\n",
       " '2012-08-08 21:21:35',\n",
       " '2012-08-20 15:40:05',\n",
       " '2012-05-01 20:04:54',\n",
       " '2012-09-24 10:57:02',\n",
       " '2012-01-28 09:37:15',\n",
       " '2011-04-11 05:55:35',\n",
       " '2010-10-08 07:22:07',\n",
       " '2012-11-27 14:22:37',\n",
       " '2011-10-20 15:05:34',\n",
       " '2012-05-19 17:55:08',\n",
       " '2011-12-07 15:17:54',\n",
       " '2014-08-09 10:16:09',\n",
       " '2012-07-13 23:15:31',\n",
       " '2014-02-15 15:56:11',\n",
       " '2012-06-18 19:06:04',\n",
       " '2012-09-14 20:57:15',\n",
       " '2012-10-28 19:33:46',\n",
       " '2013-08-24 16:38:03',\n",
       " '2011-10-07 02:15:16',\n",
       " '2011-06-29 20:16:31',\n",
       " '2012-08-02 09:05:03',\n",
       " '2010-12-14 21:37:07',\n",
       " '2011-01-05 16:59:03',\n",
       " '2012-12-11 09:17:07',\n",
       " '2012-12-12 14:09:29',\n",
       " '2011-09-01 09:16:46',\n",
       " '2012-02-13 20:21:10',\n",
       " '2010-10-15 20:20:29',\n",
       " '2012-01-03 22:05:46',\n",
       " '2011-01-29 02:00:13',\n",
       " '2011-02-06 17:03:49',\n",
       " '2012-07-12 11:40:21',\n",
       " '2012-12-26 08:55:07',\n",
       " '2011-09-21 20:43:20',\n",
       " '2011-08-04 20:29:48',\n",
       " '2011-05-29 11:33:36',\n",
       " '2012-04-02 15:25:21',\n",
       " '2011-11-22 13:12:37',\n",
       " '2013-05-17 07:05:22',\n",
       " '2012-10-27 13:28:47',\n",
       " '2012-11-11 17:58:53',\n",
       " '2012-10-10 21:30:35',\n",
       " '2012-09-12 15:43:49',\n",
       " '2013-04-14 03:04:40',\n",
       " '2011-05-27 06:45:52',\n",
       " '2012-01-21 09:07:08',\n",
       " '2012-08-05 11:54:51',\n",
       " '2012-06-06 18:12:40',\n",
       " '2011-12-31 18:53:48',\n",
       " '2012-04-24 13:23:29',\n",
       " '2013-11-27 14:46:12',\n",
       " '2011-05-05 07:18:45',\n",
       " '2012-06-21 01:36:39',\n",
       " '2011-05-18 08:21:47',\n",
       " '2011-02-13 17:30:58',\n",
       " '2011-07-31 08:52:37',\n",
       " '2012-03-21 10:50:36',\n",
       " '2013-09-04 01:59:52',\n",
       " '2012-01-08 03:48:29',\n",
       " '2013-01-10 14:17:12',\n",
       " '2012-10-18 16:44:13',\n",
       " '2012-01-23 18:09:54',\n",
       " '2013-01-01 04:48:01',\n",
       " '2012-11-18 17:35:03',\n",
       " '2012-09-04 15:21:56',\n",
       " '2012-10-03 01:43:34',\n",
       " '2012-09-24 10:04:04',\n",
       " '2014-05-12 05:22:00',\n",
       " '2011-03-30 18:43:46',\n",
       " '2012-09-30 21:44:14',\n",
       " '2011-03-08 16:45:09',\n",
       " '2012-04-12 07:00:23',\n",
       " '2010-08-13 18:01:09',\n",
       " '2011-07-09 16:47:49',\n",
       " '2012-08-14 06:32:51',\n",
       " '2012-02-02 01:01:41',\n",
       " '2011-07-29 01:01:31',\n",
       " '2011-10-26 15:52:44',\n",
       " '2012-12-30 16:27:09',\n",
       " '2012-07-25 01:46:36',\n",
       " '2013-10-21 06:36:47',\n",
       " '2012-12-06 20:37:02',\n",
       " '2012-01-09 19:13:43',\n",
       " '2011-05-22 21:37:31',\n",
       " '2011-09-27 12:32:02',\n",
       " '2011-05-02 07:07:58',\n",
       " '2011-04-16 01:47:49',\n",
       " '2011-09-28 00:12:56',\n",
       " '2013-01-16 15:25:45',\n",
       " '2013-02-11 03:49:57',\n",
       " '2012-06-14 16:32:23',\n",
       " '2011-08-22 07:25:24',\n",
       " '2012-05-25 17:42:18',\n",
       " '2011-10-13 03:32:05',\n",
       " '2012-11-07 20:22:23',\n",
       " '2012-06-06 04:48:45',\n",
       " '2013-12-12 13:43:42',\n",
       " '2012-06-25 12:31:09',\n",
       " '2011-11-03 22:52:31',\n",
       " '2012-03-23 11:40:14',\n",
       " '2013-01-21 14:31:20',\n",
       " '2012-04-10 06:46:53',\n",
       " '2012-12-12 18:12:38',\n",
       " '2012-03-15 00:05:45',\n",
       " '2011-07-14 18:57:05',\n",
       " '2012-08-15 01:09:27',\n",
       " '2013-01-28 21:57:07',\n",
       " '2011-08-16 19:06:17',\n",
       " '2014-07-28 02:19:47',\n",
       " '2012-11-12 22:53:27',\n",
       " '2012-06-24 21:26:01',\n",
       " '2012-09-05 13:24:12',\n",
       " '2012-12-24 15:29:45',\n",
       " '2012-08-22 07:58:32',\n",
       " '2012-05-28 19:10:48',\n",
       " '2011-10-04 23:35:14',\n",
       " '2010-10-04 15:49:02',\n",
       " '2011-11-12 18:31:27',\n",
       " '2012-04-30 16:19:06',\n",
       " '2012-10-20 10:29:48',\n",
       " '2011-09-28 00:02:30',\n",
       " '2012-11-09 12:08:25',\n",
       " '2010-07-26 11:26:49',\n",
       " '2010-11-19 09:35:48',\n",
       " '2011-06-15 12:35:29',\n",
       " '2012-05-12 09:51:32',\n",
       " '2011-10-29 21:38:55',\n",
       " '2012-12-06 22:59:01',\n",
       " '2011-04-03 11:18:23',\n",
       " '2010-09-19 07:23:51',\n",
       " '2012-10-11 20:31:21',\n",
       " '2010-10-31 10:29:50',\n",
       " '2012-07-25 19:27:43',\n",
       " '2012-08-31 23:26:08',\n",
       " '2011-06-02 22:03:57',\n",
       " '2011-02-19 19:16:17',\n",
       " '2013-03-07 16:15:26',\n",
       " '2012-08-31 10:49:47',\n",
       " '2011-08-03 11:35:36',\n",
       " '2012-07-03 17:08:39',\n",
       " '2012-09-01 14:39:37',\n",
       " '2012-09-23 12:26:12',\n",
       " '2012-10-04 13:38:51',\n",
       " '2011-06-05 09:59:41',\n",
       " '2011-11-14 16:56:36',\n",
       " '2010-09-24 16:36:44',\n",
       " '2011-03-15 14:48:46',\n",
       " '2012-08-12 08:13:36',\n",
       " '2012-12-24 22:36:17',\n",
       " '2012-06-05 17:19:33',\n",
       " '2012-10-15 13:17:55',\n",
       " '2012-01-12 17:08:57',\n",
       " '2012-07-12 13:09:02',\n",
       " '2011-01-19 00:02:12',\n",
       " '2011-04-29 10:45:19',\n",
       " '2013-06-12 19:56:47',\n",
       " '2012-08-30 23:36:08',\n",
       " '2012-04-14 13:17:17',\n",
       " '2011-04-29 17:04:04',\n",
       " '2013-01-10 19:36:43',\n",
       " '2012-12-21 10:47:27',\n",
       " '2010-11-27 19:51:05',\n",
       " '2011-12-19 09:02:20',\n",
       " '2012-10-02 12:48:05',\n",
       " '2012-05-10 22:19:20',\n",
       " '2011-02-01 07:55:29',\n",
       " '2012-08-28 11:36:50',\n",
       " '2012-01-15 05:00:54',\n",
       " '2011-10-22 19:28:02',\n",
       " '2012-05-05 12:49:20',\n",
       " '2012-03-28 17:21:33',\n",
       " '2012-05-04 22:56:06',\n",
       " '2012-09-18 19:47:04',\n",
       " '2012-12-06 19:23:59',\n",
       " '2013-01-06 00:25:46',\n",
       " '2011-07-13 18:16:21',\n",
       " '2011-02-02 16:20:25',\n",
       " '2012-05-04 02:18:30',\n",
       " '2011-03-17 02:46:32',\n",
       " '2012-05-10 14:47:39',\n",
       " '2012-02-22 12:54:56',\n",
       " '2014-06-17 13:56:10',\n",
       " '2012-04-29 10:35:08',\n",
       " '2011-06-30 10:26:40',\n",
       " '2012-11-13 16:25:10',\n",
       " '2013-01-09 14:10:14',\n",
       " '2012-08-30 15:25:28',\n",
       " '2011-08-05 20:13:49',\n",
       " '2012-02-29 05:30:55',\n",
       " '2011-10-18 21:58:27',\n",
       " '2012-06-17 02:06:28',\n",
       " '2012-05-22 19:53:44',\n",
       " '2013-08-30 22:39:37',\n",
       " '2011-05-21 23:57:52',\n",
       " '2011-08-06 01:00:05',\n",
       " '2012-04-09 12:18:27',\n",
       " '2012-06-27 03:47:42',\n",
       " '2012-10-24 18:49:55',\n",
       " '2012-03-16 10:59:41',\n",
       " '2012-02-01 12:42:44',\n",
       " '2011-03-08 17:04:35',\n",
       " '2011-09-01 10:09:55',\n",
       " '2012-02-14 21:11:27',\n",
       " '2012-12-29 15:38:54',\n",
       " '2011-10-07 02:29:46',\n",
       " '2011-08-23 09:01:41',\n",
       " '2011-09-18 07:37:14',\n",
       " '2012-11-21 22:37:52',\n",
       " '2011-12-30 15:23:43',\n",
       " '2014-01-09 16:51:28',\n",
       " '2012-08-07 21:02:07',\n",
       " '2011-11-24 22:07:16',\n",
       " '2010-11-23 17:01:45',\n",
       " '2012-03-30 00:57:30',\n",
       " '2011-10-23 13:40:52',\n",
       " '2011-04-16 00:34:17',\n",
       " '2011-04-08 10:12:34',\n",
       " '2013-05-17 17:38:10',\n",
       " '2012-01-09 19:14:48',\n",
       " '2012-02-23 10:35:14',\n",
       " '2012-08-09 20:13:52',\n",
       " '2013-01-10 03:38:36',\n",
       " '2011-08-22 08:51:16',\n",
       " '2010-08-29 14:41:44',\n",
       " '2013-10-14 11:26:43',\n",
       " '2012-11-19 20:59:03',\n",
       " '2011-02-23 21:29:37',\n",
       " '2012-01-24 11:30:33',\n",
       " '2010-11-10 07:40:11',\n",
       " '2013-07-06 07:31:01',\n",
       " '2012-11-29 04:37:44',\n",
       " '2010-09-16 12:29:04',\n",
       " '2012-03-05 13:34:12',\n",
       " '2012-08-08 01:14:04',\n",
       " '2011-04-19 08:24:30',\n",
       " '2011-08-24 22:39:28',\n",
       " '2013-01-11 12:54:28',\n",
       " '2011-09-06 01:40:13',\n",
       " '2012-11-12 15:23:27',\n",
       " '2012-07-27 08:31:19',\n",
       " '2012-10-29 11:34:17',\n",
       " '2011-12-13 15:04:44',\n",
       " '2013-09-07 16:07:35',\n",
       " '2010-10-27 19:04:31',\n",
       " '2013-11-07 17:55:06',\n",
       " '2011-08-10 22:25:26',\n",
       " '2013-01-04 07:40:55',\n",
       " '2013-11-19 09:12:14',\n",
       " '2011-06-01 16:57:40',\n",
       " '2011-08-18 18:56:17',\n",
       " '2012-09-03 11:33:45',\n",
       " '2011-09-07 21:05:38',\n",
       " '2011-03-25 20:32:46',\n",
       " '2012-01-09 14:48:20',\n",
       " '2011-05-27 03:36:21',\n",
       " '2012-03-10 22:15:43',\n",
       " '2012-08-14 15:56:11',\n",
       " '2011-11-08 09:43:47',\n",
       " '2012-12-11 09:17:16',\n",
       " '2011-09-26 17:30:52',\n",
       " '2011-09-22 10:43:48',\n",
       " '2011-04-07 16:48:57',\n",
       " '2011-05-01 23:30:01',\n",
       " '2012-12-15 08:30:38',\n",
       " '2011-01-18 15:09:21',\n",
       " '2010-10-18 15:36:51',\n",
       " '2012-10-28 15:31:11',\n",
       " '2012-08-17 17:28:35',\n",
       " '2012-11-02 17:12:58',\n",
       " '2012-06-05 19:10:37',\n",
       " '2012-06-21 19:12:50',\n",
       " '2014-07-22 17:14:50',\n",
       " '2010-07-31 00:48:08',\n",
       " '2014-01-20 11:38:23',\n",
       " '2012-10-04 04:00:47',\n",
       " '2012-04-06 06:50:45',\n",
       " '2010-09-16 09:41:11',\n",
       " '2011-08-20 05:06:25',\n",
       " '2012-03-26 03:53:26',\n",
       " '2010-11-15 15:07:17',\n",
       " '2012-10-07 18:39:37',\n",
       " '2010-10-13 22:24:24',\n",
       " '2012-12-07 13:36:05',\n",
       " '2012-01-25 15:50:54',\n",
       " '2013-01-11 09:01:30',\n",
       " '2012-04-11 14:34:24',\n",
       " '2012-07-18 00:35:47',\n",
       " '2011-01-18 15:07:26',\n",
       " '2011-01-26 01:48:30',\n",
       " '2011-09-11 09:03:15',\n",
       " '2011-12-17 02:19:43',\n",
       " '2012-04-18 08:49:32',\n",
       " '2012-07-05 02:15:07',\n",
       " '2011-04-01 06:02:59',\n",
       " '2011-10-27 14:31:00',\n",
       " '2012-04-23 01:04:33',\n",
       " '2013-03-03 22:52:43',\n",
       " '2012-08-24 21:58:18',\n",
       " '2012-11-18 21:08:24',\n",
       " '2013-10-22 18:48:51',\n",
       " '2010-08-04 15:45:37',\n",
       " '2012-11-25 20:34:20',\n",
       " '2012-08-23 07:51:20',\n",
       " '2012-05-02 11:56:23',\n",
       " '2013-01-22 01:02:17',\n",
       " '2011-06-16 15:57:02',\n",
       " '2012-11-02 13:36:35',\n",
       " '2011-12-13 12:46:15',\n",
       " '2010-07-24 11:51:57',\n",
       " '2010-10-04 20:26:32',\n",
       " '2013-06-25 12:13:22',\n",
       " '2011-11-09 17:35:52',\n",
       " '2014-03-24 04:49:53',\n",
       " '2012-05-23 23:52:42',\n",
       " '2012-11-15 11:15:09',\n",
       " '2012-09-24 11:07:26',\n",
       " '2012-07-15 19:28:51',\n",
       " '2011-05-06 12:11:09',\n",
       " '2013-07-04 17:03:39',\n",
       " '2011-08-09 14:43:45',\n",
       " '2011-07-25 20:22:03',\n",
       " '2012-08-19 01:17:09',\n",
       " '2012-07-26 00:54:01',\n",
       " '2014-04-03 07:12:29',\n",
       " '2010-07-20 17:36:23',\n",
       " '2011-10-06 21:56:44',\n",
       " '2012-11-02 09:59:56',\n",
       " '2013-07-21 23:15:07',\n",
       " '2012-02-26 20:36:53',\n",
       " '2012-04-04 07:49:48',\n",
       " '2011-07-03 20:01:01',\n",
       " '2011-11-20 00:21:16',\n",
       " '2013-03-10 23:19:24',\n",
       " '2012-10-08 07:05:42',\n",
       " '2013-01-17 23:35:35',\n",
       " '2011-02-16 22:11:00',\n",
       " '2013-01-08 07:40:53',\n",
       " '2012-04-22 14:16:08',\n",
       " '2012-04-27 03:39:43',\n",
       " '2013-01-28 22:20:47',\n",
       " '2012-11-07 15:25:09',\n",
       " '2011-10-10 19:24:35',\n",
       " '2012-08-14 07:11:17',\n",
       " '2012-08-28 12:10:35',\n",
       " '2012-10-23 15:58:41',\n",
       " '2012-01-23 18:43:21',\n",
       " '2012-07-30 02:19:31',\n",
       " '2012-12-20 09:02:07',\n",
       " '2012-08-21 21:57:21',\n",
       " '2011-08-03 12:37:52',\n",
       " '2011-06-22 18:13:01',\n",
       " '2014-02-19 14:36:52',\n",
       " '2010-09-13 14:14:58',\n",
       " '2012-01-09 18:49:29',\n",
       " '2011-06-13 19:39:35',\n",
       " '2012-09-02 10:57:07',\n",
       " '2012-04-13 08:45:56',\n",
       " '2012-04-20 13:05:38',\n",
       " '2012-08-13 10:19:09',\n",
       " '2012-09-21 18:25:22',\n",
       " '2011-07-08 13:51:54',\n",
       " '2012-03-20 18:55:46',\n",
       " '2012-11-27 17:39:02',\n",
       " '2010-07-29 16:28:09',\n",
       " '2013-08-28 13:42:01',\n",
       " '2010-09-27 16:54:53',\n",
       " '2012-12-16 12:11:58',\n",
       " '2012-11-10 17:53:23',\n",
       " '2011-07-11 05:45:38',\n",
       " '2012-02-22 17:23:07',\n",
       " '2011-06-01 10:05:36',\n",
       " '2012-09-01 01:19:09',\n",
       " '2013-10-03 21:08:01',\n",
       " '2012-01-22 18:24:58',\n",
       " '2012-05-17 21:07:59',\n",
       " '2012-06-11 22:49:13',\n",
       " '2011-11-28 08:13:06',\n",
       " '2012-10-02 05:08:56',\n",
       " '2012-11-21 22:03:24',\n",
       " '2011-12-25 19:58:09',\n",
       " '2011-12-09 21:53:19',\n",
       " '2013-01-06 15:01:24',\n",
       " '2012-07-14 18:54:04',\n",
       " '2011-11-01 02:04:11',\n",
       " '2012-07-09 20:22:12',\n",
       " '2010-12-08 22:26:10',\n",
       " '2012-05-11 18:19:41',\n",
       " '2012-07-28 13:47:57',\n",
       " '2012-02-15 17:26:42',\n",
       " '2011-06-17 14:09:25',\n",
       " '2012-03-11 17:38:53',\n",
       " '2013-03-26 23:26:43',\n",
       " '2012-12-12 15:30:45',\n",
       " '2012-09-07 01:30:30',\n",
       " '2011-06-13 18:13:55',\n",
       " '2010-08-11 07:22:46',\n",
       " '2011-12-03 01:55:04',\n",
       " '2012-05-26 20:01:54',\n",
       " '2012-01-07 15:42:56',\n",
       " '2012-09-07 15:34:15',\n",
       " '2011-11-16 13:58:58',\n",
       " '2012-08-31 09:30:36',\n",
       " '2013-11-02 12:30:40',\n",
       " '2012-03-28 21:45:51',\n",
       " '2011-12-16 10:36:58',\n",
       " '2010-07-25 18:10:35',\n",
       " '2011-08-05 12:44:40',\n",
       " '2012-01-07 20:35:19',\n",
       " '2013-02-05 18:57:02',\n",
       " '2013-07-10 21:44:53',\n",
       " '2013-09-04 14:01:11',\n",
       " '2011-08-04 15:45:28',\n",
       " '2012-03-31 20:30:26',\n",
       " '2013-01-05 17:49:02',\n",
       " '2012-09-18 16:01:50',\n",
       " '2010-11-01 09:53:00',\n",
       " '2011-01-13 12:55:49',\n",
       " '2011-05-12 14:35:03',\n",
       " '2012-11-24 21:55:35',\n",
       " '2013-10-22 14:59:01',\n",
       " '2011-02-20 02:32:14',\n",
       " '2011-10-13 02:11:37',\n",
       " '2012-03-19 15:25:49',\n",
       " '2012-07-20 14:07:36',\n",
       " '2011-07-18 09:53:48',\n",
       " '2012-09-16 12:23:56',\n",
       " '2012-04-27 18:09:49',\n",
       " '2012-05-10 22:20:03',\n",
       " '2011-03-19 11:03:50',\n",
       " '2011-03-23 10:13:02',\n",
       " '2012-04-02 16:25:01',\n",
       " '2012-09-19 06:12:09',\n",
       " '2011-10-02 13:51:37',\n",
       " '2011-07-07 13:54:21',\n",
       " '2012-09-17 22:51:25',\n",
       " '2012-01-13 13:09:04',\n",
       " '2010-07-27 18:06:52',\n",
       " '2010-10-22 13:32:28',\n",
       " '2012-11-03 22:39:24',\n",
       " '2010-11-29 22:54:18',\n",
       " '2010-10-19 07:20:15',\n",
       " '2012-11-25 22:01:20',\n",
       " '2011-03-19 16:31:40',\n",
       " '2011-11-07 04:23:27',\n",
       " '2011-01-28 16:22:45',\n",
       " '2010-12-11 12:56:21',\n",
       " '2012-11-21 22:26:00',\n",
       " '2012-06-12 12:14:48',\n",
       " '2011-10-05 17:25:40',\n",
       " '2011-10-04 07:06:49',\n",
       " '2011-04-11 14:55:36',\n",
       " '2012-01-21 13:17:47',\n",
       " '2012-07-04 21:06:35',\n",
       " '2014-06-27 12:41:13',\n",
       " '2011-05-02 10:34:32',\n",
       " '2012-08-15 19:40:15',\n",
       " '2012-06-29 18:12:35',\n",
       " '2012-11-10 19:15:07',\n",
       " '2013-09-02 09:57:00',\n",
       " '2012-11-16 18:57:38',\n",
       " '2011-10-13 13:39:11',\n",
       " '2012-08-01 14:15:20',\n",
       " '2014-07-04 20:38:51',\n",
       " '2012-02-22 23:36:42',\n",
       " '2012-05-16 22:36:35',\n",
       " '2012-04-01 13:59:51',\n",
       " '2012-08-14 10:51:59',\n",
       " '2010-07-25 15:32:14',\n",
       " '2010-08-03 00:57:51',\n",
       " '2011-07-18 21:02:24',\n",
       " '2011-02-17 17:10:30',\n",
       " '2011-11-16 21:22:51',\n",
       " '2013-01-05 16:38:39',\n",
       " '2012-06-05 21:46:49',\n",
       " '2011-11-02 14:48:43',\n",
       " '2011-09-21 10:15:57',\n",
       " '2011-06-20 10:35:45',\n",
       " '2012-11-28 07:26:06',\n",
       " '2011-05-07 11:50:28',\n",
       " '2011-06-08 08:38:14',\n",
       " '2010-11-25 22:44:35',\n",
       " '2014-01-16 14:48:58',\n",
       " '2011-08-01 16:07:01',\n",
       " '2012-09-06 14:49:10',\n",
       " '2012-04-26 23:05:58',\n",
       " '2011-08-24 20:51:46',\n",
       " '2011-10-19 06:47:15',\n",
       " '2011-12-15 02:38:20',\n",
       " '2011-03-30 13:19:37',\n",
       " '2011-05-27 06:53:27',\n",
       " '2012-01-09 19:32:46',\n",
       " '2012-03-25 21:22:04',\n",
       " '2013-08-28 14:56:58',\n",
       " '2012-11-21 10:35:43',\n",
       " '2012-07-11 15:51:15',\n",
       " '2012-05-05 12:56:40',\n",
       " '2012-06-24 04:43:50',\n",
       " '2012-08-28 14:13:26',\n",
       " '2012-07-08 15:14:02',\n",
       " '2012-03-26 19:53:36',\n",
       " '2012-06-26 02:37:09',\n",
       " '2014-05-09 08:26:15',\n",
       " '2012-02-03 09:50:57',\n",
       " '2012-08-23 19:00:14',\n",
       " '2010-11-28 11:58:46',\n",
       " '2011-07-01 07:33:45',\n",
       " '2010-10-26 02:19:11',\n",
       " '2012-02-24 22:38:09',\n",
       " '2011-06-10 21:56:42',\n",
       " '2011-09-14 13:00:05',\n",
       " '2012-01-16 14:16:28',\n",
       " '2011-10-22 11:40:37',\n",
       " '2012-05-17 17:29:24',\n",
       " '2012-09-04 23:14:01',\n",
       " '2012-08-23 14:10:16',\n",
       " '2012-05-14 21:49:03',\n",
       " '2011-07-07 21:25:21',\n",
       " '2011-01-11 19:20:21',\n",
       " '2011-02-09 05:44:25',\n",
       " '2011-12-28 00:47:36',\n",
       " '2014-04-24 21:35:18',\n",
       " '2011-02-06 20:06:31',\n",
       " '2012-04-11 06:28:16',\n",
       " '2011-12-30 21:59:58',\n",
       " '2011-02-17 19:44:10',\n",
       " '2012-04-12 07:02:38',\n",
       " '2012-12-31 20:32:50',\n",
       " '2011-09-26 20:24:37',\n",
       " '2011-03-08 11:34:08',\n",
       " '2012-07-05 17:06:45',\n",
       " '2010-12-16 16:11:57',\n",
       " '2011-05-05 14:41:17',\n",
       " '2011-07-15 13:30:40',\n",
       " '2013-01-14 13:37:37',\n",
       " '2012-05-11 18:49:36',\n",
       " '2011-06-06 05:40:21',\n",
       " '2012-06-04 12:11:45',\n",
       " '2012-08-17 16:02:24',\n",
       " '2012-10-18 08:03:26',\n",
       " '2012-01-17 14:29:22',\n",
       " '2012-12-14 12:35:39',\n",
       " '2012-07-10 21:11:40',\n",
       " '2011-05-23 01:31:35',\n",
       " '2012-12-04 18:32:56',\n",
       " '2011-11-13 10:21:25',\n",
       " '2011-10-04 09:56:38',\n",
       " '2012-05-18 22:12:39',\n",
       " '2012-10-23 07:09:59',\n",
       " '2012-12-18 01:23:39',\n",
       " '2011-04-02 10:36:18',\n",
       " '2011-05-09 12:17:35',\n",
       " '2011-12-15 10:18:08',\n",
       " '2012-06-28 21:48:52',\n",
       " '2012-09-22 20:34:14',\n",
       " '2012-08-20 11:23:42',\n",
       " '2013-07-19 02:17:33',\n",
       " '2010-12-16 05:25:16',\n",
       " '2011-11-20 14:41:03',\n",
       " '2012-08-07 15:33:06',\n",
       " '2012-07-30 23:22:28',\n",
       " '2010-09-21 21:07:25',\n",
       " '2012-11-12 04:11:59',\n",
       " '2012-10-05 19:27:15',\n",
       " '2011-07-01 23:45:00',\n",
       " '2013-01-22 20:08:33',\n",
       " '2012-10-06 09:30:43',\n",
       " '2012-12-09 09:49:55',\n",
       " '2012-10-09 03:37:58',\n",
       " '2012-04-08 21:24:59',\n",
       " '2012-07-16 19:15:43',\n",
       " '2012-01-12 14:52:50',\n",
       " '2012-04-02 10:34:42',\n",
       " '2012-03-27 14:42:01',\n",
       " '2013-01-12 23:08:17',\n",
       " '2011-09-05 12:56:57',\n",
       " '2011-11-16 14:09:55',\n",
       " '2013-12-02 22:36:56',\n",
       " '2011-10-07 06:19:08',\n",
       " '2011-10-11 13:22:37',\n",
       " '2011-07-10 11:54:47',\n",
       " '2011-04-19 13:03:21',\n",
       " '2011-07-21 22:23:35',\n",
       " '2012-01-04 08:17:46',\n",
       " '2012-09-18 23:34:02',\n",
       " '2011-06-16 07:48:04',\n",
       " '2010-10-07 22:52:35',\n",
       " '2011-09-30 20:51:25',\n",
       " '2012-02-15 15:00:10',\n",
       " '2012-05-21 16:42:02',\n",
       " '2012-06-22 21:02:54',\n",
       " '2012-05-02 18:53:59',\n",
       " '2012-05-19 08:50:53',\n",
       " '2012-10-29 12:58:42',\n",
       " '2012-06-19 22:18:19',\n",
       " '2012-05-07 13:39:23',\n",
       " '2013-01-16 21:21:38',\n",
       " '2011-01-17 21:54:42',\n",
       " '2012-11-05 10:51:32',\n",
       " '2012-11-20 13:48:45',\n",
       " '2012-01-15 08:06:10',\n",
       " '2012-05-11 14:33:32',\n",
       " '2011-03-01 01:07:08',\n",
       " '2014-07-22 14:22:03',\n",
       " '2012-09-10 21:12:58',\n",
       " '2011-03-29 16:04:10',\n",
       " '2012-10-15 12:04:55',\n",
       " '2012-08-18 05:43:52',\n",
       " '2012-12-20 18:43:40',\n",
       " '2012-01-12 17:29:26',\n",
       " '2012-12-04 17:56:12',\n",
       " '2013-06-16 23:32:55',\n",
       " '2012-08-09 09:51:32',\n",
       " '2011-10-16 04:06:18',\n",
       " '2011-05-17 10:16:50',\n",
       " '2012-07-03 14:47:00',\n",
       " '2013-05-31 01:02:55',\n",
       " '2011-10-21 16:42:50',\n",
       " '2011-02-21 17:58:30',\n",
       " '2012-06-28 10:01:35',\n",
       " '2012-08-13 06:36:45',\n",
       " '2011-09-29 22:46:17',\n",
       " '2012-10-29 21:20:05',\n",
       " '2013-09-06 09:39:16',\n",
       " '2011-02-08 08:43:50',\n",
       " '2011-05-20 11:57:38',\n",
       " '2012-03-14 22:10:12',\n",
       " '2012-07-02 02:33:13',\n",
       " '2011-06-03 05:22:22',\n",
       " '2013-11-18 03:13:41',\n",
       " '2010-10-02 07:49:14',\n",
       " '2011-05-19 16:36:55',\n",
       " '2010-09-17 20:20:49',\n",
       " '2011-07-17 23:03:48',\n",
       " '2012-11-28 20:13:46',\n",
       " '2011-09-19 19:04:44',\n",
       " '2011-02-09 09:33:03',\n",
       " '2010-08-11 08:46:15',\n",
       " '2012-05-30 10:40:24',\n",
       " '2011-05-16 20:31:17',\n",
       " '2010-08-10 06:53:15',\n",
       " '2012-05-01 01:50:11',\n",
       " '2012-02-17 19:04:13',\n",
       " '2012-09-17 07:06:39',\n",
       " '2011-04-20 13:00:27',\n",
       " '2012-03-29 16:44:19',\n",
       " '2010-07-20 08:52:27',\n",
       " '2011-11-19 16:08:36',\n",
       " '2012-05-11 18:35:44',\n",
       " '2012-11-13 02:26:36',\n",
       " '2012-02-12 09:04:23',\n",
       " '2012-02-07 23:50:55',\n",
       " '2011-05-17 20:29:54',\n",
       " '2014-01-08 08:34:53',\n",
       " '2011-11-22 17:26:21',\n",
       " '2012-11-06 16:44:27',\n",
       " '2014-06-11 05:08:05',\n",
       " '2011-08-10 14:58:28',\n",
       " '2012-08-15 17:39:52',\n",
       " '2012-05-02 17:48:14',\n",
       " '2012-06-01 15:01:37',\n",
       " '2013-07-22 01:35:47',\n",
       " '2013-01-04 20:53:14',\n",
       " '2011-10-18 20:05:00',\n",
       " '2011-05-26 07:33:00',\n",
       " '2011-10-05 22:30:29',\n",
       " '2010-09-16 12:33:07',\n",
       " '2012-08-09 23:28:51',\n",
       " '2012-06-02 09:32:31',\n",
       " '2011-04-08 19:33:40',\n",
       " '2010-11-23 17:48:00',\n",
       " '2011-09-23 05:43:12',\n",
       " '2011-12-08 07:43:11',\n",
       " '2012-07-26 09:24:02',\n",
       " '2010-11-23 14:37:19',\n",
       " '2012-05-12 12:22:14',\n",
       " '2011-12-11 15:35:09',\n",
       " '2013-06-03 11:31:39',\n",
       " '2014-04-25 17:11:32',\n",
       " '2011-10-12 22:42:32',\n",
       " '2013-01-21 16:18:52',\n",
       " '2014-06-05 19:03:05',\n",
       " '2013-08-23 16:13:50',\n",
       " '2012-03-31 17:02:44',\n",
       " '2012-04-04 18:48:51',\n",
       " '2012-09-21 19:22:01',\n",
       " '2012-10-17 02:53:42',\n",
       " '2012-06-27 15:35:47',\n",
       " '2011-03-20 15:53:55',\n",
       " '2013-03-31 16:58:31',\n",
       " '2010-08-06 14:14:37',\n",
       " '2012-01-15 21:26:36',\n",
       " '2012-08-19 23:20:23',\n",
       " '2010-08-25 14:12:54',\n",
       " '2012-04-06 11:17:57',\n",
       " '2011-11-11 07:53:51',\n",
       " '2011-12-25 10:05:07',\n",
       " '2012-12-16 20:41:36',\n",
       " '2012-10-03 11:46:31',\n",
       " '2012-11-09 21:34:19',\n",
       " '2012-12-18 17:37:49',\n",
       " '2011-01-23 09:26:32',\n",
       " '2011-05-14 08:04:11',\n",
       " '2011-02-01 14:57:09',\n",
       " '2012-05-09 21:41:24',\n",
       " '2012-05-04 20:55:54',\n",
       " '2012-04-11 12:37:53',\n",
       " '2012-04-06 18:21:47',\n",
       " '2012-07-24 18:40:50',\n",
       " '2012-04-03 06:05:23',\n",
       " '2010-08-12 17:31:16',\n",
       " '2012-03-25 19:00:07',\n",
       " '2012-07-28 01:24:52',\n",
       " '2011-06-10 20:09:21',\n",
       " '2011-11-06 16:25:52',\n",
       " '2012-09-21 02:59:07',\n",
       " '2011-12-19 09:05:12',\n",
       " '2010-12-12 21:23:35',\n",
       " '2011-10-09 14:01:17',\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'http://graph.facebook.com/100000071660800/picture?type=large',\n",
       " 'http://graph.facebook.com/100001122416350/picture?type=large',\n",
       " 'http://i.stack.imgur.com/0IgyC.png',\n",
       " 'http://i.stack.imgur.com/1b4QI.jpg',\n",
       " 'http://i.stack.imgur.com/1o7ij.jpg',\n",
       " 'http://i.stack.imgur.com/1sz79.png',\n",
       " 'http://i.stack.imgur.com/1uoPg.jpg',\n",
       " 'http://i.stack.imgur.com/22CD2.png',\n",
       " 'http://i.stack.imgur.com/2Je4F.jpg',\n",
       " 'http://i.stack.imgur.com/2NJpO.jpg',\n",
       " 'http://i.stack.imgur.com/36YTL.png',\n",
       " 'http://i.stack.imgur.com/3VnmB.jpg',\n",
       " 'http://i.stack.imgur.com/3lyag.png',\n",
       " 'http://i.stack.imgur.com/4LwOd.jpg',\n",
       " 'http://i.stack.imgur.com/4bZ01.jpg',\n",
       " 'http://i.stack.imgur.com/5Sqgt.png',\n",
       " 'http://i.stack.imgur.com/5Vkq8.jpg',\n",
       " 'http://i.stack.imgur.com/5ZzWR.png',\n",
       " 'http://i.stack.imgur.com/5aMmD.jpg',\n",
       " 'http://i.stack.imgur.com/5oWjh.jpg',\n",
       " 'http://i.stack.imgur.com/6BqAu.jpg',\n",
       " 'http://i.stack.imgur.com/6t0im.jpg',\n",
       " 'http://i.stack.imgur.com/73lQ8.jpg',\n",
       " 'http://i.stack.imgur.com/7A2m9.jpg',\n",
       " 'http://i.stack.imgur.com/7fpvh.jpg',\n",
       " 'http://i.stack.imgur.com/7hR5X.jpg',\n",
       " 'http://i.stack.imgur.com/7syBD.jpg',\n",
       " 'http://i.stack.imgur.com/9643V.jpg',\n",
       " 'http://i.stack.imgur.com/9H6nw.jpg',\n",
       " 'http://i.stack.imgur.com/B8ud0.png',\n",
       " 'http://i.stack.imgur.com/BF0y7.jpg',\n",
       " 'http://i.stack.imgur.com/Ba4iE.jpg',\n",
       " 'http://i.stack.imgur.com/BbqCG.jpg',\n",
       " 'http://i.stack.imgur.com/CHox8.png',\n",
       " 'http://i.stack.imgur.com/Cc7mM.jpg',\n",
       " 'http://i.stack.imgur.com/Cug0U.jpg',\n",
       " 'http://i.stack.imgur.com/DHumG.jpg',\n",
       " 'http://i.stack.imgur.com/DrRTV.jpg',\n",
       " 'http://i.stack.imgur.com/E5Eu7.jpg',\n",
       " 'http://i.stack.imgur.com/EuJtr.jpg',\n",
       " 'http://i.stack.imgur.com/F5xn2.png',\n",
       " 'http://i.stack.imgur.com/FSetO.png',\n",
       " 'http://i.stack.imgur.com/FluI8.jpg',\n",
       " 'http://i.stack.imgur.com/Frd3B.png',\n",
       " 'http://i.stack.imgur.com/GTHqI.jpg',\n",
       " 'http://i.stack.imgur.com/GU1xi.png',\n",
       " 'http://i.stack.imgur.com/Gmj2x.jpg',\n",
       " 'http://i.stack.imgur.com/Gzg7k.jpg',\n",
       " 'http://i.stack.imgur.com/HoUuO.png',\n",
       " 'http://i.stack.imgur.com/ILcTf.png',\n",
       " 'http://i.stack.imgur.com/IPbxC.jpg',\n",
       " 'http://i.stack.imgur.com/IjqM5.jpg',\n",
       " 'http://i.stack.imgur.com/Ik92X.jpg',\n",
       " 'http://i.stack.imgur.com/J2NM3.jpg',\n",
       " 'http://i.stack.imgur.com/J8e8y.jpg',\n",
       " 'http://i.stack.imgur.com/JsQBm.jpg',\n",
       " 'http://i.stack.imgur.com/KD6Fx.jpg',\n",
       " 'http://i.stack.imgur.com/KMYkT.jpg',\n",
       " 'http://i.stack.imgur.com/KNKIE.jpg',\n",
       " 'http://i.stack.imgur.com/KQ3YE.jpg?s=128&g=1',\n",
       " 'http://i.stack.imgur.com/Kua2Z.jpg',\n",
       " 'http://i.stack.imgur.com/LJ6vb.jpg',\n",
       " 'http://i.stack.imgur.com/LLgac.jpg',\n",
       " 'http://i.stack.imgur.com/LbRay.jpg',\n",
       " 'http://i.stack.imgur.com/M8Hr5.jpg',\n",
       " 'http://i.stack.imgur.com/MQ33X.jpg',\n",
       " 'http://i.stack.imgur.com/MTcDJ.jpg',\n",
       " 'http://i.stack.imgur.com/MjeHD.jpg',\n",
       " 'http://i.stack.imgur.com/MwRvs.jpg',\n",
       " 'http://i.stack.imgur.com/NeuGc.jpg',\n",
       " 'http://i.stack.imgur.com/O14Fi.jpg',\n",
       " 'http://i.stack.imgur.com/OqZrR.jpg',\n",
       " 'http://i.stack.imgur.com/PVlGM.jpg',\n",
       " 'http://i.stack.imgur.com/PXFXm.jpg',\n",
       " 'http://i.stack.imgur.com/Pmbuu.jpg',\n",
       " 'http://i.stack.imgur.com/Qnyua.jpg',\n",
       " 'http://i.stack.imgur.com/SxbWP.jpg',\n",
       " 'http://i.stack.imgur.com/TCyei.jpg',\n",
       " 'http://i.stack.imgur.com/TX6TC.jpg',\n",
       " 'http://i.stack.imgur.com/TvkT7.jpg',\n",
       " 'http://i.stack.imgur.com/U3gYU.jpg',\n",
       " 'http://i.stack.imgur.com/UWKcp.gif',\n",
       " 'http://i.stack.imgur.com/WtOby.jpg',\n",
       " 'http://i.stack.imgur.com/XFPSV.jpg',\n",
       " 'http://i.stack.imgur.com/Xwosz.jpg',\n",
       " 'http://i.stack.imgur.com/YJ0Ye.jpg?s=128&g=1',\n",
       " 'http://i.stack.imgur.com/YWmON.jpg',\n",
       " 'http://i.stack.imgur.com/YcSQq.jpg',\n",
       " 'http://i.stack.imgur.com/YwUTs.jpg',\n",
       " 'http://i.stack.imgur.com/ZU979.jpg',\n",
       " 'http://i.stack.imgur.com/Zhyym.jpg',\n",
       " 'http://i.stack.imgur.com/a0ycx.jpg',\n",
       " 'http://i.stack.imgur.com/a7yWR.jpg',\n",
       " 'http://i.stack.imgur.com/ao2Pm.jpg',\n",
       " 'http://i.stack.imgur.com/aqZSq.jpg',\n",
       " 'http://i.stack.imgur.com/axcPw.jpg',\n",
       " 'http://i.stack.imgur.com/bEHdQ.jpg',\n",
       " 'http://i.stack.imgur.com/bb8sB.jpg',\n",
       " 'http://i.stack.imgur.com/bbItm.png',\n",
       " 'http://i.stack.imgur.com/bmLJq.png',\n",
       " 'http://i.stack.imgur.com/bnim1.png',\n",
       " 'http://i.stack.imgur.com/c9A0L.png',\n",
       " 'http://i.stack.imgur.com/cCqdx.jpg',\n",
       " 'http://i.stack.imgur.com/cEINK.jpg',\n",
       " 'http://i.stack.imgur.com/dHsAN.jpg',\n",
       " 'http://i.stack.imgur.com/dcglq.jpg',\n",
       " 'http://i.stack.imgur.com/dqVjM.png',\n",
       " 'http://i.stack.imgur.com/e6fdD.jpg',\n",
       " 'http://i.stack.imgur.com/e6gnk.jpg',\n",
       " 'http://i.stack.imgur.com/e863w.jpg',\n",
       " 'http://i.stack.imgur.com/ePGCS.png',\n",
       " 'http://i.stack.imgur.com/f8n3X.jpg',\n",
       " 'http://i.stack.imgur.com/fbPUn.jpg',\n",
       " 'http://i.stack.imgur.com/g3YEU.jpg',\n",
       " 'http://i.stack.imgur.com/gIh82.jpg',\n",
       " 'http://i.stack.imgur.com/h60Xn.png',\n",
       " 'http://i.stack.imgur.com/hochp.png',\n",
       " 'http://i.stack.imgur.com/i8rnZ.jpg',\n",
       " 'http://i.stack.imgur.com/j7Ig7.jpg',\n",
       " 'http://i.stack.imgur.com/kc7hg.jpg',\n",
       " 'http://i.stack.imgur.com/km1pr.jpg',\n",
       " 'http://i.stack.imgur.com/knNVv.jpg',\n",
       " 'http://i.stack.imgur.com/lOS36.jpg',\n",
       " 'http://i.stack.imgur.com/mI2wg.png',\n",
       " 'http://i.stack.imgur.com/mIqXn.jpg',\n",
       " 'http://i.stack.imgur.com/mYIzP.png',\n",
       " 'http://i.stack.imgur.com/maMvF.jpg',\n",
       " 'http://i.stack.imgur.com/n6s2u.jpg',\n",
       " 'http://i.stack.imgur.com/nU36s.jpg?s=128&g=1',\n",
       " 'http://i.stack.imgur.com/nhtMQ.jpg',\n",
       " 'http://i.stack.imgur.com/oKsN6.jpg',\n",
       " 'http://i.stack.imgur.com/pSAVB.jpg',\n",
       " 'http://i.stack.imgur.com/pXbIh.gif',\n",
       " 'http://i.stack.imgur.com/qObut.jpg',\n",
       " 'http://i.stack.imgur.com/qfiK5.jpg?s=128&g=1',\n",
       " 'http://i.stack.imgur.com/qprPh.jpg',\n",
       " 'http://i.stack.imgur.com/r6e29.jpg',\n",
       " 'http://i.stack.imgur.com/rK2rh.png',\n",
       " 'http://i.stack.imgur.com/sQFX6.jpg',\n",
       " 'http://i.stack.imgur.com/sl6Bv.jpg',\n",
       " 'http://i.stack.imgur.com/stqX1.png',\n",
       " 'http://i.stack.imgur.com/szio3.jpg',\n",
       " 'http://i.stack.imgur.com/thOfN.gif',\n",
       " 'http://i.stack.imgur.com/tmmEr.jpg',\n",
       " 'http://i.stack.imgur.com/tyaCy.png',\n",
       " 'http://i.stack.imgur.com/u1sCz.jpg?s=128&g=1',\n",
       " 'http://i.stack.imgur.com/uKAHo.png',\n",
       " 'http://i.stack.imgur.com/uVGyl.jpg',\n",
       " 'http://i.stack.imgur.com/uimvC.gif?s=128&g=1',\n",
       " 'http://i.stack.imgur.com/uqNUS.jpg',\n",
       " 'http://i.stack.imgur.com/vyaaJ.jpg',\n",
       " 'http://i.stack.imgur.com/w5qzT.jpg',\n",
       " 'http://i.stack.imgur.com/w6rNs.jpg',\n",
       " 'http://i.stack.imgur.com/w8Goi.gif',\n",
       " 'http://i.stack.imgur.com/wMoWk.jpg',\n",
       " 'http://i.stack.imgur.com/wSfuj.png',\n",
       " 'http://i.stack.imgur.com/wmOYS.jpg?s=128&g=1',\n",
       " 'http://i.stack.imgur.com/wyuMa.jpg',\n",
       " 'http://i.stack.imgur.com/x7eKK.jpg',\n",
       " 'http://i.stack.imgur.com/xyy2Z.jpg',\n",
       " 'http://i.stack.imgur.com/zKvkR.jpg',\n",
       " 'http://i.stack.imgur.com/zSduQ.jpg',\n",
       " 'http://i.stack.imgur.com/zlV7q.jpg',\n",
       " 'http://www.gravatar.com/avatar/61c85d77bcaf5212b3a0aa2419c95f33?s=128&amp;d=identicon&amp;r=PG',\n",
       " 'http://www.gravatar.com/avatar/c7ba844e5aba2c764fe9499872deb54e?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/0f5337d47ece52ffa2a3ba366c2ca3a3?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/10950a808019775853972ea50bed1863?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/10d5751183c02f155eaedf7094174012?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/10db3ca2e511fe7b6851e3eb30aed981?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/178b62933f85027c9da370dba8ed8da3?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/2f822f8f82def56dc3237cc4ff794e9e?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/2fffdd4f8c494c29e613eefd78a6a084?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/33ff7f953a3fb4d98be320c2b71d86f8?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/37904cd3562d2f74241060ed46f4d97f?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/421afbe9424df7e10374caabccabbfd9?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/43310a8c36df3dbbca2c4d5ce158b5d0?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/43acd18dea0ebfe165c8bd6769a4bee9?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/513c598c9fd6df573ce5f4087a835560?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/56881de591adf646543e735ff28e77ea?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/569a31ff5fdd492dd39dd4bbfd9f86a7?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/5b60cd6e084f4a859eb2742a6a9ed3f6?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/5b95ed2b367e3b324597ba9271482338?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/5db52f950caf13b2702499daab09ddfe?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/62568666143087bfe90722ebeeff5609?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/669122b89437f78175d8eeaf4408c4e3?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/68b0e7c30787202b277343a9142ae0a3?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/6afa9f5c5c9ec18133611b198f106163?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/6fa70b96525f544b22fc6b051a8eb756?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/703d43e64e5d52bcaedba6af73a124e5?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/770348abdad4967b4246eaad81004f0f?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/78ac1c7e610953d897f25a1e00c7edbf?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/7d6524e509c8607ab65d9d8173071fe5?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/8d20d962d98c3b0d5be106eab8cc7ebc?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/8f4f015bc6c748fd2a4e7d700994c51e?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/91617c31b087fa08ec3a7948415f1044?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/9249785ded848233e985143ee28b08e5?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/9610fe59ef596064ac40a339343c7df6?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/98aa0766c4899d803cf24aab21221e9c?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/98fe597bf03d51503e48d2c312b4317f?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/9e51a2308408220f51b1327a4c3e0976?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/a3c50ef9c93c9ab17756f5ebea674989?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/abc177e73de48e3d81ef7afa1835bf57?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/ad6c889240f3a0301bcb17d7e63276c4?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/ad761f1a4145a1cb77f673f6efb1c453?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/b35eabc06494ff4c5b97db357e750ef0?s=128&d=identicon&r=PG&f=1',\n",
       " 'https://www.gravatar.com/avatar/b3c911f784d8fd40c687b9463c9ac3ba?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/b4a91a4f8e633988e44b64d9a4c8c5b8?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/be15d02ec5a08e675596561818f64272?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/c27fc35a92b68a498c77f020998b103b?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/c9cadf4928763c2284ec9c42a57d06a5?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/d1ff13442a2c2ae5617a92a093cdb6a9?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/e597fee4e2b9d63ef4a27829d1a183eb?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/ec1e94839a9f9b0308a5b5c243d75dc1?s=128&d=identicon&r=PG',\n",
       " 'https://www.gravatar.com/avatar/eeec80237213c91e5bab56b77ac5b8ca?s=128&d=identicon&r=PG',\n",
       " nan}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " '2011-08-12 20:30:18',\n",
       " '2013-01-05 01:35:08',\n",
       " '2010-07-19 20:52:27',\n",
       " '2010-08-28 07:10:59',\n",
       " '2012-02-09 15:13:30',\n",
       " '2010-08-31 17:41:55',\n",
       " '2012-02-03 16:12:56',\n",
       " '2012-08-08 17:40:36',\n",
       " '2010-08-30 11:48:13',\n",
       " '2011-05-28 13:02:46',\n",
       " '2011-01-20 17:13:38',\n",
       " '2010-07-29 15:43:35',\n",
       " '2010-07-28 19:34:00',\n",
       " '2012-04-10 13:48:27',\n",
       " '2011-11-17 00:00:16',\n",
       " '2011-05-21 16:04:11',\n",
       " '2010-08-27 14:38:12',\n",
       " '2011-02-11 17:52:56',\n",
       " '2010-08-30 05:10:29',\n",
       " '2010-08-15 17:26:03',\n",
       " '2010-08-28 07:10:15',\n",
       " '2011-11-21 16:45:26',\n",
       " '2010-08-06 11:15:17',\n",
       " '2011-05-03 02:40:27',\n",
       " '2011-05-02 03:02:45',\n",
       " '2010-09-16 12:15:45',\n",
       " '2010-07-19 23:06:43',\n",
       " '2010-09-19 03:24:26',\n",
       " '2010-07-28 19:22:44',\n",
       " '2011-01-01 22:49:18',\n",
       " '2012-04-05 18:24:23',\n",
       " '2012-08-01 04:44:46',\n",
       " '2010-08-08 19:59:34',\n",
       " '2011-03-22 00:36:10',\n",
       " '2010-08-20 20:55:10',\n",
       " '2012-04-05 13:12:17',\n",
       " '2010-11-16 10:24:08',\n",
       " '2012-12-15 16:02:28',\n",
       " '2012-03-28 23:53:12',\n",
       " '2010-12-01 23:57:48',\n",
       " '2010-07-30 12:11:18',\n",
       " '2011-09-01 03:05:59',\n",
       " '2012-09-29 23:06:37',\n",
       " '2010-11-20 19:19:08',\n",
       " '2011-10-28 08:23:36',\n",
       " '2012-05-29 14:05:50',\n",
       " '2010-08-21 00:04:13',\n",
       " '2010-11-16 19:10:24',\n",
       " '2012-04-26 07:10:09',\n",
       " '2011-10-12 17:52:36',\n",
       " '2010-08-06 01:20:36',\n",
       " '2011-06-02 11:49:17',\n",
       " '2010-11-30 08:02:28',\n",
       " '2012-06-13 12:59:19',\n",
       " '2012-02-04 17:49:55',\n",
       " '2010-08-20 22:35:55',\n",
       " '2010-07-28 13:59:49',\n",
       " '2011-06-09 15:53:24',\n",
       " '2010-07-26 15:31:39',\n",
       " '2010-12-16 23:13:28',\n",
       " '2010-07-19 23:19:44',\n",
       " '2012-10-06 09:36:10',\n",
       " '2012-05-10 11:54:42',\n",
       " '2012-01-08 11:58:44',\n",
       " '2010-12-07 07:46:56',\n",
       " '2012-07-31 01:06:30',\n",
       " '2012-10-25 13:01:56',\n",
       " '2011-07-13 17:12:44',\n",
       " '2011-11-07 13:54:30',\n",
       " '2010-12-14 09:17:59',\n",
       " '2010-07-24 20:15:20',\n",
       " '2012-01-14 02:49:40',\n",
       " '2010-12-04 03:37:32',\n",
       " '2011-05-26 08:34:15',\n",
       " '2010-10-08 01:39:26',\n",
       " '2012-08-11 06:24:41',\n",
       " '2011-03-30 19:05:42',\n",
       " '2013-01-03 19:38:34',\n",
       " '2010-08-17 20:09:56',\n",
       " '2012-10-09 20:46:38',\n",
       " '2011-10-29 04:45:09',\n",
       " '2010-09-30 10:44:48',\n",
       " '2010-12-04 03:46:20',\n",
       " '2012-05-27 22:18:30',\n",
       " '2012-07-28 01:19:18',\n",
       " '2012-08-07 08:57:41',\n",
       " '2010-10-25 14:59:22',\n",
       " '2011-02-21 19:51:03',\n",
       " '2012-02-01 01:10:21',\n",
       " '2011-08-23 17:29:14',\n",
       " '2011-12-29 14:00:12',\n",
       " '2010-08-17 08:29:54',\n",
       " '2011-07-29 13:58:24',\n",
       " '2012-06-16 13:21:48',\n",
       " '2010-07-20 06:41:25',\n",
       " '2010-08-03 03:02:57',\n",
       " '2011-11-07 14:37:39',\n",
       " '2012-08-02 18:04:45',\n",
       " '2013-01-18 19:50:12',\n",
       " '2011-08-06 17:52:35',\n",
       " '2011-09-13 16:05:45',\n",
       " '2012-02-13 18:27:04',\n",
       " '2010-08-16 15:29:24',\n",
       " '2012-06-26 22:45:54',\n",
       " '2010-10-21 14:42:28',\n",
       " '2010-07-27 14:20:42',\n",
       " '2012-02-08 22:05:54',\n",
       " '2012-02-10 21:49:04',\n",
       " '2010-12-04 00:36:08',\n",
       " '2012-09-03 17:14:41',\n",
       " '2011-04-09 16:49:38',\n",
       " '2012-10-02 17:46:38',\n",
       " '2012-11-18 00:37:39',\n",
       " '2010-08-06 11:09:39',\n",
       " '2013-08-08 22:45:51',\n",
       " '2011-03-15 12:03:42',\n",
       " '2011-09-30 01:37:39',\n",
       " '2011-12-13 16:07:49',\n",
       " '2011-11-16 10:17:37',\n",
       " '2011-06-01 17:43:40',\n",
       " '2011-04-11 18:45:10',\n",
       " '2010-07-21 15:23:53',\n",
       " '2010-09-16 12:57:26',\n",
       " '2010-08-04 11:38:07',\n",
       " '2011-03-29 15:14:59',\n",
       " '2010-08-20 00:23:03',\n",
       " '2010-10-19 20:50:18',\n",
       " '2012-05-18 16:56:12',\n",
       " '2010-08-16 12:03:55',\n",
       " '2013-01-23 04:29:40',\n",
       " '2010-10-02 19:21:19',\n",
       " '2011-05-07 16:13:17',\n",
       " '2011-02-17 21:56:24',\n",
       " '2012-03-12 17:25:11',\n",
       " '2010-08-09 01:44:41',\n",
       " '2010-08-21 21:19:39',\n",
       " '2010-10-04 12:05:20',\n",
       " '2010-11-16 16:24:50',\n",
       " '2011-02-27 16:45:14',\n",
       " '2010-07-23 17:11:35',\n",
       " '2012-02-20 20:00:50',\n",
       " '2011-08-18 16:24:48',\n",
       " '2011-08-22 17:22:44',\n",
       " '2011-11-25 15:52:54',\n",
       " '2010-11-23 07:50:07',\n",
       " '2010-08-31 15:58:42',\n",
       " '2010-07-20 00:06:20',\n",
       " '2011-01-19 18:49:49',\n",
       " '2011-01-22 00:50:41',\n",
       " '2011-10-31 07:10:37',\n",
       " '2010-11-03 15:49:57',\n",
       " '2014-08-01 21:37:48',\n",
       " '2010-10-12 08:48:39',\n",
       " '2010-09-10 10:48:33',\n",
       " '2010-08-01 14:16:22',\n",
       " '2010-09-18 18:15:49',\n",
       " '2010-07-19 19:14:43',\n",
       " '2011-12-07 03:12:54',\n",
       " '2012-04-25 00:13:45',\n",
       " '2011-01-06 14:22:03',\n",
       " '2010-08-20 18:16:10',\n",
       " '2011-06-09 21:40:25',\n",
       " '2012-05-04 17:38:13',\n",
       " '2010-11-18 10:57:43',\n",
       " '2011-07-07 09:58:49',\n",
       " '2011-10-03 15:41:31',\n",
       " '2010-07-27 06:26:04',\n",
       " '2011-10-07 10:58:43',\n",
       " '2011-01-18 22:11:11',\n",
       " '2010-07-27 14:04:13',\n",
       " '2012-03-07 16:16:36',\n",
       " '2011-05-19 13:31:00',\n",
       " '2012-01-25 08:48:43',\n",
       " '2011-02-19 19:49:46',\n",
       " '2012-04-20 01:29:44',\n",
       " '2011-03-13 00:58:39',\n",
       " '2012-05-04 21:08:26',\n",
       " '2010-08-14 13:06:45',\n",
       " '2011-07-05 08:57:48',\n",
       " '2012-12-10 22:29:47',\n",
       " '2011-05-08 16:52:17',\n",
       " '2014-01-17 19:50:11',\n",
       " '2012-05-11 13:22:34',\n",
       " '2013-01-12 16:13:52',\n",
       " '2012-01-12 22:18:51',\n",
       " '2010-07-19 23:27:36',\n",
       " '2010-09-03 19:22:30',\n",
       " '2013-06-23 09:57:40',\n",
       " '2012-05-31 15:27:56',\n",
       " '2012-01-15 01:23:28',\n",
       " '2010-09-01 18:23:56',\n",
       " '2011-03-16 22:34:19',\n",
       " '2011-05-07 13:05:04',\n",
       " '2010-11-06 13:01:36',\n",
       " '2011-08-17 18:38:07',\n",
       " '2011-10-30 22:51:18',\n",
       " '2011-10-24 18:52:35',\n",
       " '2012-07-25 00:59:27',\n",
       " '2010-09-05 19:18:58',\n",
       " '2011-11-16 20:17:12',\n",
       " '2010-09-21 03:00:47',\n",
       " '2010-07-26 20:22:30',\n",
       " '2010-07-21 13:50:08',\n",
       " '2012-09-02 15:57:49',\n",
       " '2010-08-19 13:44:40',\n",
       " '2012-02-23 10:35:22',\n",
       " '2011-12-17 21:19:23',\n",
       " '2012-12-14 15:27:00',\n",
       " '2011-01-06 15:28:05',\n",
       " '2012-04-04 04:25:18',\n",
       " '2010-07-20 16:10:16',\n",
       " '2010-08-09 13:16:41',\n",
       " '2010-11-16 00:15:32',\n",
       " '2011-09-23 04:35:50',\n",
       " '2012-10-13 06:30:43',\n",
       " '2010-08-19 17:14:39',\n",
       " '2010-09-18 16:14:21',\n",
       " '2010-08-19 11:22:51',\n",
       " '2012-09-26 09:28:42',\n",
       " '2012-05-26 06:33:22',\n",
       " '2010-10-04 15:04:32',\n",
       " '2011-08-17 19:48:27',\n",
       " '2010-08-22 18:15:09',\n",
       " '2012-04-27 14:49:04',\n",
       " '2010-08-25 20:17:24',\n",
       " '2012-03-04 23:57:02',\n",
       " '2013-01-22 09:54:23',\n",
       " '2010-08-14 01:41:18',\n",
       " '2010-08-05 19:41:53',\n",
       " '2012-09-26 10:10:10',\n",
       " '2012-10-24 19:51:47',\n",
       " '2011-12-08 00:40:59',\n",
       " '2010-09-16 13:13:10',\n",
       " '2010-12-03 16:23:28',\n",
       " '2010-08-06 05:15:22',\n",
       " '2010-07-19 23:00:30',\n",
       " '2012-08-22 22:05:48',\n",
       " '2012-12-08 20:35:03',\n",
       " '2010-07-27 09:51:19',\n",
       " '2010-08-30 04:55:48',\n",
       " '2010-11-30 06:19:58',\n",
       " '2012-02-29 08:43:18',\n",
       " '2010-09-27 12:00:06',\n",
       " '2010-07-21 22:13:19',\n",
       " '2010-08-21 21:36:07',\n",
       " '2012-01-14 22:11:00',\n",
       " '2012-05-05 02:32:23',\n",
       " '2010-07-27 09:09:25',\n",
       " '2010-08-25 08:02:51',\n",
       " '2013-02-12 17:26:33',\n",
       " '2010-08-20 00:16:09',\n",
       " '2012-02-15 03:43:05',\n",
       " '2010-12-04 10:54:08',\n",
       " '2012-01-12 18:45:40',\n",
       " '2010-07-19 21:23:20',\n",
       " '2011-04-09 15:57:28',\n",
       " '2012-01-24 14:03:02',\n",
       " '2010-10-14 13:44:36',\n",
       " '2011-08-17 01:00:24',\n",
       " '2012-12-25 00:08:00',\n",
       " '2010-11-18 10:31:45',\n",
       " '2010-08-02 20:31:22',\n",
       " '2012-07-24 01:09:45',\n",
       " '2010-11-21 02:00:15',\n",
       " '2010-11-10 14:38:26',\n",
       " '2010-08-19 04:47:46',\n",
       " '2012-03-28 23:57:13',\n",
       " '2010-07-24 09:52:52',\n",
       " '2010-08-09 13:51:29',\n",
       " '2011-11-12 20:20:17',\n",
       " '2010-09-13 13:16:33',\n",
       " '2012-02-14 19:15:29',\n",
       " '2010-07-31 15:46:05',\n",
       " '2011-05-20 21:19:25',\n",
       " '2010-07-30 15:43:28',\n",
       " '2011-09-22 19:40:00',\n",
       " '2010-08-19 12:50:42',\n",
       " '2011-04-10 14:26:46',\n",
       " '2011-07-06 00:37:17',\n",
       " '2011-01-29 05:40:49',\n",
       " '2010-12-02 21:56:02',\n",
       " '2011-08-12 20:31:32',\n",
       " '2011-08-19 06:50:53',\n",
       " '2010-12-13 18:42:29',\n",
       " '2010-11-08 16:06:40',\n",
       " '2012-01-11 20:30:02',\n",
       " '2012-01-31 09:53:25',\n",
       " '2010-09-08 10:28:33',\n",
       " '2013-01-12 17:31:42',\n",
       " '2010-08-16 05:45:29',\n",
       " '2011-04-18 18:56:29',\n",
       " '2010-07-19 19:33:19',\n",
       " '2012-06-27 21:39:38',\n",
       " '2010-10-21 20:53:54',\n",
       " '2010-12-09 21:31:56',\n",
       " '2010-07-21 15:53:59',\n",
       " '2010-11-13 10:25:47',\n",
       " '2012-12-21 02:15:37',\n",
       " '2013-01-19 20:36:23',\n",
       " '2011-03-07 22:00:16',\n",
       " '2012-05-09 21:55:24',\n",
       " '2010-09-29 15:57:22',\n",
       " '2010-08-14 01:09:33',\n",
       " '2012-11-09 02:54:18',\n",
       " '2010-11-02 11:17:57',\n",
       " '2011-06-05 14:57:27',\n",
       " '2010-11-03 04:01:10',\n",
       " '2012-02-14 19:46:30',\n",
       " '2012-07-27 15:17:42',\n",
       " '2011-11-02 02:33:57',\n",
       " '2010-07-26 21:17:29',\n",
       " '2010-10-14 18:00:23',\n",
       " '2010-10-16 01:09:31',\n",
       " '2012-06-15 20:18:41',\n",
       " '2010-08-01 14:25:28',\n",
       " '2012-06-09 16:33:18',\n",
       " '2012-10-08 09:31:51',\n",
       " '2011-01-21 13:40:53',\n",
       " '2012-10-10 14:37:08',\n",
       " '2010-11-05 14:04:18',\n",
       " '2010-07-21 13:53:13',\n",
       " '2010-08-08 19:02:55',\n",
       " '2010-09-01 15:02:00',\n",
       " '2010-10-25 15:54:27',\n",
       " '2012-06-20 14:14:15',\n",
       " '2010-08-29 12:04:51',\n",
       " '2010-09-18 02:32:42',\n",
       " '2011-02-18 00:06:24',\n",
       " '2012-08-21 01:17:10',\n",
       " '2010-11-02 12:04:30',\n",
       " '2010-09-02 13:10:52',\n",
       " '2012-10-02 16:37:50',\n",
       " '2011-12-25 10:12:47',\n",
       " '2011-05-20 18:21:06',\n",
       " '2010-07-26 21:58:37',\n",
       " '2010-10-21 12:17:31',\n",
       " '2012-04-04 14:19:55',\n",
       " '2011-01-28 19:20:55',\n",
       " '2012-02-28 18:05:40',\n",
       " '2010-09-18 21:07:17',\n",
       " '2012-03-25 12:42:37',\n",
       " '2010-07-26 15:37:25',\n",
       " '2010-12-15 20:14:20',\n",
       " '2010-08-02 08:58:27',\n",
       " '2012-04-03 14:29:09',\n",
       " '2010-08-27 11:15:23',\n",
       " '2012-01-04 15:35:51',\n",
       " '2010-08-08 14:33:24',\n",
       " '2012-10-29 08:07:38',\n",
       " '2010-09-12 08:12:21',\n",
       " '2011-08-17 17:18:56',\n",
       " '2013-01-10 04:46:19',\n",
       " '2010-07-29 12:02:28',\n",
       " '2010-09-03 19:38:45',\n",
       " '2012-12-22 16:49:16',\n",
       " '2012-06-08 08:57:32',\n",
       " '2013-01-26 03:25:13',\n",
       " '2010-12-31 14:56:21',\n",
       " '2010-11-03 19:01:28',\n",
       " '2010-11-17 01:34:10',\n",
       " '2010-08-07 06:03:39',\n",
       " '2012-03-31 20:21:21',\n",
       " '2010-07-21 16:56:45',\n",
       " '2012-01-03 17:44:36',\n",
       " '2012-06-18 18:25:36',\n",
       " '2012-02-09 14:52:45',\n",
       " '2011-06-26 01:49:18',\n",
       " '2012-04-10 16:50:19',\n",
       " '2012-09-02 17:11:30',\n",
       " '2010-10-04 07:15:14',\n",
       " '2012-08-30 11:14:58',\n",
       " '2012-05-23 13:17:26',\n",
       " '2012-01-12 12:58:51',\n",
       " '2012-04-13 14:21:53',\n",
       " '2010-12-14 13:12:15',\n",
       " '2011-10-24 21:02:24',\n",
       " '2010-09-30 13:14:28',\n",
       " '2010-10-19 11:07:08',\n",
       " '2012-02-14 19:22:31',\n",
       " '2010-11-09 22:31:31',\n",
       " '2011-01-25 21:13:21',\n",
       " '2012-05-05 01:58:28',\n",
       " '2010-08-19 13:45:00',\n",
       " '2010-07-27 16:37:42',\n",
       " '2011-12-29 17:15:43',\n",
       " '2011-03-17 00:09:23',\n",
       " '2011-10-29 01:17:11',\n",
       " '2010-09-03 03:49:48',\n",
       " '2011-06-19 23:56:44',\n",
       " '2012-01-25 07:01:54',\n",
       " '2010-11-16 07:52:02',\n",
       " '2011-05-31 11:07:04',\n",
       " '2010-08-12 20:51:36',\n",
       " '2010-07-21 08:01:54',\n",
       " '2010-07-21 09:02:11',\n",
       " '2011-05-16 21:48:29',\n",
       " '2012-05-23 22:55:29',\n",
       " '2011-03-13 12:45:03',\n",
       " '2010-11-16 12:19:21',\n",
       " '2010-09-16 21:32:16',\n",
       " '2010-07-19 19:13:28',\n",
       " '2011-08-29 14:04:52',\n",
       " '2010-07-20 03:35:58',\n",
       " '2011-03-30 04:20:40',\n",
       " '2012-04-04 15:57:04',\n",
       " '2010-12-14 20:00:01',\n",
       " '2012-05-05 08:28:16',\n",
       " '2012-01-22 17:59:10',\n",
       " '2010-10-19 18:41:58',\n",
       " '2011-05-16 20:51:58',\n",
       " '2010-10-13 15:07:55',\n",
       " '2012-04-04 21:14:58',\n",
       " '2010-07-26 23:04:39',\n",
       " '2010-07-19 19:22:31',\n",
       " '2011-05-08 07:16:23',\n",
       " '2010-11-27 22:27:41',\n",
       " '2012-08-03 07:55:05',\n",
       " '2010-11-21 09:44:03',\n",
       " '2011-05-16 20:26:08',\n",
       " '2012-05-30 02:39:29',\n",
       " '2011-08-15 13:52:43',\n",
       " '2012-06-08 13:01:00',\n",
       " '2012-08-27 05:03:45',\n",
       " '2010-08-05 20:38:41',\n",
       " '2010-08-21 04:42:39',\n",
       " '2011-03-13 12:26:51',\n",
       " '2011-02-11 18:44:38',\n",
       " '2012-11-13 16:23:49',\n",
       " '2014-07-30 20:09:14',\n",
       " '2010-07-22 11:19:10',\n",
       " '2010-08-13 21:22:56',\n",
       " '2010-07-21 16:53:35',\n",
       " '2010-08-30 14:19:24',\n",
       " '2011-06-10 12:53:28',\n",
       " '2010-09-20 20:39:08',\n",
       " '2010-10-02 23:58:10',\n",
       " '2010-07-30 01:01:24',\n",
       " '2010-09-03 18:51:15',\n",
       " '2011-03-01 07:17:43',\n",
       " '2010-08-12 20:25:12',\n",
       " '2012-08-26 19:59:17',\n",
       " '2012-08-09 21:13:56',\n",
       " '2010-08-20 23:58:49',\n",
       " '2012-03-27 05:13:34',\n",
       " '2011-03-29 13:26:09',\n",
       " '2010-07-20 18:17:25',\n",
       " '2012-01-18 05:23:21',\n",
       " '2011-05-08 16:12:29',\n",
       " '2010-07-27 18:33:56',\n",
       " '2010-11-04 10:02:45',\n",
       " '2010-07-21 16:50:35',\n",
       " '2010-12-15 20:10:26',\n",
       " '2012-04-10 17:38:02',\n",
       " '2012-01-12 13:58:53',\n",
       " '2010-09-06 17:27:01',\n",
       " '2010-09-13 14:07:11',\n",
       " '2012-01-19 01:24:06',\n",
       " '2011-08-12 20:29:55',\n",
       " '2011-05-26 12:05:30',\n",
       " '2012-03-29 22:41:43',\n",
       " '2012-05-22 21:18:16',\n",
       " '2012-12-23 19:16:24',\n",
       " '2012-09-26 15:47:42',\n",
       " '2010-07-21 04:19:00',\n",
       " '2010-08-23 14:58:22',\n",
       " '2012-02-09 10:48:37',\n",
       " '2010-08-13 22:29:08',\n",
       " '2010-07-21 15:43:44',\n",
       " '2012-04-24 21:22:14',\n",
       " '2010-07-21 15:38:11',\n",
       " '2012-09-06 14:48:44',\n",
       " '2011-07-05 20:15:25',\n",
       " '2010-08-18 13:50:28',\n",
       " '2010-08-28 04:09:07',\n",
       " '2012-12-24 23:25:40',\n",
       " '2011-12-06 09:02:17',\n",
       " '2010-09-02 09:35:37',\n",
       " '2010-07-22 14:10:29',\n",
       " '2012-06-18 17:49:08',\n",
       " '2010-12-03 21:05:59',\n",
       " '2011-01-18 21:34:07',\n",
       " '2011-10-12 21:36:03',\n",
       " '2011-10-13 17:34:21',\n",
       " '2010-11-20 19:17:18',\n",
       " '2010-09-27 15:13:38',\n",
       " '2010-11-27 18:14:10',\n",
       " '2012-01-20 09:04:04',\n",
       " '2011-02-05 10:49:45',\n",
       " '2010-08-09 13:05:50',\n",
       " '2012-12-11 03:48:22',\n",
       " '2010-08-31 10:13:21',\n",
       " '2011-02-06 04:09:24',\n",
       " '2010-08-20 21:30:13',\n",
       " '2013-10-14 14:50:30',\n",
       " '2010-07-21 15:38:46',\n",
       " '2011-04-13 15:39:52',\n",
       " '2012-01-12 15:37:43',\n",
       " '2012-03-29 23:16:33',\n",
       " '2012-01-05 19:49:03',\n",
       " '2010-09-30 20:40:28',\n",
       " '2010-08-18 10:56:44',\n",
       " '2010-08-19 07:05:16',\n",
       " '2012-10-11 09:43:05',\n",
       " '2010-08-04 14:02:21',\n",
       " '2012-01-25 14:29:20',\n",
       " '2010-11-18 10:36:54',\n",
       " '2010-08-19 13:47:24',\n",
       " '2010-10-04 04:31:35',\n",
       " '2010-07-27 09:19:53',\n",
       " '2011-08-05 13:32:37',\n",
       " '2010-08-12 20:20:11',\n",
       " '2012-01-19 19:35:28',\n",
       " '2010-11-18 12:14:32',\n",
       " '2011-07-06 15:37:34',\n",
       " '2012-12-16 21:59:01',\n",
       " '2011-08-09 12:43:15',\n",
       " '2010-11-09 23:09:28',\n",
       " '2010-11-17 14:15:50',\n",
       " '2010-08-31 15:30:46',\n",
       " '2010-07-26 20:08:09',\n",
       " '2011-01-04 04:04:56',\n",
       " '2010-08-19 13:01:46',\n",
       " '2010-11-15 20:28:53',\n",
       " '2010-08-14 21:52:11',\n",
       " '2010-10-22 14:03:02',\n",
       " '2011-06-17 00:57:13',\n",
       " '2010-08-13 23:58:11',\n",
       " '2010-07-27 08:43:15',\n",
       " '2012-06-25 00:08:30',\n",
       " '2012-01-12 19:58:30',\n",
       " '2010-10-20 19:41:32',\n",
       " '2011-01-27 13:22:27',\n",
       " '2010-08-06 11:17:09',\n",
       " '2010-07-20 18:16:17',\n",
       " '2010-07-21 07:04:26',\n",
       " '2011-05-26 21:39:40',\n",
       " '2010-09-09 02:35:31',\n",
       " '2010-08-16 13:01:42',\n",
       " '2010-10-01 18:31:56',\n",
       " '2012-01-04 12:31:46',\n",
       " '2010-11-10 21:11:31',\n",
       " '2010-08-03 20:07:57',\n",
       " '2011-08-09 14:53:54',\n",
       " '2011-10-04 00:41:57',\n",
       " '2011-10-08 00:01:28',\n",
       " '2012-05-12 22:06:25',\n",
       " '2011-10-19 18:24:22',\n",
       " '2010-08-09 15:36:26',\n",
       " '2012-04-10 01:06:11',\n",
       " '2010-10-31 05:14:12',\n",
       " '2011-05-07 18:40:18',\n",
       " '2011-01-21 09:42:03',\n",
       " '2010-10-04 15:40:44',\n",
       " '2010-12-31 13:17:04',\n",
       " '2010-10-04 04:48:07',\n",
       " '2010-07-21 07:15:54',\n",
       " '2012-06-09 06:06:23',\n",
       " '2011-10-06 23:34:08',\n",
       " '2011-06-02 11:09:11',\n",
       " '2010-09-27 12:01:56',\n",
       " '2010-09-16 16:27:02',\n",
       " '2010-08-04 18:29:57',\n",
       " '2010-11-19 09:49:52',\n",
       " '2011-11-09 10:39:13',\n",
       " '2012-01-20 13:18:34',\n",
       " '2012-04-11 20:19:32',\n",
       " '2010-08-12 13:30:08',\n",
       " '2012-10-23 19:47:03',\n",
       " '2011-10-15 12:53:56',\n",
       " '2010-07-29 22:32:44',\n",
       " '2011-04-08 21:02:38',\n",
       " '2011-08-16 15:34:39',\n",
       " '2012-04-02 13:14:12',\n",
       " '2010-09-09 08:59:38',\n",
       " '2010-11-17 14:17:55',\n",
       " '2010-07-29 22:30:13',\n",
       " '2012-04-11 18:11:43',\n",
       " '2011-10-07 23:59:04',\n",
       " '2013-01-12 21:14:16',\n",
       " '2010-08-22 15:08:47',\n",
       " '2010-09-06 23:56:59',\n",
       " '2011-05-05 21:04:31',\n",
       " '2010-12-14 12:57:48',\n",
       " '2012-03-25 17:39:51',\n",
       " '2010-09-10 07:40:27',\n",
       " '2010-09-17 11:11:41',\n",
       " '2012-03-28 01:07:46',\n",
       " '2011-11-19 15:44:36',\n",
       " '2011-12-06 09:05:52',\n",
       " '2010-07-21 15:13:21',\n",
       " '2011-10-13 16:41:40',\n",
       " '2010-11-16 02:49:20',\n",
       " '2011-01-16 13:48:53',\n",
       " '2010-07-29 12:44:50',\n",
       " '2011-02-18 15:05:48',\n",
       " '2012-12-19 18:45:34',\n",
       " '2012-12-30 11:40:18',\n",
       " '2010-09-27 10:48:47',\n",
       " '2010-08-14 00:58:21',\n",
       " '2011-02-22 16:13:39',\n",
       " '2011-05-20 18:39:55',\n",
       " '2011-07-04 14:40:15',\n",
       " '2010-08-19 04:35:15',\n",
       " '2012-12-30 12:48:32',\n",
       " '2011-04-01 21:35:45',\n",
       " '2011-06-13 16:44:57',\n",
       " '2012-08-07 14:28:50',\n",
       " '2010-07-27 17:57:48',\n",
       " '2012-07-26 15:51:08',\n",
       " '2012-04-03 15:32:35',\n",
       " '2010-08-19 13:45:36',\n",
       " '2010-09-11 15:18:26',\n",
       " '2011-09-21 18:09:23',\n",
       " '2010-07-19 20:07:49',\n",
       " '2010-10-02 18:10:23',\n",
       " '2010-12-15 08:53:41',\n",
       " '2012-12-20 22:55:54',\n",
       " '2010-07-27 09:20:27',\n",
       " '2012-07-27 17:51:25',\n",
       " '2010-09-27 07:25:32',\n",
       " '2011-05-15 20:20:23',\n",
       " '2010-08-16 06:37:30',\n",
       " '2010-08-02 20:35:41',\n",
       " '2012-01-25 18:55:21',\n",
       " '2010-11-10 11:57:37',\n",
       " '2010-10-19 14:34:44',\n",
       " '2011-10-11 23:16:01',\n",
       " '2011-05-20 10:56:14',\n",
       " '2011-04-12 19:43:01',\n",
       " '2010-07-28 09:35:43',\n",
       " '2012-01-04 08:21:42',\n",
       " '2010-11-21 21:02:25',\n",
       " '2011-11-08 14:43:23',\n",
       " '2010-08-27 09:36:24',\n",
       " '2012-04-11 11:52:24',\n",
       " '2010-07-21 03:48:55',\n",
       " '2010-08-04 12:32:08',\n",
       " '2010-08-06 01:08:05',\n",
       " '2011-12-25 21:55:38',\n",
       " '2011-07-08 03:50:26',\n",
       " '2010-08-19 10:45:32',\n",
       " '2011-06-26 01:51:20',\n",
       " '2010-08-16 14:38:47',\n",
       " '2010-08-23 15:35:18',\n",
       " '2010-09-03 06:27:41',\n",
       " '2010-09-16 12:31:45',\n",
       " '2011-06-30 02:48:01',\n",
       " '2012-04-24 03:43:25',\n",
       " '2011-01-21 03:10:15',\n",
       " '2010-11-18 09:52:47',\n",
       " '2010-12-04 11:07:26',\n",
       " '2011-02-05 18:31:42',\n",
       " '2011-05-31 11:05:52',\n",
       " '2010-07-19 21:50:44',\n",
       " '2010-07-20 13:08:42',\n",
       " '2010-07-21 20:29:12',\n",
       " '2010-08-07 03:55:36',\n",
       " '2012-11-20 23:53:03',\n",
       " '2011-11-09 15:07:13',\n",
       " '2012-10-02 16:35:10',\n",
       " '2012-02-16 19:54:25',\n",
       " '2013-01-03 21:32:53',\n",
       " '2012-01-06 00:22:14',\n",
       " '2010-08-19 11:06:22',\n",
       " '2010-08-14 14:06:52',\n",
       " '2010-11-22 15:36:31',\n",
       " '2012-02-22 23:48:17',\n",
       " '2010-08-01 18:56:25',\n",
       " '2010-11-21 19:28:40',\n",
       " '2012-05-04 21:14:06',\n",
       " '2011-08-17 20:15:31',\n",
       " '2011-11-16 00:43:24',\n",
       " '2012-05-12 16:33:46',\n",
       " '2010-08-19 10:29:27',\n",
       " '2011-05-20 19:55:14',\n",
       " '2011-01-21 09:55:15',\n",
       " '2010-07-26 23:10:16',\n",
       " '2011-10-12 01:14:59',\n",
       " '2012-02-29 07:17:57',\n",
       " '2010-08-08 23:05:17',\n",
       " '2012-02-14 08:09:35',\n",
       " '2012-06-03 21:38:01',\n",
       " '2010-08-03 02:49:32',\n",
       " '2011-12-19 17:41:25',\n",
       " '2010-12-15 20:06:07',\n",
       " '2011-01-27 11:08:52',\n",
       " '2011-05-16 22:32:27',\n",
       " '2012-11-13 20:22:50',\n",
       " '2010-07-27 06:23:57',\n",
       " '2010-09-07 08:58:26',\n",
       " '2010-07-19 19:16:27',\n",
       " '2010-10-22 12:52:59',\n",
       " '2010-09-09 22:52:29',\n",
       " '2010-07-27 07:38:19',\n",
       " '2012-04-11 18:14:39',\n",
       " '2010-09-05 05:23:25',\n",
       " '2010-11-17 02:44:28',\n",
       " '2010-11-16 13:30:48',\n",
       " '2010-08-04 10:55:31',\n",
       " '2010-08-20 02:25:32',\n",
       " '2012-11-20 17:15:17',\n",
       " '2011-06-30 05:50:35',\n",
       " '2012-03-28 10:08:10',\n",
       " '2010-09-11 15:27:50',\n",
       " '2011-08-18 17:27:32',\n",
       " '2011-12-30 04:07:20',\n",
       " '2010-07-27 14:14:10',\n",
       " '2011-10-07 17:23:13',\n",
       " '2010-09-02 07:18:38',\n",
       " '2011-05-07 16:26:44',\n",
       " '2012-11-21 00:03:27',\n",
       " '2010-09-10 23:05:50',\n",
       " '2011-04-06 10:55:04',\n",
       " '2010-08-30 01:22:20',\n",
       " '2011-06-02 12:47:17',\n",
       " '2011-03-01 16:16:25',\n",
       " '2010-08-06 05:27:57',\n",
       " '2012-05-02 12:24:49',\n",
       " '2012-05-15 01:58:24',\n",
       " '2012-04-27 18:19:25',\n",
       " '2011-03-17 12:05:58',\n",
       " '2011-05-27 18:21:27',\n",
       " '2010-08-26 21:24:56',\n",
       " '2010-12-01 21:09:50',\n",
       " '2010-07-29 10:57:18',\n",
       " '2010-08-29 08:58:43',\n",
       " '2014-05-14 15:17:07',\n",
       " '2011-05-08 11:53:51',\n",
       " '2010-07-29 22:35:28',\n",
       " '2010-07-27 14:03:52',\n",
       " '2011-03-01 14:19:56',\n",
       " '2010-07-27 09:44:12',\n",
       " '2010-11-16 02:30:58',\n",
       " '2012-08-27 07:28:03',\n",
       " '2010-12-01 20:17:08',\n",
       " '2011-10-12 14:02:39',\n",
       " '2010-09-07 02:04:47',\n",
       " '2011-09-20 20:42:50',\n",
       " '2011-02-20 07:03:34',\n",
       " '2012-10-29 17:08:40',\n",
       " '2010-11-17 11:14:47',\n",
       " '2010-12-31 10:46:17',\n",
       " '2012-05-05 16:59:11',\n",
       " '2011-04-20 20:08:26',\n",
       " '2011-10-24 23:26:47',\n",
       " '2010-07-27 14:10:31',\n",
       " '2012-05-15 13:51:45',\n",
       " '2014-01-11 23:27:07',\n",
       " '2010-09-20 20:44:51',\n",
       " '2010-08-17 02:01:44',\n",
       " '2010-10-20 19:12:23',\n",
       " '2010-08-27 15:07:18',\n",
       " '2011-12-06 09:09:33',\n",
       " '2012-05-02 20:00:15',\n",
       " '2010-08-10 14:59:06',\n",
       " '2010-07-26 12:38:40',\n",
       " '2012-08-21 15:17:25',\n",
       " '2010-07-20 16:13:03',\n",
       " '2010-12-13 18:20:38',\n",
       " '2012-01-12 19:54:50',\n",
       " '2011-12-15 02:44:03',\n",
       " '2010-12-15 20:08:32',\n",
       " '2010-08-18 10:40:25',\n",
       " '2010-10-22 16:15:02',\n",
       " '2010-10-02 17:30:57',\n",
       " '2010-08-02 16:57:33',\n",
       " '2010-08-13 22:35:50',\n",
       " '2010-12-05 05:18:37',\n",
       " '2010-11-02 13:02:40',\n",
       " '2012-07-01 18:46:18',\n",
       " '2010-10-02 17:06:33',\n",
       " '2010-08-05 02:36:08',\n",
       " '2010-08-30 14:00:17',\n",
       " '2011-11-04 13:18:01',\n",
       " '2010-08-15 08:59:45',\n",
       " '2010-08-19 21:44:04',\n",
       " '2010-11-30 20:22:45',\n",
       " '2010-07-31 15:36:12',\n",
       " '2012-11-27 21:59:46',\n",
       " '2010-09-05 11:43:35',\n",
       " '2011-07-12 15:26:57',\n",
       " '2010-09-05 19:19:03',\n",
       " '2010-07-27 07:24:19',\n",
       " '2010-09-24 04:09:04',\n",
       " '2010-07-27 16:04:42',\n",
       " '2011-04-01 04:40:52',\n",
       " '2010-07-22 13:49:37',\n",
       " '2011-02-11 15:01:37',\n",
       " '2012-04-10 11:54:34',\n",
       " '2010-10-08 11:30:29',\n",
       " '2011-06-22 02:03:50',\n",
       " '2010-07-27 16:50:51',\n",
       " '2010-09-23 06:10:48',\n",
       " '2013-03-06 16:07:20',\n",
       " '2011-05-20 20:15:32',\n",
       " '2012-01-16 11:22:45',\n",
       " '2012-02-01 01:31:25',\n",
       " '2012-01-05 21:14:03',\n",
       " '2013-01-23 09:37:19',\n",
       " '2011-10-12 23:52:17',\n",
       " '2012-05-29 15:04:32',\n",
       " '2012-06-26 06:00:18',\n",
       " '2011-12-08 11:04:19',\n",
       " '2010-08-31 13:03:46',\n",
       " '2010-12-04 20:42:26',\n",
       " '2011-05-04 20:45:04',\n",
       " '2010-07-21 16:04:18',\n",
       " '2011-04-06 08:35:46',\n",
       " '2011-07-05 21:24:45',\n",
       " '2011-09-20 18:06:16',\n",
       " '2011-11-02 01:43:47',\n",
       " '2010-08-28 07:11:23',\n",
       " '2011-10-30 03:14:13',\n",
       " '2010-08-22 04:16:03',\n",
       " '2010-08-22 19:24:35',\n",
       " '2012-03-23 03:15:35',\n",
       " '2010-08-22 10:02:58',\n",
       " '2011-05-05 09:27:32',\n",
       " '2011-04-09 20:39:39',\n",
       " '2012-01-30 18:19:17',\n",
       " '2011-03-12 23:55:47',\n",
       " '2011-06-25 22:37:57',\n",
       " '2011-05-04 20:52:28',\n",
       " '2011-08-12 20:30:02',\n",
       " '2010-12-10 02:48:55',\n",
       " '2010-12-17 22:00:34',\n",
       " '2012-10-16 09:05:15',\n",
       " '2010-10-05 13:54:13',\n",
       " '2010-08-06 11:11:00',\n",
       " '2011-12-04 11:09:31',\n",
       " '2010-07-20 20:50:48',\n",
       " '2010-09-02 18:56:34',\n",
       " '2011-05-20 09:23:56',\n",
       " '2012-04-24 20:50:35',\n",
       " '2010-07-29 18:59:01',\n",
       " '2010-11-08 14:12:20',\n",
       " '2011-04-05 13:34:02',\n",
       " '2010-08-17 02:52:02',\n",
       " '2010-08-17 07:28:37',\n",
       " '2010-08-28 17:07:59',\n",
       " '2012-01-19 07:12:05',\n",
       " '2010-11-10 20:58:12',\n",
       " '2012-12-19 18:50:04',\n",
       " '2012-01-04 08:04:10',\n",
       " '2011-01-28 11:07:15',\n",
       " '2012-06-09 16:41:00',\n",
       " '2012-01-25 06:09:13',\n",
       " '2010-09-03 16:27:56',\n",
       " '2011-08-12 20:29:33',\n",
       " '2010-07-19 21:40:02',\n",
       " '2010-07-27 09:22:04',\n",
       " '2012-05-31 14:35:14',\n",
       " '2010-11-15 18:07:57',\n",
       " '2010-09-24 04:13:25',\n",
       " '2010-09-16 12:35:43',\n",
       " '2011-03-20 12:49:47',\n",
       " '2012-02-06 19:18:52',\n",
       " '2011-04-05 15:19:15',\n",
       " '2010-12-14 23:13:34',\n",
       " '2010-11-18 10:10:55',\n",
       " '2011-02-16 18:00:20',\n",
       " '2012-11-28 06:04:24',\n",
       " '2011-08-12 20:30:59',\n",
       " '2011-03-01 07:40:34',\n",
       " '2011-06-01 03:42:00',\n",
       " '2012-08-20 20:11:30',\n",
       " '2010-10-04 10:16:23',\n",
       " '2011-10-21 20:00:14',\n",
       " '2012-02-16 22:03:09',\n",
       " '2012-01-22 22:24:38',\n",
       " '2012-11-27 20:13:45',\n",
       " '2012-05-02 03:02:47',\n",
       " '2010-07-23 08:18:52',\n",
       " '2010-08-31 13:02:43',\n",
       " '2012-05-15 05:29:51',\n",
       " '2011-12-12 17:50:27',\n",
       " '2010-11-06 19:47:38',\n",
       " '2010-11-15 21:39:05',\n",
       " '2012-10-02 22:10:54',\n",
       " '2011-01-03 23:15:30',\n",
       " '2010-12-04 10:52:07',\n",
       " '2010-10-04 14:08:55',\n",
       " '2010-12-04 00:18:06',\n",
       " '2010-07-19 21:53:02',\n",
       " '2014-01-27 14:38:45',\n",
       " '2012-01-25 19:18:11',\n",
       " '2010-10-09 19:17:12',\n",
       " '2011-08-16 15:52:51',\n",
       " '2010-07-22 10:16:18',\n",
       " '2011-01-30 00:05:34',\n",
       " '2010-12-04 11:27:56',\n",
       " '2010-07-21 22:57:58',\n",
       " '2010-12-14 23:17:09',\n",
       " '2012-02-01 00:27:23',\n",
       " '2010-10-04 18:24:36',\n",
       " '2010-07-21 15:09:11',\n",
       " '2012-05-05 17:27:18',\n",
       " '2011-05-11 10:03:37',\n",
       " '2011-12-14 22:49:02',\n",
       " '2010-08-25 21:24:22',\n",
       " '2011-07-13 02:37:11',\n",
       " '2011-09-22 01:48:05',\n",
       " '2012-05-30 02:16:51',\n",
       " '2010-08-08 20:22:58',\n",
       " '2010-11-17 12:03:29',\n",
       " '2010-07-19 20:33:26',\n",
       " '2010-12-17 11:13:27',\n",
       " '2012-02-14 19:39:35',\n",
       " '2010-11-23 18:37:54',\n",
       " '2011-05-08 02:26:21',\n",
       " '2010-07-21 23:00:34',\n",
       " '2012-12-12 05:49:44',\n",
       " '2011-01-21 08:38:17',\n",
       " '2010-12-14 03:02:22',\n",
       " '2010-07-21 17:54:05',\n",
       " '2010-10-14 10:31:18',\n",
       " '2012-03-28 02:27:13',\n",
       " '2012-02-09 10:00:37',\n",
       " '2010-09-16 12:08:07',\n",
       " '2010-07-26 19:53:10',\n",
       " '2011-02-25 13:25:01',\n",
       " '2012-11-03 00:22:09',\n",
       " '2010-10-06 12:22:32',\n",
       " '2012-07-26 12:21:14',\n",
       " '2012-03-27 18:47:26',\n",
       " '2012-09-07 16:57:11',\n",
       " '2010-08-20 12:28:09',\n",
       " '2012-01-05 16:56:42',\n",
       " '2011-05-27 15:30:01',\n",
       " '2011-01-02 11:22:43',\n",
       " '2010-09-08 17:47:21',\n",
       " '2010-10-01 17:21:51',\n",
       " '2012-08-31 08:37:07',\n",
       " '2010-08-04 12:32:12',\n",
       " '2010-08-29 19:54:09',\n",
       " '2012-01-19 02:47:40',\n",
       " '2012-02-09 13:25:00',\n",
       " '2010-11-08 17:07:23',\n",
       " '2014-06-16 18:42:37',\n",
       " '2011-10-12 19:40:09',\n",
       " '2012-12-17 13:18:50',\n",
       " '2010-10-20 19:13:42',\n",
       " '2010-07-28 18:17:46',\n",
       " '2010-10-20 20:29:24',\n",
       " '2010-10-05 15:08:21',\n",
       " '2010-12-15 19:33:16',\n",
       " '2010-08-19 13:25:53',\n",
       " '2010-07-30 02:42:38',\n",
       " '2011-01-03 23:07:43',\n",
       " '2010-08-22 00:34:17',\n",
       " '2010-07-21 00:09:09',\n",
       " '2010-08-13 12:31:09',\n",
       " '2010-08-17 16:17:00',\n",
       " '2012-04-19 17:44:13',\n",
       " '2010-08-22 09:47:37',\n",
       " '2010-08-16 17:13:58',\n",
       " '2010-10-04 13:02:30',\n",
       " '2010-07-30 20:00:45',\n",
       " '2010-08-03 21:19:22',\n",
       " '2010-08-14 09:41:44',\n",
       " '2010-07-21 00:54:29',\n",
       " '2010-12-09 17:39:07',\n",
       " '2013-07-29 14:48:26',\n",
       " '2010-10-11 20:31:15',\n",
       " '2010-08-14 01:32:29',\n",
       " '2011-07-06 23:18:06',\n",
       " '2011-02-18 22:29:42',\n",
       " '2011-03-30 11:35:21',\n",
       " '2010-08-30 18:02:22',\n",
       " '2010-10-20 19:29:29',\n",
       " '2011-01-27 06:46:32',\n",
       " '2010-07-21 01:35:13',\n",
       " '2010-07-19 21:29:37',\n",
       " '2010-07-30 10:03:37',\n",
       " '2010-09-11 15:37:55',\n",
       " '2010-10-24 21:18:47',\n",
       " '2010-10-19 11:13:59',\n",
       " '2010-09-20 06:05:15',\n",
       " '2010-08-21 21:24:35',\n",
       " '2011-06-17 12:29:47',\n",
       " '2010-08-22 06:37:15',\n",
       " '2010-12-09 15:05:19',\n",
       " '2010-09-16 10:21:36',\n",
       " '2012-01-05 19:05:58',\n",
       " '2010-08-22 10:07:23',\n",
       " '2012-04-08 21:58:52',\n",
       " '2012-02-21 13:06:45',\n",
       " '2010-07-27 06:51:24',\n",
       " '2011-09-23 04:36:06',\n",
       " '2010-11-03 15:51:11',\n",
       " '2011-09-01 13:57:47',\n",
       " '2011-05-08 19:03:21',\n",
       " '2010-12-13 23:50:20',\n",
       " '2010-10-02 22:51:17',\n",
       " '2010-10-19 06:16:41',\n",
       " '2010-11-18 13:14:58',\n",
       " '2012-07-17 12:44:21',\n",
       " '2012-07-28 01:40:34',\n",
       " '2010-07-22 10:15:28',\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'2010-07-19 20:19:46',\n",
       " '2010-07-20 00:21:48',\n",
       " '2010-07-29 12:38:50',\n",
       " '2010-07-29 23:43:25',\n",
       " '2010-07-29 23:45:11',\n",
       " '2010-08-03 07:58:26',\n",
       " '2010-08-03 09:14:27',\n",
       " '2010-08-05 13:06:12',\n",
       " '2010-08-19 15:10:48',\n",
       " '2010-09-01 16:59:06',\n",
       " '2010-09-16 07:09:04',\n",
       " '2010-10-07 01:30:50',\n",
       " '2010-10-14 08:57:22',\n",
       " '2010-10-14 09:00:42',\n",
       " '2010-10-20 17:16:08',\n",
       " '2010-10-22 16:04:22',\n",
       " '2010-10-24 21:24:43',\n",
       " '2010-10-25 23:52:05',\n",
       " '2010-11-30 16:38:57',\n",
       " '2010-12-08 09:15:39',\n",
       " '2011-01-09 12:50:39',\n",
       " '2011-02-06 13:02:49',\n",
       " '2011-03-02 06:16:43',\n",
       " '2011-03-04 21:40:11',\n",
       " '2011-03-11 04:09:55',\n",
       " '2011-03-11 22:18:47',\n",
       " '2011-03-23 17:06:37',\n",
       " '2011-03-24 21:11:34',\n",
       " '2011-03-29 21:45:14',\n",
       " '2011-04-08 15:18:21',\n",
       " '2011-04-11 15:04:34',\n",
       " '2011-04-29 20:13:18',\n",
       " '2011-05-13 20:14:36',\n",
       " '2011-05-24 16:26:12',\n",
       " '2011-05-28 20:11:23',\n",
       " '2011-05-31 21:06:46',\n",
       " '2011-06-17 13:28:08',\n",
       " '2011-06-19 16:36:27',\n",
       " '2011-06-24 11:18:30',\n",
       " '2011-06-26 22:07:50',\n",
       " '2011-06-29 12:49:22',\n",
       " '2011-06-29 15:58:28',\n",
       " '2011-07-11 16:03:44',\n",
       " '2011-07-29 10:18:27',\n",
       " '2011-07-29 16:48:36',\n",
       " '2011-07-31 14:12:19',\n",
       " '2011-07-31 19:40:29',\n",
       " '2011-08-03 13:56:26',\n",
       " '2011-08-04 13:34:19',\n",
       " '2011-08-08 17:33:16',\n",
       " '2011-08-10 01:42:02',\n",
       " '2011-08-12 02:44:42',\n",
       " '2011-08-15 17:32:32',\n",
       " '2011-08-15 23:14:42',\n",
       " '2011-08-23 09:26:16',\n",
       " '2011-08-24 14:05:47',\n",
       " '2011-08-25 19:13:01',\n",
       " '2011-09-06 19:45:37',\n",
       " '2011-09-08 13:27:07',\n",
       " '2011-09-08 15:23:15',\n",
       " '2011-09-13 16:31:02',\n",
       " '2011-09-15 18:48:52',\n",
       " '2011-09-16 16:30:41',\n",
       " '2011-09-16 17:48:00',\n",
       " '2011-09-17 14:19:21',\n",
       " '2011-09-19 08:38:35',\n",
       " '2011-09-19 15:10:56',\n",
       " '2011-09-22 21:10:08',\n",
       " '2011-09-23 15:11:02',\n",
       " '2011-09-23 16:17:59',\n",
       " '2011-09-23 18:16:31',\n",
       " '2011-09-26 13:48:30',\n",
       " '2011-09-26 16:00:33',\n",
       " '2011-10-01 22:24:44',\n",
       " '2011-10-05 22:02:12',\n",
       " '2011-10-06 18:21:48',\n",
       " '2011-10-07 14:36:36',\n",
       " '2011-10-12 07:11:43',\n",
       " '2011-10-12 20:43:23',\n",
       " '2011-10-16 16:54:57',\n",
       " '2011-10-17 13:31:39',\n",
       " '2011-10-18 20:10:27',\n",
       " '2011-10-18 20:12:24',\n",
       " '2011-11-03 04:49:37',\n",
       " '2011-11-11 12:16:26',\n",
       " '2011-11-18 15:25:49',\n",
       " '2011-11-21 20:42:57',\n",
       " '2011-11-22 22:41:08',\n",
       " '2011-11-25 21:59:41',\n",
       " '2011-11-28 19:35:10',\n",
       " '2011-12-10 09:44:15',\n",
       " '2011-12-13 06:07:32',\n",
       " '2011-12-13 20:34:11',\n",
       " '2011-12-19 00:35:20',\n",
       " '2011-12-19 15:14:31',\n",
       " '2011-12-20 18:04:15',\n",
       " '2011-12-23 03:32:41',\n",
       " '2011-12-23 13:59:16',\n",
       " '2011-12-23 14:12:10',\n",
       " '2011-12-27 14:40:39',\n",
       " '2011-12-30 08:53:36',\n",
       " '2012-01-01 15:01:58',\n",
       " '2012-01-03 13:44:56',\n",
       " '2012-01-03 15:38:23',\n",
       " '2012-01-04 15:40:50',\n",
       " '2012-01-09 00:13:04',\n",
       " '2012-01-09 15:02:02',\n",
       " '2012-01-10 14:02:42',\n",
       " '2012-01-11 19:40:42',\n",
       " '2012-01-12 21:13:45',\n",
       " '2012-01-20 10:35:10',\n",
       " '2012-01-30 11:55:34',\n",
       " '2012-01-31 08:26:25',\n",
       " '2012-02-03 15:03:49',\n",
       " '2012-02-11 08:37:19',\n",
       " '2012-02-12 20:11:06',\n",
       " '2012-02-17 15:02:28',\n",
       " '2012-02-19 06:43:26',\n",
       " '2012-02-21 16:01:16',\n",
       " '2012-02-22 14:02:31',\n",
       " '2012-02-22 21:19:53',\n",
       " '2012-02-22 23:35:05',\n",
       " '2012-02-22 23:50:15',\n",
       " '2012-02-27 04:49:15',\n",
       " '2012-02-29 08:38:07',\n",
       " '2012-03-06 19:10:53',\n",
       " '2012-03-12 13:21:27',\n",
       " '2012-03-13 17:23:23',\n",
       " '2012-03-14 22:37:12',\n",
       " '2012-03-15 16:32:53',\n",
       " '2012-03-17 00:07:38',\n",
       " '2012-03-18 17:06:10',\n",
       " '2012-03-20 15:51:10',\n",
       " '2012-03-22 11:05:40',\n",
       " '2012-03-24 08:59:01',\n",
       " '2012-03-24 09:06:35',\n",
       " '2012-03-24 20:31:39',\n",
       " '2012-03-26 17:25:30',\n",
       " '2012-03-29 11:04:53',\n",
       " '2012-04-01 12:15:05',\n",
       " '2012-04-02 19:17:41',\n",
       " '2012-04-10 20:14:16',\n",
       " '2012-04-11 08:19:31',\n",
       " '2012-04-13 14:27:28',\n",
       " '2012-04-13 16:13:33',\n",
       " '2012-04-16 15:05:04',\n",
       " '2012-04-17 17:31:52',\n",
       " '2012-04-17 19:00:13',\n",
       " '2012-04-20 17:22:54',\n",
       " '2012-04-25 08:35:24',\n",
       " '2012-05-02 10:54:44',\n",
       " '2012-05-03 14:56:56',\n",
       " '2012-05-09 16:46:29',\n",
       " '2012-05-10 07:49:25',\n",
       " '2012-05-14 19:15:34',\n",
       " '2012-05-16 15:58:31',\n",
       " '2012-05-16 20:38:14',\n",
       " '2012-05-18 14:11:07',\n",
       " '2012-05-18 14:17:07',\n",
       " '2012-05-22 14:25:22',\n",
       " '2012-05-22 19:24:15',\n",
       " '2012-05-23 13:37:15',\n",
       " '2012-05-24 12:55:29',\n",
       " '2012-05-24 14:16:59',\n",
       " '2012-05-28 17:32:46',\n",
       " '2012-05-29 14:16:08',\n",
       " '2012-05-31 13:53:54',\n",
       " '2012-06-01 12:15:12',\n",
       " '2012-06-01 12:28:28',\n",
       " '2012-06-03 18:04:59',\n",
       " '2012-06-04 17:37:36',\n",
       " '2012-06-05 13:57:13',\n",
       " '2012-06-06 14:42:25',\n",
       " '2012-06-07 16:05:18',\n",
       " '2012-06-07 21:44:55',\n",
       " '2012-06-10 13:44:45',\n",
       " '2012-06-11 21:17:43',\n",
       " '2012-06-14 13:12:56',\n",
       " '2012-06-14 13:55:05',\n",
       " '2012-06-15 10:40:36',\n",
       " '2012-06-15 15:18:08',\n",
       " '2012-06-18 22:42:51',\n",
       " '2012-06-22 18:54:32',\n",
       " '2012-06-25 14:45:41',\n",
       " '2012-06-27 03:11:39',\n",
       " '2012-06-29 14:38:26',\n",
       " '2012-07-01 10:07:50',\n",
       " '2012-07-05 13:36:54',\n",
       " '2012-07-07 09:47:44',\n",
       " '2012-07-10 12:16:26',\n",
       " '2012-07-10 12:34:19',\n",
       " '2012-07-11 16:15:53',\n",
       " '2012-07-12 14:57:46',\n",
       " '2012-07-12 16:44:55',\n",
       " '2012-07-12 18:58:17',\n",
       " '2012-07-12 22:23:31',\n",
       " '2012-07-18 12:40:43',\n",
       " '2012-07-18 12:42:29',\n",
       " '2012-07-18 15:47:39',\n",
       " '2012-07-18 18:53:06',\n",
       " '2012-07-20 15:51:03',\n",
       " '2012-07-24 14:00:08',\n",
       " '2012-07-25 09:07:39',\n",
       " '2012-07-26 11:52:35',\n",
       " '2012-07-26 12:28:06',\n",
       " '2012-07-27 13:10:49',\n",
       " '2012-07-31 02:05:44',\n",
       " '2012-07-31 02:11:46',\n",
       " '2012-08-02 14:00:33',\n",
       " '2012-08-02 19:40:28',\n",
       " '2012-08-03 13:12:34',\n",
       " '2012-08-04 10:53:08',\n",
       " '2012-08-05 22:20:29',\n",
       " '2012-08-07 14:36:18',\n",
       " '2012-08-07 15:48:43',\n",
       " '2012-08-08 20:03:09',\n",
       " '2012-08-08 22:29:00',\n",
       " '2012-08-09 20:08:07',\n",
       " '2012-08-09 23:16:04',\n",
       " '2012-08-10 17:09:37',\n",
       " '2012-08-14 12:05:12',\n",
       " '2012-08-14 12:08:44',\n",
       " '2012-08-14 12:14:58',\n",
       " '2012-08-14 12:16:51',\n",
       " '2012-08-14 12:19:02',\n",
       " '2012-08-14 12:20:07',\n",
       " '2012-08-14 12:30:06',\n",
       " '2012-08-14 12:37:39',\n",
       " '2012-08-14 12:38:09',\n",
       " '2012-08-14 12:38:31',\n",
       " '2012-08-14 12:41:56',\n",
       " '2012-08-14 12:42:15',\n",
       " '2012-08-14 12:49:34',\n",
       " '2012-08-14 12:50:15',\n",
       " '2012-08-14 12:50:27',\n",
       " '2012-08-14 12:54:38',\n",
       " '2012-08-14 12:56:02',\n",
       " '2012-08-14 12:56:41',\n",
       " '2012-08-14 12:56:53',\n",
       " '2012-08-14 13:00:49',\n",
       " '2012-08-14 13:00:57',\n",
       " '2012-08-14 20:28:12',\n",
       " '2012-08-14 20:33:32',\n",
       " '2012-08-14 20:35:34',\n",
       " '2012-08-14 20:36:08',\n",
       " '2012-08-14 20:37:33',\n",
       " '2012-08-14 20:38:11',\n",
       " '2012-08-14 20:40:00',\n",
       " '2012-08-14 20:40:58',\n",
       " '2012-08-14 20:44:28',\n",
       " '2012-08-14 20:46:00',\n",
       " '2012-08-14 20:49:38',\n",
       " '2012-08-17 21:27:28',\n",
       " '2012-08-18 19:41:06',\n",
       " '2012-08-21 16:37:43',\n",
       " '2012-08-21 19:26:31',\n",
       " '2012-08-22 21:34:16',\n",
       " '2012-08-24 01:55:31',\n",
       " '2012-08-25 21:59:13',\n",
       " '2012-08-25 22:00:40',\n",
       " '2012-08-26 07:34:30',\n",
       " '2012-08-27 09:02:34',\n",
       " '2012-08-27 18:39:17',\n",
       " '2012-08-28 14:32:07',\n",
       " '2012-08-28 14:39:13',\n",
       " '2012-08-28 19:20:14',\n",
       " '2012-08-28 19:43:42',\n",
       " '2012-08-30 13:54:29',\n",
       " '2012-08-31 12:42:15',\n",
       " '2012-08-31 16:52:57',\n",
       " '2012-09-02 08:15:24',\n",
       " '2012-09-02 16:05:52',\n",
       " '2012-09-02 16:12:23',\n",
       " '2012-09-02 16:16:30',\n",
       " '2012-09-03 09:37:51',\n",
       " '2012-09-03 09:49:54',\n",
       " '2012-09-03 16:09:16',\n",
       " '2012-09-04 12:24:22',\n",
       " '2012-09-05 12:26:10',\n",
       " '2012-09-05 16:03:21',\n",
       " '2012-09-05 22:17:53',\n",
       " '2012-09-06 15:04:11',\n",
       " '2012-09-07 12:25:49',\n",
       " '2012-09-07 13:32:57',\n",
       " '2012-09-07 22:01:46',\n",
       " '2012-09-08 21:45:15',\n",
       " '2012-09-10 20:11:46',\n",
       " '2012-09-11 14:20:48',\n",
       " '2012-09-12 15:49:44',\n",
       " '2012-09-13 16:45:45',\n",
       " '2012-09-13 16:46:03',\n",
       " '2012-09-13 16:46:23',\n",
       " '2012-09-14 15:48:41',\n",
       " '2012-09-16 15:58:16',\n",
       " '2012-09-16 15:58:42',\n",
       " '2012-09-16 16:03:56',\n",
       " '2012-09-20 13:41:44',\n",
       " '2012-09-20 13:45:11',\n",
       " '2012-09-20 13:45:50',\n",
       " '2012-09-21 08:19:59',\n",
       " '2012-09-21 12:56:44',\n",
       " '2012-09-22 15:51:57',\n",
       " '2012-09-23 21:15:12',\n",
       " '2012-09-24 01:56:12',\n",
       " '2012-09-24 01:58:52',\n",
       " '2012-09-24 14:11:43',\n",
       " '2012-09-24 14:34:12',\n",
       " '2012-09-25 20:09:10',\n",
       " '2012-09-25 21:05:25',\n",
       " '2012-09-25 21:28:02',\n",
       " '2012-09-27 00:34:58',\n",
       " '2012-09-27 14:24:46',\n",
       " '2012-09-28 20:26:10',\n",
       " '2012-09-28 22:56:23',\n",
       " '2012-09-29 07:10:24',\n",
       " '2012-09-29 20:45:20',\n",
       " '2012-09-29 21:44:00',\n",
       " '2012-09-30 22:29:07',\n",
       " '2012-09-30 22:30:47',\n",
       " '2012-09-30 22:39:02',\n",
       " '2012-09-30 23:33:52',\n",
       " '2012-10-02 04:47:11',\n",
       " '2012-10-02 05:07:56',\n",
       " '2012-10-02 16:21:11',\n",
       " '2012-10-02 19:51:14',\n",
       " '2012-10-03 13:48:21',\n",
       " '2012-10-07 18:52:42',\n",
       " '2012-10-07 19:23:59',\n",
       " '2012-10-08 09:58:41',\n",
       " '2012-10-08 15:33:31',\n",
       " '2012-10-08 15:37:37',\n",
       " '2012-10-09 19:16:23',\n",
       " '2012-10-10 10:55:26',\n",
       " '2012-10-10 10:56:20',\n",
       " '2012-10-10 15:44:24',\n",
       " '2012-10-11 10:14:29',\n",
       " '2012-10-11 10:29:06',\n",
       " '2012-10-11 10:35:55',\n",
       " '2012-10-11 14:27:38',\n",
       " '2012-10-11 21:24:31',\n",
       " '2012-10-15 12:08:48',\n",
       " '2012-10-15 12:10:22',\n",
       " '2012-10-15 12:11:57',\n",
       " '2012-10-17 16:13:43',\n",
       " '2012-10-20 17:59:01',\n",
       " '2012-10-21 03:35:59',\n",
       " '2012-10-21 13:47:21',\n",
       " '2012-10-21 17:01:03',\n",
       " '2012-10-23 17:33:51',\n",
       " '2012-10-23 20:56:02',\n",
       " '2012-10-24 13:51:05',\n",
       " '2012-10-25 10:27:57',\n",
       " '2012-10-25 15:53:41',\n",
       " '2012-10-25 17:57:06',\n",
       " '2012-10-29 19:14:37',\n",
       " '2012-10-30 01:02:51',\n",
       " '2012-10-30 23:52:23',\n",
       " '2012-10-31 14:07:58',\n",
       " '2012-11-01 17:31:26',\n",
       " '2012-11-01 23:18:34',\n",
       " '2012-11-03 20:27:15',\n",
       " '2012-11-04 11:33:23',\n",
       " '2012-11-04 17:34:17',\n",
       " '2012-11-04 20:40:23',\n",
       " '2012-11-04 20:41:01',\n",
       " '2012-11-05 08:31:57',\n",
       " '2012-11-07 16:29:57',\n",
       " '2012-11-11 21:13:31',\n",
       " '2012-11-12 17:27:10',\n",
       " '2012-11-15 16:40:19',\n",
       " '2012-11-15 21:49:11',\n",
       " '2012-11-16 12:27:18',\n",
       " '2012-11-18 22:15:04',\n",
       " '2012-11-20 18:22:19',\n",
       " '2012-11-25 11:55:19',\n",
       " '2012-11-25 21:03:00',\n",
       " '2012-11-27 19:02:45',\n",
       " '2012-12-01 15:51:14',\n",
       " '2012-12-01 21:02:53',\n",
       " '2012-12-05 01:51:18',\n",
       " '2012-12-05 14:50:07',\n",
       " '2012-12-07 19:49:06',\n",
       " '2012-12-08 15:27:55',\n",
       " '2012-12-11 16:18:09',\n",
       " '2012-12-11 17:42:44',\n",
       " '2012-12-13 02:44:48',\n",
       " '2012-12-13 15:35:10',\n",
       " '2012-12-14 17:58:58',\n",
       " '2012-12-17 07:22:05',\n",
       " '2012-12-17 15:49:56',\n",
       " '2012-12-20 14:45:39',\n",
       " '2012-12-20 15:40:04',\n",
       " '2012-12-20 20:11:14',\n",
       " '2012-12-21 23:15:04',\n",
       " '2012-12-26 08:56:18',\n",
       " '2012-12-27 15:58:48',\n",
       " '2012-12-27 18:57:43',\n",
       " '2012-12-28 05:30:59',\n",
       " '2012-12-30 14:46:33',\n",
       " '2012-12-31 14:35:16',\n",
       " '2013-01-03 19:28:05',\n",
       " '2013-01-04 00:08:21',\n",
       " '2013-01-04 09:48:00',\n",
       " '2013-01-04 10:47:48',\n",
       " '2013-01-06 12:19:18',\n",
       " '2013-01-07 14:29:48',\n",
       " '2013-01-08 20:43:45',\n",
       " '2013-01-09 13:25:40',\n",
       " '2013-01-09 13:51:10',\n",
       " '2013-01-10 08:21:02',\n",
       " '2013-01-11 15:45:23',\n",
       " '2013-01-13 16:56:31',\n",
       " '2013-01-13 16:58:24',\n",
       " '2013-01-15 13:15:24',\n",
       " '2013-01-15 14:35:04',\n",
       " '2013-01-15 15:25:35',\n",
       " '2013-01-17 16:05:33',\n",
       " '2013-01-20 10:51:02',\n",
       " '2013-01-21 15:49:52',\n",
       " '2013-01-21 18:29:11',\n",
       " '2013-01-22 18:12:08',\n",
       " '2013-01-23 15:29:42',\n",
       " '2013-01-24 13:34:38',\n",
       " '2013-01-26 11:56:19',\n",
       " '2013-01-29 12:38:24',\n",
       " '2013-01-30 21:40:10',\n",
       " '2013-02-07 14:43:26',\n",
       " '2013-02-07 14:47:36',\n",
       " '2013-02-07 20:59:40',\n",
       " '2013-02-11 18:19:14',\n",
       " '2013-02-12 23:07:15',\n",
       " '2013-02-13 15:09:23',\n",
       " '2013-02-13 15:36:11',\n",
       " '2013-02-13 15:36:35',\n",
       " '2013-02-14 16:37:21',\n",
       " '2013-02-14 16:37:32',\n",
       " '2013-02-15 13:32:21',\n",
       " '2013-02-15 16:21:19',\n",
       " '2013-02-17 22:11:26',\n",
       " '2013-02-18 17:58:02',\n",
       " '2013-02-27 00:36:24',\n",
       " '2013-03-01 22:35:12',\n",
       " '2013-03-07 18:27:16',\n",
       " '2013-04-03 19:39:22',\n",
       " '2013-04-05 13:24:46',\n",
       " '2013-04-24 13:38:54',\n",
       " '2013-04-26 04:51:32',\n",
       " '2013-04-30 14:38:58',\n",
       " '2013-05-06 21:42:10',\n",
       " '2013-05-17 23:20:46',\n",
       " '2013-05-22 19:28:12',\n",
       " '2013-05-30 18:37:30',\n",
       " '2013-06-18 03:39:53',\n",
       " '2013-06-20 03:05:29',\n",
       " '2013-06-20 14:08:50',\n",
       " '2013-06-20 14:09:08',\n",
       " '2013-06-21 20:36:04',\n",
       " '2013-06-28 19:20:40',\n",
       " '2013-06-30 09:12:59',\n",
       " '2013-07-06 12:17:41',\n",
       " '2013-07-18 19:01:48',\n",
       " '2013-07-21 10:56:15',\n",
       " '2013-07-22 08:03:56',\n",
       " '2013-08-03 19:17:05',\n",
       " '2013-08-05 01:05:53',\n",
       " '2013-08-19 19:39:04',\n",
       " '2013-08-22 18:11:28',\n",
       " '2013-08-23 19:02:08',\n",
       " '2013-08-26 08:19:37',\n",
       " '2013-08-28 09:17:38',\n",
       " '2013-08-29 17:09:07',\n",
       " '2013-09-02 15:19:30',\n",
       " '2013-09-03 15:34:18',\n",
       " '2013-09-09 15:42:20',\n",
       " '2013-09-16 23:41:12',\n",
       " '2013-09-19 22:15:27',\n",
       " '2013-09-27 16:24:39',\n",
       " '2013-10-02 14:22:50',\n",
       " '2013-10-02 14:23:17',\n",
       " '2013-10-22 15:04:01',\n",
       " '2013-11-07 13:12:47',\n",
       " '2013-11-14 07:50:25',\n",
       " '2013-11-14 15:30:19',\n",
       " '2013-11-21 10:45:54',\n",
       " '2013-11-29 10:18:11',\n",
       " '2013-12-10 11:15:20',\n",
       " '2013-12-31 14:16:04',\n",
       " '2014-01-15 22:05:35',\n",
       " '2014-01-26 10:13:50',\n",
       " '2014-01-28 18:56:51',\n",
       " '2014-02-09 19:20:50',\n",
       " '2014-02-12 01:23:48',\n",
       " '2014-02-26 16:04:21',\n",
       " '2014-02-27 21:30:26',\n",
       " '2014-02-27 21:31:05',\n",
       " '2014-02-27 21:31:53',\n",
       " '2014-02-27 21:32:30',\n",
       " '2014-03-08 22:01:28',\n",
       " '2014-03-24 07:24:04',\n",
       " '2014-04-20 13:17:01',\n",
       " '2014-04-20 22:19:48',\n",
       " '2014-04-21 22:12:50',\n",
       " '2014-05-14 17:23:17',\n",
       " '2014-05-29 22:18:49',\n",
       " '2014-06-01 07:24:29',\n",
       " '2014-07-02 13:12:05',\n",
       " '2014-07-29 23:11:30',\n",
       " '2014-08-02 14:01:10',\n",
       " '2014-08-05 12:21:53',\n",
       " '2014-08-07 20:48:13',\n",
       " '2014-08-14 21:14:00',\n",
       " nan}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'A.R',\n",
       " 'ALSTAT',\n",
       " 'Aarthi',\n",
       " 'Abhishek Shivkumar',\n",
       " 'Adam Bailey',\n",
       " 'Adam Bowen',\n",
       " 'Aina',\n",
       " 'Alberto',\n",
       " 'Alby',\n",
       " 'Alex',\n",
       " 'Alexander Galkin',\n",
       " 'Alexandre Passos',\n",
       " 'Alix Axel',\n",
       " 'Andreas Mueller',\n",
       " 'Andree',\n",
       " 'Andrew',\n",
       " 'Andrew Rosenberg',\n",
       " 'Angelo',\n",
       " 'Aniko',\n",
       " 'Anony-Mousse',\n",
       " 'Antonio Pedro Ramos',\n",
       " 'Arnold Neumaier',\n",
       " 'Artem Kaznatcheev',\n",
       " 'Arun',\n",
       " 'Arvin',\n",
       " 'Atilla Ozgur',\n",
       " 'AweSIM',\n",
       " 'B Seven',\n",
       " 'BB',\n",
       " 'BGreene',\n",
       " 'BYS2',\n",
       " 'Backlin',\n",
       " 'Ben',\n",
       " 'Ben Bolker',\n",
       " 'BenBarnes',\n",
       " 'BlueTrin',\n",
       " 'Bradford',\n",
       " 'Bryan',\n",
       " 'Bullmoose',\n",
       " 'CWT',\n",
       " 'Carl Witthoft',\n",
       " 'Cassie',\n",
       " 'CharlesM',\n",
       " 'Chase',\n",
       " 'Chris',\n",
       " 'Chris Farmer',\n",
       " 'Chris Taylor',\n",
       " 'ChrisArmstrong',\n",
       " 'ChrisS',\n",
       " 'Chthonic Project',\n",
       " 'DWin',\n",
       " 'Dail',\n",
       " 'Dan M.',\n",
       " 'Darren Young',\n",
       " 'David',\n",
       " 'David Roberts',\n",
       " 'David Robinson',\n",
       " 'Davide Giraudo',\n",
       " 'DevelBD',\n",
       " 'Developer',\n",
       " 'Dilip Sarwate',\n",
       " 'Dimitriy V. Masterov',\n",
       " 'Dirk Eddelbuettel',\n",
       " 'DonAndre',\n",
       " 'Dougal',\n",
       " 'Dov',\n",
       " 'DrewConway',\n",
       " 'Druss2k',\n",
       " 'ECII',\n",
       " 'EMS',\n",
       " 'EOL',\n",
       " 'Edward',\n",
       " 'Emre',\n",
       " 'EnergyNumbers',\n",
       " 'Ethan Shepherd',\n",
       " 'Faheem Mitha',\n",
       " 'Faith',\n",
       " 'Felipe',\n",
       " 'Francisco Roldán',\n",
       " 'Frank',\n",
       " 'Fred',\n",
       " 'Frederik Brinck Jensen',\n",
       " 'G. Grothendieck',\n",
       " 'GEdgar',\n",
       " 'Gang Liu',\n",
       " 'Garrith Graham',\n",
       " 'Gavin Simpson',\n",
       " 'Gerenuk',\n",
       " 'Girishkumar',\n",
       " 'Glen_b',\n",
       " 'Gonten',\n",
       " 'Graham Jones',\n",
       " 'GrayR',\n",
       " 'Guy',\n",
       " 'Henrik',\n",
       " 'Henry',\n",
       " 'Henry B.',\n",
       " 'Ian Stuart',\n",
       " 'Indrajit',\n",
       " 'Ismeet Kaur',\n",
       " 'Iterator',\n",
       " 'Ivan',\n",
       " 'Jacob',\n",
       " 'Jaime',\n",
       " 'James',\n",
       " 'James Bowery',\n",
       " 'James Waters',\n",
       " 'Jason S',\n",
       " 'Javad',\n",
       " 'Jawad',\n",
       " 'Jebego',\n",
       " 'Jens',\n",
       " 'Jeremy _',\n",
       " 'Jergason',\n",
       " 'Jim',\n",
       " 'John ',\n",
       " 'John A. Ramey',\n",
       " 'John Colby',\n",
       " 'John D. Cook',\n",
       " 'John Horton',\n",
       " 'John Leidegren',\n",
       " 'Jonas',\n",
       " 'Joris Meys',\n",
       " 'Jose',\n",
       " 'Josh',\n",
       " \"Josh O'Brien\",\n",
       " 'Joshua',\n",
       " 'Juanan',\n",
       " 'Julie',\n",
       " 'Kaish',\n",
       " 'Kaoukkos',\n",
       " 'Kavka',\n",
       " 'Kerry',\n",
       " 'Kevin',\n",
       " 'Kim N',\n",
       " 'Kit',\n",
       " 'Klas Lindbäck',\n",
       " 'Knightgu',\n",
       " 'KronoS',\n",
       " 'Kyle',\n",
       " 'Kyle Brandt',\n",
       " 'LPP',\n",
       " 'Largh',\n",
       " 'LazyCat',\n",
       " 'Legend',\n",
       " 'LonelyBear',\n",
       " 'Lori',\n",
       " 'Lostsoul',\n",
       " 'Lucas',\n",
       " 'Lukapple',\n",
       " 'Luke404',\n",
       " 'M C',\n",
       " 'MOLi',\n",
       " 'MRocklin',\n",
       " 'MYaseen208',\n",
       " 'Macro',\n",
       " 'Mahin',\n",
       " 'Mahmoud Ghandi ',\n",
       " 'MainMa',\n",
       " 'Marc Shivers',\n",
       " 'Marc in the box',\n",
       " 'Marcel',\n",
       " 'Marek Sebera',\n",
       " 'Mark Lavin',\n",
       " 'Martin Mystery',\n",
       " 'Martin Pohl',\n",
       " 'Mat',\n",
       " 'Mathematics',\n",
       " 'Mew 3.4',\n",
       " 'Michael Barker',\n",
       " 'Michael Chernick',\n",
       " 'Michael Hardy',\n",
       " 'Michael McGowan',\n",
       " 'Michael.Z',\n",
       " 'Mihran Hovsepyan',\n",
       " 'Mike Byrne',\n",
       " 'Mike Furlender',\n",
       " 'MikeRand',\n",
       " 'Ming-Chih Kao',\n",
       " 'Misha',\n",
       " 'MisterH',\n",
       " 'Mobius Pizza',\n",
       " 'Mohammad',\n",
       " 'Mohammed',\n",
       " 'Mutuelinvestor',\n",
       " 'NJH',\n",
       " 'Nick',\n",
       " 'Nick Sabbe',\n",
       " 'Nikratio',\n",
       " 'Noam Peled',\n",
       " 'OSlOlSO',\n",
       " 'Oleksandr Pshenychnyy',\n",
       " 'Oliver',\n",
       " 'Olivier',\n",
       " 'Owen',\n",
       " 'P Sellaz',\n",
       " 'PepsiCo',\n",
       " 'Peter',\n",
       " 'Peter H',\n",
       " 'Peter Shor ',\n",
       " 'PherricOxide',\n",
       " 'Philipp',\n",
       " 'Prasenjit',\n",
       " 'Programmer',\n",
       " 'Qin',\n",
       " 'Qnan',\n",
       " 'Quant Guy',\n",
       " 'Rachel',\n",
       " 'Ramnath',\n",
       " 'Ran',\n",
       " 'Ranabir',\n",
       " 'Raphael',\n",
       " 'Renato Dinhani Conceição',\n",
       " 'Ricardo González-Gil',\n",
       " 'Richie Cotton',\n",
       " 'Roland',\n",
       " 'Roman Luštrik',\n",
       " 'Ron Gejman',\n",
       " 'Ross Ahmed ',\n",
       " 'Ross Millikan',\n",
       " 'STW',\n",
       " 'S_H',\n",
       " 'Sachin Shekhar',\n",
       " 'Sam',\n",
       " 'Sam Roberts',\n",
       " 'Sander',\n",
       " 'Sanjay',\n",
       " 'Sasha',\n",
       " 'SaveTheRbtz',\n",
       " 'Seth',\n",
       " 'Seyhmus Güngören',\n",
       " 'Shahzad',\n",
       " 'Sidd',\n",
       " 'Sigvard',\n",
       " 'Sinan Ünür',\n",
       " 'Skiminok',\n",
       " 'Sklivvz',\n",
       " 'Soo',\n",
       " 'Sriram ',\n",
       " 'StasK',\n",
       " 'Strin',\n",
       " 'Surjya Narayana Padhi',\n",
       " 'Sycren',\n",
       " 'Szabolcs',\n",
       " 'TenaliRaman',\n",
       " 'ThePiachu',\n",
       " 'Thom Blake',\n",
       " 'Throwback1986',\n",
       " 'Timo',\n",
       " 'Tizz',\n",
       " 'Tom Au',\n",
       " 'Tom Ritter',\n",
       " 'Tomas T.',\n",
       " 'Tommy',\n",
       " 'Travis',\n",
       " 'Turukawa',\n",
       " 'Two Cents',\n",
       " 'Tyler Rinker',\n",
       " 'Ugo',\n",
       " 'Upul',\n",
       " 'User',\n",
       " 'Uwe Schmitt',\n",
       " 'Victor Lin',\n",
       " 'Victor May',\n",
       " 'Vighnesh',\n",
       " 'Vilx-',\n",
       " 'Vincent Zoonekynd',\n",
       " 'Vinterwoo',\n",
       " 'Volomike',\n",
       " 'Wee',\n",
       " 'Whymarrh',\n",
       " 'Wilduck',\n",
       " 'Xiaowen Li',\n",
       " 'Yang',\n",
       " 'Yiming Lu',\n",
       " 'Zach',\n",
       " 'aL3xa',\n",
       " 'aardvarkk',\n",
       " 'aaronjg',\n",
       " 'ablmf',\n",
       " 'adharris',\n",
       " 'aioobe',\n",
       " 'alang',\n",
       " 'alfa',\n",
       " 'amit',\n",
       " 'andreister',\n",
       " 'andrew cooke',\n",
       " 'andrija',\n",
       " 'anonymous_4322',\n",
       " 'apeescape',\n",
       " 'aramis',\n",
       " 'awshepard',\n",
       " 'banjollity',\n",
       " 'baz',\n",
       " 'bdh_dtu',\n",
       " 'ben',\n",
       " 'bgbg',\n",
       " 'bootech',\n",
       " 'brandon',\n",
       " 'burritoboy',\n",
       " 'caerolus',\n",
       " 'carlosdc',\n",
       " 'caseyr547',\n",
       " 'check123',\n",
       " 'chk',\n",
       " 'chutsu',\n",
       " 'cohoz',\n",
       " 'crible',\n",
       " 'cups',\n",
       " 'dannytoone',\n",
       " 'dark_charlie',\n",
       " 'datayoda',\n",
       " 'datta ram',\n",
       " 'dchandler',\n",
       " 'deinst',\n",
       " 'dmckee',\n",
       " 'doug',\n",
       " 'drknexus',\n",
       " 'dsimcha',\n",
       " 'duffymo',\n",
       " 'eznme',\n",
       " 'fabee',\n",
       " 'feelfree',\n",
       " 'ffriend',\n",
       " 'fred basset',\n",
       " 'gabriel',\n",
       " 'gisol',\n",
       " 'gorcee',\n",
       " 'gsk3',\n",
       " 'h.l.m',\n",
       " 'hackartist',\n",
       " 'heltonbiker',\n",
       " 'icobes',\n",
       " 'j0N45',\n",
       " 'jasonweiyi',\n",
       " 'jazzvibes',\n",
       " 'jb.',\n",
       " 'jeannot',\n",
       " 'jkg',\n",
       " 'johanbev',\n",
       " 'jonsca',\n",
       " 'joran',\n",
       " 'joriki',\n",
       " 'jthetzel',\n",
       " 'jwegner',\n",
       " 'keyboardP',\n",
       " 'kino',\n",
       " 'kmore',\n",
       " 'kohske',\n",
       " 'krammer',\n",
       " 'learner',\n",
       " 'lockedoff',\n",
       " 'maknelly',\n",
       " 'maple_shaft',\n",
       " 'maximus',\n",
       " 'mbq',\n",
       " 'metrobalderas',\n",
       " 'mga',\n",
       " 'mhoran_psprep',\n",
       " 'mihsathe',\n",
       " 'mosaic',\n",
       " 'mpiktas',\n",
       " 'mrdwab',\n",
       " 'muzhig',\n",
       " nan,\n",
       " 'nate',\n",
       " 'natorro',\n",
       " 'neuron',\n",
       " 'nibot',\n",
       " 'niko',\n",
       " 'nimrodm',\n",
       " 'nograpes',\n",
       " 'nostock',\n",
       " 'nullglob',\n",
       " 'ojblass',\n",
       " 'olchauvin',\n",
       " 'pater',\n",
       " 'petrelharp',\n",
       " 'petrichor',\n",
       " 'pharmine',\n",
       " 'picmate',\n",
       " 'pixel',\n",
       " 'pootzko',\n",
       " 'porst17',\n",
       " 'powerbar',\n",
       " 'prasenjit',\n",
       " 'prototoast',\n",
       " 'pyCthon',\n",
       " 'qmsource',\n",
       " 'quarkdown27',\n",
       " 'rahul',\n",
       " 'ralu',\n",
       " 'richardh',\n",
       " 'rnorberg',\n",
       " 'rumbleB',\n",
       " 's093294',\n",
       " 'sanity',\n",
       " 'sbg',\n",
       " 'sebhofer',\n",
       " 'sharky',\n",
       " 'shoonya',\n",
       " 'shootingstars',\n",
       " 'sinoTrinity',\n",
       " 'skyde',\n",
       " 'somori',\n",
       " 'stevenvh',\n",
       " 'subash',\n",
       " 'swiecki',\n",
       " 'sydeulissie',\n",
       " 'tdc',\n",
       " 'thedorkknight',\n",
       " 'thelatemail',\n",
       " 'thiton',\n",
       " 'tnaser',\n",
       " 'tutu',\n",
       " 'ulidtko',\n",
       " 'user1009703',\n",
       " 'user1134241',\n",
       " 'user1134516',\n",
       " 'user1140126',\n",
       " 'user1149913',\n",
       " 'user11667',\n",
       " 'user1172558',\n",
       " 'user1180428',\n",
       " 'user1203297',\n",
       " 'user1257313',\n",
       " 'user1265067',\n",
       " 'user12714',\n",
       " 'user1313',\n",
       " 'user1554592',\n",
       " 'user1569715',\n",
       " 'user1665220',\n",
       " 'user1727485',\n",
       " 'user1731927',\n",
       " 'user1738753',\n",
       " 'user1753987',\n",
       " 'user1785104',\n",
       " 'user1788005',\n",
       " 'user1873',\n",
       " 'user1911070',\n",
       " 'user1924327',\n",
       " 'user2760',\n",
       " 'user3125',\n",
       " 'user333',\n",
       " 'user3671',\n",
       " 'user3897',\n",
       " 'user54511',\n",
       " 'user5455',\n",
       " 'user570593',\n",
       " 'user594694',\n",
       " 'user697473',\n",
       " 'user714852',\n",
       " 'user779747',\n",
       " 'user862',\n",
       " 'user8968',\n",
       " 'user918967',\n",
       " 'user9325',\n",
       " 'user969113',\n",
       " 'user974896',\n",
       " 'user975904',\n",
       " 'user975964',\n",
       " 'user976991',\n",
       " 'user995434',\n",
       " 'user9964',\n",
       " 'utdiscant',\n",
       " 'vedran',\n",
       " 'vignesh kumar rathakumar',\n",
       " 'viki.omega9',\n",
       " 'vonPetrushev',\n",
       " 'vonjd',\n",
       " 'vzn',\n",
       " 'whuber',\n",
       " 'winwaed',\n",
       " 'wvoq',\n",
       " 'xan',\n",
       " 'xiaohan2012',\n",
       " 'yO2gO',\n",
       " 'zca0',\n",
       " 'zyx',\n",
       " 'ʞɔıu'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " 'user10525',\n",
       " 'user13253',\n",
       " 'user14071',\n",
       " 'user28',\n",
       " 'user5644',\n",
       " 'user7997',\n",
       " 'user9410'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Fill this columns\n",
    "#strings with empty string ' ' \n",
    "display(set(users_posts['DisplayName']))\n",
    "display(set(users_posts['WebsiteUrl']))\n",
    "display(set(users_posts['Location']))\n",
    "display(set(users_posts['AboutMe']))\n",
    "display(set(users_posts['Body']))\n",
    "display(set(users_posts['Title']))\n",
    "display(set(users_posts['Tags']))\n",
    "#numeric with 0\n",
    "display(set(users_posts['Age']))\n",
    "display(set(users_posts['AcceptedAnswerId']))\n",
    "display(set(users_posts['ViewCount']))\n",
    "display(set(users_posts['AnswerCount']))\n",
    "display(set(users_posts['FavoriteCount']))\n",
    "display(set(users_posts['LastEditorUserId']))\n",
    "display(set(users_posts['ParentId']))\n",
    "#Min datetime\n",
    "display(set(users_posts['LastEditDate']))\n",
    "\n",
    "#clean\n",
    "#drop columns\n",
    "display(set(users_posts['ProfileImageUrl']))\n",
    "display(set(users_posts['CommunityOwnedDate']))\n",
    "display(set(users_posts['ClosedDate']))\n",
    "display(set(users_posts['OwnerDisplayName']))\n",
    "display(set(users_posts['LastEditorDisplayName']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill strings\n",
    "users_posts[['DisplayName','WebsiteUrl','Location','AboutMe','Body','Title','Tags']] = users_posts[['DisplayName','WebsiteUrl','Location','AboutMe','Body','Title','Tags']].fillna(' ')\n",
    "#Fill numeric\n",
    "users_posts[['Age','AcceptedAnswerId','ViewCount','AnswerCount','FavoriteCount','LastEditorUserId','ParentId']] = users_posts[['Age','AcceptedAnswerId','ViewCount','AnswerCount','FavoriteCount','LastEditorUserId','ParentId']].fillna(0)\n",
    "\n",
    "#Fill datetime\n",
    "import datetime\n",
    "\n",
    "users_posts['LastEditDate'] = users_posts['LastEditDate'].fillna(datetime.datetime.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>percent_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ProfileImageUrl</th>\n",
       "      <td>ProfileImageUrl</td>\n",
       "      <td>92.749346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CommunityOwnedDate</th>\n",
       "      <td>CommunityOwnedDate</td>\n",
       "      <td>95.313382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ClosedDate</th>\n",
       "      <td>ClosedDate</td>\n",
       "      <td>98.688466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OwnerDisplayName</th>\n",
       "      <td>OwnerDisplayName</td>\n",
       "      <td>98.105847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LastEditorDisplayName</th>\n",
       "      <td>LastEditorDisplayName</td>\n",
       "      <td>99.463580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 column_name  percent_missing\n",
       "ProfileImageUrl              ProfileImageUrl        92.749346\n",
       "CommunityOwnedDate        CommunityOwnedDate        95.313382\n",
       "ClosedDate                        ClosedDate        98.688466\n",
       "OwnerDisplayName            OwnerDisplayName        98.105847\n",
       "LastEditorDisplayName  LastEditorDisplayName        99.463580"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_missing = users_posts.isnull().sum() * 100 / len(users_posts)\n",
    "missing_value_df = pd.DataFrame({'column_name': users_posts.columns,\n",
    "                                 'percent_missing': percent_missing})\n",
    "missing_value_df[(percent_missing>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns\n",
    "\n",
    "users_posts = users_posts.drop(['ProfileImageUrl','CommunityOwnedDate','ClosedDate','OwnerDisplayName','LastEditorDisplayName'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>percent_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [column_name, percent_missing]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_missing = users_posts.isnull().sum() * 100 / len(users_posts)\n",
    "missing_value_df = pd.DataFrame({'column_name': users_posts.columns,\n",
    "                                 'percent_missing': percent_missing})\n",
    "missing_value_df[(percent_missing>0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Adjust the data types in order to avoid future issues. Which ones should be changed? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId                int64\n",
       "Reputation            int64\n",
       "CreationDate         object\n",
       "DisplayName          object\n",
       "LastAccessDate       object\n",
       "WebsiteUrl           object\n",
       "Location             object\n",
       "AboutMe              object\n",
       "Views                 int64\n",
       "UpVotes               int64\n",
       "DownVotes             int64\n",
       "AccountId             int64\n",
       "Age                 float64\n",
       "postId                int64\n",
       "PostTypeId            int64\n",
       "AcceptedAnswerId    float64\n",
       "CreaionDate          object\n",
       "Score                 int64\n",
       "ViewCount           float64\n",
       "Body                 object\n",
       "LasActivityDate      object\n",
       "Title                object\n",
       "Tags                 object\n",
       "AnswerCount         float64\n",
       "CommentCount          int64\n",
       "FavoriteCount       float64\n",
       "LastEditorUserId    float64\n",
       "LastEditDate         object\n",
       "ParentId            float64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(users_posts.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>...</th>\n",
       "      <th>Body</th>\n",
       "      <th>LasActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>ParentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [userId, Reputation, CreationDate, DisplayName, LastAccessDate, WebsiteUrl, Location, AboutMe, Views, UpVotes, DownVotes, AccountId, Age, postId, PostTypeId, AcceptedAnswerId, CreaionDate, Score, ViewCount, Body, LasActivityDate, Title, Tags, AnswerCount, CommentCount, FavoriteCount, LastEditorUserId, LastEditDate, ParentId]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>...</th>\n",
       "      <th>Body</th>\n",
       "      <th>LasActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>ParentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [userId, Reputation, CreationDate, DisplayName, LastAccessDate, WebsiteUrl, Location, AboutMe, Views, UpVotes, DownVotes, AccountId, Age, postId, PostTypeId, AcceptedAnswerId, CreaionDate, Score, ViewCount, Body, LasActivityDate, Title, Tags, AnswerCount, CommentCount, FavoriteCount, LastEditorUserId, LastEditDate, ParentId]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>...</th>\n",
       "      <th>Body</th>\n",
       "      <th>LasActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>ParentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [userId, Reputation, CreationDate, DisplayName, LastAccessDate, WebsiteUrl, Location, AboutMe, Views, UpVotes, DownVotes, AccountId, Age, postId, PostTypeId, AcceptedAnswerId, CreaionDate, Score, ViewCount, Body, LasActivityDate, Title, Tags, AnswerCount, CommentCount, FavoriteCount, LastEditorUserId, LastEditDate, ParentId]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>...</th>\n",
       "      <th>Body</th>\n",
       "      <th>LasActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>ParentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [userId, Reputation, CreationDate, DisplayName, LastAccessDate, WebsiteUrl, Location, AboutMe, Views, UpVotes, DownVotes, AccountId, Age, postId, PostTypeId, AcceptedAnswerId, CreaionDate, Score, ViewCount, Body, LasActivityDate, Title, Tags, AnswerCount, CommentCount, FavoriteCount, LastEditorUserId, LastEditDate, ParentId]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>...</th>\n",
       "      <th>Body</th>\n",
       "      <th>LasActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>ParentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [userId, Reputation, CreationDate, DisplayName, LastAccessDate, WebsiteUrl, Location, AboutMe, Views, UpVotes, DownVotes, AccountId, Age, postId, PostTypeId, AcceptedAnswerId, CreaionDate, Score, ViewCount, Body, LasActivityDate, Title, Tags, AnswerCount, CommentCount, FavoriteCount, LastEditorUserId, LastEditDate, ParentId]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>...</th>\n",
       "      <th>Body</th>\n",
       "      <th>LasActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>ParentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [userId, Reputation, CreationDate, DisplayName, LastAccessDate, WebsiteUrl, Location, AboutMe, Views, UpVotes, DownVotes, AccountId, Age, postId, PostTypeId, AcceptedAnswerId, CreaionDate, Score, ViewCount, Body, LasActivityDate, Title, Tags, AnswerCount, CommentCount, FavoriteCount, LastEditorUserId, LastEditDate, ParentId]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>...</th>\n",
       "      <th>Body</th>\n",
       "      <th>LasActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>ParentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [userId, Reputation, CreationDate, DisplayName, LastAccessDate, WebsiteUrl, Location, AboutMe, Views, UpVotes, DownVotes, AccountId, Age, postId, PostTypeId, AcceptedAnswerId, CreaionDate, Score, ViewCount, Body, LasActivityDate, Title, Tags, AnswerCount, CommentCount, FavoriteCount, LastEditorUserId, LastEditDate, ParentId]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(users_posts[((users_posts['Age']%1)!=0)])\n",
    "display(users_posts[((users_posts['AcceptedAnswerId']%1)!=0)])\n",
    "display(users_posts[((users_posts['ViewCount']%1)!=0)])\n",
    "display(users_posts[((users_posts['AnswerCount']%1)!=0)])\n",
    "display(users_posts[((users_posts['FavoriteCount']%1)!=0)])\n",
    "display(users_posts[((users_posts['LastEditorUserId']%1)!=0)])\n",
    "display(users_posts[((users_posts['ParentId']%1)!=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId                int64\n",
       "Reputation            int64\n",
       "CreationDate         object\n",
       "DisplayName          object\n",
       "LastAccessDate       object\n",
       "WebsiteUrl           object\n",
       "Location             object\n",
       "AboutMe              object\n",
       "Views                 int64\n",
       "UpVotes               int64\n",
       "DownVotes             int64\n",
       "AccountId             int64\n",
       "Age                 float64\n",
       "postId                int64\n",
       "PostTypeId            int64\n",
       "AcceptedAnswerId    float64\n",
       "CreaionDate          object\n",
       "Score                 int64\n",
       "ViewCount           float64\n",
       "Body                 object\n",
       "LasActivityDate      object\n",
       "Title                object\n",
       "Tags                 object\n",
       "AnswerCount         float64\n",
       "CommentCount          int64\n",
       "FavoriteCount       float64\n",
       "LastEditorUserId    float64\n",
       "LastEditDate         object\n",
       "ParentId            float64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "userId               int64\n",
       "Reputation           int64\n",
       "CreationDate        object\n",
       "DisplayName         object\n",
       "LastAccessDate      object\n",
       "WebsiteUrl          object\n",
       "Location            object\n",
       "AboutMe             object\n",
       "Views                int64\n",
       "UpVotes              int64\n",
       "DownVotes            int64\n",
       "AccountId            int64\n",
       "Age                  int64\n",
       "postId               int64\n",
       "PostTypeId           int64\n",
       "AcceptedAnswerId     int64\n",
       "CreaionDate         object\n",
       "Score                int64\n",
       "ViewCount            int64\n",
       "Body                object\n",
       "LasActivityDate     object\n",
       "Title               object\n",
       "Tags                object\n",
       "AnswerCount          int64\n",
       "CommentCount         int64\n",
       "FavoriteCount        int64\n",
       "LastEditorUserId     int64\n",
       "LastEditDate        object\n",
       "ParentId             int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Should change these, because are integer but within data type float\n",
    "\n",
    "display(users_posts.dtypes)\n",
    "users_posts = users_posts.astype({'Age':'int','AcceptedAnswerId':'int','ViewCount':'int','AnswerCount':'int','FavoriteCount':'int','LastEditorUserId':'int','ParentId':'int'})\n",
    "display(users_posts.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>DownVotes</th>\n",
       "      <th>AccountId</th>\n",
       "      <th>Age</th>\n",
       "      <th>postId</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>ParentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>3.896200e+04</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.00000</td>\n",
       "      <td>38962.000000</td>\n",
       "      <td>38962.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6079.063087</td>\n",
       "      <td>7281.091679</td>\n",
       "      <td>1400.648016</td>\n",
       "      <td>914.799677</td>\n",
       "      <td>43.841050</td>\n",
       "      <td>6.490214e+05</td>\n",
       "      <td>12.975078</td>\n",
       "      <td>22960.799651</td>\n",
       "      <td>1.639290</td>\n",
       "      <td>4259.335224</td>\n",
       "      <td>4.083081</td>\n",
       "      <td>472.509573</td>\n",
       "      <td>0.627586</td>\n",
       "      <td>2.014630</td>\n",
       "      <td>0.61098</td>\n",
       "      <td>2318.515117</td>\n",
       "      <td>11951.882167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5224.896435</td>\n",
       "      <td>15164.527714</td>\n",
       "      <td>3423.886887</td>\n",
       "      <td>2296.527060</td>\n",
       "      <td>161.797079</td>\n",
       "      <td>5.958926e+05</td>\n",
       "      <td>18.804939</td>\n",
       "      <td>13696.932471</td>\n",
       "      <td>0.595892</td>\n",
       "      <td>11038.875897</td>\n",
       "      <td>6.561843</td>\n",
       "      <td>2423.961137</td>\n",
       "      <td>1.514881</td>\n",
       "      <td>2.674018</td>\n",
       "      <td>3.52762</td>\n",
       "      <td>4673.215379</td>\n",
       "      <td>14494.357588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1317.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.565530e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11325.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4856.000000</td>\n",
       "      <td>909.000000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.567810e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22373.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4620.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9651.000000</td>\n",
       "      <td>7931.000000</td>\n",
       "      <td>1050.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.091370e+06</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>33688.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2116.000000</td>\n",
       "      <td>22468.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>55226.000000</td>\n",
       "      <td>87393.000000</td>\n",
       "      <td>20932.000000</td>\n",
       "      <td>11442.000000</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>2.840547e+06</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>48325.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>114363.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>175495.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>233.00000</td>\n",
       "      <td>53985.000000</td>\n",
       "      <td>48314.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             userId    Reputation         Views       UpVotes     DownVotes  \\\n",
       "count  38962.000000  38962.000000  38962.000000  38962.000000  38962.000000   \n",
       "mean    6079.063087   7281.091679   1400.648016    914.799677     43.841050   \n",
       "std     5224.896435  15164.527714   3423.886887   2296.527060    161.797079   \n",
       "min       -1.000000      1.000000      0.000000      0.000000      0.000000   \n",
       "25%     1317.000000    147.000000     16.000000      4.000000      0.000000   \n",
       "50%     4856.000000    909.000000    124.000000     65.000000      1.000000   \n",
       "75%     9651.000000   7931.000000   1050.000000    582.000000     16.000000   \n",
       "max    55226.000000  87393.000000  20932.000000  11442.000000   1920.000000   \n",
       "\n",
       "          AccountId           Age        postId    PostTypeId  \\\n",
       "count  3.896200e+04  38962.000000  38962.000000  38962.000000   \n",
       "mean   6.490214e+05     12.975078  22960.799651      1.639290   \n",
       "std    5.958926e+05     18.804939  13696.932471      0.595892   \n",
       "min   -1.000000e+00      0.000000      1.000000      1.000000   \n",
       "25%    1.565530e+05      0.000000  11325.250000      1.000000   \n",
       "50%    4.567810e+05      0.000000  22373.500000      2.000000   \n",
       "75%    1.091370e+06     29.000000  33688.500000      2.000000   \n",
       "max    2.840547e+06     94.000000  48325.000000      7.000000   \n",
       "\n",
       "       AcceptedAnswerId         Score      ViewCount   AnswerCount  \\\n",
       "count      38962.000000  38962.000000   38962.000000  38962.000000   \n",
       "mean        4259.335224      4.083081     472.509573      0.627586   \n",
       "std        11038.875897      6.561843    2423.961137      1.514881   \n",
       "min            0.000000    -19.000000       0.000000      0.000000   \n",
       "25%            0.000000      1.000000       0.000000      0.000000   \n",
       "50%            0.000000      2.000000       0.000000      0.000000   \n",
       "75%            0.000000      5.000000     253.000000      1.000000   \n",
       "max       114363.000000    192.000000  175495.000000    136.000000   \n",
       "\n",
       "       CommentCount  FavoriteCount  LastEditorUserId      ParentId  \n",
       "count  38962.000000    38962.00000      38962.000000  38962.000000  \n",
       "mean       2.014630        0.61098       2318.515117  11951.882167  \n",
       "std        2.674018        3.52762       4673.215379  14494.357588  \n",
       "min        0.000000        0.00000         -1.000000      0.000000  \n",
       "25%        0.000000        0.00000          0.000000      0.000000  \n",
       "50%        1.000000        0.00000          0.000000   4620.000000  \n",
       "75%        3.000000        0.00000       2116.000000  22468.000000  \n",
       "max       45.000000      233.00000      53985.000000  48314.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OUTLIERS\n",
    "\n",
    "users_posts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3 = users_posts.quantile(0.75)\n",
    "Q1 = users_posts.quantile(0.25)\n",
    "IQR = Q3 - Q1\n",
    "lower_limit = Q1 - (IQR * 1.5) \n",
    "upper_limit = Q3 + (IQR * 1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32874/790282299.py:1: FutureWarning: Automatic reindexing on DataFrame vs Series comparisons is deprecated and will raise ValueError in a future version. Do `left, right = left.align(right, axis=1, copy=False)` before e.g. `left == right`\n",
      "  outliers = users_posts[((users_posts < (Q1 - 1.5 * IQR)) |(users_posts > (Q3 + 1.5 * IQR))).any(axis=1)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>...</th>\n",
       "      <th>Body</th>\n",
       "      <th>LasActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>ParentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;CrossValidated&lt;/strong&gt; is for stat...</td>\n",
       "      <td>2013-01-10 19:43:24</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2014-04-23 13:43:43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>2011-03-21 17:40:28</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2011-03-21 17:40:28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>2011-03-21 17:46:43</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2011-03-21 17:46:43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;p&gt;\"Statistics\" can refer variously to the (wi...</td>\n",
       "      <td>2011-03-30 19:23:14</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>919</td>\n",
       "      <td>2011-03-30 19:23:14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>Community</td>\n",
       "      <td>2010-07-19 06:55:26</td>\n",
       "      <td>http://meta.stackexchange.com/</td>\n",
       "      <td>on the server farm</td>\n",
       "      <td>&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;\\r\\n\\r\\n&lt;p&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>5007</td>\n",
       "      <td>...</td>\n",
       "      <td>This generic tag is only rarely suitable; use ...</td>\n",
       "      <td>2011-03-30 19:23:14</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>919</td>\n",
       "      <td>2011-03-30 19:23:14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38957</th>\n",
       "      <td>45934</td>\n",
       "      <td>11</td>\n",
       "      <td>2014-05-21 17:13:23</td>\n",
       "      <td>jasonweiyi</td>\n",
       "      <td>2014-06-08 13:36:47</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;p&gt;I built a suggestion system, which provides...</td>\n",
       "      <td>2014-02-09 12:34:41</td>\n",
       "      <td>Statistical test for a gradually self-improvin...</td>\n",
       "      <td>&lt;hypothesis-testing&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>805</td>\n",
       "      <td>2013-05-13 04:41:41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38958</th>\n",
       "      <td>46192</td>\n",
       "      <td>36</td>\n",
       "      <td>2014-05-26 15:29:30</td>\n",
       "      <td>user1738753</td>\n",
       "      <td>2014-09-10 19:52:34</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;p&gt;Currently, I am working on a count data set...</td>\n",
       "      <td>2012-10-18 15:11:09</td>\n",
       "      <td>Testing for Spatial Autocorrelation in a Negat...</td>\n",
       "      <td>&lt;r&gt;&lt;spatial&gt;&lt;correlation&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>930</td>\n",
       "      <td>2012-10-18 15:07:40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38959</th>\n",
       "      <td>46522</td>\n",
       "      <td>235</td>\n",
       "      <td>2014-06-01 17:25:18</td>\n",
       "      <td>Andy Blankertz</td>\n",
       "      <td>2014-09-13 18:03:07</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>&lt;p&gt;Actuary for a life insurance company.&lt;/p&gt;\\r\\n</td>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;p&gt;I'm studying a data set in R using both reg...</td>\n",
       "      <td>2011-10-24 12:21:54</td>\n",
       "      <td>Mismatch between significant variables from lo...</td>\n",
       "      <td>&lt;r&gt;&lt;modeling&gt;&lt;classification&gt;&lt;regression&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0001-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38960</th>\n",
       "      <td>52371</td>\n",
       "      <td>221</td>\n",
       "      <td>2014-07-19 13:36:58</td>\n",
       "      <td>Karel Petranek</td>\n",
       "      <td>2014-07-20 09:32:16</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;p&gt;When doing research in Economy, one frequen...</td>\n",
       "      <td>2012-04-27 13:48:30</td>\n",
       "      <td>What are the most useful sources of economics ...</td>\n",
       "      <td>&lt;sources&gt;</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0001-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38961</th>\n",
       "      <td>55226</td>\n",
       "      <td>119</td>\n",
       "      <td>2014-09-04 07:34:48</td>\n",
       "      <td>Klas Lindbäck</td>\n",
       "      <td>2014-09-08 11:07:33</td>\n",
       "      <td>http://N/A</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>&lt;p&gt;Working with a wide range of languages, but...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;p&gt;How about making a solution based on the Si...</td>\n",
       "      <td>2011-09-28 07:26:03</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0001-01-01 00:00:00</td>\n",
       "      <td>16174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23664 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  Reputation         CreationDate     DisplayName  \\\n",
       "0          -1           1  2010-07-19 06:55:26       Community   \n",
       "1          -1           1  2010-07-19 06:55:26       Community   \n",
       "2          -1           1  2010-07-19 06:55:26       Community   \n",
       "3          -1           1  2010-07-19 06:55:26       Community   \n",
       "4          -1           1  2010-07-19 06:55:26       Community   \n",
       "...       ...         ...                  ...             ...   \n",
       "38957   45934          11  2014-05-21 17:13:23      jasonweiyi   \n",
       "38958   46192          36  2014-05-26 15:29:30     user1738753   \n",
       "38959   46522         235  2014-06-01 17:25:18  Andy Blankertz   \n",
       "38960   52371         221  2014-07-19 13:36:58  Karel Petranek   \n",
       "38961   55226         119  2014-09-04 07:34:48   Klas Lindbäck   \n",
       "\n",
       "            LastAccessDate                      WebsiteUrl  \\\n",
       "0      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "1      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "2      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "3      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "4      2010-07-19 06:55:26  http://meta.stackexchange.com/   \n",
       "...                    ...                             ...   \n",
       "38957  2014-06-08 13:36:47                                   \n",
       "38958  2014-09-10 19:52:34                                   \n",
       "38959  2014-09-13 18:03:07                                   \n",
       "38960  2014-07-20 09:32:16                                   \n",
       "38961  2014-09-08 11:07:33                      http://N/A   \n",
       "\n",
       "                 Location                                            AboutMe  \\\n",
       "0      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "1      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "2      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "3      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "4      on the server farm  <p>Hi, I'm not really a person.</p>\\r\\n\\r\\n<p>...   \n",
       "...                   ...                                                ...   \n",
       "38957                                                                          \n",
       "38958                                                                          \n",
       "38959                       <p>Actuary for a life insurance company.</p>\\r\\n   \n",
       "38960                                                                          \n",
       "38961              Sweden  <p>Working with a wide range of languages, but...   \n",
       "\n",
       "       Views  UpVotes  ...                                               Body  \\\n",
       "0          0     5007  ...  <p><strong>CrossValidated</strong> is for stat...   \n",
       "1          0     5007  ...                                                      \n",
       "2          0     5007  ...                                                      \n",
       "3          0     5007  ...  <p>\"Statistics\" can refer variously to the (wi...   \n",
       "4          0     5007  ...  This generic tag is only rarely suitable; use ...   \n",
       "...      ...      ...  ...                                                ...   \n",
       "38957      1        0  ...  <p>I built a suggestion system, which provides...   \n",
       "38958      1        0  ...  <p>Currently, I am working on a count data set...   \n",
       "38959     13       27  ...  <p>I'm studying a data set in R using both reg...   \n",
       "38960      2        0  ...  <p>When doing research in Economy, one frequen...   \n",
       "38961      2        3  ...  <p>How about making a solution based on the Si...   \n",
       "\n",
       "           LasActivityDate                                              Title  \\\n",
       "0      2013-01-10 19:43:24                                                      \n",
       "1      2011-03-21 17:40:28                                                      \n",
       "2      2011-03-21 17:46:43                                                      \n",
       "3      2011-03-30 19:23:14                                                      \n",
       "4      2011-03-30 19:23:14                                                      \n",
       "...                    ...                                                ...   \n",
       "38957  2014-02-09 12:34:41  Statistical test for a gradually self-improvin...   \n",
       "38958  2012-10-18 15:11:09  Testing for Spatial Autocorrelation in a Negat...   \n",
       "38959  2011-10-24 12:21:54  Mismatch between significant variables from lo...   \n",
       "38960  2012-04-27 13:48:30  What are the most useful sources of economics ...   \n",
       "38961  2011-09-28 07:26:03                                                      \n",
       "\n",
       "                                            Tags  AnswerCount  CommentCount  \\\n",
       "0                                                           0             0   \n",
       "1                                                           0             0   \n",
       "2                                                           0             0   \n",
       "3                                                           0             0   \n",
       "4                                                           0             0   \n",
       "...                                          ...          ...           ...   \n",
       "38957                       <hypothesis-testing>            1             2   \n",
       "38958                  <r><spatial><correlation>            1             2   \n",
       "38959  <r><modeling><classification><regression>            3             0   \n",
       "38960                                  <sources>            7             5   \n",
       "38961                                                       0             0   \n",
       "\n",
       "      FavoriteCount  LastEditorUserId         LastEditDate ParentId  \n",
       "0                 0                -1  2014-04-23 13:43:43        0  \n",
       "1                 0                -1  2011-03-21 17:40:28        0  \n",
       "2                 0                -1  2011-03-21 17:46:43        0  \n",
       "3                 0               919  2011-03-30 19:23:14        0  \n",
       "4                 0               919  2011-03-30 19:23:14        0  \n",
       "...             ...               ...                  ...      ...  \n",
       "38957             0               805  2013-05-13 04:41:41        0  \n",
       "38958             1               930  2012-10-18 15:07:40        0  \n",
       "38959             0                 0  0001-01-01 00:00:00        0  \n",
       "38960             8                 0  0001-01-01 00:00:00        0  \n",
       "38961             0                 0  0001-01-01 00:00:00    16174  \n",
       "\n",
       "[23664 rows x 29 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers = users_posts[((users_posts < (Q1 - 1.5 * IQR)) |(users_posts > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers.to_csv('outliers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
